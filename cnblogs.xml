<?xml version="1.0" encoding="UTF-8" ?>
    <rss version="2.0">
    <channel>
        <title><![CDATA[ 主页 - 博客园 ]]></title>
        <link><![CDATA[ https://www.cnblogs.com/ ]]></link>
        <lastBuildDate>2025-12-22T12:33:50.500Z</lastBuildDate>
        <description><![CDATA[
        主页 - 博客园 RSS
    ]]></description>
        <language>zh-cn</language>
        <item>
    <title><![CDATA[ 【Agent】MemOS 源码笔记---(7)---MemScheduler 细节 ]]></title>
    <link>https://www.cnblogs.com/rossiXYZ/p/19364457</link>
    <guid>f3e434fdaea83dcfce7317d536ec9114</guid>
    <description>
    <![CDATA[ 
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/rossiXYZ/p/19364457" title="发布于 2025-12-22 20:15">
    <span role="heading" aria-level="2">【Agent】MemOS 源码笔记---(7)---MemScheduler 细节</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="agentmemos-源码笔记---7---memscheduler-细节">【Agent】MemOS 源码笔记---(7)---MemScheduler 细节</h1>
<p></p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#agentmemos-源码笔记---7---memscheduler-细节" rel="noopener nofollow">【Agent】MemOS 源码笔记---(7)---MemScheduler 细节</a><ul><li><a href="#0x00-摘要" rel="noopener nofollow">0x00 摘要</a></li><li><a href="#0x01-组件关系" rel="noopener nofollow">0x01 组件关系</a></li><li><a href="#0x02-实现" rel="noopener nofollow">0x02 实现</a><ul><li><a href="#21-generalscheduler" rel="noopener nofollow">2.1 GeneralScheduler</a><ul><li><a href="#211-消息处理" rel="noopener nofollow">2.1.1 消息处理</a><ul><li><a href="#消息类型与processor注册" rel="noopener nofollow">消息类型与processor注册</a></li><li><a href="#消息分发过程" rel="noopener nofollow">消息分发过程</a></li></ul></li><li><a href="#212-流程" rel="noopener nofollow">2.1.2 流程</a></li><li><a href="#213-完整代码" rel="noopener nofollow">2.1.3 完整代码</a></li></ul></li><li><a href="#22-schedulerdispatcher" rel="noopener nofollow">2.2 SchedulerDispatcher</a><ul><li><a href="#221-分发逻辑" rel="noopener nofollow">2.2.1 分发逻辑</a></li><li><a href="#222-流程" rel="noopener nofollow">2.2.2 流程</a></li><li><a href="#223-代码" rel="noopener nofollow">2.2.3 代码</a></li></ul></li><li><a href="#23-schedulerretriever" rel="noopener nofollow">2.3 SchedulerRetriever</a><ul><li><a href="#231-功能" rel="noopener nofollow">2.3.1 功能</a></li><li><a href="#232-详细步骤说明" rel="noopener nofollow">2.3.2 详细步骤说明</a></li><li><a href="#233-调用关系" rel="noopener nofollow">2.3.3 调用关系</a></li><li><a href="#234-实现" rel="noopener nofollow">2.3.4 实现</a><ul><li><a href="#记忆检索流程search-方法" rel="noopener nofollow">记忆检索流程（search 方法）</a></li><li><a href="#记忆全流程处理与重排process_and_rerank_memories-方法" rel="noopener nofollow">记忆全流程处理与重排（process_and_rerank_memories 方法）</a></li><li><a href="#llm-记忆重排流程rerank_memories-方法" rel="noopener nofollow">LLM 记忆重排流程（rerank_memories 方法）</a></li><li><a href="#代码" rel="noopener nofollow">代码</a></li></ul></li></ul></li></ul></li><li><a href="#0x03-关联" rel="noopener nofollow">0x03 关联</a><ul><li><a href="#31-schedulerdispatcher-和-generalscheduler" rel="noopener nofollow">3.1 SchedulerDispatcher 和 GeneralScheduler</a></li><li><a href="#32-generalscheduler-和-treetextmemory" rel="noopener nofollow">3.2 GeneralScheduler 和 TreeTextMemory</a></li><li><a href="#33-总结" rel="noopener nofollow">3.3 总结</a></li><li><a href="#34-moscore" rel="noopener nofollow">3.4 MosCore</a></li></ul></li><li><a href="#0xff-参考" rel="noopener nofollow">0xFF 参考</a></li></ul></li></ul></div><p></p>
<h2 id="0x00-摘要">0x00 摘要</h2>
<p>记忆调度就像大脑的注意力机制，动态决定在合适的时刻调用合适的记忆。</p>
<p>在 MemOS 中，<strong>记忆调度（Memory Scheduling）</strong> 通过对【不同使用效率（参数&gt;激活&gt;工作&gt;其他明文）的记忆】的相互调度，让模型能更高效、准确地获取用户所需的记忆。在对话和任务进行时，通过预测用户后续对话所需记忆并提前调入高效率记忆类型如激活记忆工作记忆，加速推理链路。</p>
<p>记忆调度的具体实现就是MemScheduler ，这是一个与 MemOS 系统并行运行的并发记忆管理系统，它协调 AI 系统中工作记忆、长时记忆和激活记忆之间的记忆操作。它通过事件驱动调度处理记忆检索、更新和压缩。该系统特别适合需要动态记忆管理的对话代理和推理系统。</p>
<p>备注：本文基于MemOS的文档和源码进行学习，似乎其文档并没有跟上源码更新的速度，而且也有部分功能未实现或者未开源。</p>
<p>前一篇介绍了 MemScheduler 的总体概念，本篇来看看实现细节。</p>
<h2 id="0x01-组件关系">0x01 组件关系</h2>
<p>最核心的几个组件关系如下：</p>
<ul>
<li>GeneralScheduler 是整个调度系统的核心组件，继承自 BaseScheduler，负责处理不同类型的消息（查询、回答、添加等），并协调其他组件完成具体任务。</li>
<li>SchedulerDispatcher 是消息分发器，负责将不同类型的消息分发给对应的processor，支持并行处理（可选）。</li>
<li>SchedulerRetriever 是记忆检索器，负责从记忆库中搜索、重排序和过滤记忆项，提供智能记忆管理功能。</li>
</ul>
<p><img alt="MemScheduler-7-1" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1850883/202512/1850883-20251217235057999-1525991890.png" class="lazyload"></p>
<p>MemOS-main\src\memos\templates\mem_scheduler_prompts.py 中可以看到使用的prompt。</p>
<pre><code>PROMPT_MAPPING = {
    "intent_recognizing": INTENT_RECOGNIZING_PROMPT,
    "memory_reranking": MEMORY_RERANKING_PROMPT,
    "query_keywords_extraction": QUERY_KEYWORDS_EXTRACTION_PROMPT,
    "memory_filtering": MEMORY_FILTERING_PROMPT,
    "memory_redundancy_filtering": MEMORY_REDUNDANCY_FILTERING_PROMPT,
    "memory_combined_filtering": MEMORY_COMBINED_FILTERING_PROMPT,
    "memory_answer_ability_evaluation": MEMORY_ANSWER_ABILITY_EVALUATION_PROMPT,
}
</code></pre>
<h2 id="0x02-实现">0x02 实现</h2>
<h3 id="21-generalscheduler">2.1 GeneralScheduler</h3>
<p>BaseScheduler是基类，而GeneralScheduler才是真正起作用的派生类。</p>
<p>GeneralScheduler采用生产者-消费者模型进行工作，是基于消息队列的内存调度器。</p>
<ul>
<li>
<p>该类是 MemOS 中基于<code>BaseScheduler</code>的具体调度器实现，专注于处理查询、回答和添加内存等核心任务。核心组件架构：</p>
<ul>
<li>消息队列：使用 memos_message_queue 来存储待处理的消息。</li>
<li>消费者线程：使用 _message_consumer 方法持续轮询队列并处理消息。</li>
<li>分发器：SchedulerDispatcher 负责将消息路由到对应的processor。</li>
<li>处理器：针对不同消息类型的具体processor。</li>
</ul>
</li>
<li>
<p>核心功能包括：</p>
<ul>
<li>注册消息类型（查询 / 回答 / 添加）注册对应的processor，实现任务的定向分发；</li>
<li>处理查询消息时，提取关键词、更新查询监控、执行内存检索与重排序，并支持激活内存的周期性更新；</li>
<li>处理添加内存消息时，记录新增内存日志并同步到监控系统；</li>
<li>基于会话对话轮次逻辑，根据意图检测和时间触发机制决定决定是否执行内存检索，确保工作内存的时效性和相关性。</li>
</ul>
</li>
<li>
<p>特色是基于消息类型的模块化处理机制，结合意图识别和时间触发的混合调度策略，支持按用户和内存立方体分组处理消息，保证多用户场景下的内存隔离和处理效率。</p>
</li>
</ul>
<h4 id="211-消息处理">2.1.1 消息处理</h4>
<p>SchedulerDispatcher将消息路由到适当的处理器。</p>
<h5 id="消息类型与processor注册">消息类型与processor注册</h5>
<p>初始化时注册了三种消息processor。</p>
<pre><code class="language-python">        # register handlers
        handlers = {
            QUERY_LABEL: self._query_message_consumer,
            ANSWER_LABEL: self._answer_message_consumer,
            ADD_LABEL: self._add_message_consumer,
        }
        self.dispatcher.register_handlers(handlers)
</code></pre>
<ul>
<li>_query_message_consumer：处理用户查询 QUERY_LABEL，触发记忆检索和重排序</li>
<li>_answer_message_consumer：处理系统回答 ANSWER_LABEL。</li>
<li>_add_message_consumer：处理新增记忆请求 ADD_LABEL。</li>
</ul>
<p>QUERY_LABEL在memos/mem_scheduler/schemas/general_schemas.py文件中定义为一个常量：</p>
<ul>
<li>QUERY_LABEL="query"</li>
<li>ANSWER_LABEL ="anSwer"</li>
<li>ADD_LABEL = "add"</li>
</ul>
<p>这是一个预定义的字符串常量，值为"query"。</p>
<h5 id="消息分发过程">消息分发过程</h5>
<ul>
<li>消息提交：通过submit_messages 将ScheduleMessageItem放入消息队列。</li>
<li>消息消费：_message_consumer 线程定期从队列中取出所有消息。</li>
<li>消息分发：调用self.dispatcher.dispatch(messages)按消息标签分发到对应的processor。</li>
</ul>
<h4 id="212-流程">2.1.2 流程</h4>
<p>GeneralScheduler的流程如下。</p>
<p><img alt="MemScheduler-7-2" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1850883/202512/1850883-20251217235110795-709389211.png" class="lazyload"></p>
<h4 id="213-完整代码">2.1.3 完整代码</h4>
<pre><code class="language-python">class GeneralScheduler(BaseScheduler):
    """通用调度器，实现查询、回答和添加内存等具体任务的处理逻辑"""

    def __init__(self, config: GeneralSchedulerConfig):
        """使用给定定配置初始化通用调度器"""
        super().__init__(config)

        # 查询关键词数量限制（从配置获取，默认20）
        self.query_key_words_limit = self.config.get("query_key_words_limit", 20)

        # 注册消息processor（按消息标签绑定对应的处理方法）
        handlers = {
            QUERY_LABEL: self._query_message_consumer,  # 处理查询消息
            ANSWER_LABEL: self._answer_message_consumer,  # 处理回答消息
            ADD_LABEL: self._add_message_consumer,  # 处理添加内存消息
        }
        self.dispatcher.register_handlers(handlers)

    def _query_message_consumer(self, messages: list[ScheduleMessageItem]) -&gt; None:
        """
        处理和响应队列中的查询触发消息

        参数:
            messages: 要处理的查询消息列表
        """
        # 按用户和内存立方体分组处理消息
        grouped_messages = self.dispatcher.group_messages_by_user_and_cube(messages=messages)

        # 验证消息合法性（确保标签与消息类型匹配）
        self.validate_schedule_messages(messages=messages, label=QUERY_LABEL)

        # 遍历分组消息，按用户和内存立方体处理
        for user_id in grouped_messages:
            for mem_cube_id in grouped_messages[user_id]:
                messages = grouped_messages[user_id][mem_cube_id]
                if len(messages) == 0:
                    return

                # 获取当前内存立方体实例
                mem_cube = messages[0].mem_cube

                # 从消息更新当前上下文（线程安全）
                self._set_current_context_from_message(msg=messages[0])

                # 更新查询监控器（为每个消息注册查询记录）
                for msg in messages:
                    # 若监控器不存在则创建
                    self.monitor.register_query_monitor_if_not_exists(
                        user_id=user_id, mem_cube_id=mem_cube_id
                    )

                    # 提取查询内容和关键词
                    query = msg.content
                    query_keywords = self.monitor.extract_query_keywords(query=query)

                    # 关键词提取失败时的 fallback 逻辑
                    if len(query_keywords) == 0:
                        stripped_query = query.strip()
                        # 根据语言类型选择分词方式
                        if is_all_english(stripped_query):
                            words = stripped_query.split()  # 英文按空格分词
                        elif is_all_chinese(stripped_query):
                            words = stripped_query  # 中文按字符处理
                        else:
                            words = stripped_query  # 混合语言默认按字符处理

                        # 取前N个关键词（去重）
                        query_keywords = list(set(words[: self.query_key_words_limit]))

                    # 创建查询监控项并添加到数据库
                    item = QueryMonitorItem(
                        user_id=user_id,
                        mem_cube_id=mem_cube_id,
                        query_text=query,
                        keywords=query_keywords,
                        max_keywords=DEFAULT_MAX_QUERY_KEY_WORDS,
                    )

                    # 同步到数据库
                    query_db_manager = self.monitor.query_monitors[user_id][mem_cube_id]
                    query_db_manager.obj.put(item=item)
                    query_db_manager.sync_with_orm()  # 添加后同步到数据库

                # 提取所有查询内容
                queries = [msg.content for msg in messages]

                # 执行会话轮次处理（检索相关内存）
                cur_working_memory, new_candidates = self.process_session_turn(
                    queries=queries,
                    user_id=user_id,
                    mem_cube_id=mem_cube_id,
                    mem_cube=mem_cube,
                    top_k=self.top_k,
                )

                # 重排序内存重排序并替换工作内存
                new_order_working_memory = self.replace_working_memory(
                    user_id=user_id,
                    mem_cube_id=mem_cube_id,
                    mem_cube=mem_cube,
                    original_memory=cur_working_memory,
                    new_memory=new_candidates,
                )

                # 若启用激活内存，执行周期性更新
                if self.enable_activation_memory:
                    self.update_activation_memory_periodically(
                        interval_seconds=self.monitor.act_mem_update_interval,
                        label=QUERY_LABEL,
                        user_id=user_id,
                        mem_cube_id=mem_cube_id,
                        mem_cube=messages[0].mem_cube,
                    )

    def _answer_message_consumer(self, messages: list[ScheduleMessageItem]) -&gt; None:
        """
        处理和响应队列中的回答触发消息

        参数:
          messages: 要处理的回答消息列表
        """
        # 按用户和内存立方体分组处理消息
        grouped_messages = self.dispatcher.group_messages_by_user_and_cube(messages=messages)

        # 验证消息合法性
        self.validate_schedule_messages(messages=messages, label=ANSWER_LABEL)

        # 遍历分组消息，更新上下文（具体回答逻辑可在此扩展）
        for user_id in grouped_messages:
            for mem_cube_id in grouped_messages[user_id]:
                messages = grouped_messages[user_id][mem_cube_id]
                if len(messages) == 0:
                    return

                # 从消息更新当前上下文
                self._set_current_context_from_message(msg=messages[0])

    def _add_message_consumer(self, messages: list[ScheduleMessageItem]) -&gt; None:
        """处理和响应队列中的添加内存消息"""
        # 按用户和内存立方体分组处理消息
        grouped_messages = self.dispatcher.group_messages_by_user_and_cube(messages=messages)

        # 验证消息合法性
        self.validate_schedule_messages(messages=messages, label=ADD_LABEL)
        try:
            # 遍历分组消息，处理每个添加内存请求
            for user_id in grouped_messages:
                for mem_cube_id in grouped_messages[user_id]:
                    messages = grouped_messages[user_id][mem_cube_id]
                    if len(messages) == 0:
                        return

                    # 从消息更新当前上下文
                    self._set_current_context_from_message(msg=messages[0])

                    # 处理每条消息中的内存ID列表
                    for msg in messages:
                        try:
                            # 解析消息内容中的内存ID列表（JSON格式）
                            userinput_memory_ids = json.loads(msg.content)
                        except Exception as e:
                            userinput_memory_ids = []

                        # 遍历内存ID，记录添加日志（跳过工作内存类型）
                        mem_cube = msg.mem_cube
                        for memory_id in userinput_memory_ids:
                            mem_item: TextualMemoryItem = mem_cube.text_mem.get(memory_id=memory_id)
                            mem_type = mem_item.metadata.memory_type
                            mem_content = mem_item.memory

                            # 跳过工作内存（通常不需要手动添加）
                            if mem_type == WORKING_MEMORY_TYPE:
                                continue

                            # 记录添加内存的日志
                            self.log_adding_memory(
                                memory=mem_content,
                                memory_type=mem_type,
                                user_id=msg.user_id,
                                mem_cube_id=msg.mem_cube_id,
                                mem_cube=msg.mem_cube,
                                log_func_callback=self._submit_web_logs,
                            )

        except Exception as e:
            logger.error(f"Error: {e}", exc_info=True)

    def process_session_turn(
        self,
        queries: str | list[str],
        user_id: UserID | str,
        mem_cube_id: MemCubeID | str,
        mem_cube: GeneralMemCube,
        top_k: int = 10,
    ) -&gt; tuple[list[TextualMemoryItem], list[TextualMemoryItem]] | None:
        """
        处理对话轮次：
        - 当查询列表达到窗口大小时，触发内存检索；
        - 若检索被触发，立即切换到新内存。
        """

        # 获取文本内存实例（仅支持TreeTextMemory类型）
        text_mem_base = mem_cube.text_mem
        if not isinstance(text_mem_base, TreeTextMemory):
            logger.error(
                f"未实现！期望TreeTextMemory，但获取到{type(text_mem_base).__name__} "
                f"（mem_cube_id={mem_cube_id}，user_id={user_id}）。 "
                f"text_mem_base值：{text_mem_base}",
                exc_info=True,
            )
            return None

        # 获取当前工作内存及文本内容
        cur_working_memory: list[TextualMemoryItem] = text_mem_base.get_working_memory()
        text_working_memory: list[str] = [w_m.memory for w_m in cur_working_memory]
        # 检测查询意图（判断是否需要触发检索）
        intent_result = self.monitor.detect_intent(
            q_list=queries, text_working_memory=text_working_memory
        )

        # 检查是否达到时间触发条件
        time_trigger_flag = False
        if self.monitor.timed_trigger(
            last_time=self.monitor.last_query_consume_time,
            interval_seconds=self.monitor.query_trigger_interval,
        ):
            time_trigger_flag = True

        # 根据意图和时间触发条件决定是否执行检索
        if (not intent_result["trigger_retrieval"]) and (not time_trigger_flag):
            return None
        elif (not intent_result["trigger_retrieval"]) and time_trigger_flag:
            intent_result["trigger_retrieval"] = True
            intent_result["missing_evidences"] = queries
        else:
            logger.info(
                f"触发查询调度（user_id={user_id}，mem_cube_id={mem_cube_id}）。 "
                f"缺失证据：{intent_result['missing_evidences']}"
            )

        # 处理缺失的证据（需要检索的内容）
        missing_evidences = intent_result["missing_evidences"]
        num_evidence = len(missing_evidences)
        # 为每个证据分配Top-K配额（确保至少返回1条）
        k_per_evidence = max(1, top_k // max(1, num_evidence))
        new_candidates = []

        # 为每个缺失证据执行检索
        for item in missing_evidences:
            info = {
                "user_id": user_id,
                "session_id": "",
            }

            # 执行检索
            results: list[TextualMemoryItem] = self.retriever.search(
                query=item,
                mem_cube=mem_cube,
                top_k=k_per_evidence,
                method=self.search_method,
                info=info,
            )
            new_candidates.extend(results)

        # 返回当前工作内存和新检索到的候选内存
        return cur_working_memory, new_candidates

</code></pre>
<h3 id="22-schedulerdispatcher">2.2 SchedulerDispatcher</h3>
<p>SchedulerDispatcher 负责将消息路由到对应的processor。SchedulerDispatcher 不使用任何模型进行消息分发或分类判断。下面是它的工作原理：</p>
<ul>
<li>该类是基于线程池的消息分发器，核心作用是根据消息标签（label）将消息路由到对应的processor，实现消息的定向批量处理。</li>
<li>核心功能包括：支持单个 / 批量注册消息processor、按用户 ID 和内存立方体 ID 分组消息、串行 / 并行两种分发模式、优雅关闭和任务监控。</li>
<li>特色是采用上下文感知线程池（ContextThreadPoolExecutor），支持并行任务执行以提升效率；消息分组机制保证同用户同内存立方体的消息集中处理，避免上下文混乱；完善的任务生命周期管理（取消、等待、异常捕获）确保系统稳定。</li>
</ul>
<h4 id="221-分发逻辑">2.2.1 分发逻辑</h4>
<p>SchedulerDispatcher 是基于消息标签进行操作的，而不是使用任何机器学习模型或大语言模型进行分类，具体分发逻辑如下：</p>
<ul>
<li>消息分组：将消息按标签进行分组。</li>
<li>processor查找：依据标签查找注册的processor。处理函数在初始化时被明确注册。</li>
<li>执行模式：
<ul>
<li>串行模式：直接调用processor</li>
<li>并行模式：通过线程池异步执行</li>
</ul>
</li>
</ul>
<p>消息通过其标签属性进行分发</p>
<pre><code class="language-python">class ScheduleMessageItem:
    def __init__(self, label: str, content: str, ...):
        self.label = label  # 分发键
        self.content = content
        # ... 其他属性
</code></pre>
<p>调度器使用简单的字典查找来路由消息到处理函数，在 SchedulerDispatcher.dispatch() 中：</p>
<pre><code class="language-python">label_groups = defaultdict(list)
for message in msg_list:
    label_groups[message.label].append(message)  # 按标签分组

for label, msgs in label_groups.items():
    handler = self.handlers.get(label, self._default_message_handler)
    handler(msgs)  # 基于标签直接调用函数
</code></pre>
<p>当启用并行分发时，也只是并行执行处理函数，但仍不使用模型进行路由：</p>
<pre><code class="language-python">if self.enable_parallel_dispatch and self.dispatcher_executor is not None:
    future = self.dispatcher_executor.submit(handler, msgs)  # 并行执行
</code></pre>
<p>消息流程示例</p>
<pre><code class="language-python">传入消息
├── 带有标签的消息 "query"
│       ├── 没有模型参与确定使用哪个处理函数
│       │
│       ├── 按标签分组
│       │
│       └── 查找 "query" 处理函数
│           ├── 调用已注册的 _query_message_consumer 函数直接调用
│
└── 并行处理（如果启用）
</code></pre>
<h4 id="222-流程">2.2.2 流程</h4>
<p>SchedulerDispatcher的流程如下。</p>
<p><img alt="MemScheduler-7-3" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1850883/202512/1850883-20251217235131608-1552561046.png" class="lazyload"></p>
<h4 id="223-代码">2.2.3 代码</h4>
<pre><code class="language-python">class SchedulerDispatcher(BaseSchedulerModule):
    """
    基于线程池的消息分发器，根据消息标签将消息路由到专用processor。

    特性：
    - 每个消息标签对应独立的线程池处理逻辑
    - 支持批量消息处理
    - 支持优雅关闭
    - 支持批量注册processor
    """

    def __init__(self, max_workers=30, enable_parallel_dispatch=False):
        super().__init__()
        self.max_workers = max_workers  # 线程池最大工作线程数

        # 仅在并行模式下初始化线程池
        self.enable_parallel_dispatch = enable_parallel_dispatch
        self.thread_name_prefix = "dispatcher"  # 线程名称前缀
        if self.enable_parallel_dispatch:
            self.dispatcher_executor = ContextThreadPoolExecutor(
                max_workers=self.max_workers, thread_name_prefix=self.thread_name_prefix
            )
        else:
            self.dispatcher_executor = None  # 串行模式下线程池为None

        self.handlers: dict[str, Callable] = {}  # 已注册的消息processor（标签→processor映射）
        self._running = False  # 分发器运行状态标识
        self._futures = set()  # 用于监控的活跃任务集合

    def register_handler(self, label: str, handler: Callable[[list[ScheduleMessageItem]], None]):
        """
        为特定消息标签注册processor函数。

        参数:
            label: 要处理的消息标签
            handler: 处理该标签消息的可调用对象，接收消息列表作为参数
        """
        self.handlers[label] = handler

    def register_handlers(
        self, handlers: dict[str, Callable[[list[ScheduleMessageItem]], None]]
    ) -&gt; None:
        """
        从字典中批量注册多个processor。

        参数:
            handlers: 标签到processor函数的映射字典，格式：{标签: processor可调用对象}
        """
        for label, handler in handlers.items():
            # 验证标签类型（必须为字符串）
            if not isinstance(label, str):
                continue
            # 验证processor是否可调用
            if not callable(handler):
                continue
            # 注册单个processor
            self.register_handler(label=label, handler=handler)

    def _default_message_handler(self, messages: list[ScheduleMessageItem]) -&gt; None:
        """默认消息processor（当找不到对应标签的processor时使用）"""
        logger.debug(f"使用默认消息processor处理消息：{messages}")

    def group_messages_by_user_and_cube(
        self, messages: list[ScheduleMessageItem]
    ) -&gt; dict[str, dict[str, list[ScheduleMessageItem]]]:
        """
        将消息按用户ID和内存立方体ID分组，生成嵌套字典结构。

        参数:
            messages: 要分组的ScheduleMessageItem对象列表

        返回:
            嵌套字典，结构如下：
            {
                "user_id_1": {
                    "mem_cube_id_1": [消息1, 消息2, ...],
                    "mem_cube_id_2": [消息3, 消息4, ...],
                    ...
                },
                "user_id_2": {
                    ...
                },
                ...
            }
            其中每个消息保持原始ScheduleMessageItem对象
        """
        # 初始化嵌套默认字典（自动创建不存在的键）
        grouped_dict = defaultdict(lambda: defaultdict(list))

        # 按用户ID→内存立方体ID的层级分组消息
        for msg in messages:
            grouped_dict[msg.user_id][msg.mem_cube_id].append(msg)

        # 将默认字典转换为普通字典，输出更简洁
        return {user_id: dict(cube_groups) for user_id, cube_groups in grouped_dict.items()}

    def _handle_future_result(self, future: Future):
        """处理异步任务结果，捕获异常并清理任务集合"""
        self._futures.remove(future)  # 从活跃任务集合中移除已完成任务
         future.result()  # 获取任务结果，触发可能的异常

    def dispatch(self, msg_list: list[ScheduleMessageItem]):
        """
        将消息列表分发到对应的processor。

        参数:
            msg_list: 要处理的ScheduleMessageItem对象列表
        """
        if not msg_list:
            return

        # 按消息标签分组（同一标签的消息交给同一个processor）
        label_groups = defaultdict(list)
        for message in msg_list:
            label_groups[message.label].append(message)

        # 处理每个标签对应的消息组
        for label, msgs in label_groups.items():
            # 获取该标签对应的processor，无则使用默认processor
            handler = self.handlers.get(label, self._default_message_handler)
            # 并行模式：提交任务到线程池异步执行
            if self.enable_parallel_dispatch and self.dispatcher_executor is not None:
                # 提交任务到线程池，捕获变量避免循环引用问题
                future = self.dispatcher_executor.submit(handler, msgs)
                self._futures.add(future)  # 将任务添加到活跃任务集合
                # 绑定任务完成回调函数
                future.add_done_callback(self._handle_future_result)
                logger.info(f"已将 {len(msgs)} 条消息作为异步任务分发")
            # 串行模式：直接调用processor同步执行
            else:
                handler(msgs)

    def join(self, timeout: float | None = None) -&gt; bool:
        """等待所有已分发任务完成。

        参数:
            timeout: 最大等待时间（秒），None表示无限等待

        返回:
            bool: 所有任务完成返回True，超时返回False
        """
        # 串行模式下无异步任务，直接返回True
        if not self.enable_parallel_dispatch or self.dispatcher_executor is None:
            return True

        # 等待所有活跃任务完成
        done, not_done = concurrent.futures.wait(
            self._futures, timeout=timeout, return_when=concurrent.futures.ALL_COMPLETED
        )

        # 检查已完成任务中的异常
        for future in done:
            try:
                future.result()
            except Exception:
                logger.error("关闭过程中processor执行失败", exc_info=True)

        # 返回是否所有任务都已完成
        return len(not_done) == 0
</code></pre>
<h3 id="23-schedulerretriever">2.3 SchedulerRetriever</h3>
<p>SchedulerRetriever 是MemOS中负责记忆检索和重排序的核心模块，是连接查询和记忆存储的关键桥梁，负责确保系统能够准确、高效地检索和排序相关记忆。</p>
<p>内存检索与推理的本质是：在正确时间找到正确信息。</p>
<h4 id="231-功能">2.3.1 功能</h4>
<p>SchedulerRetriever 的功能如下：</p>
<ul>
<li>记忆检索功能
<ul>
<li>搜索实现：通过 search 方法在文本记忆库中查找与查询相关的记忆项</li>
<li>支持多种搜索模式：
<ul>
<li>快速搜索（fast mode）</li>
<li>精细搜索（fine mode）</li>
<li>多类型记忆检索：同时搜索长期记忆和用户记忆</li>
</ul>
</li>
</ul>
</li>
<li>记忆重排序
<ul>
<li>LLM辅助重排序：使用 rerank_memories 方法通过大语言模型对检索到的记忆进行重新排序</li>
<li>智能相关性判断：基于查询内容判断记忆的相关性，提升记忆使用的准确性</li>
</ul>
</li>
<li>记忆处理与过滤
<ul>
<li>记忆去重：使用 filter_vector_based_similar_memories 过滤过于相似的记忆项</li>
<li>长度过滤：使用 filter_too_short_memories 移除过短的记忆项</li>
<li>无关记忆过滤：通过 filter_unrelated_memories 移除与当前查询历史无关的记忆</li>
<li>冗余记忆过滤：通过 filter_redundant_memories 移除重复的记忆项</li>
</ul>
</li>
<li>记忆整合：在 process_and_rerank_memories 方法中整合原始记忆和新检索的记忆，进行统一处理</li>
</ul>
<h4 id="232-详细步骤说明">2.3.2 详细步骤说明</h4>
<p>SchedulerRetriever 的详细步骤如下：</p>
<ul>
<li>搜索阶段：根据查询在 TreeTextMemory 中搜索相关记忆项</li>
<li>合并阶段：将原始记忆和新检索的记忆合并成一个列表</li>
<li>相似度过滤：使用相似度移除过于相似的记忆项</li>
<li>长度过滤：移除过短的记忆项（默认阈值为6个字符）</li>
<li>去重处理：确保记忆项唯一性，同时保持原有顺序</li>
<li>LLM重排序：利用大语言模型根据查询相关性对记忆进行重新排序</li>
<li>无关记忆过滤：移除与查询历史无关的记忆项</li>
<li>返回结果：返回优化后的记忆列表供系统使用</li>
</ul>
<h4 id="233-调用关系">2.3.3 调用关系</h4>
<p>一般来说，业界的<strong>检索时机</strong>有两种主要方法：</p>
<ul>
<li><strong>主动检索</strong>：在每轮开始时自动加载记忆，确保上下文始终可用，但会为不需要记忆访问的轮次引入不必要延迟</li>
<li><strong>反应式检索</strong>（内存即工具）：代理被赋予查询记忆的工具，自行决定何时检索上下文，更高效但需要额外LLM调用</li>
</ul>
<p>SchedulerRetriever 作为 BaseScheduler 的核心组件，主要在处理用户查询和更新工作记忆时被调用，负责记忆的检索、处理、重排序和过滤等核心功能，具体场景为：</p>
<ul>
<li>
<p>初始化：在BaseScheduler 的initialize_modules中建立SchedulerRetriever 实例。</p>
</li>
<li>
<p>查询处理：当 GeneralScheduler 接收到 QUERY_LABEL 消息时，调用 process_session_turn 方法触发 self.retriever.search() 来检索相关记忆。</p>
</li>
<li>
<p>工作记忆更新：在 replace_working_memory 过程中对候选记忆进行处理和重排序</p>
</li>
</ul>
<pre><code class="language-python">└── GeneralScheduler._query_message_consumer()
  └── BaseScheduler.process_session_turn()
      └── SchedulerRetriever.search() --------&gt; 检索相关记忆

└── BaseScheduler.replace_working_memory()
  └── SchedulerRetriever.process_and_rerank_memories() ------&gt; 处理和重排序记忆
      └──SchedulerRetriever.filter_unrelated_memories() -----&gt; 过滤无关记忆

</code></pre>
<h4 id="234-实现">2.3.4 实现</h4>
<h5 id="记忆检索流程search-方法">记忆检索流程（search 方法）</h5>
<p>作为对比，从业界角度看，一般会从多个维度评估潜在记忆，单纯依赖基于向量的相关性是常见陷阱。相似性得分可能找出概念相似但过时或琐碎的记忆。最有效策略是结合所有三个维度的混合方法：</p>
<ul>
<li><strong>相关性</strong>（语义相似性）：与当前对话的概念关联度</li>
<li><strong>新鲜度</strong>（基于时间）：记忆创建的时间远近</li>
<li><strong>重要性</strong>（显著性）：记忆的整体关键程度</li>
</ul>
<p>SchedulerRetriever的  search方法会 调用TreeTextMemory的功能来进行搜索。</p>
<p><img alt="MemScheduler-7-4" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1850883/202512/1850883-20251217235146352-1069858221.png" class="lazyload"></p>
<h5 id="记忆全流程处理与重排process_and_rerank_memories-方法">记忆全流程处理与重排（process_and_rerank_memories 方法）</h5>
<p><img alt="MemScheduler-7-5" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1850883/202512/1850883-20251217235158437-1421234719.png" class="lazyload"></p>
<h5 id="llm-记忆重排流程rerank_memories-方法">LLM 记忆重排流程（rerank_memories 方法）</h5>
<p><img alt="MemScheduler-7-6" loading="lazy" data-src="https://img2024.cnblogs.com/blog/1850883/202512/1850883-20251217235208004-2039529273.png" class="lazyload"></p>
<h5 id="代码">代码</h5>
<pre><code class="language-python">class SchedulerRetriever(BaseSchedulerModule):
    """
    MemOS 记忆检索与优化调度器
    核心功能：检索树状文本内存、合并新旧记忆、多维度过滤、LLM 智能重排，为 Agent 提供精准记忆支持
    继承自 BaseSchedulerModule，遵循 MemOS 调度器模块统一接口规范
    """
    def __init__(self, process_llm: BaseLLM, config: BaseSchedulerConfig):
        """
        初始化检索调度器实例
        Args:
            process_llm: 用于记忆重排的 LLM 实例（BaseLLM 基类对象）
            config: 调度器配置对象（BaseSchedulerConfig 基类，包含过滤、检索等参数）
        """
        super().__init__()  # 调用父类 BaseSchedulerModule 的初始化方法

        # 超参数配置：记忆过滤阈值
        self.filter_similarity_threshold = 0.75  # 相似度过滤阈值（≥0.75 的记忆视为重复）
        self.filter_min_length_threshold = 6     # 长度过滤阈值（字符数＜6 的记忆视为过短，将被过滤）

        self.config: BaseSchedulerConfig = config  # 保存调度器配置
        self.process_llm = process_llm             # 保存用于重排的 LLM 实例

        # 初始化记忆过滤器：委托处理「无关记忆」和「冗余记忆」过滤逻辑
        self.memory_filter = MemoryFilter(process_llm=process_llm, config=config)

    def search(
        self,
        query: str,
        mem_cube: GeneralMemCube,
        top_k: int,
        method: str = TreeTextMemory_SEARCH_METHOD,
        info: dict | None = None,
    ) -&gt; list[TextualMemoryItem]:
        """
        根据查询语句在文本内存中检索相关记忆
        Args:
            query: 检索查询字符串（如 Agent 的当前对话查询）
            mem_cube: 记忆立方体（GeneralMemCube，MemOS 中存储各类记忆的核心数据结构）
            top_k: 返回的Top-K 相关记忆数量
            method: 检索方法（默认使用 TreeTextMemory 的标准搜索方法）
            info: 检索附加信息（需包含 user_id、session_id，用于记录记忆使用历史）

        Returns:
            检索到的文本型记忆项列表（list[TextualMemoryItem]）；检索失败返回空列表
        """
        text_mem_base = mem_cube.text_mem  # 从记忆立方体中提取文本内存核心实例
        try:
            # 仅支持 TreeTextMemory 的两种检索方法
            if method in [TreeTextMemory_SEARCH_METHOD, TreeTextMemory_FINE_SEARCH_METHOD]:
                # 断言文本内存类型为 TreeTextMemory（确保检索方法兼容性）
                assert isinstance(text_mem_base, TreeTextMemory)
                # 若未传入 info，打印警告并初始化空信息（用于历史记录存储）
                if info is None:
                    info = {"user_id": "", "session_id": ""}

                # 根据检索方法设置模式：快速搜索（fast）或精细搜索（fine）
                mode = "fast" if method == TreeTextMemory_SEARCH_METHOD else "fine"
                # 检索长期记忆（LongTermMemory）
                results_long_term = text_mem_base.search(
                    query=query, top_k=top_k, memory_type="LongTermMemory", mode=mode, info=info
                )
                # 检索用户记忆（UserMemory）
                results_user = text_mem_base.search(
                    query=query, top_k=top_k, memory_type="UserMemory", mode=mode, info=info
                )
                # 合并长期记忆和用户记忆的检索结果
                results = results_long_term + results_user
            else:
                # 不支持的检索方法：抛出未实现异常（传入文本内存类型）
                raise NotImplementedError(str(type(text_mem_base)))
        except Exception as e:
            # 检索异常：打印错误日志（包含堆栈信息），返回空列表
            logger.error(f"Fail to search. The exeption is {e}.", exc_info=True)
            results = []
        return results

    def rerank_memories(
        self, queries: list[str], original_memories: list[str], top_k: int
    ) -&gt; Tuple[list[str], bool]:
        """
        基于 LLM 对记忆进行相关性重排（根据查询语句调整记忆顺序）
        Args:
            queries: 用于判断相关性的查询列表（通常为当前对话查询）
            original_memories: 待重排的记忆文本列表
            top_k: 重排后返回的 Top-K 记忆数量

        Returns:
            Tuple[重排后的记忆文本列表（长度≤top_k）, 重排成功标志（bool）]

        Note:
            若 LLM 重排失败（如 JSON 解析异常），降级为原始记忆顺序（截断至 top_k）
        """

        logger.info(f"Starting memory reranking for {len(original_memories)} memories")

        # 构建 LLM 重排提示词（使用 "memory_reranking" 模板）
        prompt = self.build_prompt(
            "memory_reranking",
            queries=[f"[0] {queries[0]}"],  # 格式化查询（仅取第一个查询，标记为 [0]）
            current_order=[f"[{i}] {mem}" for i, mem in enumerate(original_memories)],  # 格式化原始记忆顺序
        )

        # 调用 LLM 生成重排结果（构造用户角色消息）
        response = self.process_llm.generate([{"role": "user", "content": prompt}])

        try:
            # 解析 LLM 响应中的 JSON 数据（期望格式：{"new_order": [索引列表], "reasoning": "重排理由"}）
            response = extract_json_dict(response)
            new_order = response["new_order"][:top_k]  # 提取 Top-K 索引顺序
            # 根据新索引映射回原始记忆文本
            text_memories_with_new_order = [original_memories[idx] for idx in new_order]
            success_flag = True  # 重排成功标志
        except Exception as e:
            # 重排异常（如 JSON 解析失败、键缺失）：打印错误日志，使用原始顺序降级
            text_memories_with_new_order = original_memories[:top_k]  # 截断原始记忆至 top_k
            success_flag = False  # 重排失败标志
        return text_memories_with_new_order, success_flag

    def process_and_rerank_memories(
        self,
        queries: list[str],
        original_memory: list[TextualMemoryItem],
        new_memory: list[TextualMemoryItem],
        top_k: int = 10,
    ) -&gt; Tuple[Optional[list[TextualMemoryItem]], bool]:
        """
        记忆全流程处理：合并新旧记忆 → 过滤冗余/过短记忆 → LLM 重排 → 返回优化后记忆
        Args:
            queries: 用于重排的查询列表（判断记忆相关性的依据）
            original_memory: 原始记忆列表（已存储的历史记忆，TextualMemoryItem 类型）
            new_memory: 新记忆列表（待合并的新增记忆，TextualMemoryItem 类型）
            top_k: 最终返回的最大记忆数量（默认 10）

        Returns:
            Tuple[优化后的 TextualMemoryItem 列表（长度≤top_k）, 处理成功标志（bool）]；失败返回 (None, False)
        """
        # 1. 合并原始记忆和新记忆（形成完整记忆池）
        combined_memory = original_memory + new_memory

        # 2. 构建「归一化文本→记忆对象」的映射（用于后续重排后还原记忆对象）
        # transform_name_to_key：文本归一化函数（如去空格、小写，确保匹配一致性）
        memory_map = {
            transform_name_to_key(name=mem_obj.memory): mem_obj for mem_obj in combined_memory
        }

        # 2. 提取所有记忆的文本内容（用于过滤和重排）
        combined_text_memory = [m.memory for m in combined_memory]

        # 4. 相似度过滤：移除过于相似的记忆（基于向量相似度，阈值 0.75）
        filtered_combined_text_memory = filter_vector_based_similar_memories(
            text_memories=combined_text_memory,
            similarity_threshold=self.filter_similarity_threshold,
        )

        # 5. 长度过滤：移除过短记忆（字符数＜6 的记忆视为无效，阈值 6）
        filtered_combined_text_memory = filter_too_short_memories(
            text_memories=filtered_combined_text_memory,
            min_length_threshold=self.filter_min_length_threshold,
        )

        # 6. 去重：基于归一化文本的字典去重（保留原始顺序）
        unique_memory = list(dict.fromkeys(filtered_combined_text_memory))

        # 7. LLM 智能重排：按与查询的相关性调整记忆顺序
        text_memories_with_new_order, success_flag = self.rerank_memories(
            queries=queries,
            original_memories=unique_memory,
            top_k=top_k,
        )

        # 8. 映射回原始记忆对象（从文本列表还原为 TextualMemoryItem 列表）
        memories_with_new_order = []
        for text in text_memories_with_new_order:
            normalized_text = transform_name_to_key(name=text)  # 文本归一化（确保与 memory_map 键匹配）
            if normalized_text in memory_map:  # 检查归一化文本是否存在于映射中
                memories_with_new_order.append(memory_map[normalized_text])
            else:
                # 日志警告：记忆文本未找到映射（可能是归一化异常或记忆对象缺失）
                logger.warning(
                    f"Memory text not found in memory map. text: {text};\n"
                    f"Keys of memory_map: {memory_map.keys()}"
                )

        return memories_with_new_order, success_flag

    def filter_unrelated_memories(
        self,
        query_history: list[str],
        memories: list[TextualMemoryItem],
    ) -&gt; Tuple[list[TextualMemoryItem], bool]:
        """
        过滤与查询历史无关的记忆（委托给 MemoryFilter 实现）
        Args:
            query_history: 查询历史列表（Agent 的对话历史）
            memories: 待过滤的记忆列表（TextualMemoryItem 类型）
        Returns:
            Tuple[过滤后的记忆列表, 过滤成功标志]
        """
        return self.memory_filter.filter_unrelated_memories(query_history, memories)

    def filter_redundant_memories(
        self,
        query_history: list[str],
        memories: list[TextualMemoryItem],
    ) -&gt; Tuple[list[TextualMemoryItem], bool]:
        """
        过滤冗余记忆（委托给 MemoryFilter 实现）
        Args:
            query_history: 查询历史列表（判断冗余的上下文依据）
            memories: 待过滤的记忆列表（TextualMemoryItem 类型）
        Returns:
            Tuple[过滤后的记忆列表, 过滤成功标志]
        """
        return self.memory_filter.filter_redundant_memories(query_history, memories)

    def filter_unrelated_and_redundant_memories(
        self,
        query_history: list[str],
        memories: list[TextualMemoryItem],
    ) -&gt; Tuple[list[TextualMemoryItem], bool]:
        """
        同时过滤「无关记忆」和「冗余记忆」（基于 LLM 分析，委托给 MemoryFilter 实现）
        Args:
            query_history: 查询历史列表（上下文依据）
            memories: 待过滤的记忆列表（TextualMemoryItem 类型）
        Returns:
            Tuple[过滤后的记忆列表, 过滤成功标志]
        """
        return self.memory_filter.filter_unrelated_and_redundant_memories(query_history, memories)


</code></pre>
<h2 id="0x03-关联">0x03 关联</h2>
<p>SchedulerDispatcher、GeneralScheduler 和 TreeTextMemory 之间存在明确的依赖和协作关系。</p>
<h3 id="31-schedulerdispatcher-和-generalscheduler">3.1 SchedulerDispatcher 和 GeneralScheduler</h3>
<p>SchedulerDispatcher 负责消息的分发和处理，是一个底层的消息处理组件。GeneralScheduler 是一个更高级别的调度器，利用 SchedulerDispatcher 来管理不同类型的消息（如查询、回答、添加记忆等）。</p>
<ul>
<li>GeneralScheduler 依赖 SchedulerDispatcher</li>
<li>SchedulerDispatcher 是 GeneralScheduler 的一个组件，在 GeneralScheduler 的 init 方法中，创建了一个 SchedulerDispatcher 实例并赋值给 self.dispatcher</li>
</ul>
<p>GeneralScheduler 使用 self.dispatcher 来：</p>
<ul>
<li>注册消息处理器（handlers）</li>
<li>分发消息（通过 self.dispatcher.dispatch()）</li>
<li>批量处理消息（通过 self.dispatcher.group_messages_by_user_and_cube()）</li>
<li>等待任务完成（通过 self.dispatcher.join()）</li>
<li>关闭调度器（通过 self.dispatcher.shutdown()）</li>
</ul>
<h3 id="32-generalscheduler-和-treetextmemory">3.2 GeneralScheduler 和 TreeTextMemory</h3>
<p>TreeTextMemory 是记忆存储和检索的核心组件，提供了添加、搜索、更新和删除记忆的功能，GeneralScheduler 利用 TreeTextMemory 来管理记忆的生命周期，处理与记忆相关的各种操作。</p>
<ul>
<li>GeneralScheduler 依赖 TreeTextMemory来执行实际的记忆操作</li>
<li>TreeTextMemory 提供了 GeneralScheduler 所需的记忆存储和检索功能</li>
</ul>
<p>GeneralScheduler 通过 mem_cube.text_mem 访问 TreeTextMemory 实例，GeneralScheduler 使用 TreeTextMemory 来：</p>
<ul>
<li>TreeTextMemory 包含了 MemoryManager 和 Searcher 等组件，这些组件被 GeneralScheduler 间接使用</li>
<li>获取工作记忆（通过 text_mem_base.get_working_memory()）</li>
<li>搜索记忆（通过 self.retriever.search()，其中 retriever 使用 TreeTextMemory 的搜索功能），检索过程涉及：
<ul>
<li>使用 TaskGoalParser 解析查询</li>
<li>使用 GraphMemoryRetriever 从图数据库检索记忆</li>
<li>使用 Reranker 对检索到的记忆进行重排序</li>
<li>使用 MemoryReasoner 进行推理以优化结果</li>
</ul>
</li>
<li>添加记忆（通过 self.memory_manager.add()，其中 memory_manager 与 TreeTextMemory 相关联）</li>
<li>替换工作记忆（通过 text_mem_base.replace_working_memory()）</li>
<li>获取记忆（通过 mem_cube.text_mem.get(memory_id=memory_id)）</li>
<li>GeneralScheduler 接收不同类型的消息（查询、回答、添加记忆等），根据消息类型，GeneralScheduler 调用相应的处理器，处理器使用 TreeTextMemory 的方法来执行具体操作</li>
</ul>
<p>GeneralScheduler 和 TreeTextMemory 之间是一种协作关系，其中：</p>
<ul>
<li>GeneralScheduler 是一个高级调度器，负责处理不同类型的消息并协调记忆操作</li>
<li>TreeTextMemory 是记忆操作的实际执行者，提供了存储、检索、更新和组织记忆的功能</li>
</ul>
<p>两者通过定义良好的接口进行交互，GeneralScheduler 调用 TreeTextMemory 的方法来执行具体操作<br>
这种设计实现了关注点分离，使调度逻辑和记忆管理逻辑可以独立开发和维护。</p>
<p>协作范例如下：</p>
<pre><code class="language-python"># GeneralScheduler 处理查询消息的简化流程
def process_query(self, query, mem_cube):
    # 1. 获取当前工作记忆
    current_memory = mem_cube.text_mem.get_working_memory()
    # 2. 检索相关记忆
    new_candidates = mem_cube.text_mem.search(query, top_k=self.top_k)
    # 3. 更新工作记忆
    mem_cube.text_mem.replace_working_memory(new_candidates)
</code></pre>
<h3 id="33-总结">3.3 总结</h3>
<p>三者之间的依赖关系可以概括为：</p>
<ul>
<li>GeneralScheduler → SchedulerDispatcher（使用其进行消息分发）</li>
<li>GeneralScheduler → TreeTextMemory（使用其进行记忆操作）</li>
</ul>
<p>具体来说：</p>
<ul>
<li>SchedulerDispatcher 是一个消息分发器，负责将不同类型的消息路由到相应的处理器</li>
<li>GeneralScheduler 是一个高级调度器，它使用 SchedulerDispatcher 来管理消息，并使用 TreeTextMemory 来处理与记忆相关的操作</li>
<li>TreeTextMemory 是记忆存储和检索的核心组件，为 GeneralScheduler 提供底层的记忆操作支持</li>
</ul>
<p>这种设计使得系统能够有效地处理不同类型的消息，并对记忆进行相应的操作，实现了消息处理和记忆管理的解耦。</p>
<h3 id="34-moscore">3.4 MosCore</h3>
<p>MosCore 是一个很好的范例。</p>
<pre><code class="language-python">MosCore
- uses GeneralMemCube
- uses GeneralScheduler
  - uses MemoryManager
  - uses TreeTextMemory
- uses UserManager
</code></pre>
<p>在如下代码中，可以管窥。</p>
<pre><code class="language-python">class MOSCore:
    """
    The MOSCore (Memory Operating System Core) class manages multiple MemCube objects and their operations.
    It provides methods for creating, searching, updating, and deleting MemCubes, supporting multi-user scenarios.
    MOSCore acts as an operating system layer for handling and orchestrating MemCube instances.
    """

    def __init__(self, config: MOSConfig, user_manager: UserManager | None = None):
        self.config = config
        self.user_id = config.user_id
        self.session_id = config.session_id
        self.chat_llm = LLMFactory.from_config(config.chat_model)
        self.mem_reader = MemReaderFactory.from_config(config.mem_reader)
        self.chat_history_manager: dict[str, ChatHistory] = {}
        # use thread safe dict for multi-user product-server scenario
        self.mem_cubes: OptimizedThreadSafeDict[str, GeneralMemCube] = (
            OptimizedThreadSafeDict() if user_manager is not None else {}
        )
        self._register_chat_history()

        # Use provided user_manager or create a new one
        if user_manager is not None:
            self.user_manager = user_manager
        else:
            self.user_manager = UserManager(user_id=self.user_id if self.user_id else "root")

        # Initialize mem_scheduler
        self._mem_scheduler_lock = Lock()
        self.enable_mem_scheduler = self.config.get("enable_mem_scheduler", False)
        if self.enable_mem_scheduler:
            self._mem_scheduler = self._initialize_mem_scheduler()
            self._mem_scheduler.mem_cubes = self.mem_cubes
        else:
            self._mem_scheduler: GeneralScheduler = None

</code></pre>
<h2 id="0xff-参考">0xFF 参考</h2>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-12-22 20:16">2025-12-22 20:15</span>&nbsp;
<a href="https://www.cnblogs.com/rossiXYZ">罗西的思考</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19364457);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19364457', targetLink: 'https://www.cnblogs.com/rossiXYZ/p/19364457', title: '【Agent】MemOS 源码笔记---(7)---MemScheduler 细节' })">举报</a>
</div>
         ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ 数据主权的守护者：为什么你需要本地知识库？ ]]></title>
    <link>https://www.cnblogs.com/zhangguilandocpartner/p/19384254</link>
    <guid>92900da29a5df3a45eb493d8907c0475</guid>
    <description>
    <![CDATA[ 
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/zhangguilandocpartner/p/19384254" title="发布于 2025-12-22 20:12">
    <span role="heading" aria-level="2">数据主权的守护者：为什么你需要本地知识库？</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="数据主权的守护者为什么你需要本地知识库">数据主权的守护者：为什么你需要本地知识库？</h1>
<h2 id="引言数据时代的隐私危机">引言：数据时代的隐私危机</h2>
<p>在人工智能和大数据蓬勃发展的今天，我们的文件、知识和数据正以前所未有的速度被数字化。然而，一个不容忽视的问题日益凸显：当我们将个人或企业的核心知识上传到云端，我们是否正在失去对数据的主控权？云服务商的数据泄露事件频发，AI模型对用户数据的"白嫖"行为引发广泛担忧。在这样的背景下，本地知识库应运而生，为我们提供了一个全新的解决方案。</p>
<h2 id="什么是本地知识库">什么是本地知识库？</h2>
<h3 id="知识库的基本概念">知识库的基本概念</h3>
<p>知识库本质上是一个智能化的知识管理系统，它能够对各类文件内容进行深度解析、搜索和问答。与传统的文件管理系统不同，知识库能够理解文件的内在含义，而不仅仅是表面的文件名或关键词。</p>
<h3 id="本地知识库的核心特征">本地知识库的核心特征</h3>
<p>本地知识库最大的特点在于其运行环境——它完全部署在用户的本地设备上，无论是个人电脑还是企业服务器。用户可以将PDF、Word、图片、Excel、视频、音频等各种格式的文件"投影"到知识库中，系统会深度解析这些文件的内容，建立起一个完全私有的知识体系。</p>
<h2 id="为什么本地知识库成为刚需">为什么本地知识库成为刚需？</h2>
<h3 id="数据安全不可妥协的底线">数据安全：不可妥协的底线</h3>
<p>在数字经济时代，数据已经成为个人和企业的核心资产。云知识库虽然便捷，但存在固有的安全风险：文件数据可能被窃取、被AI模型无偿使用。而本地知识库确保了数据的绝对安全——不会上传任何文件、断网可用、自主可控。这种安全性的保障，对于保护知识产权、维护数据隐私具有不可替代的价值。</p>
<h3 id="深度解析超越传统搜索的能力">深度解析：超越传统搜索的能力</h3>
<p>本地知识库的强大之处在于其深度解析能力。它不仅能够识别文本内容，还能解析图片中的文字、视频中的画面、音频中的语音，甚至是文件中的公式和表格。这种全方位的解析能力，使得搜索和问答的准确性和深度达到了前所未有的水平。</p>
<h3 id="多模态搜索全新的知识检索体验">多模态搜索：全新的知识检索体验</h3>
<p>知识库支持多种搜索形式：文本相似搜索、图片相似搜索、语音相似搜索、视频相似搜索等。用户可以通过图片搜索相关的文档，通过文本搜索匹配的图片，实现真正意义上的多模态知识检索。</p>
<h2 id="本地知识库的核心优势">本地知识库的核心优势</h2>
<h3 id="隐私保护与数据主权">隐私保护与数据主权</h3>
<p>本地知识库最大的优势在于用户完全掌握自己的数据。所有操作都在本地完成，不依赖外部云服务，从根本上杜绝了数据泄露的风险。对于企业而言，这意味着核心资产的安全保障；对于个人用户，这意味着隐私权的真正实现。</p>
<h3 id="文件变动实时同步">文件变动实时同步</h3>
<p>本地知识库具备文件监听功能，能够实时监测本地文件的变更——包括增加、删除、修改、移动和重命名，并自动同步到知识库中。这种智能化的同步机制，确保了知识库始终与用户的文件系统保持同步。</p>
<h3 id="高度可定制性">高度可定制性</h3>
<p>用户可以根据自己的需求自由定制本地知识库的存储架构、模型架构和查询逻辑。这种灵活性使得本地知识库能够适应各种不同的使用场景和需求。</p>
<h2 id="应用场景从个人到企业">应用场景：从个人到企业</h2>
<h3 id="智能客服与问答系统">智能客服与问答系统</h3>
<p>企业可以将产品信息、服务文档等沉淀到本地知识库中，构建智能客服系统。当用户咨询时，系统会先搜索知识库中的相关信息，然后传递给问答模型生成准确回答。这种基于本地知识库的智能客服，既保证了回答的准确性，又确保了数据的安全性。</p>
<h3 id="企业知识管理">企业知识管理</h3>
<p>本地知识库能够帮助企业打破部门间的知识壁垒，实现高效的知识共享和管理。员工可以快速检索内部资料，新员工能够通过知识库快速学习企业流程和规范。</p>
<h3 id="智能推荐系统">智能推荐系统</h3>
<p>对于电商和企业服务，本地知识库可以用于构建智能推荐系统。基于用户行为和知识库中的商品信息，系统能够实现精准、多样的商品推荐。</p>
<h2 id="本地知识库与云知识库的对比">本地知识库与云知识库的对比</h2>
<h3 id="安全性对比">安全性对比</h3>
<p>本地知识库的所有操作都在用户设备上进行，不上传任何文件数据；而云知识库虽然跨设备便捷，但存在数据外泄的风险。在数据安全日益重要的今天，本地知识库的优势愈发明显。</p>
<h3 id="适用场景差异">适用场景差异</h3>
<p>云知识库适合需要跨设备访问、对安全性要求不极高的场景；而本地知识库则是数据敏感型个人和企业的首选。</p>
<h2 id="结语重掌数据主权的时代已来临">结语：重掌数据主权的时代已来临</h2>
<p>在数据成为新时代石油的今天，掌握数据主权意味着掌握核心竞争力。本地知识库为我们提供了一个安全、高效、可控的知识管理解决方案。它不仅保护了我们的隐私和知识产权，更为我们开启了一种全新的知识利用方式。</p>
<p>随着技术的不断发展，本地知识库必将成为个人和企业知识管理的标准配置。选择本地知识库，就是选择对自己数据的完全掌控，就是选择在数字时代的安全与自由。</p>
<p><img alt="btywyzv7" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3745994/202512/3745994-20251222201152992-1975120989.png" class="lazyload"></p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-12-22 20:13">2025-12-22 20:12</span>&nbsp;
<a href="https://www.cnblogs.com/zhangguilandocpartner">aaabbbcccd</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19384254);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19384254', targetLink: 'https://www.cnblogs.com/zhangguilandocpartner/p/19384254', title: '数据主权的守护者：为什么你需要本地知识库？' })">举报</a>
</div>
         ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ 吴恩达深度学习课程四：计算机视觉  第三周：检测算法 （一）目标定位与特征点检测 ]]></title>
    <link>https://www.cnblogs.com/Goblinscholar/p/19384242</link>
    <guid>469880176f4c26905d70177d26106101</guid>
    <description>
    <![CDATA[ 
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/Goblinscholar/p/19384242" title="发布于 2025-12-22 20:08">
    <span role="heading" aria-level="2">吴恩达深度学习课程四：计算机视觉  第三周：检测算法 （一）目标定位与特征点检测</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>此分类用于记录吴恩达深度学习课程的学习笔记。<br>
课程相关信息链接如下：</p>
<ol>
<li>原课程视频链接：<a href="https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&amp;from_spmid=playlist.playlist-detail.0.0&amp;is_story_h5=false&amp;mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&amp;plat_id=116&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&amp;share_source=COPY&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1713085655&amp;unique_k=DfBgvFW&amp;up_id=8654113&amp;vd_source=e035e9878d32f414b4354b839a4c31a4" target="_blank" rel="noopener nofollow">[双语字幕]吴恩达深度学习deeplearning.ai</a></li>
<li>github课程资料，含课件与笔记:<a href="https://github.com/robbertliu/deeplearning.ai-andrewNG" target="_blank" rel="noopener nofollow">吴恩达深度学习教学资料</a></li>
<li>课程配套练习（中英）与答案：<a href="https://blog.csdn.net/u013733326/article/details/79827273" target="_blank" rel="noopener nofollow">吴恩达深度学习课后习题与答案</a></li>
</ol>
<p>本篇为第四课的第三周内容，<a href="https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&amp;vd_source=e035e9878d32f414b4354b839a4c31a4&amp;p=130" target="_blank" rel="noopener nofollow">3.1</a>到<a href="https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&amp;vd_source=e035e9878d32f414b4354b839a4c31a4&amp;p=131" target="_blank" rel="noopener nofollow">3.2</a>的内容。</p>
<hr>
<p>本周为第四课的第三周内容，这一课所有内容的中心只有一个：<strong>计算机视觉</strong>。应用在深度学习里，就是专门用来进行图学习的模型和技术，是在之前全连接基础上的“特化”，也是相关专业里的一个重要研究大类。<br>
<strong>这一整节课都存在大量需要反复理解的内容和机器学习、数学基础。</strong> 因此我会尽可能的补足基础，用比喻和实例来演示每个部分，从而帮助理解。<br>
第三周的内容将从<strong>图像分类</strong>进一步拓展到<strong>目标检测（Object Detection）</strong> 这一更具挑战性的计算机视觉任务。<br>
与分类任务只需回答“图中有什么”不同，目标检测需要同时解决“ <strong>有什么</strong>”以及“<strong>在什么位置</strong>”两个问题，因此在模型结构设计、训练方式和评价标准上都更为复杂。<br>
本篇的内容关于目标定位与特征点检测。</p>
<h1 id="1目标定位">1.目标定位</h1>
<p>这个标题想必不用解释，在原本的分类算法中，我们构建模型来识别输入的图像“”是什么“，而现在，我们想进一步进行定位，找到检测内容在图像中的位置。<br>
这并不是另起炉灶，而是在原本的分类基础上的进一步发展，因此在这部分，我们就来看看如何从”是什么“到”在什么位置“。</p>
<h2 id="11-图像分类的传播逻辑">1.1 图像分类的传播逻辑</h2>
<p>图像分类的逻辑对我们来说早就不陌生了，我们再简单梳理一遍，来看这样课程里这样一个例子:<br>
<img src="https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251222200657746-1727850064.png" alt="image.png" loading="lazy"><br>
你会发现，在监督学习中，我们对每个样本都规定了它的”标准答案“。<br>
因此，网络才能在”传播——对比——改正“这样的循环中不断学习，最终实现分类。<br>
用通俗的话来说，这其中的一个关键点在于：<strong>模型并不知道汽车是什么，但是你通过标签告诉它：这是汽车。</strong><br>
而目标定位也是同样的逻辑：<strong>我们不仅要通过标签告诉模型什么是汽车，还要告诉它汽车在哪</strong>。</p>
<h2 id="12-从分类到定位">1.2 从分类到定位</h2>
<p>通过上一部分的了解，我们知道，要实现从分类到定位的逻辑，首先就要在标签中增加检测目标的位置信息。<br>
<img src="https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251222200659358-1846109048.png" alt="image.png" loading="lazy"><br>
我们知道，在分类问题中，可以用标签 0 和 1 来表示二分类，用独热编码来表示多分类。<br>
但显然，表示目标在图像中的位置是一个更复杂，需要更多参数的问题。<br>
你可能会想，可不可以在标签中增加四个参数对，来表示检测目标的四角？<br>
这的确是一种方法，但实际上，我们使用的是<strong>另一种参数量更少且更普适的方法</strong>：<br>
<img src="https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251222200658515-1899177316.png" alt="image.png" loading="lazy"><br>
再规范一下：<br>
在目标检测中，通常使用一个<strong>边界框（Bounding Box）</strong> 来描述目标在图像中的位置与大小。<br>
相比直接记录四个顶点坐标，更常见也更通用的做法是用<strong>中心点 + 宽高</strong>来表示：</p>
<ol>
<li><span class="math inline">\(b_x\)</span>：表示目标边界框<strong>中心点在水平方向上的位置</strong>，通常是相对于整张图像宽度的归一化坐标，取值范围为 <span class="math inline">\([0,1]\)</span>。</li>
<li><span class="math inline">\(b_y\)</span>：表示目标边界框<strong>中心点在垂直方向上的位置</strong>，同样是相对于图像高度归一化后的坐标。</li>
<li><span class="math inline">\(b_w\)</span>：表示目标边界框的<strong>宽度</strong>，一般以图像宽度为基准进行归一化，用来描述目标在水平方向上所占的比例。</li>
<li><span class="math inline">\(b_h\)</span>：表示目标边界框的<strong>高度</strong>，同样以图像高度为基准归一化，用来描述目标在垂直方向上所占的比例。</li>
</ol>
<p>通过这四个参数，模型不仅能够判断“<strong>是什么</strong>”，还可以同时给出“<strong>在什么位置、占多大范围</strong>”，从而完成最基本的目标定位任务。</p>
<h2 id="13-定位问题的完整样本标签">1.3 定位问题的完整样本标签</h2>
<p>了解了如何表示检测目标在图像中的位置信息后，继续延续上面的例子，来看看如何完整地表示一个样本的标签：<br>
<img src="https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251222200658627-1495275469.png" alt="image.png" loading="lazy"></p>
<p>同样再次列举一下：<br>
在<strong>单目标检测</strong>的设定下，一个样本的标签通常由以下几部分组成：</p>
<ol>
<li><span class="math inline">\(p_c\)</span>：表示<strong>当前图像中是否存在需要检测的目标</strong>，这是一个二值变量，通常取值为 <span class="math inline">\(0\)</span> 或 <span class="math inline">\(1\)</span>。<br>
当 <span class="math inline">\(p_c = 1\)</span> 时，表示图像中确实存在目标，此时后续的位置信息和类别信息才是“有效”的；<br>
当 <span class="math inline">\(p_c = 0\)</span> 时，表示图像中不存在目标，<strong>其余参数通常被忽略或置为 0。</strong></li>
<li><span class="math inline">\(b_x,b_y,b_h,b_w\)</span> :表示检测目标的<strong>位置信息</strong>。</li>
<li><span class="math inline">\(c_n\)</span>：表示目标所属的<strong>类别信息</strong>。<br>
在二分类问题中，它可以是一个标量；<br>
在多分类问题中，通常采用<strong>独热编码</strong> 的形式，用一个向量来表示目标属于哪一类。</li>
</ol>
<p>因此，在单目标检测任务中，一个完整的标签可以被表示为：</p>
<p></p><div class="math display">\[y=(p_c,b_x,b_y,b_w,b_h,c_1,c_2,…,c_n)
\]</div><p></p><p>它同时包含了<strong>是否存在目标、目标的位置与尺度，以及目标的类别信息</strong>。</p>
<p>这样，我们就在数据层面上为目标检测问题完成了准备工作。<br>
为了逻辑的完整性，下一步网络如何学习目标检测的问题就放在下一篇来展开。<br>
下面我们简单拓展一下课程里提到的另一项应用：特征点检测。</p>
<h1 id="2特征点检测keypoint-detection">2.特征点检测（Keypoint Detection）</h1>
<p>在我们了解了如何对图像中的目标进行定位后，结合实际，你会发现在很多实际任务中，我们关注的目标并不是汽车、行人这类需要用边界框描述的宏观物体，而是一些<strong>更精细、更局部的结构</strong>，例如：</p>
<ul>
<li>人的眼角、瞳孔</li>
<li>人体的关节位置</li>
<li>笔尖的位置</li>
<li>面部轮廓上的关键点</li>
</ul>
<p>这类目标本身<strong>几乎不具备“面积”或“尺寸”概念</strong>，用一个边界框来描述反而显得冗余。<br>
在这种情况下，目标往往可以直接用<strong>一个二维坐标点</strong>来表示。</p>
<p>因此，与目标检测类似，我们同样可以通过<strong>人工标注关键点坐标</strong>，构建训练样本，让神经网络学习这个关键点是否存在，以及如果存在，它在图像中的具体位置。<br>
从建模角度看，特征点检测本质上是一个对连续坐标进行预测的<strong>回归问题</strong>。<br>
这种任务，就叫<strong>特征点检测（Keypoint Detection）</strong>。<br>
来看课程里这样一个例子：<br>
<img src="https://img2024.cnblogs.com/blog/3708248/202512/3708248-20251222200657530-300000923.png" alt="image.png" loading="lazy"><br>
我们可以通过检测得到的特征点做进一步应用，比如根据面部特征点分布判断人的表情，根据人的关节特征点位置预测人的动作等等。</p>
<h1 id="3总结">3.总结</h1>
<table>
<thead>
<tr>
<th>概念</th>
<th>原理</th>
<th>比喻</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像分类</td>
<td>通过标签告诉模型“输入图像<strong>是什么</strong>”，模型在传播—对比—修正中学习从图像到类别的映射关系</td>
<td>像给学生一张照片，并直接告诉他：“这是汽车”</td>
</tr>
<tr>
<td>目标定位</td>
<td>在分类的基础上，进一步通过标签告诉模型目标<strong>在图像中的什么位置</strong></td>
<td>不仅说“这是汽车”，还用手指指出“汽车在这里”</td>
</tr>
<tr>
<td>边界框（Bounding Box）</td>
<td>用一个矩形区域描述目标的位置与大小，统一表示空间信息</td>
<td>在照片上用框把目标圈出来</td>
</tr>
<tr>
<td><span class="math inline">\(b_x, b_y\)</span></td>
<td>描述目标中心点在图像中的相对位置（归一化坐标）</td>
<td>用“地图坐标”标出目标的中心点</td>
</tr>
<tr>
<td><span class="math inline">\(b_w, b_h\)</span></td>
<td>描述目标在水平方向和垂直方向所占的比例</td>
<td>告诉别人这个框“有多宽、多高”</td>
</tr>
<tr>
<td>中心点 + 宽高表示法</td>
<td>相比四个角坐标，参数更少、形式更统一，便于网络学习</td>
<td>不报四个角地址，而是说“在市中心，方圆两公里”</td>
</tr>
<tr>
<td><span class="math inline">\(p_c\)</span>（目标存在性）</td>
<td>指示图像中是否存在需要检测的目标，决定后续标签是否有效</td>
<td>先确认“房间里有没有人”，再讨论他站在哪</td>
</tr>
<tr>
<td>类别向量 <span class="math inline">\(c_1,\dots,c_n\)</span></td>
<td>表示目标属于哪一类，可用标量或独热编码</td>
<td>给被圈出来的目标贴上“身份标签”</td>
</tr>
<tr>
<td>特征点检测</td>
<td>不用边界框，而是直接预测关键点的二维坐标</td>
<td>不给人画框，只标出“眼睛、关节、笔尖的位置”</td>
</tr>
</tbody>
</table>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-12-22 20:08">2025-12-22 20:08</span>&nbsp;
<a href="https://www.cnblogs.com/Goblinscholar">哥布林学者</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19384242);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19384242', targetLink: 'https://www.cnblogs.com/Goblinscholar/p/19384242', title: '吴恩达深度学习课程四：计算机视觉  第三周：检测算法 （一）目标定位与特征点检测' })">举报</a>
</div>
         ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ RAG效果差？7个指标让你的准确率大幅提升 ]]></title>
    <link>https://www.cnblogs.com/aifrontiers/p/19384101</link>
    <guid>e73abccfc464399a78b66353a752b574</guid>
    <description>
    <![CDATA[ 
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/aifrontiers/p/19384101" title="发布于 2025-12-22 18:55">
    <span role="heading" aria-level="2">RAG效果差？7个指标让你的准确率大幅提升</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<p>原文: <a href="https://mp.weixin.qq.com/s/VV29xpdOMEkbz4iXmD_szg" target="_blank" rel="noopener nofollow">https://mp.weixin.qq.com/s/VV29xpdOMEkbz4iXmD_szg</a></p>
<p>在上一篇 <a href="https://mp.weixin.qq.com/s/am89yasxAvuYUToEAWNyTA" target="_blank" rel="noopener nofollow">RAG评测完整指南：指标、测试和最佳实践</a> 中，我们对RAG系统中各个模块的评估方法进行系统的阐述，并没有详细介绍每个模块设计的具体指标、指标的计算方法。从本篇开始，将深入RAG系统的各个模块（如检索、排序、生成），探讨各个模块有哪些评估指标，及这些指标的计算方法。</p>
<h1 id="1-核心要点">1. 核心要点</h1>
<ul>
<li>RAG系统的检索环节本质是排序任务，目标是返回与用户查询高度相关的上下文片段列表，为生成环节提供精准支撑。</li>
<li>评估RAG排序质量时，需具备&lt;查询，上下文&gt;对形式的预测结果，作为真实标签的二元或分级相关性得分，并确定Top-K参数，即衡量前K个检索结果中，有多少是相关的。</li>
<li>预测指标：准确率（Precision at K）、召回率（Recall at K）评估</li>
<li>排序指标：NDCG、MRR、MAP等评估检索准确性与排序合理性</li>
</ul>
<h1 id="2-什么是rag系统的检索核心">2. 什么是RAG系统的检索核心？</h1>
<p>RAG系统的核心流程包含<strong>检索-生成</strong>两大环节，其中，检索环节本质是排序任务：系统根据用户的查询意图，从海量知识库中筛选、排序相关上下文片段，最终返回相关度Top-K的结果作为模型生成答案的依据。</p>
<p>想象这样的场景，当用户向RAG系统提问<em>Transformer模型的核心创新点是什么？</em></p>
<p>RAG系统的执行流程大致如下：a) 首先，从知识库中检索相关文档片段，包括注意力机制原理、编码器-解码器结构说明、与传统模型的差异对比等内容；b) 通过排序算法进一步筛选最贴合查询的片段优先呈现给生成模块；c）最终输出准确、有依据的答案。这一过程中，检索排序质量直接决定了生成答案的准确性与可靠性，若检索排序失误导致无关上下文被优先选用，可能引发生成内容偏离主题或出现事实错误。</p>
<p>下面，我们聊聊RAG系统、推荐系统和检索系统的共同点、差异点。</p>
<p><strong>三者的相同点</strong></p>
<table>
<thead>
<tr>
<th>核心目标</th>
<th>从大规模数据集中，根据特定需求，筛选出符合条件的信息，避免对全量数据进行遍历计算，本质是信息过滤与精准触达的工具。</th>
</tr>
</thead>
<tbody>
<tr>
<td>底层技术</td>
<td>都需要构建数据索引（如倒排索引、向量索引），依赖相似度计算算法（如余弦相似度、BM25），且都需要处理海量非结构化 / 结构化数据的存储与快速查询，提升信息获取效率</td>
</tr>
</tbody>
</table>
<p><strong>三者的差异点</strong></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>RAG 系统</th>
<th>推荐系统</th>
<th>检索系统</th>
</tr>
</thead>
<tbody>
<tr>
<td>用户需求类型</td>
<td>隐性 / 显性需求，需要 「理解 + 生成的深度加工」</td>
<td>隐性需求为主，用户无明确查询</td>
<td>显性需求为主，用户输入明确关键词 / 条件</td>
</tr>
<tr>
<td>数据流向</td>
<td>用户查询 → 检索知识 → 大模型生成回答</td>
<td>用户行为 → 分析偏好 → 推送内容</td>
<td>用户查询 → 匹配索引 → 返回结果</td>
</tr>
<tr>
<td>输出形式</td>
<td>自然语言文本（回答、摘要等）</td>
<td>内容列表（商品、视频、文章）</td>
<td>结果列表（文档、链接、数据条目）</td>
</tr>
<tr>
<td>典型应用场景</td>
<td>智能问答、知识库助手、企业客服机器人</td>
<td>电商商品推荐、短视频推荐、新闻推荐</td>
<td>搜索引擎（百度 / 谷歌）、文件检索、数据库查询</td>
</tr>
</tbody>
</table>
<p>本篇将涵盖RAG系统检索排序的核心指标，从基础的Precision、Recall到复杂的NDCG、MAP等。</p>
<h1 id="3-评估原则">3. 评估原则</h1>
<p>在深入探讨具体指标前，需要定义好检索排序评估的基本原则，主要围绕输入数据、相关性定义、Top-K参数三大核心要素。</p>
<h2 id="31-输入数据">3.1 输入数据</h2>
<p>这里介绍的都是需要真值的指标，因此，评估RAG检索/排序质量时，数据需要满足&lt;<strong>预测结果，真实标签</strong>&gt;数据对的模式，具体定义如下：</p>
<ul>
<li>预测结果：RAG检索/排序模块针对每个查询生成的上下文排序列表（含查询-上下文对及相关性得分/排名）。</li>
<li>真实标签：反映上下文与查询实际相关性的标注（二元标签或分级得分），用于验证排序结果的准确性。</li>
</ul>
<p>RAG系统的排序评估数据集通常结构如下：</p>
<table>
<thead>
<tr>
<th>查询ID</th>
<th>上下文ID</th>
<th>预测值（相关性得分）</th>
</tr>
</thead>
<tbody>
<tr>
<td>查询1</td>
<td>片段A</td>
<td>0.92</td>
</tr>
<tr>
<td>查询1</td>
<td>片段B</td>
<td>0.78</td>
</tr>
<tr>
<td>查询1</td>
<td>片段C</td>
<td>0.15</td>
</tr>
</tbody>
</table>
<p>其中，<strong>上下文</strong>可为知识库中的文档片段、句子或段落，是RAG系统生成答案的直接依据。为判断检索排序优劣，需将预测结果与真实标签对比，示例如下：</p>
<table>
<thead>
<tr>
<th>查询ID</th>
<th>上下文ID</th>
<th>预测值（相关性得分）</th>
<th>目标值（实际相关性）</th>
</tr>
</thead>
<tbody>
<tr>
<td>查询1</td>
<td>片段A</td>
<td>0.92</td>
<td>1（高度相关）</td>
</tr>
<tr>
<td>查询1</td>
<td>片段B</td>
<td>0.78</td>
<td>0（无关）</td>
</tr>
<tr>
<td>查询1</td>
<td>片段C</td>
<td>0.15</td>
<td>1（中等相关）</td>
</tr>
</tbody>
</table>
<h2 id="32-什么是相关性">3.2 什么是相关性？</h2>
<p>相关性是RAG排序评估的核心，指上下文片段对回答用户查询的有用性，直接决定该片段是否能为生成模块提供有效支撑。相关性可分为两类：</p>
<ul>
<li>二元相关性：仅判断<strong>相关</strong>或<strong>无关</strong>（如1=对用户查询有用，0=对用户查询没用），如上面表格中<strong>目标值（实际相关性）</strong>字段所示，基于简单的人工标注或规则判断（如上下文是否包含查询核心关键词）。</li>
<li>分级相关性：按有用程度划分等级（如1-5星），例如：5星=完全覆盖查询答案，4星=大部分支撑答案，3星=部分辅助信息，2星=微弱相关，1星=无关。</li>
</ul>
<p>在RAG场景中，相关性标注需重点关注<strong>上下文是否能直接用于生成准确答案</strong>，即使某片段与查询主题相关，但缺乏关键信息（如仅提及概念名称却未解释核心），仍可能被标注为低相关性。</p>
<p>多数检索排序指标要求输入二元相关性数据，因此需对分级得分进行后处理，如将4-5星视为<strong>相关</strong>，1-3星视为<strong>无关</strong>；若存在标注偏差（如不同标注者标准不一致），可通过加权或归一化调整阈值。</p>
<h2 id="33-top-k参数">3.3 Top-K参数</h2>
<p><img alt="1" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222184431570-1650624694.png" class="lazyload"></p>
<p>Top-K参数是RAG检索排序评估的关键截断点，在RAG系统实际应用中，选取前K个片段作为模型上下文生成内容。</p>
<ul>
<li>选择依据：K值需匹配RAG系统的实际配置，如生成答案时最多使用5个上下文片段，则K=5；也可测试多个K值（如K=3、K=5、K=10），分析不同检索深度对排序质量的影响。</li>
<li>核心意义：用户（或生成模块）仅关注Top-K结果，对于低排名的上下文几乎无实际作用。例如，若RAG系统配置K=5，那么评估需重点关注前5个检索结果的相关性与排序合理性，无需过度关注第6名之后的片段。</li>
</ul>
<p>你可能会有疑问，一定要设置这个K值？答案是未必。当知识库规模极小（如仅数十个片段），且需全量评估时可省略。但，RAG应用场景中的知识库数据一般都比较大，尤其是在企业级应用中，全量评估既无必要也无效率，因此Top-K截断是评估的必备环节。</p>
<p>RAG系统的排序指标可分为预测、排序两大类，分别对应准确性、合理性两大评估维度：</p>
<ul>
<li>预测指标：评估准确性，是否能精准筛选出与查询相关的上下文。</li>
<li>排序指标：评估合理性，是否能将更相关的上下文排在更靠前的位置。</li>
</ul>
<h1 id="4-检索排序指标详解">4. 检索排序指标详解</h1>
<h2 id="41-预测指标">4.1 预测指标</h2>
<h3 id="411-精确率-precisionk">4.1.1 精确率 (Precision@K)</h3>
<p><strong>定义</strong>：衡量前K个检索结果中，有多少是相关的。</p>
<p><img alt="2" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222184646752-339394347.png" class="lazyload"></p>
<p><strong>核心逻辑</strong>：衡量RAG系统<strong>精准筛选</strong>的能力，Top-K结果中相关片段越多，生成答案的依据越可靠。示例，若K=5，Top-5检索结果中有3个相关片段，则Precision@5=3/5=60%。</p>
<p><strong>局限性</strong>：受<strong>查询相关上下文总数</strong>影响，若某查询仅存在2个相关片段，即使K=10，最高准确率也仅20%，难以跨查询平均比较。</p>
<p>怎么理解这个问题，我们以推荐场景为例来解释。推荐系统给用户 A 推荐了 10 个项目，而用户 A 实际的相关项目（也就是用户真正感兴趣、符合需求的项目）只有 3 个，不管推荐算法如何迭代优化，精确率的上限就是30%。</p>
<p>为什么难以平均和比较？假设另一个用户 B，相关项目有 8 个。同样给他推荐 10 个项目，精确率上限能达到80%。对比用户 A 30%的精确率上限，两者的上限差异，<strong>不全是推荐算法或机制的好坏导致的</strong>，更多是因为两人本身的相关项目数量不同。</p>
<p>如果直接把这两个用户的精确率平均，结果并不能真实反映推荐系统的整体性能，因为这个平均值被用户的相关项目基数干扰了。</p>
<h3 id="412-召回率recallk">4.1.2 召回率（Recall<strong>@</strong>K）</h3>
<p><strong>定义</strong>：在前K个检索结果中，检索到的相关文档的数量占总相关文档数量的比例。</p>
<p><img alt="3" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222184704022-1867248335.png" class="lazyload"></p>
<p><strong>核心逻辑</strong>：衡量RAG系统全面覆盖的能力，即能否将所有相关上下文尽可能纳入Top-K结果。示例：某查询共有8个相关上下文，Top-10检索结果中包含5个，则Recall@10=5/8=62.5%。</p>
<p><strong>局限性</strong>：与K值正相关，K值越大，召回率越高，需结合实际系统配置的K值进行评估，而非单纯追求高召回。</p>
<h3 id="413-f分数f-score">4.1.3 F分数（F-score）</h3>
<p><strong>定义</strong>：平衡Precision与Recall的综合指标，通过Beta参数调整二者权重。</p>
<p><img alt="4" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222184823659-2037218871.png" class="lazyload"></p>
<p>当 <span class="math inline">\(\beta=1\)</span>时，则会得到标准的 F1 分数，即精确率和召回率的调和平均值。</p>
<p><strong>核心逻辑</strong>：Precision关注精准度，Recall关注覆盖度，F值可根据RAG场景需求调整优先级， <span class="math inline">\(\beta=1\)</span>时为F1分数（二者同等重要）， <span class="math inline">\(\beta&gt;1\)</span>时侧重Recall， <span class="math inline">\(\beta&lt;1\)</span>时侧重Precision。</p>
<h2 id="42-排序指标">4.2 排序指标</h2>
<p>以前文介绍的精确率和召回率指标为例，这些并不<strong>考虑排名。</strong>它们对排名前 K 的相关项的具体位置漠不关心。</p>
<p>我们来举例说明，考虑两个列表，它们都包含 5 个匹配项（共 10 个）。在左侧列表中，相关项位于列表顶部；在右侧列表中，相关项位于列表底部。只要相关项的总数保持不变，精确率就始终为 50%。</p>
<p><img alt="5" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222184840470-249548423.png" class="lazyload"></p>
<p>不同于预测指标，排序指标聚焦排序合理性，评估RAG系统对相关上下文的排序能力，相关度越高的片段应排在越靠前的位置，这对生成效率与答案质量至关重要。</p>
<h3 id="421-平均倒数排名mean-reciprocal-rankmrr">4.2.1 平均倒数排名（(Mean Reciprocal Rank，MRR）</h3>
<p><strong>定义</strong>：所有查询中<strong>第一个相关上下文的倒数排名</strong>的平均值。</p>
<p><img alt="6" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222184942877-1034910417.png" class="lazyload"></p>
<p><strong>核心逻辑</strong>：衡量RAG系统快速命中关键上下文的能力，第一个相关片段的排名越靠前，生成模块越能快速获取有效依据。</p>
<p>以下图为例，解释MRR的计算过程</p>
<p><img alt="7" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222185013164-1559961396.png" class="lazyload"></p>
<p>查询1的第一个相关片段排在第3位（倒数排名=1/3），查询2的第一个相关片段排在第1位（倒数排名=1），查询3的第一个相关片段排在第2位（倒数排名=1/2），查询4的第一个相关片段排在第3位（倒数排名=1/3），则MRR=(0.33 + 1 + 0.5 + 0.33) / 4 = 0.54。</p>
<p><strong>局限性</strong>：仅关注第一个相关片段，忽略后续相关片段的排序质量，若后续相关片段排名过低，仍可能影响生成效果。</p>
<h3 id="422-平均精度均值mean-average-precision-map">4.2.2 平均精度均值（Mean Average Precision, MAP）</h3>
<p><strong>定义</strong>：计算多个查询的平均精度（AP）来衡量<a href="https://zhida.zhihu.com/search?content_id=247682406&amp;content_type=Article&amp;match_order=1&amp;q=%E6%A3%80%E7%B4%A2%E6%8E%92%E5%BA%8F&amp;zhida_source=entity" target="_blank" rel="noopener nofollow">检索排序</a>性能。MAP首先计算每个查询的平均精确率，然后取所有查询的平均值。</p>
<p><img alt="8" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222185108204-661347408.png" class="lazyload"></p>
<p>以上图为例，查询1 Top-6结果中，相关片段位于第1、4、5位，则AP=(1/1 + 2/4 + 3/5) / 3 ≈ (1+0.5+0.6) / 3 ≈ 0.7。另外，查询2 Top-6按照类似的逻辑，得到AP为0.8，则所有查询的AP平均值即为MAP = （0.8 + 0.7）/ 2 = 0.75。</p>
<p>下图给出了多个查询的详细计算过程，</p>
<p><img alt="9" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222185116787-1253861924.png" class="lazyload"></p>
<p><strong>核心逻辑</strong>：同时评估筛选准确性与排序合理性，既要求Top-K结果多包含相关片段，也要求相关片段排在更靠前的位置。</p>
<p><strong>局限性</strong>：计算逻辑较复杂，不易向非技术人员解释。</p>
<h3 id="423--命中率hit-rate">4.2.3  命中率（Hit Rate）</h3>
<p><strong>定义</strong>：Top-K检索结果中<strong>至少包含一个相关上下文</strong>的查询占比。</p>
<p><img alt="10" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222185155277-1693391900.png" class="lazyload"></p>
<p>将每个查询结果统计一个二元分数，1表示前 K 个搜索结果中至少有一个相关项目，0表示其他情况。然后，计算所有查询的平均命中率。以上图为例，对于三次查询，有2个查询结果中是包含相关项目的，则命中率为 2 / 3 = 0.67。</p>
<p><strong>核心逻辑</strong>：衡量RAG系统基础有效性，能否为大多数查询找到有效支撑上下文，是RAG系统的及格线指标。</p>
<p><strong>局限性</strong>：仅关注是否存在相关片段，不关注相关片段的数量与排名，无法反映排序质量的细节。</p>
<h3 id="424-归一化折损累积增益normalized-discounted-cumulative-gainndcg">4.2.4 归一化折损累积增益（Normalized Discounted Cumulative Gain，NDCG）</h3>
<p><strong>定义</strong>：综合了文档的相关性和它们在排名中的位置，用于衡量排名质量。它不仅考虑了相关文档的位置，还为较早出现的相关文档分配更高的权重。NDCG特别适用于文档具有不同程度相关性的场景。</p>
<p><img alt="11" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3719074/202512/3719074-20251222185204395-70917889.png" class="lazyload"></p>
<p>根据上图，可以看出NDCG的计算过程：先计算折损累积增益（DCG），相关得分除以实际排名的对数折损；再计算理想DCG（IDCG），所有相关上下文按最高相关度排序后的DCG；NDCG=DCG/IDCG。</p>
<p><strong>核心逻辑</strong>：相关度越高的上下文排在越靠前的位置，NDCG越接近1；无关上下文排在前面则NDCG降低。支持分级相关性得分（如1-5星），更贴合RAG场景的精细评估。</p>
<p><strong>优势</strong>：归一化后可跨查询、跨数据集比较，同时兼顾相关性与排名，是RAG排序评估的黄金指标之一。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-12-22 18:55">2025-12-22 18:55</span>&nbsp;
<a href="https://www.cnblogs.com/aifrontiers">AI-Frontiers</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19384101);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19384101', targetLink: 'https://www.cnblogs.com/aifrontiers/p/19384101', title: 'RAG效果差？7个指标让你的准确率大幅提升' })">举报</a>
</div>
         ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ 推荐算法中的位置消偏，如何解决？ ]]></title>
    <link>https://www.cnblogs.com/GlenTt/p/19383094</link>
    <guid>277b7d4f5c8b62f97ca0b41e1a23bafe</guid>
    <description>
    <![CDATA[ 
		<h2>
			<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/GlenTt/p/19383094" title="发布于 2025-12-22 16:22">
    <span role="heading" aria-level="2">推荐算法中的位置消偏，如何解决？</span>
    

</a>

		</h2>
		<div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h3 id="位置偏差指的是在用户--item-不变的前提下把同一条内容放在不同位置用户的点击概率会系统性变化这部分变化是由位置本身引起的而不是内容质量变化引起的"><strong>位置偏差指的是：在用户 / item 不变的前提下，把同一条内容放在不同位置，用户的点击概率会系统性变化</strong>——这部分变化是由位置本身引起的，而不是内容质量变化引起的。</h3>
<p>1.Recommending What Video to Watch Next: A Multitask Ranking System（google）</p>
<p>2.PAL: A Position-bias Aware Learning Framework for CTR Prediction in Live Recommender Systems（华为）</p>
<hr>
<h2 id="一googlerecommending-what-video-to-watch-next">一、Google：Recommending What Video to Watch Next</h2>
<p><img alt="image" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3670197/202512/3670197-20251222160346128-1897569845.png" class="lazyload"><br>
<img alt="image" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3670197/202512/3670197-20251222160441684-811142632.png" class="lazyload"></p>
<h3 id="1-消偏的核心思想">1. 消偏的核心思想</h3>
<p>YouTube 的“下一个看什么”模型使用线上日志训练，日志里存在明显的 <strong>selection bias / position bias</strong>：</p>
<p>排在前面的候选被看见、被点击的概率更高，并不完全因为它质量更高，而是因为上游系统已经给过“加成”。</p>
<p>这篇文章的思路，可以概括为一句话：</p>
<blockquote>
<p><strong>把“观测到的点击”拆成“内容本身的效用（utility）”和“各种展示条件带来的偏置（propensity）之和，在模型结构中用两个塔分别建模。</strong><br>
形式上，相当于在 logit 上做一个分解：</p>
</blockquote>
<p>logit(CTR(<em>x</em>,bias_feat))=<em>f</em>main​(<em>x</em>)+<em>g</em>bias​(bias_feat)</p>
<ul>
<li><em>f</em>main​(<em>x</em>)：用户–视频的“真实偏好”（希望尽量去掉位置等偏差）；</li>
<li><em>g</em>bias​(⋅)：位置、设备、候选来源、上一版系统评分等造成的偏移。</li>
</ul>
<p>训练时，<strong>两者相加</strong>来拟合日志中的点击；</p>
<p>排序上线时，可以选择只依赖 <em>f</em>main​(<em>x</em>)，或者显著弱化/归一化 <em>g</em>bias​，从而达到“减小偏差”的目的。</p>
<h3 id="2-具体实现结构">2. 具体实现结构</h3>
<ol>
<li>
<p><strong>Main Tower：多任务 MMoE Ranking 网络</strong></p>
<ul>
<li>输入：用户特征、历史行为（watch history）、候选视频特征、上下文特征等；</li>
<li>结构：embedding → shared-bottom DNN → MMoE（多个专家 + 每个任务各自的 gate）；</li>
<li>输出：多个任务头（click、watch time、满意度等）。</li>
</ul>
</li>
<li>
<p>这部分主要为 <strong>学习稳定的用户–视频表达</strong>，且要兼顾多个优化目标。</p>
</li>
<li>
<p><strong>Shallow Tower（Bias Tower）：浅层偏差网络</strong></p>
<ul>
<li>输入：只选与 selection bias 强相关的特征：position、上游排序分、设备、入口、语言等；</li>
<li>结构：一到两层的小 DNN；</li>
<li>输出：一个标量偏置 <em>g</em>bias​，加到 main tower 的 logit 上。</li>
</ul>
</li>
<li>
<p><strong>特征 Dropout：防止“全靠位置解释点击”</strong>如果简单把 position 交给 shallow tower，模型可能会走“捷径”：直接用位置来解释大部分点击，main tower 反而学不到有效的内容信号。因此训练中在 bias 特征上做较强的 dropout（或者给 shallow tower 很低的容量、较强正则），让 main tower 也必须承担足够的解释力。这样 <em>f</em>main​(<em>x</em>) 更接近“去位置偏差”的效用。</p>
</li>
<li>
<p><strong>Serving 策略</strong></p>
<ul>
<li>在排序阶段，常见做法是 <strong>不给 position 相关特征，或者固定为统一值</strong>，从而弱化甚至移除 <em>g</em>bias​ 对排序的影响；</li>
<li>如果某些模块确实需要“带偏的真实 CTR”（例如收益评估），还可以单独把 bias tower 再加回去算一个“现实 CTR”。</li>
</ul>
</li>
</ol>
<h3 id="3-各部分在消偏中的作用">3. 各部分在消偏中的作用</h3>
<ul>
<li><strong>Main Tower</strong>：主要承载“真实兴趣”的建模，是最终用于排序的核心分数；MMoE 解决多目标之间梯度冲突，使这个表示尽可能稳定、泛化好。</li>
<li><strong>Shallow Tower</strong>：把短期、强烈、但结构较简单的 bias（位置、入口、曝光策略）吸收入一个可控制的通道，避免 main tower 对这些因素过拟合。</li>
<li><strong>Bias 特征 Dropout / 小容量设计</strong>：强迫 main tower 不得不解释一部分点击信号，否则 shallow tower 会把所有监督“吃掉”，去偏效果就会失真。</li>
</ul>
<h3 id="4实验结果">4.实验结果</h3>
<p><img alt="image" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3670197/202512/3670197-20251222160735172-498688773.png" class="lazyload"></p>
<p>1.训练样本的分布，被点击的样本其位置特征越靠前<br>
2.最后的position bias符合预期，越靠前，其position bias越大，当推理去除时，对靠后的样本，补偿就越大，起到一个debias 的作用</p>
<h3 id="5评价">5.评价</h3>
<ol>
<li><strong>偏差建模是“隐式”的，而不是严格的因果建模</strong>虽然直观上可以解释为“utility + propensity”，但从优化上看只是对 logit 做了一个可学习的分解，<strong>没有任何硬约束保证 <strong><em>f</em>main​</strong> 就是真实效用、</strong><em>g</em>bias​** 就是纯粹偏差**。两者之间存在可交换性：网络完全可以把一部分内容效用塞进 bias tower，只要整体 loss 不变。</li>
<li>能够处理的不只是 position bias，而是一揽子的“条件偏差”从工程角度很灵活，但从理论上说，它把所有与展示条件相关的因素统统扔进一个 shallow tower，难以区分各类 bias 的贡献，不适合做精细的公平性/因果分析。</li>
<li><strong>sample selection bias 仍然存在</strong>模型仍旧在上一版推荐系统生成的候选上训练，这天生带来 selection bias（没被召回的 item 没有 label）。shallow tower 只是在 ranking 这一层做了结构上的消偏，对候选生成阶段的偏差基本无能为力。</li>
<li><strong>“消偏效果”高度依赖工程细节</strong>例如：bias 特征放哪些、dropout 比例多大、shallow tower 容量如何设定、线上是否真的去掉 bias 特征等。这些并没有统一理论指引，更多依赖经验和 A/B 实验。</li>
</ol>
<hr>
<h2 id="二华为palposition-bias-aware-learning">二、华为：PAL（Position-bias Aware Learning）</h2>
<h3 id="1-消偏的核心思想-1">1. 消偏的核心思想</h3>
<p>PAL 背景更具体：在一些线上业务中，<strong>训练 CTR 模型时可以使用 position 特征，但排序预测阶段并不知道最终位置</strong>——先要根据 CTR 排序，排序结果再决定位置。这就产生了典型的“训练有位置、预测没位置”的矛盾。</p>
<p>工业界常见的粗暴做法：</p>
<ul>
<li>训练时让模型使用位置特征；</li>
<li>预测时给所有候选一个“默认位置”（比如 1、5、10），用该位置值喂进模型。</li>
</ul>
<p>不同默认位置会得到完全不同的排序结果，且很不稳定。</p>
<p>PAL 的核心思想是把点击拆成两个阶段的概率乘积，并对每一项分别建模：</p>
<p><em>p</em>(click∣<em>x</em>,<em>pos</em>)=<em>p</em>(seen∣<em>pos</em>)⋅<em>p</em>(click∣<em>x</em>,seen)</p>
<p>再做两个关键假设：</p>
<ol>
<li><strong>“是否被看到”只由 position 决定</strong>即 <em>p</em>(seen∣<em>x</em>,<em>pos</em>)≈<em>p</em>(seen∣<em>pos</em>)；</li>
<li><strong>在已经被看到的前提下，点击与 position 无关</strong>即 <em>p</em>(click∣<em>x</em>,<em>pos</em>,seen)≈<em>p</em>(click∣<em>x</em>,seen)。</li>
</ol>
<p>如此一来：</p>
<ul>
<li>第一项是纯粹的 <strong>位置曝光概率</strong>；</li>
<li>第二项可视为 <strong>“在所有 item 都被同样看见”时的真实兴趣点击率</strong>。</li>
</ul>
<p>训练阶段用乘积来拟合日志 CTR，</p>
<p>排序阶段直接用第二项 <em>p</em>(click∣<em>x</em>,seen)，自然就不再需要位置特征。</p>
<h3 id="2-具体实现结构-1">2. 具体实现结构</h3>
<ol>
<li>
<p><strong>ProbSeen(pos)：位置曝光模块</strong></p>
<ul>
<li>输入：position（可能再加少量页面级上下文）；</li>
<li>结构：embedding → 小型 DNN → sigmoid 输出到 [0,1]；</li>
<li>含义：学“该位置下被真正注意到的概率”，近似传统点击模型里的 examination 函数。</li>
</ul>
</li>
<li>
<p><strong>pCTR(x)：条件 CTR 模块</strong></p>
<ul>
<li>输入：用户、item、上下文等特征，但 <strong>不包含 position</strong>；</li>
<li>结构：任意现成 CTR 网络（DeepFM / Wide&amp;Deep / DIN等）；</li>
<li>输出：在“已看到”的前提下，点击概率。</li>
</ul>
</li>
<li>
<p><strong>联合训练</strong></p>
<ul>
<li>合成点击预测：</li>
</ul>
</li>
</ol>
<p><em>p</em>^​(<em>y</em>=1∣<em>x</em>,<em>pos</em>)=<em>ProbSeen</em>(<em>pos</em>)×<em>pCTR</em>(<em>x</em>)</p>
<ul>
<li>使用普通二分类交叉熵对观测到的 click label 做监督；</li>
<li>两个模块参数一次性 end-to-end 更新。</li>
</ul>
<p>这种训练方式保证了两部分<strong>在同一个目标下联合适配</strong>，而不是“先估曝光率，再单独训 CTR”。</p>
<ol start="4">
<li>
<p><strong>Serving 策略</strong></p>
<ul>
<li>排序阶段只上线 pCTR 模块：score = pCTR(x)；</li>
<li>如果系统某处需要“带位置”的现实 CTR，可再用离线学习到的 ProbSeen(pos) 补回。</li>
</ul>
</li>
</ol>
<h3 id="3-各部分在消偏中的作用-1">3. 各部分在消偏中的作用</h3>
<ul>
<li><strong>ProbSeen</strong>：显式承担位置偏差的建模任务，是 PAL 的“偏差塔”。</li>
<li><strong>pCTR</strong>：负责学习“内容兴趣 + 用户偏好”，在结构上完全与 position 解耦，是 PAL 想要的“去偏 CTR”。</li>
<li><strong>乘积形式</strong>：保证这两个模块都在概率域是可解释的；与加法形态相比，这更像传统 exam–click 模型：先看见，再点击。</li>
</ul>
<h3 id="4-批判性评价">4. 批判性评价</h3>
<ol>
<li>
<p><strong>核心假设过于理想化</strong></p>
<ul>
<li>实际上，用户是否“看到”一个推荐，不仅取决于位置，还取决于 <strong>item 的显眼程度、封面、标题长度、用户滚动速度、屏幕大小等</strong>；更合理的应是 <em>p</em>(seen∣<em>x</em>,<em>pos</em>)，而不只是 <em>pos</em>。</li>
<li>即便在“已被看到”条件下，位置仍可能影响点击，例如：首条更可信、后面的更像广告，等等。这些都会违背“点击与 position 无关”的假设。</li>
</ul>
</li>
<li>
<p>因此，PAL 在理论上是一个比较“干净”的分解，但在现实业务中一定会有系统偏差残留在 pCTR 模块里，消偏效果取决于场景是否“近似满足”这些假设。</p>
</li>
<li>
<p><strong>存在可辨识性问题</strong>数学上，如果把 ProbSeen 整体乘以常数 <em>c</em>，再把 pCTR 整体除以 <em>c</em>，乘积不变，loss 一样小。这说明两个模块在纯数据层面并不是强可辨识的。实际训练中，谁负责什么，很大程度依赖于：</p>
<ul>
<li>初始化方式；</li>
<li>两边网络容量；</li>
<li>正则化强度。</li>
</ul>
</li>
<li>
<p>这会导致结果对实现细节很敏感，很难保证“ProbSeen 真的是位置曝光概率”。</p>
</li>
<li>
<p><strong>只能处理“位置”这一维的偏差</strong>在很多业务里，selection bias 来自多种因素：入口、流量策略、探索策略、历史曝光频次等。PAL 把 bias 完全归因于 position，<strong>无法显式建模其他偏差来源</strong>；这些因素要么被 pCTR 强行吸收，要么只能另想办法处理。</p>
</li>
<li>
<p><strong>训练数据仍来源于偏置系统</strong>PAL 没有改变训练样本本身“来源于旧系统排序”的事实，因此在候选集选择层面的偏差仍旧存在，只是对 position 这一维度做了结构化处理。</p>
</li>
</ol>
<hr>
<h2 id="三两篇工作在消偏上的共同点与差异">三、两篇工作在消偏上的共同点与差异</h2>
<p><strong>共同点</strong></p>
<ul>
<li>
<p>都承认：日志里的点击混杂了“用户真实偏好”和“展示条件偏差”，不能直接当作“公平的真实喜好”；</p>
</li>
<li>
<p>都采用了 <strong>“主塔 + 偏差塔”</strong> 的结构分解：</p>
<ul>
<li>主塔强调内容/用户的效用；</li>
<li>偏差塔强调位置、入口、上游系统排序等引入的倾斜；</li>
</ul>
</li>
<li>
<p>排序上线时，都倾向于 <strong>只用主塔</strong>（或显著弱化偏差塔）来提高公平性和泛化。</p>
</li>
</ul>
<p><strong>差异</strong></p>
<ul>
<li>
<p><strong>分解形式不同</strong></p>
<ul>
<li>Google：logit 加法，偏差与效用是可加的；</li>
<li>PAL：概率乘法，偏差是“被看到概率”，效用是“看到后的点击概率”，解释接近经典点击模型。</li>
</ul>
</li>
<li>
<p><strong>偏差塔输入的范围不同</strong></p>
<ul>
<li>Google 的 shallow tower 可以吃多种 bias 特征（位置、上游排序分、设备等），更工程化；</li>
<li>PAL 的 ProbSeen 只吃 position，更专注但也更受限。</li>
</ul>
</li>
<li>
<p><strong>在“训练有位置、预测无位置”问题上的针对性</strong></p>
<ul>
<li>Google 方案更像是“即便预测时还有 position，我也让模型知道它只是偏置的一部分，有机会在 serving 时关掉它”；</li>
<li>PAL 则直接从概率分解出发，设计出一个在结构上就可以天然“训练有位置、预测不需要位置”的框架。</li>
</ul>
</li>
</ul>
<h3 id="实践经验">实践经验</h3>
<p>当前采用 shallow tower 引入位置特征，并与主塔通过加法或乘法进行交互，本质上是在整体样本分布下学习不同位置对应的平均点击贡献。然而在真实数据中，位置与内容质量高度相关，靠前位置天然聚集了用户更感兴趣的内容，靠后位置则相反，因此 shallow tower 学到的并非纯粹的位置因果效应，<strong><strong>而是位置偏置与内容分布偏置的混合结果</strong></strong>。训练阶段引入 shallow tower、推理阶段将其移除，会导致模型在训练与推理阶段的函数形式不一致，主塔参数已隐含依赖 shallow tower 的补偿效果，<strong><strong>从而难以保证去除的仅是位置贡献</strong></strong>。</p>
<p>无论采用加法还是乘法交互，该问题本质上都无法通过建模形式解决，实际表现往往是训练指标改善有限、线上效果不稳定，目前认为该类基于相关性分解的位置消偏思路在当前数据分布下存在根本局限。</p>

</div>
<div class="clear"></div>

		<p class="postfoot">
			posted on 
<span id="post-date" data-last-update-days="0.12361111111111112" data-date-updated="2025-12-22 19:20">2025-12-22 16:22</span>&nbsp;
<a href="https://www.cnblogs.com/GlenTt">GRITJW</a>&nbsp;
阅读(<span id="post_view_count">18</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19383094);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19383094', targetLink: 'https://www.cnblogs.com/GlenTt/p/19383094', title: '推荐算法中的位置消偏，如何解决？' })">举报</a>

		</p>
	 ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ MySQL 在哪些场景下不会写 binlog ]]></title>
    <link>https://www.cnblogs.com/ivictor/p/19382809</link>
    <guid>a9fcc8e5588c8766e623484def22e168</guid>
    <description>
    <![CDATA[ 
		<h1 class="postTitle">
			<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/ivictor/p/19382809" title="发布于 2025-12-22 15:47">
    <span role="heading" aria-level="2">MySQL 在哪些场景下不会写 binlog</span>
    

</a>

		</h1>
		<div class="clear"></div>
		<div class="postBody">
			<div id="cnblogs_post_body" class="blogpost-body blogpost-body-html">
<h1 data-tool="mdnice编辑器"><span style="color: rgba(0, 128, 0, 1)">背景</span></h1>
<p data-tool="mdnice编辑器"><span>在 MySQL 中，慢日志不仅可以记录在文件中，还可以记录在表中。具体是记录在文件还是表中是由<code><span>log_output</span></code><span>参数决定的。</span></span></p>
<p data-tool="mdnice编辑器"><span>该参数默认为<code><span>FILE</span></code><span>，即慢日志默认会记录在文件中。如果参数中包含<code><span>TABLE</span></code><span>，则慢日志还会记录在<code><span>mysql.slow_log</span></code><span>中，而<code><span>mysql.slow_log</span></code><span>使用的是 CSV 存储引擎。</span></span></span></span></span></p>
<p data-tool="mdnice编辑器"><span>最初研究这一问题，是为了确认在主从复制以及组复制（MGR）环境下，<code><span>mysql.slow_log</span></code><span>表中的慢日志是否会同步到其他节点。</span></span></p>
<p data-tool="mdnice编辑器"><span>随着分析的深入，发现 MySQL 实际上提供了多种机制和开关，用于确保操作不会写入 binlog。</span></p>
<p data-tool="mdnice编辑器"><span>由于&nbsp;<strong>ROW 格式</strong><span>&nbsp;是目前最常用的 binlog 格式，本文将从&nbsp;<strong>ROW 模式下 MySQL 判断操作是否写入 binlog 的实现逻辑</strong><span>&nbsp;入手，逐步引出相关控制开关，并分析它们各自的使用场景。</span></span></span></p>
<h1 data-tool="mdnice编辑器"><span style="color: rgba(0, 128, 0, 1)">ROW 格式下判断操作是否写入 binlog 的实现逻辑</span></h1>
<p data-tool="mdnice编辑器"><span>在 ROW 格式下，将数据变化记录到 binlog 的核心是在<code><span>binlog_log_row</span></code><span>函数中实现的：</span></span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>int<span>&nbsp;<span>binlog_log_row<span>(TABLE *table,&nbsp;<span>const<span>&nbsp;uchar *before_record,<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span>const<span>&nbsp;uchar *after_record, Log_func *log_func)<span>&nbsp;<span>{<span><br>bool<span>&nbsp;error =&nbsp;false<span>;<span><br><span>&nbsp; THD *const<span>&nbsp;thd = table-&gt;in_use;<span><br>// 判断当前操作是否需要写入 binlog<span><br>if<span>&nbsp;(check_table_binlog_row_based(thd, table)) {<span><br><span>&nbsp; &nbsp; ...<span><br><span>&nbsp; &nbsp;&nbsp;if<span>&nbsp;(likely(!(error = write_locked_table_maps(thd)))) {<span><br><span>&nbsp; &nbsp; &nbsp;&nbsp;boolconst<span>&nbsp;has_trans = thd-&gt;lex-&gt;sql_command == SQLCOM_CREATE_TABLE ||<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;table-&gt;file-&gt;has_transactions();<span><br><span>&nbsp; &nbsp; &nbsp;&nbsp;// 根据操作类型，将行镜像写入 binlog<span><br><span>&nbsp; &nbsp; &nbsp; error = (*log_func)(thd, table, has_trans, before_record, after_record);<span><br><span>&nbsp; &nbsp; }<span><br><span>&nbsp; }<span><br><span><br>return<span>&nbsp;error ? HA_ERR_RBR_LOGGING_FAILED :&nbsp;0<span>;<span><br><span>}<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>首先调用&nbsp;<code><span>check_table_binlog_row_based</span></code><span>&nbsp;判断当前操作是否需要写入 binlog，若需要，则会针对不同的操作类型，调用不同的函数来处理。具体来说：</span></span></p>
<ul class="list-paddingleft-1">
<li><span>INSERT：<code><span>Write_rows_log_event::binlog_row_logging_function</span></code><span>。</span></span></li>
<li><span>UPDATE：<code><span>Update_rows_log_event::binlog_row_logging_function</span></code><span>。</span></span></li>
<li><span>DELETE：<code><span>Delete_rows_log_event::binlog_row_logging_function</span></code><span>。</span></span></li>
</ul>
<p data-tool="mdnice编辑器"><span>接下来，重点看看<code><span>check_table_binlog_row_based</span></code><span>函数的处理逻辑。</span></span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>static<span>&nbsp;<span>bool<span>&nbsp;<span>check_table_binlog_row_based<span>(THD *thd, TABLE *table)<span>&nbsp;<span>{<span><br>if<span>&nbsp;(table-&gt;s-&gt;cached_row_logging_check ==&nbsp;-1<span>) {<span><br><span>&nbsp; &nbsp;&nbsp;<span>int<span>&nbsp;<span>const<span>&nbsp;<span>check<span>(table-&gt;s-&gt;tmp_table == NO_TMP_TABLE &amp;&amp;<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; !table-&gt;no_replicate &amp;&amp;<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; binlog_filter-&gt;db_ok(table-&gt;s-&gt;db.str))<span>;<span><br><span>&nbsp; &nbsp; table-&gt;s-&gt;cached_row_logging_check = check;<span><br><span>&nbsp; }<span><br><span><br><span>&nbsp; assert(table-&gt;s-&gt;cached_row_logging_check ==&nbsp;0<span>&nbsp;||<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;table-&gt;s-&gt;cached_row_logging_check ==&nbsp;1<span>);<span><br><span><br>return<span>&nbsp;(thd-&gt;is_current_stmt_binlog_format_row() &amp;&amp;<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; table-&gt;s-&gt;cached_row_logging_check &amp;&amp;<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (thd-&gt;variables.option_bits &amp; OPTION_BIN_LOG) &amp;&amp;<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mysql_bin_log.is_open());<span><br><span>}<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>要返回 false，只需满足以下任意一个条件：</span></p>
<ul class="list-paddingleft-1">
<li><span>当前 SQL 语句不能以 ROW 格式记录到 binlog 中：<code><span>thd-&gt;is_current_stmt_binlog_format_row()</span></code><span>为 false，例如 DDL 语句。</span></span></li>
<li><span>表不允许写入 binlog：<code><span>table-&gt;s-&gt;cached_row_logging_check</span></code><span>为 false。</span></span></li>
<li><span>当前线程未启用 binlog：<code><span>thd-&gt;variables.option_bits &amp; OPTION_BIN_LOG</span></code><span>为 false。</span></span></li>
<li><span>binlog 未打开：<code><span>mysql_bin_log.is_open()</span></code><span>&nbsp;为 false。</span></span></li>
</ul>
<p data-tool="mdnice编辑器"><span>因为第一个条件和第四个条件为 false 的情况并不常见，下面将重点分析<code><span>table-&gt;s-&gt;cached_row_logging_check</span></code><span>和<code><span>thd-&gt;variables.option_bits &amp; OPTION_BIN_LOG</span></code><span>为 false 时的场景。</span></span></span></p>
<h1 data-tool="mdnice编辑器"><span style="color: rgba(0, 128, 0, 1)">cached_row_logging_check 为 false 的场景</span></h1>
<p data-tool="mdnice编辑器"><code><span>table-&gt;s-&gt;cached_row_logging_check</span></code><span>的赋值逻辑如下：</span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>&nbsp;&nbsp;if<span>&nbsp;(table-&gt;s-&gt;cached_row_logging_check ==&nbsp;-1<span>) {<span><br><span>&nbsp; &nbsp;&nbsp;<span>int<span>&nbsp;<span>const<span>&nbsp;<span>check<span>(table-&gt;s-&gt;tmp_table == NO_TMP_TABLE &amp;&amp;<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; !table-&gt;no_replicate &amp;&amp;<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; binlog_filter-&gt;db_ok(table-&gt;s-&gt;db.str))<span>;<span><br><span>&nbsp; &nbsp; table-&gt;s-&gt;cached_row_logging_check = check;<span><br><span>&nbsp; }<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>要使其为 false，必须满足以下任意一个条件：</span></p>
<ol class="list-paddingleft-1">
<li><span>当前表是临时表：&nbsp;<code><span>table-&gt;s-&gt;tmp_table == NO_TMP_TABLE</span></code><span>为 false。</span></span></li>
<li><span>库名不满足 --replicate-do-db、--replicate-ignore-db 复制规则：<code><span>binlog_filter-&gt;db_ok(table-&gt;s-&gt;db.str)</span></code><span>为 false。</span></span></li>
<li><span>表设置了 no_replicate。该属性是在<code><span>open_table_from_share()</span></code><span>函数中根据表的类型和存储引擎能力标志设置的。</span></span></li>
</ol>
<p data-tool="mdnice编辑器"><span>no_replicate 的设置逻辑如下：</span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>&nbsp;&nbsp;if<span>&nbsp;((share-&gt;table_category == TABLE_CATEGORY_LOG) ||<span><br><span>&nbsp; &nbsp; &nbsp; (share-&gt;table_category == TABLE_CATEGORY_RPL_INFO) ||<span><br><span>&nbsp; &nbsp; &nbsp; (share-&gt;table_category == TABLE_CATEGORY_GTID)) {<span><br><span>&nbsp; &nbsp; outparam-&gt;no_replicate =&nbsp;true<span>;<span><br><span>&nbsp; }&nbsp;else<span>&nbsp;if<span>&nbsp;(outparam-&gt;file) {<span><br><span>&nbsp; &nbsp;&nbsp;const<span>&nbsp;handler::Table_flags flags = outparam-&gt;file-&gt;ha_table_flags();<span><br><span>&nbsp; &nbsp; outparam-&gt;no_replicate =<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; !(flags &amp; (HA_BINLOG_STMT_CAPABLE | HA_BINLOG_ROW_CAPABLE)) ||<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; (flags &amp; HA_HAS_OWN_BINLOGGING);<span><br><span>&nbsp; }&nbsp;else<span>&nbsp;{<span><br><span>&nbsp; &nbsp; outparam-&gt;no_replicate =&nbsp;false<span>;<span><br><span>&nbsp; }<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>可以看到，no_replicate 会在以下几种情况设置为 true。</span></p>
<p data-tool="mdnice编辑器"><span>一、特殊类别的表。包括：</span></p>
<ul class="list-paddingleft-1">
<li><span>TABLE_CATEGORY_LOG 类别的表，具体包括 mysql.general_log, mysql.slow_log。</span></li>
<li><span>TABLE_CATEGORY_RPL_INFO 类别的表，具体包括 mysql.slave_relay_log_info，mysql.slave_master_info，mysql.slave_worker_info。</span></li>
<li><span>TABLE_CATEGORY_GTID 类别的表，具体包括 mysql.gtid_executed。</span></li>
</ul>
<p data-tool="mdnice编辑器"><span>二、根据存储引擎的能力标志判断。</span></p>
<p data-tool="mdnice编辑器"><span>这些标志是每个存储引擎单独设置的，一般是在<code><span>m_int_table_flags</span></code><span>或<code><span>table_flags</span></code><span>函数中定义的，主要是用来向 Server 层声明：这个存储引擎的表，支持哪些能力/约束。与复制相关的标志有三个：</span></span></span></p>
<ul class="list-paddingleft-1">
<li><span>HA_BINLOG_STMT_CAPABLE：支持 STATEMENT 格式 binlog。</span></li>
<li><span>HA_BINLOG_ROW_CAPABLE：支持 ROW 格式 binlog</span></li>
<li><span>HA_HAS_OWN_BINLOGGING：该引擎自己管理 binlog（如 NDB Cluster）。</span></li>
</ul>
<p data-tool="mdnice编辑器"><span>在 MySQL 支持的存储引擎中，只有 perfschema（对应 performance_schema）和 temptable（MySQL 8.0 引入的内部临时表存储引擎，主要用来替代老的 MEMORY/MyISAM 内部临时表）不会设置 HA_BINLOG_STMT_CAPABLE 或 HA_BINLOG_ROW_CAPABLE。</span></p>
<p data-tool="mdnice编辑器"><span>所以，针对 performance_schema 表的操作不会写入 binlog。</span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code># ls mysql-8.4.3/storage/<span><br><span>archive &nbsp;blackhole &nbsp;csv &nbsp;example &nbsp;federated &nbsp;heap &nbsp;innobase &nbsp;myisam &nbsp;myisammrg &nbsp;ndb &nbsp;perfschema &nbsp;secondary_engine_mock &nbsp;temptable<span><br></span></span></span></code></span></pre>
<h1 data-tool="mdnice编辑器"><span style="color: rgba(0, 128, 0, 1)">OPTION_BIN_LOG 为 false 的场景</span></h1>
<p data-tool="mdnice编辑器"><code><span>thd-&gt;variables</span></code><span>保存当前线程的会话级系统变量状态。其中，option_bits 是一个位图（bitmap），用于记录多个线程级选项标志，OPTION_BIN_LOG 则表示是否将当前线程的操作写入 binlog。</span></p>
<p data-tool="mdnice编辑器"><span>以下是几种典型场景。</span></p>
<p data-tool="mdnice编辑器"><span>一、显式关闭会话级 binlog</span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code>SET<span>&nbsp;SESSION<span>&nbsp;sql_log_bin =&nbsp;0<span>;<span><br></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>该参数对应的回调函数是<code><span>fix_sql_log_bin_after_update</span></code><span>。</span></span></p>
<p data-tool="mdnice编辑器"><span>当<code><span>sql_log_bin = 1</span></code><span>时，打开 OPTION_BIN_LOG，反之，则清除 OPTION_BIN_LOG。</span></span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>static<span>&nbsp;<span>bool<span>&nbsp;<span>fix_sql_log_bin_after_update<span>(sys_var *, THD *thd,<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;enum_var_type type [[maybe_unused]])<span>&nbsp;<span>{<span><br><span>&nbsp; assert(type == OPT_SESSION);<span><br><span><br><span>&nbsp;&nbsp;if<span>&nbsp;(thd-&gt;variables.sql_log_bin)<span><br><span>&nbsp; &nbsp; thd-&gt;variables.option_bits |= OPTION_BIN_LOG;<span><br><span>&nbsp;&nbsp;else<span><br><span>&nbsp; &nbsp; thd-&gt;variables.option_bits &amp;= ~OPTION_BIN_LOG;<span><br><span><br><span>&nbsp;&nbsp;return<span>&nbsp;false<span>;<span><br><span>}<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>二、从库未启用 log_replica_updates</span></p>
<p data-tool="mdnice编辑器"><span>当实例作为从库运行，且未开启 log_replica_updates 时，从库 SQL 线程重放的操作默认不写 binlog。</span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>void<span>&nbsp;<span>set_slave_thread_options<span>(THD *thd)<span>&nbsp;<span>{<span><br><span>&nbsp; ...<span><br><span>&nbsp; ulonglong options = thd-&gt;variables.option_bits | OPTION_BIG_SELECTS;<span><br><span>&nbsp;&nbsp;if<span>&nbsp;(opt_log_replica_updates)<span><br><span>&nbsp; &nbsp; options |= OPTION_BIN_LOG;<span><br><span>&nbsp;&nbsp;else<span><br><span>&nbsp; &nbsp; options &amp;= ~OPTION_BIN_LOG;<span><br><span>&nbsp; ...<span><br><span>}<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>三、使用<code><span>Disable_binlog_guard</span></code><span>临时关闭 binlog</span></span></p>
<p data-tool="mdnice编辑器"><code><span>Disable_binlog_guard</span></code><span>用于在特定代码块内临时关闭 binlog，并在离开作用域时自动恢复原状态。</span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>class<span>&nbsp;<span>Disable_binlog_guard<span>&nbsp;{<span><br><span>&nbsp;public<span>:<span><br><span>explicit<span>&nbsp;<span>Disable_binlog_guard<span>(THD *thd)<span><br><span>&nbsp; &nbsp; &nbsp; :&nbsp;<span>m_thd<span>(thd)<span>,<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<span>m_binlog_disabled<span>(thd-&gt;variables.option_bits &amp; OPTION_BIN_LOG)<span>&nbsp;<span>{<span><br><span>&nbsp; &nbsp; thd-&gt;variables.option_bits &amp;= ~OPTION_BIN_LOG;<span><br><span>&nbsp; }<span><br><span><br><span>&nbsp; ~Disable_binlog_guard() {<span><br><span>&nbsp; &nbsp;&nbsp;if<span>&nbsp;(m_binlog_disabled) m_thd-&gt;variables.option_bits |= OPTION_BIN_LOG;<span><br><span>&nbsp; }<span><br><span><br>private<span>:<span><br><span>&nbsp; THD *const<span>&nbsp;m_thd;<span><br>constbool<span>&nbsp;m_binlog_disabled;<span><br><span>};<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>Disable_binlog_guard 被调用的场景有：</span></p>
<p data-tool="mdnice编辑器"><span>3.1 实例初始化（<code><span>--initialize</span></code><span>）</span></span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>static<span>&nbsp;<span>bool<span>&nbsp;<span>handle_bootstrap_impl<span>(handle_bootstrap_args *args)<span>&nbsp;<span>{<span><br><span>&nbsp; ...<span><br>if<span>&nbsp;(opt_initialize) {<span><br><span>&nbsp; &nbsp; assert(thd-&gt;system_thread == SYSTEM_THREAD_SERVER_INITIALIZE);<span><br><span><br><span>&nbsp; &nbsp; sysd::notify("STATUS=Initialization of MySQL system tables in progress\n"<span>);<span><br><span>&nbsp; &nbsp;&nbsp;<span><br><span>&nbsp; &nbsp;&nbsp;<span>const<span>&nbsp;Disable_binlog_guard&nbsp;<span>disable_binlog<span>(thd)<span>;<span><br><span>&nbsp; &nbsp;&nbsp;<span>const<span>&nbsp;Disable_sql_log_bin_guard&nbsp;<span>disable_sql_log_bin<span>(thd)<span>;<span><br><span><br><span>&nbsp; &nbsp; Compiled_in_command_iterator comp_iter;<span><br><span>&nbsp; &nbsp; rc = process_iterator(thd, &amp;comp_iter,&nbsp;true<span>);<span><br><span><br><span>&nbsp; &nbsp; thd-&gt;system_thread = SYSTEM_THREAD_INIT_FILE;<span><br><span><br><span>&nbsp; &nbsp; sysd::notify("STATUS=Initialization of MySQL system tables "<span>,<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;rc ?&nbsp;"unsuccessful"<span>&nbsp;:&nbsp;"successful"<span>,&nbsp;"\n"<span>);<span><br><span><br><span>&nbsp; &nbsp;&nbsp;if<span>&nbsp;(rc !=&nbsp;0<span>) {<span><br><span>&nbsp; &nbsp; &nbsp;&nbsp;returntrue<span>;<span><br><span>&nbsp; &nbsp; }<span><br><span>&nbsp; }<span><br><span>&nbsp; ...<span><br>returnfalse<span>;<span><br><span>}<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>3.2 实例升级</span></p>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code><span>bool<span>&nbsp;<span>upgrade_system_schemas<span>(THD *thd)<span>&nbsp;<span>{<span><br><span>Disable_autocommit_guard&nbsp;<span>autocommit_guard<span>(thd)<span>;<span><br><span>&nbsp; Bootstrap_error_handler bootstrap_error_handler;<span><br><span><br><span>Server_option_guard&lt;<span>bool<span>&gt;&nbsp;<span>acl_guard<span>(&amp;opt_noacl,&nbsp;<span>true<span>)<span>;<span><br><span>Server_option_guard&lt;<span>bool<span>&gt;&nbsp;<span>general_log_guard<span>(&amp;opt_general_log,&nbsp;<span>false<span>)<span>;<span><br><span>Server_option_guard&lt;<span>bool<span>&gt;&nbsp;<span>slow_log_guard<span>(&amp;opt_slow_log,&nbsp;<span>false<span>)<span>;<span><br><span>Disable_binlog_guard&nbsp;<span>disable_binlog<span>(thd)<span>;<span><br><span>Disable_sql_log_bin_guard&nbsp;<span>disable_sql_log_bin<span>(thd)<span>;<span><br><span>&nbsp; ...<span><br><span>&nbsp; bootstrap_error_handler.set_log_error(false<span>);<span><br>bool<span>&nbsp;err =<span><br><span>&nbsp; &nbsp; &nbsp; fix_mysql_tables(thd) || fix_sys_schema(thd) || upgrade_help_tables(thd);<span><br>if<span>&nbsp;(!err) {<span><br><span>&nbsp; &nbsp;&nbsp;<span>/*<span><br><span>&nbsp; &nbsp; &nbsp; Initialize structures necessary for federated server from mysql.servers<span><br><span>&nbsp; &nbsp; &nbsp; table.<span><br><span>&nbsp; &nbsp; */<span><br><span>&nbsp; &nbsp; servers_init(thd);<span><br><span>&nbsp; &nbsp; err = (DBUG_EVALUATE_IF("force_fix_user_schemas"<span>,&nbsp;true<span>,<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dd::bootstrap::DD_bootstrap_ctx::instance()<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .is_server_upgrade_from_before(<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; bootstrap::SERVER_VERSION_80011))<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;? check.check_all_schemas(thd)<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;: check.check_system_schemas(thd)) ||<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; check.repair_tables(thd) ||<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; dd::tables::DD_properties::instance().set<span>(<span><br><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; thd,&nbsp;"MYSQLD_VERSION_UPGRADED"<span>, MYSQL_VERSION_ID);<span><br><span>&nbsp; }<span><br><span>&nbsp; ...<span><br>return<span>&nbsp;dd::end_transaction(thd, err);<span><br><span>}<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>3.3 CREATE SERVER, ALTER SERVER 和 DROP SERVER 操作。</span></p>
<p data-tool="mdnice编辑器"><span>3.4 INSTALL COMPONENT, UNINSTALL COMPONENT 操作。</span></p>
<p data-tool="mdnice编辑器"><span>3.5 INSTALL PLUGIN, UNINSTALL PLUGIN 操作。</span></p>
<p data-tool="mdnice编辑器"><span>3.6 一些内部操作，例如 ALTER TABLE 过程中创建/删除临时表、DROP DATABASE 时清理数据库对象、更新数据字典表、后台线程自动更新列直方图。</span></p>
<p data-tool="mdnice编辑器"><span>除了上面介绍的这些场景，通过将&nbsp;<code><span>thd-&gt;lex-&gt;no_write_to_binlog</span></code><span>&nbsp;设置为<code><span>true</span></code><span>（<code><span>thd-&gt;lex</span></code><span>表示当前 SQL 语句的语法解析上下文），可以在语句级别控制该语句不写入 binlog。</span></span></span></span></p>
<h1 data-tool="mdnice编辑器"><span style="color: rgba(0, 128, 0, 1)">NO_WRITE_TO_BINLOG 为 true 的场景</span></h1>
<p data-tool="mdnice编辑器"><span>以下场景会将 no_write_to_binlog 设置为 true。</span></p>
<ol class="list-paddingleft-1">
<li><span><span>SHUTDOWN、RESTART 命令。</span></span></li>
<li><span>RESET 系列命令，包括：RESET MASTER, RESET SLAVE, RESET PERSIST。</span></li>
<li><span>显式指定<code><span>NO_WRITE_TO_BINLOG</span></code><span>或<code><span>LOCAL</span></code><span>。部分维护类 SQL 命令（OPTIMIZE, ANALYZE, REPAIR, FLUSH）支持在语句中显式指定不写 binlog，如，</span></span></span></li>
</ol>
<pre data-tool="mdnice编辑器"><span data-cacheurl="" data-remoteid="" data-lazy-bgimg="https://mmbiz.qpic.cn/mmbiz_svg/BO1qQiajiacVlibSwibjXiaMnylbHVGpQcib8UWIFerYZgIR9sxBIBCgO6hYKkExWrhsjMPIQa1yIOMmrRGFx1dEib6vlCuJygOxAZF/640?wx_fmt=svg&amp;from=appmsg" data-fail="0"><code>OPTIMIZE<span>&nbsp;NO_WRITE_TO_BINLOG<span>&nbsp;TABLE<span>&nbsp;t1;<span><br>ANALYZE<span>&nbsp;LOCAL<span>&nbsp;TABLE<span>&nbsp;t1;<span><br>REPAIR<span>&nbsp;NO_WRITE_TO_BINLOG<span>&nbsp;TABLE<span>&nbsp;t1;<span><br>FLUSH<span>&nbsp;LOCAL<span>&nbsp;PRIVILEGES<span>;<span><br></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></span></pre>
<p data-tool="mdnice编辑器"><span>需要注意的是，对于<code><span>FLUSH</span></code><span>命令，即使未显式指定<code><span>NO_WRITE_TO_BINLOG</span></code><span>，以下命令默认也不会写入 binlog：NO_WRITE_TO_BINLOG，FLUSH LOGS、FLUSH BINARY LOGS、FLUSH TABLES WITH READ LOCK、FLUSH TABLES tbl_name ... FOR EXPORT。</span></span></span></p>
<h1 data-tool="mdnice编辑器"><span style="color: rgba(0, 128, 0, 1)">总结</span></h1>
<p data-tool="mdnice编辑器"><span>虽然上面列举的场景较多，但实际上并不需要大家刻意去记。</span></p>
<p data-tool="mdnice编辑器"><span>简单来说，</span></p>
<ul class="list-paddingleft-1">
<li>
<p><span>凡是 MySQL 内部自动执行的操作（即非用户手动执行的操作），通常不会写入 binlog。 典型场景包括：实例初始化与升级、<code><span>mysql.slow_log</span></code><span>表的写入、数据字典的维护、<code><span>performance_schema</span></code><span>表数据的更新等。</span></span></span></p>
</li>
<li>
<p><span>对 mysql 库下的表进行 <span>DML 操作，只要不属于上面提到的特殊类别的表，基本都会写入 binlog。</span></span></p>
<p><span>但若执行的是 DDL 操作（如 truncate），基本都会写入 binlog。</span></p>
</li>
<li>
<p><span>对 performance_schema 中的表进行 DML、DDL 操作会提示权限不足，即便是用 root 用户执行。但部分表允许执行 truncate 操作，且 truncate 操作不会写入 binlog。</span></p>
</li>
</ul>
</div>
<div class="clear"></div>

		</div>
		<div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-12-22 15:48">2025-12-22 15:47</span>&nbsp;
<a href="https://www.cnblogs.com/ivictor">iVictor</a>&nbsp;
阅读(<span id="post_view_count">24</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19382809);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19382809', targetLink: 'https://www.cnblogs.com/ivictor/p/19382809', title: 'MySQL 在哪些场景下不会写 binlog' })">举报</a>
</div>
	 ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ 具身智能：零基础入门睿尔曼机械臂（六）——手眼标定代码库详解，从原理到实践 ]]></title>
    <link>https://www.cnblogs.com/ChenAI-TGF/p/19382703</link>
    <guid>07f193168dc99b013739523a18c701fb</guid>
    <description>
    <![CDATA[ 
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/ChenAI-TGF/p/19382703" title="发布于 2025-12-22 15:34">
    <span role="heading" aria-level="2">具身智能：零基础入门睿尔曼机械臂（六）——手眼标定代码库详解，从原理到实践</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="前言">前言</h1>
<p>手眼标定是机器人视觉领域的关键技术，它解决了机械臂与相机之间的坐标转换问题，为精准抓取、视觉伺服等应用奠定基础。上一篇博客中我们讲解了手眼标定的概念以及原理，本文将详细解析睿尔曼官方提供的完整的手眼标定代码库，包括其结构设计、核心功能及使用方法，帮助读者快速掌握手眼标定的实现流程。</p>
<p>@</p><div class="toc"><div class="toc-container-header">目录</div><ul><li><a href="#前言" rel="noopener nofollow">前言</a></li><li><a href="#项目概述" rel="noopener nofollow">项目概述</a></li><li><a href="#项目结构" rel="noopener nofollow">项目结构</a></li><li><a href="#核心原理" rel="noopener nofollow">核心原理</a><ul><li><a href="#1-眼在手上模式eye-in-hand" rel="noopener nofollow">1. 眼在手上模式（Eye-in-Hand）</a></li><li><a href="#2-眼在手外模式eye-to-hand" rel="noopener nofollow">2. 眼在手外模式（Eye-to-Hand）</a></li></ul></li><li><a href="#核心代码解析" rel="noopener nofollow">核心代码解析</a><ul><li><a href="#一核心计算模块深度解析" rel="noopener nofollow">一、核心计算模块深度解析</a><ul><li><a href="#1-眼在手上标定compute_in_handpy" rel="noopener nofollow">1. 眼在手上标定（compute_in_hand.py）</a><ul><li><a href="#11-初始化与配置加载" rel="noopener nofollow">1.1 初始化与配置加载</a></li><li><a href="#12-标定板角点检测与相机外参计算" rel="noopener nofollow">1.2 标定板角点检测与相机外参计算</a></li><li><a href="#13-相机标定与位姿转换" rel="noopener nofollow">1.3 相机标定与位姿转换</a></li><li><a href="#14-手眼标定核心计算" rel="noopener nofollow">1.4 手眼标定核心计算</a></li></ul></li><li><a href="#2-眼在手外标定compute_to_handpy" rel="noopener nofollow">2. 眼在手外标定（compute_to_hand.py）</a><ul><li><a href="#21-核心差异点" rel="noopener nofollow">2.1 核心差异点</a></li><li><a href="#22-标定目标差异" rel="noopener nofollow">2.2 标定目标差异</a></li></ul></li></ul></li><li><a href="#二辅助模块解析" rel="noopener nofollow">二、辅助模块解析</a><ul><li><a href="#1-数据采集模块collect_datapy" rel="noopener nofollow">1. 数据采集模块（collect_data.py）</a></li><li><a href="#2-配置文件configyaml" rel="noopener nofollow">2. 配置文件（config.yaml）</a></li><li><a href="#3-日志模块libslog_settingpy" rel="noopener nofollow">3. 日志模块（libs/log_setting.py）</a></li><li><a href="#4-位姿处理工具save_posespy--save_poses2py" rel="noopener nofollow">4. 位姿处理工具（save_poses.py / save_poses2.py）</a></li></ul></li><li><a href="#三代码串联与整体流程" rel="noopener nofollow">三、代码串联与整体流程</a><ul><li><a href="#1-数据流向图" rel="noopener nofollow">1. 数据流向图</a></li></ul></li></ul></li><li><a href="#总结" rel="noopener nofollow">总结</a></li></ul></div><br>
<img alt="在这里插入图片描述" loading="lazy" data-src="https://img2024.cnblogs.com/blog/3724025/202512/3724025-20251222153302574-1876873088.png" class="lazyload"><p></p>
<h1 id="项目概述">项目概述</h1>
<p>该代码库专注于实现两种常见的手眼标定模式：</p>
<ul>
<li><strong>眼在手上（Eye-in-Hand）</strong>：相机固定在机械臂末端，随机械臂一起运动</li>
<li><strong>眼在手外（Eye-to-Hand）</strong>：相机固定在机械臂外部，位置保持不变</li>
</ul>
<p>通过采集机械臂位姿和标定板图像，计算出相机与机械臂之间的坐标变换矩阵，最终实现目标从相机坐标系到机械臂基坐标系的转换。</p>
<h1 id="项目结构">项目结构</h1>
<pre><code>hand_eye_calibration/
├── README.md               # 项目说明文档
├── collect_data.py         # 数据采集程序
├── compute_in_hand.py      # 眼在手上标定计算
├── compute_to_hand.py      # 眼在手外标定计算
├── config.yaml             # 配置文件
├── requirements.txt        # 依赖包列表
├── save_poses.py           # 位姿数据处理工具
├── save_poses2.py          # 位姿数据处理工具
├── picture/                # 图片资源
└── libs/                   # 辅助工具库
    ├── auxiliary.py        # 辅助函数
    └── log_setting.py      # 日志配置
</code></pre>
<h1 id="核心原理">核心原理</h1>
<p>手眼标定的本质是求解坐标变换矩阵，两种模式分别对应不同的数学模型，上一章已经详细讲过其中的数学原理，这边我们简单复习一下：</p>
<h2 id="1-眼在手上模式eye-in-hand">1. 眼在手上模式（Eye-in-Hand）</h2>
<p>需要求解相机相对于机械臂末端的变换矩阵 $H^{EE}_{CAM}$，核心方程为：<br>
$$A_2^{-1} \cdot A_1 \cdot X = X \cdot B_2 \cdot B_1^{-1}$$<br>
其中：</p>
<ul>
<li>$A$ 表示机械臂末端在基坐标系下的位姿变换</li>
<li>$B$ 表示标定板在相机坐标系下的位姿变换</li>
<li>$X$ 即为待求的相机与末端的变换矩阵</li>
</ul>
<h2 id="2-眼在手外模式eye-to-hand">2. 眼在手外模式（Eye-to-Hand）</h2>
<p>需要求解相机相对于机械臂基坐标系的变换矩阵 $H^{ROB}_{CAM}$，核心方程同样遵循 $AX=XB$ 形式，只是矩阵定义不同。</p>
<h1 id="核心代码解析">核心代码解析</h1>
<h2 id="一核心计算模块深度解析">一、核心计算模块深度解析</h2>
<h3 id="1-眼在手上标定compute_in_handpy">1. 眼在手上标定（compute_in_hand.py）</h3>
<p>该模块是眼在手上模式的核心计算单元，负责将采集到的图像和位姿数据转换为相机与机械臂末端的坐标变换关系。</p>
<h4 id="11-初始化与配置加载">1.1 初始化与配置加载</h4>
<pre><code class="language-python">current_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),"eye_hand_data")
images_path = os.path.join("eye_hand_data",find_latest_data_folder(current_path))
file_path = os.path.join(images_path,"poses.txt")

with open("config.yaml", 'r', encoding='utf-8') as file:
    data = yaml.safe_load(file)
XX = data.get("checkerboard_args").get("XX")  # 标定板长度方向角点数
YY = data.get("checkerboard_args").get("YY")  # 标定板宽度方向角点数
L = data.get("checkerboard_args").get("L")    # 角点间距（米）
</code></pre>
<ul>
<li><strong>路径处理</strong>：通过<code>find_latest_data_folder</code>自动定位最新采集的数据文件夹，确保使用最新标定数据</li>
<li><strong>配置加载</strong>：从<code>config.yaml</code>读取标定板参数，支持不同规格标定板的灵活适配</li>
</ul>
<h4 id="12-标定板角点检测与相机外参计算">1.2 标定板角点检测与相机外参计算</h4>
<pre><code class="language-python"># 标定板3D坐标定义（世界坐标系）
objp = np.zeros((XX * YY, 3), np.float32)
objp[:, :2] = np.mgrid[0:XX, 0:YY].T.reshape(-1, 2)
objp = L * objp  # 转换为实际物理尺寸

# 角点检测
for i in range(1, len(images_num) + 1):
    img = cv2.imread(image_file)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, corners = cv2.findChessboardCorners(gray, (XX, YY), None)
    if ret:
        obj_points.append(objp)
        # 亚像素优化：提高角点检测精度到亚像素级别
        corners2 = cv2.cornerSubPix(gray, corners, (5, 5), (-1, -1), criteria)
        img_points.append(corners2)
</code></pre>
<ul>
<li><strong>世界坐标系定义</strong>：将标定板视为世界坐标系，Z轴为0，X/Y轴由角点网格定义</li>
<li><strong>亚像素优化</strong>：通过<code>cv2.cornerSubPix</code>将角点坐标从像素级优化到亚像素级（精度达0.1像素以下），为后续标定提供更高精度的输入</li>
</ul>
<h4 id="13-相机标定与位姿转换">1.3 相机标定与位姿转换</h4>
<pre><code class="language-python"># 相机标定：获取标定板在相机坐标系下的位姿
ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(
    obj_points, img_points, size, None, None)

# 机械臂位姿处理：将poses.txt转换为齐次变换矩阵
poses_main(file_path)
tool_pose = np.loadtxt("RobotToolPose.csv", delimiter=',')

# 提取旋转矩阵和平移向量
R_tool = []
t_tool = []
for i in range(int(N)):
    R_tool.append(tool_pose[0:3,4*i:4*i+3])  # 旋转矩阵部分
    t_tool.append(tool_pose[0:3,4*i+3])      # 平移向量部分
</code></pre>
<ul>
<li><strong>相机外参计算</strong>：<code>cv2.calibrateCamera</code>返回的<code>rvecs</code>和<code>tvecs</code>分别是标定板在相机坐标系下的旋转向量和平移向量</li>
<li><strong>机械臂位姿转换</strong>：<code>poses_main</code>函数将机械臂末端的位姿（通常是X,Y,Z,_rx,ry,rz）转换为齐次变换矩阵，存储于<code>RobotToolPose.csv</code></li>
</ul>
<h4 id="14-手眼标定核心计算">1.4 手眼标定核心计算</h4>
<pre><code class="language-python"># 使用Tsai算法求解手眼变换
R, t = cv2.calibrateHandEye(R_tool, t_tool, rvecs, tvecs, cv2.CALIB_HAND_EYE_TSAI)
</code></pre>
<ul>
<li><strong>参数解析</strong>：
<ul>
<li><code>R_tool</code>/<code>t_tool</code>：机械臂末端在基坐标系下的旋转矩阵/平移向量序列</li>
<li><code>rvecs</code>/<code>tvecs</code>：标定板在相机坐标系下的旋转向量/平移向量序列</li>
<li><code>cv2.CALIB_HAND_EYE_TSAI</code>：采用Tsai-Lenz算法（计算速度快，精度高）</li>
</ul>
</li>
<li><strong>输出</strong>：<code>R</code>为相机相对于机械臂末端的旋转矩阵，<code>t</code>为对应的平移向量</li>
</ul>
<h3 id="2-眼在手外标定compute_to_handpy">2. 眼在手外标定（compute_to_hand.py）</h3>
<p>该模块与<code>compute_in_hand.py</code>结构高度相似，但存在关键差异：</p>
<h4 id="21-核心差异点">2.1 核心差异点</h4>
<pre><code class="language-python"># 机械臂位姿处理函数不同
from save_poses2 import poses2_main
...
poses2_main(file_path)  # 替代compute_in_hand.py中的poses_main
</code></pre>
<ul>
<li><strong>位姿转换逻辑</strong>：<code>poses2_main</code>与<code>poses_main</code>的区别在于机械臂位姿的变换方向不同：
<ul>
<li>眼在手上：需要机械臂末端相对于基坐标系的变换（$A$矩阵）</li>
<li>眼在手外：需要基坐标系相对于机械臂末端的变换（$A^{-1}$矩阵）</li>
</ul>
</li>
</ul>
<h4 id="22-标定目标差异">2.2 标定目标差异</h4>
<pre><code class="language-python"># 眼在手外模式下，calibrateHandEye输出的是相机相对于基坐标系的变换
R, t = cv2.calibrateHandEye(R_tool, t_tool, rvecs, tvecs, cv2.CALIB_HAND_EYE_TSAI)
</code></pre>
<ul>
<li>眼在手上：$X = H^{EE}_{CAM}$（相机→末端）</li>
<li>眼在手外：$X = H^{ROB}_{CAM}$（相机→基坐标系）</li>
</ul>
<h2 id="二辅助模块解析">二、辅助模块解析</h2>
<h3 id="1-数据采集模块collect_datapy">1. 数据采集模块（collect_data.py）</h3>
<p>该模块实现图像与位姿的同步采集，确保每一组数据的时间一致性：</p>
<pre><code class="language-python"># 机械臂位姿获取
socket_command = '{"command": "get_current_arm_state"}'
state, pose = send_cmd(client, socket_command)

# 数据保存
with open(filename, 'a+') as f:
    pose_ = [str(i) for i in pose]
    new_line = f'{",".join(pose_)}\n'
    f.write(new_line)
cv2.imwrite(image_path, cv_img)
</code></pre>
<ul>
<li><strong>同步机制</strong>：通过按键触发（'s'键），确保同一时刻采集的图像和位姿被关联存储</li>
<li><strong>数据格式</strong>：
<ul>
<li>位姿：<code>poses.txt</code>中每行存储一组机械臂位姿（X,Y,Z,rx,ry,rz）</li>
<li>图像：按序号命名（1.jpg, 2.jpg...），与位姿文件行号一一对应</li>
</ul>
</li>
</ul>
<h3 id="2-配置文件configyaml">2. 配置文件（config.yaml）</h3>
<pre><code class="language-yaml">checkerboard_args:
  XX: 9        # 长度方向角点数
  YY: 6        # 宽度方向角点数
  L: 0.02      # 角点间距（米）
</code></pre>
<ul>
<li><strong>标定板参数</strong>：需根据实际使用的标定板修改，直接影响世界坐标系到图像坐标系的转换精度</li>
<li><strong>扩展性</strong>：可通过添加其他参数（如相机内参路径、日志级别等）增强配置灵活性</li>
</ul>
<h3 id="3-日志模块libslog_settingpy">3. 日志模块（libs/log_setting.py）</h3>
<pre><code class="language-python">class CommonLog(object):
    def __console(self, level, message):
        # 文件日志（按天轮转）
        fh = TimedRotatingFileHandler(
            self.logname, when='MIDNIGHT', interval=1, encoding='utf-8')
        # 控制台日志
        ch = logging.StreamHandler()
        
        self.logger.addHandler(fh)
        self.logger.addHandler(ch)
        # 记录日志后移除处理器，避免重复输出
        self.logger.removeHandler(ch)
        self.logger.removeHandler(fh)
</code></pre>
<ul>
<li><strong>双重输出</strong>：同时向控制台和文件输出日志，方便调试和记录</li>
<li><strong>日志轮转</strong>：按天分割日志文件，避免单文件过大，便于追溯历史标定过程</li>
</ul>
<h3 id="4-位姿处理工具save_posespy--save_poses2py">4. 位姿处理工具（save_poses.py / save_poses2.py）</h3>
<p>虽然未提供完整代码，但根据上下文可推断其功能：</p>
<ul>
<li><strong>功能</strong>：将机械臂位姿（欧拉角）转换为齐次变换矩阵</li>
<li><strong>核心公式</strong>：<pre><code class="language-python"># 欧拉角转旋转矩阵（以ZYX顺序为例）
R = Rz(rz) * Ry(ry) * Rx(rx)
# 构造齐次矩阵
H = [[R[0][0], R[0][1], R[0][2], x],
     [R[1][0], R[1][1], R[1][2], y],
     [R[2][0], R[2][1], R[2][2], z],
     [0,       0,       0,       1]]
</code></pre>
</li>
<li><strong>差异点</strong>：<code>save_poses2.py</code>会计算变换矩阵的逆，以适应眼在手外的数学模型</li>
</ul>
<h2 id="三代码串联与整体流程">三、代码串联与整体流程</h2>
<h3 id="1-数据流向图">1. 数据流向图</h3>
<pre><code>采集阶段：
机械臂 → [get_current_arm_state] → 位姿数据 → 写入poses.txt
   ↑
   同步触发（'s'键）
   ↓
相机 → [RealSense SDK] → 图像帧 → 保存为.jpg文件

计算阶段：
poses.txt → [save_poses.py/2] → 机械臂变换矩阵(R_tool, t_tool)
   ↓
.jpg文件 → [角点检测] → 图像点集 → [calibrateCamera] → 标定板位姿(rvecs, tvecs)
   ↓
[calibrateHandEye] → 手眼变换矩阵(R, t) → 输出旋转矩阵/平移向量/四元数
</code></pre>
<h1 id="总结">总结</h1>
<p>本代码库提供了一套完整的手眼标定解决方案，通过模块化设计实现了数据采集、标定计算等核心功能。无论是眼在手上还是眼在手外模式，都能通过简单的操作流程完成标定。实际应用中，需注意机械臂运动姿态的多样性和标定板的稳定性，以获得更高的标定精度。</p>
<p>通过本文的解析，相信读者已经对该代码库有了全面的了解，可以根据实际需求进行二次开发或直接应用于机器人视觉项目中。</p>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-12-22 15:34">2025-12-22 15:34</span>&nbsp;
<a href="https://www.cnblogs.com/ChenAI-TGF">TTGF</a>&nbsp;
阅读(<span id="post_view_count">49</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19382703);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19382703', targetLink: 'https://www.cnblogs.com/ChenAI-TGF/p/19382703', title: '具身智能：零基础入门睿尔曼机械臂（六）——手眼标定代码库详解，从原理到实践' })">举报</a>
</div>
         ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ 在Odoo18中实现多选下拉框搜索功能 ]]></title>
    <link>https://www.cnblogs.com/lifuquan/p/19382615</link>
    <guid>a998f258bcb168de64661dc3aadcb055</guid>
    <description>
    <![CDATA[ 
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/lifuquan/p/19382615" title="发布于 2025-12-22 15:22">
    <span role="heading" aria-level="2">在Odoo18中实现多选下拉框搜索功能</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="背景需求">背景需求</h2>
<p>最近在开发一个Odoo项目时，客户提出了一个特定的搜索需求：希望在列表页面中展示多个多选下拉框作为过滤条件。用户选中任意下拉选项时，列表需要实时查询并显示对应的结果。</p>
<p>这种设计相较于Odoo原生搜索更为直观，特别是当用户需要同时基于多个维度筛选数据时，操作更加便捷。<br>
<img src="https://img2024.cnblogs.com/blog/597896/202512/597896-20251222144812190-2038429530.png" alt="image" loading="lazy"></p>
<h2 id="odoo原生搜索的局限性">Odoo原生搜索的局限性</h2>
<p>Odoo作为一款国际化的开源ERP系统，其搜索功能设计理念与国内用户的使用习惯存在一定差异：</p>
<ul>
<li>搜索模式单一：默认采用"搜索框+预设过滤器"的模式</li>
<li>多条件过滤不够直观：需要点击过滤器图标，在弹出窗口中配置多个条件</li>
<li>用户体验差异：国外用户习惯文本搜索+条件组合，国内用户更习惯可视化的多选过滤</li>
</ul>
<p><img src="https://img2024.cnblogs.com/blog/597896/202512/597896-20251222145301116-1805738145.png" alt="image" loading="lazy"></p>
<h2 id="解决方案自定义控件开发">解决方案：自定义控件开发</h2>
<p>面对这种需求差异，我们决定采用Odoo的自定义开发能力。Odoo提供了灵活的扩展机制，特别是基于QWeb模板引擎，我们可以通过以下方式实现自定义搜索控件：</p>
<ol>
<li>自定义多选下拉框组件</li>
<li>集成到搜索面板</li>
<li>重写列表视图控制器</li>
<li>动态构建搜索条件</li>
</ol>
<h2 id="完整方案实现">完整方案实现</h2>
<h5 id="1-多选下拉框组件-xml模板">1. 多选下拉框组件 (XML模板)</h5>
<p>首先需要在XML文件中定义自定义下拉框控件视图(multi_select_widget.xml)：</p>
<pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;templates xml:space="preserve"&gt;
    &lt;t t-name="multi_select" owl="1"&gt;
        &lt;div class="multiselect-container" t-ref="multi_select_dropdown"&gt;
            &lt;div class="form-control" t-on-click="toggleDropdown"&gt;
                &lt;span t-if="state.selected.size === 0"&gt;
                    &lt;t t-esc="props.placeholder || 'Select options'"/&gt;
                &lt;/span&gt;
                &lt;div t-if="state.selected.size === 1" class="selected-options" &gt;
                    &lt;span class="badge bg-primary me-1" 
                              t-esc="[...state.selected][0]"/&gt;
                &lt;/div&gt;
                &lt;div t-if="state.selected.size &gt; 1" class="selected-options" &gt;
                    &lt;span class="badge bg-primary me-1"&gt;已选择&lt;t t-esc="state.selected.size"&gt;&lt;/t&gt;个&lt;t t-esc="props.fieldName"/&gt;&lt;/span&gt;
                &lt;/div&gt;
            &lt;/div&gt;
            
            &lt;div t-if="state.isOpen" class="dropdown-menu show"&gt;
                &lt;t t-foreach="props.options" t-as="option" t-key="option"&gt;
                    &lt;a href="#" class="dropdown-item" 
                       t-att-class="{'active': state.selected.has(option)}"
                       t-on-click="(ev) =&gt; this.selectOption(option, ev)"&gt;
                        &lt;t t-esc="option"/&gt;
                    &lt;/a&gt;
                &lt;/t&gt;
            &lt;/div&gt;
            &lt;style&gt;
                .multiselect-container{
                    margin: 3px;
                    width: 200px;
                }
            &lt;/style&gt;
        &lt;/div&gt;
    &lt;/t&gt;
&lt;/templates&gt;
</code></pre>
<h3 id="2-多选下拉框组件逻辑-javascript">2. 多选下拉框组件逻辑 (JavaScript)</h3>
<p>业务逻辑我们用js来实现(multi_select_widget.js)</p>
<pre><code>import { Component, useState, useRef, onMounted, onWillUnmount } from "@odoo/owl";

export class MultiSelectField extends Component {
    static template = "multi_select";
    static props = {
        options: Array,
        placeholder: { type: String, optional: true },
        fieldName: String,
        onChange: Function,
    };

    setup() {
        this.dropdownRef = useRef("multi_select_dropdown");
        this.state = useState({
            isOpen: false,
            selected: new Set(),
        });

        this.clickOutsideHandler = null;
        this.keydownHandler = null;

        onMounted(() =&gt; {
            this.setupEventListeners();
        });

        onWillUnmount(() =&gt; {
            this.cleanupEventListeners();
        });
    }

    toggleDropdown() {
        this.state.isOpen = !this.state.isOpen;
    }

    selectOption = (option, ev) =&gt; {
        if (this.state.selected.has(option)) {
            this.state.selected.delete(option);
        } else {
            this.state.selected.add(option);
        }
        this.props.onChange(this.props.fieldName, [...this.state.selected]);
    }

    setupEventListeners() {
        this.clickOutsideHandler = (event) =&gt; {
            if (!this.dropdownRef || !this.dropdownRef.el) return;

            if (!this.dropdownRef.el.contains(event.target)) {
                this.state.isOpen = false;
            }
        }

        this.keydownHandler = (event) =&gt; {
            if (event.key === 'Escape' &amp;&amp; this.state.isOpen) {
                event.preventDefault();
                event.stopPropagation();
                event.stopImmediatePropagation();
                this.state.isOpen = false;
            }
        }

        document.addEventListener('mousedown', this.clickOutsideHandler, true);
        document.addEventListener('touchstart', this.clickOutsideHandler, true);
        document.addEventListener('keydown', this.keydownHandler, true);
    }

    cleanupEventListeners() {
        if (this.clickOutsideHandler) {
            document.removeEventListener('mousedown', this.clickOutsideHandler, true);
            document.removeEventListener('touchstart', this.clickOutsideHandler, true);
        }

        if (this.keydownHandler) {
            document.removeEventListener('keydown', this.keydownHandler, true);
        }

        this.clickOutsideHandler = null;
        this.keydownHandler = null;
    }
}
</code></pre>
<h3 id="3自定义搜索面板-xml模板">3.自定义搜索面板 (XML模板)</h3>
<p>同样定义一个xml(search_widget.xml)</p>
<pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;templates xml:space="preserve"&gt;
    &lt;t t-name="custom_search_panel" owl="1"&gt;
        &lt;div class="custom-search-panel" t-att-data-loading="state.loading"&gt;
            &lt;!-- 加载状态 --&gt;
            &lt;t t-if="state.loading"&gt;
                &lt;div class="loading-state text-center p-3"&gt;
                    &lt;i class="fa fa-spinner fa-spin me-2"&gt;&lt;/i&gt;
                    &lt;span&gt;正在加载数据...&lt;/span&gt;
                &lt;/div&gt;
            &lt;/t&gt;

            &lt;!-- 错误状态 --&gt;
            &lt;t t-if="state.error"&gt;
                &lt;div class="error-state alert alert-warning m-3"&gt;
                    &lt;i class="fa fa-exclamation-triangle me-2"&gt;&lt;/i&gt;
                    &lt;span t-esc="state.error"&gt;&lt;/span&gt;
                &lt;/div&gt;
            &lt;/t&gt;

            &lt;!-- 正常状态 --&gt;
            &lt;t t-if="!state.loading and !state.error"&gt;
                &lt;div class="search-filters-container"&gt;
                    &lt;!-- 多选下拉框组件 --&gt;
                    &lt;MultiSelectField 
                        fieldName="field_a" 
                        options="state.dropdownData.field_a" 
                        placeholder="'字段A筛选'"
                        onChange="(field, values) =&gt; handleSelection(field, values)"
                    /&gt;
                    
                    &lt;MultiSelectField 
                        fieldName="field_b" 
                        options="state.dropdownData.field_b" 
                        placeholder="'字段B筛选'"
                        onChange="(field, values) =&gt; handleSelection(field, values)"
                    /&gt;
                    
                    &lt;MultiSelectField 
                        fieldName="field_c" 
                        options="state.dropdownData.field_c" 
                        placeholder="'字段C筛选'"
                        onChange="(field, values) =&gt; handleSelection(field, values)"
                    /&gt;
                &lt;/div&gt;
            &lt;/t&gt;
            
            &lt;style&gt;
                .custom-search-panel {
                    padding: 16px;
                    background: #f8f9fa;
                    border-bottom: 1px solid #dee2e6;
                }
                
                .search-filters-container {
                    display: flex;
                    flex-wrap: wrap;
                    align-items: center;
                    gap: 12px;
                }
                
                .loading-state {
                    color: #6c757d;
                }
                
                .error-state {
                    max-width: 600px;
                    margin: 0 auto;
                }
            &lt;/style&gt;
        &lt;/div&gt;
    &lt;/t&gt;
&lt;/templates&gt;
</code></pre>
<h3 id="4搜索面板业务逻辑-javascript">4.搜索面板业务逻辑 (JavaScript)</h3>
<p>search_widget.js</p>
<pre><code>import { Component, useState, onWillStart } from "@odoo/owl";
import { registry } from "@web/core/registry";
import { useService } from "@web/core/utils/hooks";
import { MultiSelectField } from "./multi_select_widget";

export class CustomSearchPanel extends Component {
    static template = "custom_search_panel";
    static components = { MultiSelectField };

    setup() {
        // 获取服务
        this.ormService = useService("orm");
        
        // 初始化响应式状态
        this.state = useState({
            dropdownData: {
                field_a: [],
                field_b: [],
                field_c: [],
            },
            selectedValues: {
                field_a: [],
                field_b: [],
                field_c: [],
            },
            loading: false,
            error: null,
        });

        // 组件挂载前加载数据
        onWillStart(async () =&gt; {
            await this.loadDropdownData();
        });
    }

    // 加载下拉框数据
    loadDropdownData = async () =&gt; {
        this.state.loading = true;
        this.state.error = null;
        
        try {
            // 调用后端方法获取下拉框数据
            const dropdownData = await this.ormService.call(
                "your.model.name",  // 替换为实际模型名
                "get_filter_dropdown_data",  // 后端方法名
                [],
                {}
            );
            
            this.state.dropdownData = dropdownData;
        } catch (error) {
            console.error("加载下拉框数据失败:", error);
            this.state.error = "加载筛选数据失败，请稍后重试";
        } finally {
            this.state.loading = false;
        }
    }

    // 处理选择变化
    handleSelection = async (fieldName, selectedValues) =&gt; {
        // 更新选中值
        this.state.selectedValues[fieldName] = selectedValues;
        
        // 生成搜索条件
        const domain = this.generateSearchDomain();
        
        // 触发搜索更新
        this.triggerSearchUpdate(domain);
    }

    // 生成搜索条件
    generateSearchDomain() {
        const domain = [];
        
        Object.entries(this.state.selectedValues).forEach(([field, values]) =&gt; {
            if (values &amp;&amp; values.length &gt; 0) {
                // 使用 'in' 操作符支持多选
                domain.push([field, 'in', values]);
            }
        });
        
        return domain;
    }

    // 触发搜索更新
    triggerSearchUpdate(domain) {
        // 更新搜索模型
        this.env.searchModel.updateDomain(domain);
        
        // 发送自定义事件通知列表刷新
        this.env.bus.trigger('custom_search:updated', { 
            domain,
            timestamp: Date.now()
        });
    }
}

// 注册组件
registry.category("view_components").add("custom_search_panel", CustomSearchPanel);
</code></pre>
<h3 id="5自定义列表控制器-javascript">5.自定义列表控制器 (JavaScript)</h3>
<pre><code>import { registry } from "@web/core/registry";
import { listView } from "@web/views/list/list_view";
import { ListController } from "@web/views/list/list_controller";
import { CustomSearchPanel } from "./search_widget";
import { useBus } from "@web/core/utils/hooks";

// 扩展原生列表控制器
export class CustomListController extends ListController {
    static components = {
        ...ListController.components,
        SearchPanel: CustomSearchPanel,  // 替换搜索组件
    };
    
    static template = "web.ListView";

    setup() {
        super.setup();
        
        // 监听自定义搜索事件
        useBus(this.env.bus, "custom_search:updated", (ev) =&gt; {
            this.handleCustomSearch(ev.detail.domain);
        });
    }

    // 处理自定义搜索
    async handleCustomSearch(domain) {
        try {
            // 显示加载状态
            this.model.isLoading = true;
            this.render();
            
            // 加载数据
            await this.model.load({ domain });
            
            // 更新分页信息
            if (this.model.data) {
                this.model.pager.limit = this.model.data.length;
            }
        } catch (error) {
            console.error("搜索数据失败:", error);
        } finally {
            this.model.isLoading = false;
            this.render();
        }
    }
}

// 注册自定义列表视图
registry.category("views").add("custom_multi_select_list", {
    ...listView,
    Controller: CustomListController,
    display: {
        controlPanel: {
        'bottom-left': false,
        'bottom-right': false,
        },
    },
});
</code></pre>
<h3 id="6后端数据接口-python">6.后端数据接口 (Python)</h3>
<pre><code># models/your_model.py
from odoo import models, fields, api

class YourModel(models.Model):
    _name = 'your.model.name'
    _description = '示例模型'
    
    # 定义字段
    field_a = fields.Selection([
        ('option1', '选项1'),
        ('option2', '选项2'),
        ('option3', '选项3'),
    ], string='字段A')
    
    field_b = fields.Char(string='字段B')
    field_c = fields.Many2one('related.model', string='字段C')
    
    # 获取下拉框数据的方法
    @api.model
    def get_filter_dropdown_data(self):
        """返回所有下拉框的选项数据"""
        return {
            'field_a': self._get_field_a_options(),
            'field_b': self._get_field_b_options(),
            'field_c': self._get_field_c_options(),
        }
    
    def _get_field_a_options(self):
        """获取字段A的选项"""
        return [
            display_value 
            for value, display_value in self._fields['field_a'].selection
        ]
    
    def _get_field_b_options(self):
        """获取字段B的去重值"""
        records = self.search_read(
            [('field_b', '!=', False)],
            ['field_b'],
            limit=100
        )
        return sorted(list(set([
            record['field_b'] 
            for record in records 
            if record['field_b']
        ])))
    
    def _get_field_c_options(self):
        """获取字段C的关联选项"""
        related_records = self.env['related.model'].search_read(
            [],
            ['name'],
            limit=50
        )
        return [record['name'] for record in related_records]
</code></pre>
<h3 id="7-视图配置-xml">7. 视图配置 (XML)</h3>
<pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;odoo&gt;
  
    &lt;!-- 自定义列表视图 --&gt;
    &lt;record id="view_custom_list" model="ir.ui.view"&gt;
        &lt;field name="name"&gt;your.model.custom.list&lt;/field&gt;
        &lt;field name="model"&gt;your.model.name&lt;/field&gt;
        &lt;field name="arch" type="xml"&gt;
            &lt;list js_class="custom_multi_select_list"&gt;
                &lt;field name="name" string="名称"/&gt;
                &lt;field name="field_a" string="字段A"/&gt;
                &lt;field name="field_b" string="字段B"/&gt;
                &lt;field name="field_c" string="字段C"/&gt;
                &lt;!-- 其他字段 --&gt;
            &lt;/list&gt;
        &lt;/field&gt;
    &lt;/record&gt;
&lt;/odoo&gt;
</code></pre>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-12-22 15:23">2025-12-22 15:22</span>&nbsp;
<a href="https://www.cnblogs.com/lifuquan">李怀瑾</a>&nbsp;
阅读(<span id="post_view_count">65</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19382615);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19382615', targetLink: 'https://www.cnblogs.com/lifuquan/p/19382615', title: '在Odoo18中实现多选下拉框搜索功能' })">举报</a>
</div>
         ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ 流式数据集：效率提升 100 倍！ ]]></title>
    <link>https://www.cnblogs.com/huggingface/p/19382568</link>
    <guid>1315e90e0d5d815aa6cac9012f6d16aa</guid>
    <description>
    <![CDATA[ 
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/huggingface/p/19382568" title="发布于 2025-12-22 15:17">
    <span role="heading" aria-level="2">流式数据集：效率提升 100 倍！</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h2 id="快速了解tldr">快速了解（TLDR）</h2>
<blockquote>
<p>现在只需一行代码，就能通过 <code>load_dataset('dataset', streaming=True)</code> 以流式方式加载数据集，无需下载！</p>
<p>无需复杂配置、不占磁盘空间、不再担心 “磁盘已满” 或 429 请求过多错误，立即开始训练 TB 级数据集！<br>
性能非常强劲：在 64×H100、256 个并发 worker 环境下，流式加载速度甚至超过本地 SSD！<br>
我们优化后的流式系统：请求数减少 100 倍 → 数据解析速度提升 10 倍 → 样本处理速度翻倍 → 即使在 256 个并发 worker 下也 0 崩溃。</p>
</blockquote>
<p><img alt="数据流式加载可视化" loading="lazy" data-src="https://img-s2.andfun.cn/devrel/posts/2025/12/2800184c9855d.gif" class="lazyload"></p>
<p>在机器学习中，特别是在处理 TB 级别的数据时，数据加载一直是个大难题。我们自己在训练 <a href="https://huggingface.co/blog/smollm3" target="_blank" rel="noopener nofollow">SmolLM3</a> 时也深有体会，有段时间每次训练前都得等上 3 小时下载数据。</p>
<p>虽然 <code>datasets</code> 库早就支持流式加载，但在大规模训练中依然面临瓶颈。今天，这一切都变了 🔥。我们花了几个月优化后端，全面提升流式数据集的速度与效率。</p>
<p>那我们到底做了哪些优化？⤵️</p>
<h2 id="一样简单的-api更强大的性能">一样简单的 API，更强大的性能</h2>
<p>首先最重要的一点：<strong>改进后的接口依然兼容原来的用法</strong>。你只需要加上 <code>streaming=True</code>，就能流式加载 Hugging Face 上的任意数据集，依旧简单直接。🚀</p>
<pre><code class="language-python">from datasets import load_dataset

# Stream a dataset instead of downloading it
dataset = load_dataset("HuggingFaceM4/FineVisionMax", split="train", streaming=True)
# Get the first example
print(next(iter(dataset)))
</code></pre>
<p>全球成千上万的 AI 开发者每天都在使用 <code>datasets</code>，现在他们无需改动任何代码，就能直接享受到更高的性能。</p>
<h2 id="问题挑战大规模流式加载">问题挑战：大规模流式加载</h2>
<p>流式加载一直是快速了解数据集的好方法，但在训练模型时，大多数人仍然选择将数据预先下载到本地，或使用 S3 等云存储——我们在训练 <a href="https://huggingface.co/blog/smolvlm2" target="_blank" rel="noopener nofollow">SmolVLM</a> 时也是这么做的。</p>
<p>我们希望改变这种情况，于是在开发 <a href="https://github.com/huggingface/nanoVLM" target="_blank" rel="noopener nofollow">nanoVLM</a> 时，尝试直接从 Hugging Face Hub 进行流式读取。</p>
<p>但很快就遇到一个严重问题：<strong>一次测试运行在不到一分钟的时间内发出了超过 10 万个请求，结果我们的 IP 被 Hub 屏蔽了！</strong>😅</p>
<p>问题的根源在于：<strong>每个 <code>DataLoader</code> 的 worker 都在独立初始化数据集</strong>，这导致大量冗余请求，形成了“请求风暴”，其中大部分其实是没必要的。</p>
<p>于是我们对启动逻辑进行了深度优化，<strong>最终将启动请求量减少了 100 倍</strong>。总体性能提升如下：</p>
<ul>
<li><strong>数据文件解析速度：提升 10 倍</strong></li>
<li><strong>启动请求效率：提高最多 100 倍</strong></li>
<li><strong>流式速度：提升最多 2 倍</strong></li>
<li><strong>在途请求效率：提升最多 2 倍</strong></li>
</ul>
<h2 id="技术揭秘我们具体改了什么">技术揭秘：我们具体改了什么？</h2>
<p>我们主要优化了两个阶段：<strong>启动阶段</strong> 和 <strong>流式加载阶段</strong>。</p>
<h3 id="1-启动优化-️">1. 启动优化 ⚡️</h3>
<p>初始的数据文件解析阶段会触发大量请求。我们进行了以下两项关键优化：</p>
<ul>
<li>
<p><strong>持久化数据文件缓存</strong>：现在所有 <code>DataLoader</code> worker 会共享数据文件列表缓存。第一个 worker 从 Hub 获取文件列表，其余 worker 直接从本地缓存中读取，从而<strong>几乎完全消除启动阶段的请求</strong>，大幅缩短加载时间，彻底告别“请求风暴”。</p>
</li>
<li>
<p><strong>优化文件解析逻辑</strong>：我们精简了初始 worker 向 Hub 请求文件列表的 API 调用数量，将多个请求进行打包处理，进一步<strong>降低启动延迟</strong>。</p>
</li>
</ul>
<h3 id="2-流式加载优化-️">2. 流式加载优化 🏎️</h3>
<p>为了提升训练过程中的流式吞吐量，我们新增了两个关键功能：</p>
<ul>
<li>
<p><strong>Parquet 数据预取（Prefetching）</strong>：我们为 Parquet 格式的数据集启用了预取功能。这意味着，在模型处理当前数据块的同时，<code>datasets</code> 库会在后台<strong>提前加载下一块数据</strong>。这样可以让整个数据管道始终保持“满负荷”，确保 GPU 不会因等待数据而处于空闲状态，大大提升训练效率。</p>
</li>
<li>
<p><strong>可配置缓冲机制（Buffering）</strong>：针对高级用户，我们开放了缓冲区的配置参数，支持自定义设置<strong>预取数量和数据块大小</strong>，方便根据自身硬件和网络情况进行 I/O 优化。</p>
</li>
</ul>
<p>以下是如何将默认流式请求大小从 32MiB 提升到 128MiB，并启用预取的示例代码：</p>
<pre><code class="language-python">import pyarrow
import pyarrow.dataset

fragment_scan_options = pyarrow.dataset.ParquetFragmentScanOptions(
    cache_options=pyarrow.CacheOptions(
        prefetch_limit=1,
        range_size_limit=128 &lt;&lt; 20
    ),
)
ds = load_dataset(parquet_dataset_id, streaming=True, fragment_scan_options=fragment_scan_options)
</code></pre>
<p>通过这些优化，你的数据加载速度可以提升一倍，训练效率更高！</p>
<h2 id="为什么比-s3-还快背后是-xet-技术">为什么比 S3 还快？背后是 Xet 技术</h2>
<p>Hugging Face 使用了 <strong>Xet 存储系统</strong>：这是一种去重式存储方案，上传和下载速度极快。与传统远程存储不同，Xet 会跳过重复数据，只传输独特内容。</p>
<p>比如，在 Hugging Face 上传大型数据集时，Xet 的去重机制大幅减少了数据传输量，上传更快；数据一上传完，就可以立即开始流式读取。</p>
<p>对于 Parquet 文件，Xet 利用 <a href="https://huggingface.co/blog/parquet-cdc" target="_blank" rel="noopener nofollow">Parquet 内容定义切块（CDC）</a> 来实现去重，进一步加快传输速度。</p>
<p>此外，我们还推出了 <code>pyspark_huggingface</code> 包，支持 Spark 直接读写 HF 数据集，内置对 Parquet CDC 和 Xet 的支持，大幅加快大数据处理。</p>
<h2 id="想自定义流式管道可以">想自定义流式管道？可以！</h2>
<p>有些数据格式 <code>datasets</code> 库还不支持，或者你希望获得更高的控制权，我们也提供了强大的自定义流式能力。</p>
<p><code>huggingface_hub</code> 库中的 <a href="https://huggingface.co/docs/huggingface_hub/guides/hf_file_system" target="_blank" rel="noopener nofollow">HfFileSystem</a> 可高效读取远程数据集文件：</p>
<pre><code class="language-python">from huggingface_hub import HfFileSystem

path = f"hf://datasets/{dataset_id}/{path_in_repo}"
with HfFileSystem().open(path) as f:
    # loop with .read() or .readline() to stream data
    # or do random access with .seek()
</code></pre>
<p>将 <code>HfFileSystem</code> 传入 PyTorch 的 <code>DataLoader</code> 时，会<strong>复用 <code>.ls()</code> 和 <code>.glob()</code> 的缓存结果</strong>，从而<strong>避免在列举数据文件时产生额外的网络请求</strong>，进一步提升流式加载的效率和稳定性。</p>
<h2 id="极限测试我们把它用在-nanovlm-上了">极限测试：我们把它用在 nanoVLM 上了！</h2>
<p>目前我们正在使用这些流式优化功能训练下一代 SmolVLM 模型。得益于新改进，流式加载比我们集群上的多层硬盘系统还要快，<strong>几乎等同于从本地 SSD 读取数据的速度</strong>！</p>
<p>过去，为了避免慢速网络，我们还要把数据拷贝到本地 SSD——整个过程花费 3 小时。而现在，直接流式加载，训练马上开始！</p>
<p>更多细节请看：<a href="https://github.com/huggingface/nanoVLM" target="_blank" rel="noopener nofollow">nanoVLM GitHub</a></p>
<h2 id="快速上手立见成效">快速上手，立见成效</h2>
<p>这些强大的新功能已经集成到 <code>datasets</code> 和 <code>huggingface_hub</code> 库中。想要体验全新的流式加载性能，只需升级你的库版本，并查阅<a href="https://huggingface.co/docs/datasets/stream" target="_blank" rel="noopener nofollow">官方文档</a>即可开始使用：</p>
<pre><code class="language-bash">pip install --upgrade datasets huggingface_hub
</code></pre>
<p>为庆祝这一更新，我们已将 FineVision 所有数据源合并并预先打乱成一个统一数据集：<a href="https://huggingface.co/datasets/HuggingFaceM4/FineVisionMax" target="_blank" rel="noopener nofollow">FineVisionMax</a>。你可以直接用它来训练 VLM 模型，无需手动处理多个数据集！</p>
<pre><code class="language-python">from datasets import load_dataset

# Stream a dataset instead of downloading it
dataset = load_dataset("HuggingFaceM4/FineVisionMax", split="train", streaming=True)
# Get the first example
print(next(iter(dataset)))
</code></pre>
<p>想了解我们是如何大规模运行的？欢迎查看：<a href="https://github.com/huggingface/nanoVLM" target="_blank" rel="noopener nofollow">nanoVLM 项目</a></p>
<p>祝你流式加载愉快！🤗</p>
<blockquote>
<p>英文原文: <a href="https://huggingface.co/blog/streaming-datasets" target="_blank" rel="noopener nofollow">https://huggingface.co/blog/streaming-datasets</a></p>
<p>原文作者: Andres Marafioti, Quentin Lhoest, ben burtenshaw, Pedro Cuenca, merve</p>
<p>译者: Luke,  Hugging Face Fellow</p>
</blockquote>

</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0" data-date-updated="2025-12-22 15:18">2025-12-22 15:17</span>&nbsp;
<a href="https://www.cnblogs.com/huggingface">HuggingFace</a>&nbsp;
阅读(<span id="post_view_count">0</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19382568);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19382568', targetLink: 'https://www.cnblogs.com/huggingface/p/19382568', title: '流式数据集：效率提升 100 倍！' })">举报</a>
</div>
         ]]>
    </description>
    </item>
<item>
    <title><![CDATA[ 从 MCP 到 Agent Skills，AI Ready 的 .NET 10 正当时 ]]></title>
    <link>https://www.cnblogs.com/sheng-jie/p/19381647</link>
    <guid>a5ed226215532db4d76e49fdf856585d</guid>
    <description>
    <![CDATA[ 
            <h1 class="postTitle">
                <a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/sheng-jie/p/19381647" title="发布于 2025-12-22 11:58">
    <span role="heading" aria-level="2">从 MCP 到 Agent Skills，AI Ready 的 .NET 10 正当时</span>
    

</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="使用-net-file-based-apps-编写高效-agent-skills-脚本指南">使用 .NET File-Based Apps 编写高效 Agent Skills 脚本指南</h1>
<h2 id="前言">前言</h2>
<p>AI 工具生态正在经历一场深刻的变革。从 Anthropic 推出的 <strong>Model Context Protocol (MCP)</strong> 到最新发布的 <strong>Agent Skills</strong>，我们见证了 AI 能力扩展方式的演进：MCP 为 AI 提供了访问外部数据和工具的标准化接口，而 Agent Skills 则更进一步，专注于将人类的专业知识和工作流程封装为 AI 可理解、可执行的格式。</p>
<p><strong>Agent Skills</strong> (<a href="https://agentskills.io" title="https://agentskills.io" target="_blank" rel="noopener nofollow">https://agentskills.io</a>) 中可以包含可执行脚本（位于 <code>scripts/</code> 目录），官方规范支持 Python、Bash、JavaScript 等语言。而 <strong>.NET 10 引入的 File-Based Apps</strong> 特性为这些脚本的编写提供了一个强大的新选择：单个 <code>.cs</code> 文件即可作为完整应用运行，内联依赖声明，支持跨平台部署和 Native AOT 编译。</p>
<p>本文将带你了解 Agent Skills 的规范，以及如何使用 .NET File-Based Apps 编写高效、可靠的 Skill 脚本。你会发现，.NET 为 Agent Skills 的脚本开发提供了类型安全、高性能和优秀跨平台支持的独特优势。</p>
<h2 id="一什么是-agent-skills">一、什么是 Agent Skills？</h2>
<p><img src="https://img2024.cnblogs.com/blog/577140/202512/577140-20251222115747883-716068741.png" alt="" loading="lazy"></p>
<h3 id="11-核心概念">1.1 核心概念</h3>
<p>Agent Skills 是一种轻量级、开放的格式，用于扩展 AI Agent 的能力和专业知识。本质上，<strong>一个 Skill 就是一个包含 <code>SKILL.md</code> 文件的文件夹</strong>。</p>
<p>该文件包含：</p>
<ul>
<li><strong>元数据</strong>：<code>name</code> 和 <code>description</code>（最少必需）</li>
<li><strong>指令</strong>：告诉 Agent 如何执行特定任务的 Markdown 文档</li>
<li><strong>可选资源</strong>：脚本、模板、参考文档等</li>
</ul>
<p>Skills 的核心价值在于：</p>
<ul>
<li><strong>专业知识封装</strong>：将特定领域的程序化知识（procedural knowledge）和公司/团队/用户特定的上下文打包</li>
<li><strong>按需加载</strong>：Agent 启动时只加载 Skill 的 <code>name</code> 和 <code>description</code>，任务匹配时才加载完整指令</li>
<li><strong>可执行能力</strong>：可以包含脚本、工具，扩展 Agent 的实际操作能力</li>
<li><strong>版本化管理</strong>：Skills 就是文件，易于编辑、版本控制和分享</li>
</ul>
<h3 id="12-agent-skills-能做什么">1.2 Agent Skills 能做什么？</h3>
<p>根据官方文档，Agent Skills 的主要应用场景包括：</p>
<h4 id="领域专长domain-expertise">领域专长（Domain Expertise）</h4>
<p>将专业知识打包为可复用的指令：</p>
<ul>
<li><strong>法律审查流程</strong>：标准化的合同审查检查清单和审批流程</li>
<li><strong>数据分析管道</strong>：统一的数据清洗、转换、可视化工作流</li>
<li><strong>代码审查规范</strong>：团队的编码标准、安全检查、性能优化指南</li>
<li><strong>医疗诊断协议</strong>：基于症状的诊断路径和治疗建议流程</li>
</ul>
<h4 id="新能力new-capabilities">新能力（New Capabilities）</h4>
<p>赋予 Agent 原本不具备的操作能力：</p>
<ul>
<li><strong>创建演示文稿</strong>：根据内容自动生成 PPT/Keynote</li>
<li><strong>构建 MCP 服务器</strong>：生成 Model Context Protocol 服务器代码</li>
<li><strong>分析数据集</strong>：执行统计分析、生成可视化图表</li>
<li><strong>处理 PDF 文档</strong>：拆分、合并、提取文本、填写表单</li>
</ul>
<h4 id="可重复工作流repeatable-workflows">可重复工作流（Repeatable Workflows）</h4>
<p>将多步骤任务转化为一致且可审计的流程：</p>
<ul>
<li><strong>CI/CD 流水线</strong>：标准化的构建、测试、部署步骤</li>
<li><strong>客户入职流程</strong>：账户创建、权限配置、培训材料发送</li>
<li><strong>月度报告生成</strong>：数据收集、分析、报告编写、分发</li>
<li><strong>代码重构任务</strong>：识别问题、建议改进、执行修改、验证测试</li>
</ul>
<h4 id="跨工具互操作interoperability">跨工具互操作（Interoperability）</h4>
<p>一次编写，多处使用：</p>
<ul>
<li>在 <strong>GitHub Copilot</strong> 中辅助编码</li>
<li>在 <strong>Cursor</strong> 中进行项目重构</li>
<li>在 <strong>Claude</strong> 中进行文档写作</li>
<li>在 <strong>Goose</strong> 中执行自动化任务</li>
</ul>
<p>这种标准化使得组织知识可以在不同工具间无缝流转。</p>
<h3 id="13-谁在支持-agent-skills">1.3 谁在支持 Agent Skills？</h3>
<p>目前已有多个主流 AI 开发工具支持该标准：</p>
<p><img src="https://img2024.cnblogs.com/blog/577140/202512/577140-20251222115747805-798970849.png" alt="" loading="lazy"></p>
<h2 id="二agent-skills-规范解读">二、Agent Skills 规范解读</h2>
<h3 id="21-基本目录结构">2.1 基本目录结构</h3>
<p>一个最简单的 Skill 只需要包含 <code>SKILL.md</code> 文件：</p>
<pre><code>skill-name/
└── SKILL.md          # 必需
</code></pre>
<p>可选的支持目录：</p>
<pre><code>skill-name/
├── SKILL.md          # 必需：技能描述和使用说明
├── scripts/          # 可选：可执行脚本
│   └── tool.py
├── references/       # 可选：详细参考文档
│   └── REFERENCE.md
└── assets/          # 可选：静态资源
    └── template.json
</code></pre>
<h3 id="22-skillmd-格式规范">2.2 SKILL.md 格式规范</h3>
<p><code>SKILL.md</code> 文件包含两部分：<strong>YAML frontmatter</strong> 和 <strong>Markdown 正文</strong>。</p>
<h4 id="frontmatter-必需字段">Frontmatter 必需字段</h4>
<pre><code class="language-yaml">---
name: skill-name
description: 描述技能功能和使用场景的文字，应包含帮助 Agent 识别相关任务的关键词
---
</code></pre>
<p><strong>name 字段规则</strong>：</p>
<ul>
<li>长度：1-64 字符</li>
<li>字符：仅允许小写字母、数字和连字符 (<code>a-z</code>, <code>-</code>)</li>
<li>不能以连字符开头或结尾</li>
<li>不能包含连续的连字符 (<code>--</code>)</li>
<li><strong>必须与父目录名完全一致</strong></li>
</ul>
<p><strong>description 字段规则</strong>：</p>
<ul>
<li>长度：1-1024 字符</li>
<li>应说明技能的功能和使用时机</li>
<li>包含关键词帮助 Agent 识别适用场景</li>
</ul>
<h4 id="可选字段">可选字段</h4>
<pre><code class="language-yaml">---
name: pdf-processing
description: Extract text from PDFs and merge multiple documents
license: MIT
compatibility: Requires Python 3.8+ and poppler-utils
metadata:
  author: your-org
  version: "1.0.0"
---
</code></pre>
<h3 id="23-渐进式信息披露">2.3 渐进式信息披露</h3>
<p>Agent Skills 采用渐进式加载策略来优化 token 使用：</p>
<ol>
<li><strong>元数据阶段</strong> (~100 tokens)：启动时加载所有 Skills 的 <code>name</code> 和 <code>description</code></li>
<li><strong>指令阶段</strong> (&lt;5000 tokens 推荐)：激活 Skill 时加载完整的 <code>SKILL.md</code></li>
<li><strong>资源阶段</strong> (按需)：仅在需要时加载 <code>scripts/</code>、<code>references/</code>、<code>assets/</code> 中的文件</li>
</ol>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>保持 <code>SKILL.md</code> 在 500 行以内</li>
<li>将详细参考资料移至 <code>references/</code> 目录</li>
<li>避免深层嵌套的文件引用</li>
</ul>
<h2 id="三使用-net-file-based-apps-编写-agent-skills-脚本">三、使用 .NET File-Based Apps 编写 Agent Skills 脚本</h2>
<p>在进入具体实现之前，我们先来看看为什么 .NET File-Based Apps 是编写 Agent Skills 中 <code>scripts/</code> 目录下可执行脚本的优秀选择。</p>
<h3 id="30-net-作为脚本语言的独特优势">3.0 .NET 作为脚本语言的独特优势</h3>
<p>根据 Agent Skills 规范，<code>scripts/</code> 目录中的可执行代码应该自包含或明确记录依赖、包含有用的错误消息、优雅处理边界情况。常见的脚本语言包括 Python、Bash、JavaScript，而 .NET File-Based Apps 为此提供了一个强大的替代方案：</p>
<h4 id="对比其他脚本语言方案">对比其他脚本语言方案</h4>
<p>在为 Agent Skills 编写 <code>scripts/</code> 目录下的可执行脚本时，.NET File-Based Apps 与其他常用语言的对比：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>.NET File-Based Apps</th>
<th>Python 脚本</th>
<th>Node.js 脚本</th>
</tr>
</thead>
<tbody>
<tr>
<td>单文件运行</td>
<td>✅ <code>dotnet file.cs</code></td>
<td>✅ <code>python file.py</code></td>
<td>✅ <code>node file.js</code></td>
</tr>
<tr>
<td>依赖声明</td>
<td>✅ 文件内声明</td>
<td>⚠️ 需 <code>requirements.txt</code></td>
<td>⚠️ 需 <code>package.json</code></td>
</tr>
<tr>
<td>类型安全</td>
<td>✅ 编译时检查</td>
<td>❌ 运行时错误</td>
<td>⚠️ 需 TypeScript</td>
</tr>
<tr>
<td>性能</td>
<td>✅ Native AOT 编译</td>
<td>⚠️ 解释执行</td>
<td>⚠️ JIT 编译</td>
</tr>
<tr>
<td>跨平台部署</td>
<td>✅ 单个可执行文件</td>
<td>⚠️ 需 Python 运行时</td>
<td>⚠️ 需 Node.js 运行时</td>
</tr>
<tr>
<td>企业级库支持</td>
<td>✅ 丰富的 NuGet 生态</td>
<td>✅ PyPI 生态</td>
<td>✅ npm 生态</td>
</tr>
<tr>
<td>AI Agent 可读性</td>
<td>✅ 结构清晰、自文档化</td>
<td>✅ 简洁</td>
<td>✅ 简洁</td>
</tr>
</tbody>
</table>
<h4 id="net-的三大杀手锏">.NET 的三大杀手锏</h4>
<p><strong>1. 真正的自包含</strong><br>
依赖声明直接写在代码文件头部，Agent 一眼就能看懂需要什么包、什么版本，无需查找外部配置文件。</p>
<pre><code class="language-csharp">#:package PdfSharpCore@1.3.65
#:package Spectre.Console@0.49.1
</code></pre>
<p><strong>2. 从开发到生产无缝过渡</strong><br>
开发时直接运行 <code>.cs</code> 文件，生产时一键发布为 Native AOT 可执行文件，启动速度可达毫秒级，内存占用极小。</p>
<p><strong>3. AI 友好的代码结构</strong><br>
.NET 的强类型和清晰的语法结构，让 AI Agent 更容易理解代码意图、发现潜在问题、提出改进建议。</p>
<h3 id="31-什么是-net-file-based-apps">3.1 什么是 .NET File-Based Apps？</h3>
<p>.NET 10 引入的 File-Based Apps 特性允许将单个 <code>.cs</code> 文件作为完整的应用程序运行，无需传统的项目文件（<code>.csproj</code>）。关键特性包括：</p>
<ul>
<li><strong>单文件即应用</strong>：一个 <code>.cs</code> 文件包含完整程序</li>
<li><strong>内联依赖声明</strong>：通过特殊注释声明 NuGet 包</li>
<li><strong>直接运行</strong>：<code>dotnet file.cs</code> 即可执行</li>
<li><strong>支持发布</strong>：可以发布为独立可执行文件或 Native AOT</li>
<li><strong>零配置</strong>：无需 <code>csproj</code>、<code>sln</code> 等项目文件</li>
</ul>
<h3 id="32-file-based-apps-如何适配-agent-skills-需求">3.2 File-Based Apps 如何适配 Agent Skills 需求？</h3>
<p>Agent Skills 规范强调「简洁」、「自包含」、「可理解」，.NET File-Based Apps 的设计理念与之完美契合：</p>
<h4 id="适配点-1渐进式复杂度">适配点 1：渐进式复杂度</h4>
<pre><code class="language-csharp">// 入门：10 行代码的简单工具
#!/usr/bin/env dotnet
if (args.Length == 0) { Console.WriteLine("Hello, Agent!"); return; }
Console.WriteLine($"Processing: {args[0]}");

// 进阶：添加依赖和错误处理
#:package Newtonsoft.Json@13.0.3
using Newtonsoft.Json;
try { /* 处理逻辑 */ }
catch (Exception ex) { Console.Error.WriteLine(ex.Message); return 1; }

// 生产：发布为高性能可执行文件
#:property PublishAot=true
// 一行命令：dotnet publish -r win-x64
</code></pre>
<p>从原型到生产，同一个文件，逐步迭代，没有项目结构的重构成本。</p>
<h4 id="适配点-2ai-agent-的理解成本">适配点 2：AI Agent 的理解成本</h4>
<p><strong>Python 方案</strong>：Agent 需要找到并理解多个文件</p>
<pre><code>my-skill/
├── tool.py          # Agent 需要读取
├── requirements.txt # Agent 需要读取
└── README.md        # Agent 需要读取
</code></pre>
<p><strong>.NET 方案</strong>：一个文件包含所有信息</p>
<pre><code>my-skill/
└── scripts/
    └── tool.cs      # 依赖、逻辑、配置全在这里
</code></pre>
<p>Agent 只需读取一个文件，就能了解：</p>
<ul>
<li>需要什么依赖（<code>#:package</code>）</li>
<li>如何运行（<code>#!/usr/bin/env dotnet</code>）</li>
<li>做什么事（代码逻辑）</li>
<li>如何部署（<code>#:property</code>）</li>
</ul>
<h4 id="适配点-3企业级可靠性">适配点 3：企业级可靠性</h4>
<p>.NET 的类型系统在 Agent 驱动的开发中尤为重要：</p>
<pre><code class="language-csharp">// 编译时就能发现错误，而不是运行时崩溃
string pdfPath = args[0];  // Agent 知道这是字符串
int pageCount = GetPageCount(pdfPath);  // Agent 知道返回值是整数

// Python 中相同的错误可能运行时才暴露
# pdf_path = args[0]  # 类型不明确
# page_count = get_page_count(pdf_path)  # 返回值类型不明确
</code></pre>
<p>这意味着 Agent 在生成或修改代码时，有更多的安全护栏。</p>
<h4 id="适配点-4性能与资源效率">适配点 4：性能与资源效率</h4>
<p>AI Agent 可能频繁调用 Skills，启动性能至关重要：</p>
<pre><code class="language-bash"># Python 脚本启动
$ time python tool.py input.pdf
real    0m0.234s  # 需要加载解释器

# .NET File-Based App 启动
$ time dotnet tool.cs input.pdf
real    0m0.089s  # JIT 编译

# Native AOT 编译后
$ time ./tool input.pdf
real    0m0.012s  # 接近原生 C++ 性能
</code></pre>
<p>对于 Agent 执行的自动化任务，这种性能差异会累积成显著的时间节省。</p>
<h3 id="33-实战案例split-pdf-skill">3.3 实战案例：split-pdf Skill</h3>
<p>让我们以一个实际的 PDF 拆分工具为例，演示如何开发符合规范的 Agent Skill。</p>
<h4 id="步骤-1创建目录结构">步骤 1：创建目录结构</h4>
<pre><code class="language-bash">mkdir -p split-pdf/scripts
cd split-pdf
</code></pre>
<h4 id="步骤-2编写-skillmd">步骤 2：编写 SKILL.md</h4>
<p><code>SKILL.md</code> 是 Agent Skill 的核心，包含元数据和使用说明。创建 <code>SKILL.md</code> 文件：</p>
<pre><code class="language-markdown">---
name: split-pdf
description: Split PDF files into separate single-page documents or extract specific page ranges. Use when you need to divide a PDF into multiple files, extract particular pages, or process PDF pages individually. Works with multi-page PDF documents.
license: MIT
---

# Split PDF

将 PDF 文件拆分为多个单页文件或提取指定页面范围。

## 使用场景

- 将多页 PDF 拆分为独立的单页文件
- 提取 PDF 的特定页面范围
- 需要单独处理 PDF 各个页面时

## 使用方法

使用 `scripts/split-pdf.cs` 脚本进行 PDF 拆分：

### 拆分页面
# 拆分所有页面
dotnet scripts/split-pdf.cs input.pdf output-dir/
# 拆分第 1-5 页
dotnet scripts/split-pdf.cs input.pdf output-dir/ 1-5


## 输出格式

拆分后的文件命名格式：`{原文件名}_page_{页码}.pdf`

## 依赖项

- PdfSharpCore 1.3.65 - PDF 操作核心库
- Spectre.Console 0.49.1 - 美化的控制台输出



**注意**：
- `name` 必须与目录名 `split-pdf` 完全一致
- `description` 包含关键词 "split", "PDF", "pages" 帮助 Agent 识别场景
</code></pre>
<h4 id="步骤-3编写-file-based-app-脚本">步骤 3：编写 File-Based App 脚本</h4>
<p>在 <code>scripts/</code> 目录下创建 <code>.NET File-Based App 脚本 </code>split-pdf.cs`：</p>
<pre><code class="language-csharp">#!/usr/bin/env dotnet
#:package PdfSharpCore@1.3.65
#:package Spectre.Console@0.49.1
#:property PublishAot=true

using PdfSharpCore.Pdf;
using PdfSharpCore.Pdf.IO;
using Spectre.Console;
using System;
using System.IO;

// ==================== 参数校验 ====================
if (args.Length &lt; 2)
{
    AnsiConsole.MarkupLine("[red]错误: 参数不足[/]");
    AnsiConsole.MarkupLine("[yellow]用法: dotnet split-pdf.cs &lt;PDF文件&gt; &lt;输出目录&gt; [页面范围][/]");
    return 1;
}

var pdfPath = args[0];
var outputDir = args[1];
var pageRange = args.Length &gt;= 3 ? args[2] : null;

// 验证文件
if (!File.Exists(pdfPath))
{
    AnsiConsole.MarkupLine($"[red]错误: 文件不存在: {pdfPath}[/]");
    return 1;
}

// 创建输出目录
Directory.CreateDirectory(outputDir);

// ==================== 拆分 PDF ====================
try
{
    using var inputDocument = PdfReader.Open(pdfPath, PdfDocumentOpenMode.Import);
    var totalPages = inputDocument.PageCount;
    
    // 解析页面范围
    int startPage = 1, endPage = totalPages;
    if (!string.IsNullOrEmpty(pageRange))
    {
        var parts = pageRange.Split('-');
        if (parts.Length == 2 &amp;&amp; 
            int.TryParse(parts[0], out startPage) &amp;&amp; 
            int.TryParse(parts[1], out endPage))
        {
            startPage = Math.Max(1, Math.Min(startPage, totalPages));
            endPage = Math.Max(startPage, Math.Min(endPage, totalPages));
        }
    }

    var baseName = Path.GetFileNameWithoutExtension(pdfPath);
    
    await AnsiConsole.Progress()
        .StartAsync(async ctx =&gt;
        {
            var task = ctx.AddTask("拆分 PDF 页面", maxValue: endPage - startPage + 1);

            for (int i = startPage; i &lt;= endPage; i++)
            {
                using var outputDocument = new PdfDocument();
                outputDocument.AddPage(inputDocument.Pages[i - 1]);
                
                var outputPath = Path.Combine(outputDir, $"{baseName}_page_{i:D3}.pdf");
                outputDocument.Save(outputPath);
                
                task.Increment(1);
                await Task.CompletedTask;
            }
        });

    AnsiConsole.MarkupLine($"[green]✅ 拆分完成！已生成 {endPage - startPage + 1} 个文件[/]");
    return 0;
}
catch (Exception ex)
{
    AnsiConsole.MarkupLine($"[red]❌ 错误: {ex.Message}[/]");
    return 1;
}
</code></pre>
<p><strong>关键要素解析</strong>：</p>
<ol>
<li><strong>Shebang 行</strong>：<code>#!/usr/bin/env dotnet</code> - 使脚本可在 Unix 系统直接执行</li>
<li><strong>依赖声明</strong>：<code>#:package</code> 指令声明 NuGet 包及版本</li>
<li><strong>发布配置</strong>：<code>#:property PublishAot=true</code> - 支持 Native AOT 编译</li>
<li><strong>顶层语句</strong>：无需 <code>Main</code> 方法，直接编写逻辑</li>
<li><strong>返回值</strong>：使用 <code>return</code> 返回退出码</li>
</ol>
<h2 id="四测试与验证">四、测试与验证</h2>
<h3 id="41-本地测试">4.1 本地测试</h3>
<h4 id="直接运行测试">直接运行测试</h4>
<pre><code class="language-bash"># 准备测试 PDF 文件
cd split-pdf

# 测试拆分所有页面
dotnet scripts/split-pdf.cs test.pdf ./output/

# 测试拆分指定范围
dotnet scripts/split-pdf.cs test.pdf ./output/ 1-3

# 验证输出
ls ./output/
# 应该看到：test_page_001.pdf, test_page_002.pdf, test_page_003.pdf
</code></pre>
<h4 id="错误处理测试">错误处理测试</h4>
<pre><code class="language-bash"># 测试文件不存在
dotnet scripts/split-pdf.cs nonexistent.pdf ./output/

# 测试参数不足
dotnet scripts/split-pdf.cs

# 测试无效页面范围
dotnet scripts/split-pdf.cs test.pdf ./output/ 100-200
</code></pre>
<h3 id="42-agent-集成测试">4.2 Agent 集成测试</h3>
<p>在支持 Agent Skills 的环境中测试（如 GitHub Copilot）：</p>
<ol>
<li>将 Skill 放置在 <code>.github/skills/</code> 目录下</li>
<li>重启或刷新 Agent</li>
<li>测试 Agent 是否能发现和使用该 Skill</li>
</ol>
<pre><code>用户提问：请帮我把这个 PDF 文件拆分成单独的页面

Agent 行为：
1. 识别任务涉及 PDF 拆分
2. 查找并激活 split-pdf skill
3. 读取 SKILL.md 了解使用方法
4. 执行：dotnet scripts/split-pdf.cs document.pdf ./pages/
5. 向用户报告结果
</code></pre>
<h2 id="五最佳实践">五、最佳实践</h2>
<h3 id="51-脚本设计原则">5.1 脚本设计原则</h3>
<h4 id="清晰的参数设计">清晰的参数设计</h4>
<pre><code class="language-csharp">// ❌ 不好：参数含义不明
dotnet tool.cs input output 1

// ✅ 好：参数含义清晰
dotnet split-pdf.cs document.pdf ./pages/ 1-10
</code></pre>
<h4 id="友好的错误提示">友好的错误提示</h4>
<pre><code class="language-csharp">if (args.Length &lt; 2)
{
    AnsiConsole.MarkupLine("[red]错误: 参数不足[/]");
    AnsiConsole.MarkupLine("[yellow]用法: dotnet split-pdf.cs &lt;PDF文件&gt; &lt;输出目录&gt; [页面范围][/]");
    AnsiConsole.MarkupLine("[gray]示例: dotnet split-pdf.cs input.pdf ./output/ 1-5[/]");
    return 1;
}
</code></pre>
<h4 id="进度反馈">进度反馈</h4>
<p>使用 Spectre.Console 提供直观的进度显示：</p>
<pre><code class="language-csharp">await AnsiConsole.Progress()
    .StartAsync(async ctx =&gt;
    {
        var task = ctx.AddTask("处理中", maxValue: totalItems);
        foreach (var item in items)
        {
            // 处理逻辑
            task.Increment(1);
        }
    });
</code></pre>
<h3 id="52-依赖管理">5.2 依赖管理</h3>
<h4 id="选择稳定的包版本">选择稳定的包版本</h4>
<pre><code class="language-csharp">// ✅ 指定明确版本
#:package PdfSharpCore@1.3.65
#:package Spectre.Console@0.49.1

// ❌ 避免使用不稳定版本
#:package SomePackage@*
#:package BetaPackage@2.0.0-beta
</code></pre>
<h4 id="最小化依赖">最小化依赖</h4>
<p>只引入必需的包，减少潜在的兼容性问题。</p>
<h3 id="53-文档编写">5.3 文档编写</h3>
<h4 id="description-关键词策略">Description 关键词策略</h4>
<p>在 <code>description</code> 中包含：</p>
<ul>
<li><strong>动作关键词</strong>：split, extract, convert, merge</li>
<li><strong>领域关键词</strong>：PDF, document, pages</li>
<li><strong>场景关键词</strong>：when you need to divide, when working with</li>
</ul>
<pre><code class="language-yaml"># ✅ 好的 description
description: Split PDF files into separate single-page documents or extract specific page ranges. Use when you need to divide a PDF into multiple files, extract particular pages, or process PDF pages individually.

# ❌ 不够好
description: PDF tool for splitting.
</code></pre>
<h4 id="提供清晰的示例">提供清晰的示例</h4>
<p>在 <code>SKILL.md</code> 正文中提供：</p>
<ul>
<li>常见用例的示例</li>
<li>不同参数组合的效果</li>
<li>预期的输出格式</li>
</ul>
<h3 id="54-跨平台兼容性">5.4 跨平台兼容性</h3>
<h4 id="路径处理">路径处理</h4>
<pre><code class="language-csharp">// ✅ 使用 Path.Combine
var outputPath = Path.Combine(outputDir, $"{baseName}_page_{i:D3}.pdf");

// ❌ 避免硬编码路径分隔符
var outputPath = outputDir + "\\" + baseName + "_page_" + i + ".pdf";
</code></pre>
<h4 id="编码处理">编码处理</h4>
<pre><code class="language-csharp">// 确保控制台正确显示 Unicode
Console.OutputEncoding = System.Text.Encoding.UTF8;
</code></pre>
<h2 id="六进阶话题">六、进阶话题</h2>
<h3 id="61-支持多个相关工具">6.1 支持多个相关工具</h3>
<p>可以在一个 Skill 中包含多个相关脚本：</p>
<pre><code>pdf-toolkit/
├── SKILL.md
└── scripts/
    ├── split.cs
    ├── merge.cs
    └── extract-text.cs
</code></pre>
<p>在 <code>SKILL.md</code> 中说明每个工具的用途和使用场景。</p>
<h3 id="62-使用-references-目录">6.2 使用 References 目录</h3>
<p>对于复杂的 Skill，将详细文档分离：</p>
<pre><code>data-analysis/
├── SKILL.md           # 简要说明和快速开始
├── scripts/
│   └── analyze.cs
└── references/
    ├── REFERENCE.md   # 详细 API 参考
    ├── examples.md    # 更多示例
    └── algorithms.md  # 算法说明
</code></pre>
<p>在 <code>SKILL.md</code> 中引用：</p>
<pre><code class="language-markdown">详细的 API 参考请见 [REFERENCE.md](references/REFERENCE.md "REFERENCE.md")。
</code></pre>
<h3 id="63-native-aot-优化">6.3 Native AOT 优化</h3>
<p>对于性能敏感的工具，启用 Native AOT：</p>
<pre><code class="language-csharp">#:property PublishAot=true
#:property InvariantGlobalization=true  // 减小体积
</code></pre>
<p>发布时使用：</p>
<pre><code class="language-bash">dotnet publish scripts/tool.cs -r win-x64 --property:PublishAot=true
</code></pre>
<p><strong>Native AOT 优势</strong>：</p>
<ul>
<li>极快的启动时间</li>
<li>较小的内存占用</li>
<li>无需安装 .NET 运行时</li>
<li>单个可执行文件</li>
</ul>
<h3 id="64-团队协作">6.4 团队协作</h3>
<p>将 Skills 放入版本控制：</p>
<pre><code>.github/
└── skills/
    ├── split-pdf/
    ├── data-analysis/
    └── code-review/
</code></pre>
<p>在团队 README 中说明：</p>
<ul>
<li>如何安装 Skills</li>
<li>如何贡献新 Skills</li>
<li>编码规范和测试要求</li>
</ul>
<h2 id="七意义与展望">七、意义与展望</h2>
<h3 id="71-对开发者的价值">7.1 对开发者的价值</h3>
<p>Agent Skills 为开发者带来三大核心价值：</p>
<p><strong>能力复用</strong>：一次编写，在 Copilot、Cursor、Claude 等多个 Agent 产品中使用，还可跨团队共享或通过 GitHub 公开发布。</p>
<p><strong>知识沉淀</strong>：将团队最佳实践固化为版本化的 Skills，如代码审查规范、部署流程、数据分析模板等，确保工作流程的一致性。</p>
<p><strong>提升效率</strong>：通过明确的指导让 Agent 更准确地执行复杂任务，减少试错和修正，提供一致的输出质量。</p>
<h3 id="72-net-file-based-apps-的机遇">7.2 .NET File-Based Apps 的机遇</h3>
<p>File-Based Apps 为 .NET 带来了新的应用场景：</p>
<p><strong>降低门槛</strong>：从复杂的项目结构到单文件脚本，让 Python、Node.js 开发者也能轻松尝试 .NET。</p>
<p><strong>AI 协作优势</strong>：强类型系统帮助 AI 更准确理解代码、即时编译反馈加速修正、丰富的 API 文档提升 AI 可读性。</p>
<p><strong>性能差异化</strong>：Native AOT 编译在批量任务中的性能优势（启动快 90%+），不仅提升用户体验，也降低云端成本。</p>
<p><strong>生态拓展</strong>：.NET 开发者可以将企业实践打包为 Skills，在 AI Agent 生态中展现 .NET 价值，同时 .NET 的实践经验也能反哺 Agent Skills 标准演进。</p>
<h3 id="73-未来展望">7.3 未来展望</h3>
<p>Agent Skills 生态正在快速发展，可以期待：</p>
<ul>
<li><strong>Skills 市场</strong>：类似 npm 的包管理器和分发平台</li>
<li><strong>工具链集成</strong>：IDE 内置 Agent Skills 模板和验证工具</li>
<li><strong>应用拓展</strong>：从个人工具到企业知识库、教育培训、行业解决方案</li>
</ul>
<p>.NET File-Based Apps 在这个生态中的定位清晰：作为 Agent Skills 脚本的强类型、高性能选择，为开发者提供从原型到生产的无缝体验。</p>
<h2 id="八总结">八、总结</h2>
<p>通过本文的探索，我们看到 .NET File-Based Apps 作为 Agent Skills 脚本语言的独特价值：<strong>单文件自包含、强类型安全、Native AOT 高性能</strong>，完美匹配 Agent Skills 规范对脚本的要求。</p>
<h3 id="核心收获">核心收获</h3>
<p>本文从 Agent Skills 规范入手，通过 split-pdf 实战案例，展示了如何用 .NET File-Based Apps 编写高质量的 Skill 脚本，并探讨了其在 AI 时代的应用机遇。</p>
<h3 id="开始行动">开始行动</h3>
<p><strong>对于 .NET 开发者</strong>：用熟悉的 C# 为 Agent Skills 编写脚本，将专业知识封装为可复用的 Skills，在 AI 时代发挥 .NET 的价值。</p>
<p><strong>对于其他技术栈开发者</strong>：.NET File-Based Apps 和 Python 一样简洁，但提供了类型安全和 Native AOT 性能。值得尝试：<code>winget install Microsoft.DotNet.SDK.10</code> 然后创建你的第一个 <code>.cs</code> 脚本。</p>
<p><strong>下一步</strong>：动手实现你的第一个 Agent Skill，将专业知识转化为可复用的能力，在 AI 时代发挥更大价值。</p>
<h2 id="参考资源">参考资源</h2>
<ul>
<li><a href="https://agentskills.io/" title="Agent Skills 官方网站" target="_blank" rel="noopener nofollow">Agent Skills 官方网站</a></li>
<li><a href="https://agentskills.io/specification" title="Agent Skills 规范" target="_blank" rel="noopener nofollow">Agent Skills 规范</a></li>
<li><a href="https://github.com/agentskills/agentskills" title="Agent Skills GitHub" target="_blank" rel="noopener nofollow">Agent Skills GitHub</a></li>
<li><a href="https://learn.microsoft.com/en-us/dotnet/core/sdk/file-based-apps" target="_blank" rel="noopener nofollow">.NET File-Based Apps 文档</a></li>
<li><a href="https://github.com/anthropics/skills" title="anthropics/skills" target="_blank" rel="noopener nofollow">anthropics/skills</a></li>
</ul>
<hr>

</div>
<div id="MySignature" role="contentinfo">
    <div style="display: block; border: 2px solid #6ecaa8; padding: 10px; background: aliceblue">  
<img src="https://files.cnblogs.com/files/sheng-jie/maf-course-card-scan.bmp">
<blockquote>
<b>👆面向.NET开发者的AI Agent 开发课程【.NET+AI | 智能体开发进阶】已上线，欢迎扫码加入学习。👆</b>
</blockquote>
</div>

<img src="https://files.cnblogs.com/files/sheng-jie/scan-follow.bmp">
<blockquote>
<b>
关注我的公众号『向 AI 而行』，我们微信不见不散。
<br>
阅罢此文，如果您觉得本文不错并有所收获，请【打赏】或【推荐】，也可【评论】留下您的问题或建议与我交流。

你的支持是我不断创作和分享的不竭动力！</b>
</blockquote>

<div style="display: block; border: 2px solid #6ecaa8; padding: 10px; background: aliceblue" id="AllanboltSignature">    
        <div>作者：<a href="http://www.jianshu.com/u/39ec0e6b1844" target="_blank">『圣杰』</a></div>
        <div>出处：<a href="http://www.cnblogs.com/sheng-jie/" target="_blank">http://www.cnblogs.com/sheng-jie/</a></div>
        <div>本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接，否则保留追究法律责任的权利。</div>  
</div>
</div>
<div class="clear"></div>

            </div>
            <div class="postDesc">posted @ 
<span id="post-date" data-last-update-days="0.12152777777777778" data-date-updated="2025-12-22 14:53">2025-12-22 11:58</span>&nbsp;
<a href="https://www.cnblogs.com/sheng-jie">「圣杰」</a>&nbsp;
阅读(<span id="post_view_count">505</span>)&nbsp;
评论(<span id="post_comment_count">2</span>)&nbsp;
&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(19381647);return false;">收藏</a>&nbsp;
<a href="javascript:void(0)" onclick="reportManager.report({ currentUserId: '', targetType: 'blogPost', targetId: '19381647', targetLink: 'https://www.cnblogs.com/sheng-jie/p/19381647', title: '从 MCP 到 Agent Skills，AI Ready 的 .NET 10 正当时' })">举报</a>
</div>
         ]]>
    </description>
    </item>
        </channel>
        </rss>