{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "Concept Bottleneck Models-概念瓶颈模型用于可解释决策：进展、分类体系 与未来方向综述",
      "link": "https://www.cnblogs.com/lemonzhang/p/19592426",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/lemonzhang/p/19592426\" id=\"cb_post_title_url\" title=\"发布于 2026-02-08 19:27\">\n    <span>Concept Bottleneck Models-概念瓶颈模型用于可解释决策：进展、分类体系 与未来方向综述</span>\n    \n\n</a>\n\n\t\t</h1>\n\t\t<div class=\"clear\"></div>\n\t\t<div class=\"postBody\">\n\t\t\t    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        深度神经网络虽然表现出优异的性能，但其不透明性限制了其在需要透明度和人工监管的高风险领域中的应用。概念瓶颈模型(Concept Bottleneck Models, CBMs)通过引入一个人类可理解的概念层来连接输入与决策，从而解决了这一差距，实现了语义解释和测试时干预。本综述从四个维度提供了一个统一的CBMs概览：概念获取、基于概念的决策制定、概念干预和概念评估。我们总结了概念构建的演变过程，从人工标注到基于词典的挖掘、大语言模型(LLM)/视觉语言模型(VLM）引导的生成，以及通过原型和扩散模型实现的视觉关联发现；回顾了超越严格瓶颈的新兴CBM架构；并整合了强调忠实度、稀疏性和可干预性的评估与干预协议，这些对医疗保健等高风险领域尤为重要。我们综合了零散的文献，并勾勒了基于概念的可解释决策面临的关键挑战和未来方向。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p><span style=\"font-size: 16px;\">深度神经网络虽然表现出优异的性能，但其不透明性限制了其在需要透明度和人工监管的高风险领域中的应用。概念瓶颈模型(Concept Bottleneck Models, CBMs)通过引入一个人类可理解的概念层来连接输入与决策，从而解决了这一差距，实现了语义解释和测试时干预。本综述从四个维度提供了一个统一的CBMs概览：概念获取、基于概念的决策制定、概念干预和概念评估。我们总结了概念构建的演变过程，从人工标注到基于词典的挖掘、大语言模型(LLM)/视觉语言模型(VLM）引导的生成，以及通过原型和扩散模型实现的视觉关联发现；回顾了超越严格瓶颈的新兴CBM架构；并整合了强调忠实度、稀疏性和可干预性的评估与干预协议，这些对医疗保健等高风险领域尤为重要。我们综合了零散的文献，并勾勒了基于概念的可解释决策面临的关键挑战和未来方向。</span></p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:bash;gutter:true;\">@article{Wang2026CBMSurvey,\n  title   = {Concept Bottleneck Models for Explainable Decision Making: A Survey of Progress, Taxonomy, and Future Directions},\n  author  = {Wang, Chunjiang and Li, Fan and Hu, Wenbo and Yan, Rui and Zhang, Kun and Zhou, Shaohua Kevin},\n  journal = {ResearchGate Preprint},\n  year    = {2026},\n  doi     = {10.13140/RG.2.2.30356.16002},\n  url     = {https://www.researchgate.net/publication/399898851_Concept_Bottleneck_Models_for_Explainable_Decision_Making_A_Survey_of_Progress_Taxonomy_and_Future_Directions}\n}\n</pre>\n</div>\n<p>&nbsp;This blog is from kkzhang at <a href=\"https://www.cnblogs.com/lemonzhang/p/19592426\" target=\"_blank\">https://www.cnblogs.com/lemonzhang/p/19592426</a>.</p>\n<h1><span lang=\"EN-US\">1 </span><span>引言</span></h1>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>深度神经网络在视觉、语言和多模态学习方面取得了强大的性能，使其在医疗保健</span><sup><span lang=\"EN-US\">[1]</span></sup><span>、医学</span><sup><span lang=\"EN-US\">[2]</span></sup><span>和金融</span><sup><span lang=\"EN-US\">[3]</span></sup><span>等现实世界中得到广泛采用。然而，它们的决策过程往往是不透明的，这在需要信任、问责制和人类监督的高风险环境中造成了风险</span><sup><span lang=\"EN-US\">[4]</span></sup><span>。这种性能与可解释性之间的差距推动了可解释</span><span lang=\"EN-US\">AI (XAI)</span><span>的发展，旨在使模型的推理过程对人类而言是可理解、可验证和可修正的。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念瓶颈模型</span><span lang=\"EN-US\">(CBMs)</span><span>已成为解决这一挑战的一个原则性且有影响力的范式</span><sup><span lang=\"EN-US\">[5]</span></sup><span>。</span><span lang=\"EN-US\">CBMs</span><span>不是直接将输入映射到输出，而是通过在输入和决策之间引入一个人类可理解的中间概念层，显式地对预测过程进行因式分解。这种结构化的分解实现了语义解释，便于专家检查中间推理过程，并通过概念修正支持测试时干预。这些特性将</span><span lang=\"EN-US\">CBMs</span><span>与事后解释技术</span><sup><span lang=\"EN-US\">[6]</span></sup><span>区分开来，并将其定位为可解释和可控决策的统一框架。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>早期的概念瓶颈模型依赖于手动定义和标注的概念（例如视觉属性或临床发现）来提供透明的中间接口，但受到标注成本、覆盖范围不全和标签噪声的限制</span><sup><span lang=\"EN-US\">[5]</span></sup><span>。最近的进展可以组织为基于概念推理的四个阶段：概念获取正从人工策展转向可扩展的词典挖掘、</span><span lang=\"EN-US\">LLM/VLM</span><span>引导的生成，以及通过原型或扩散模型实现的视觉接地发现，这些都得益于大规模基础模型</span><sup><span lang=\"EN-US\">[7, 8, 9]</span></sup><span>；基于概念的决策正从严格的瓶颈向软性、混合、概率和基于能量的设计演变，这些设计在保留概念接口的同时提高了预测能力</span><sup><span lang=\"EN-US\">[10, 11, 12, 13]</span></sup><span>；概念干预越来越多地支持结构化和感知依赖关系的修正，以便通过概念层更好地传播人类反馈</span><sup><span lang=\"EN-US\">[14, 15]</span></sup><span>；概念评估正从准确性扩展到以可解释性为中心的指标（例如忠实度、干预下的一致性、对噪声或缺失概念的鲁棒性），但在视觉接地、语义稳定性和与人类推理对齐方面仍面临挑战</span><sup><span lang=\"EN-US\">[8]</span></sup><span>。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>尽管进展迅速，但</span><span lang=\"EN-US\">CBM</span><span>文献仍然是碎片化的。现有的综述</span><sup><span lang=\"EN-US\">[16]</span></sup><span>和评论通常侧重于属性学习、基于原型的解释或一般的可解释性方法，但它们并未捕捉到</span><span lang=\"EN-US\">CBMs</span><span>作为涵盖概念获取、决策架构、干预机制和评估协议的综合框架的更广泛演变。特别是，</span><span lang=\"EN-US\">CBMs</span><span>与基础模型、可编辑学习和多模态交互的近期融合尚未得到系统性的综合。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>本综述的主要贡献总结如下：</span><span lang=\"EN-US\"> (1) </span><span>提出了涵盖概念获取、决策、干预和评估的</span><span lang=\"EN-US\">CBMs</span><span>统一分类法。</span><span lang=\"EN-US\"> (2) </span><span>系统回顾了概念构建方法，从人工标注到</span><span lang=\"EN-US\">LLM</span><span>引导和基于原型的发现。</span><span lang=\"EN-US\"> (3) </span><span>综合了具有不同瓶颈设计和可编辑程度的新兴</span><span lang=\"EN-US\">CBM</span><span>架构。</span><span lang=\"EN-US\"> (4) </span><span>整合了评估协议和基准，重点关注忠实度、稀疏性、可干预性和医学应用。</span><span>我们在</span><span lang=\"EN-US\"><a href=\"https://github.com/kkzhang95/Awesome_Concept_Bottleneck_Models\" rel=\"noopener nofollow\"><span>Awesome Concept Bottleneck Models</span></a></span><span>维护了一个包含代表性</span><span lang=\"EN-US\">CBM</span><span>论文、代码和基准的更新仓库，以支持复现和社区使用。</span></p>\n<h1><span lang=\"EN-US\">2 </span><span>预备知识</span></h1>\n<h2><span lang=\"EN-US\">2.1 </span><span>概念瓶颈模型</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念瓶颈模型</span><span lang=\"EN-US\">(CBMs)<sup>[5]</sup></span><span>是一类可解释的神经架构，通过称为概念的人类可理解的中间变量，显式地将预测分解为两个阶段。形式上，</span><span lang=\"EN-US\">CBM</span><span>将从输入$x \\in \\mathcal{X}$</span><span>到目标$y \\in \\mathcal{Y}$</span><span>的映射因式分解为：</span><span lang=\"EN-US\"></span></p>\n<p>\\begin{equation}<br />\tx \\;\\rightarrow\\; c \\;\\rightarrow\\; y,<br />\\end{equation}&nbsp;<span>其中$c = (c_1, \\ldots, c_K) \\in \\mathcal{C}^K$</span><span>表示一组可解释的概念，如视觉属性、解剖结构、病理发现或语义描述。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>一个典型的</span><span lang=\"EN-US\">CBM</span><span>由两个组件组成：概念预测器$f_{\\theta}: \\mathcal{X} \\rightarrow \\mathcal{C}^K$</span><span>，它将原始输入映射到概念激活；以及任务预测器$g_{\\phi}: \\mathcal{C}^K \\rightarrow \\mathcal{Y}$</span><span>，它仅基于预测的概念产生最终预测。整体模型可以写为：</span></p>\n<p>\\begin{equation}<br />\t\\hat{y} = g_{\\phi}(f_{\\theta}(x)).<br />\\end{equation}</p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>这种结构实现了</span><span lang=\"EN-US\">CBMs</span><span>的两个决定性能力：</span><span lang=\"EN-US\">(i) </span><span>语义可解释性，因为每个概念维度都与人类可识别的属性对齐；</span><span lang=\"EN-US\">(ii) </span><span>可干预性，因为用户可以在测试时检查或修改</span><span>以纠正推理错误。这些特性激发了本综述稍后讨论的一系列架构变体，包括放宽严格瓶颈约束、结合概念间结构化依赖关系或利用多模态基础模型等外部知识的模型。</span></p>\n<p><img alt=\"image\" class=\"lazyload\" style=\"display: block; margin-left: auto; margin-right: auto;\" />&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 150%; text-align: center;\"><span>图</span><span lang=\"EN-US\">1 CBM</span><span>流程概览。该图展示了概念是如何被获取、整合至决策制定、进行干预以及评估，从而生成可解释预测的。</span></p>\n<h2><span lang=\"EN-US\">2.2 </span><span>分类法</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>图</span><span lang=\"EN-US\">1</span><span>展示了概念瓶颈模型的四部分分类法，而表</span><span lang=\"EN-US\">1</span><span>总结了这些维度上的代表性方法。我们区分了四个主要维度：</span><span lang=\"EN-US\">(1) </span><span>概念获取，关注如何构建人类可解释的概念，范围从专家标注到自动发现和语言诱导的词汇表；</span><span lang=\"EN-US\">(2) </span><span>基于概念的决策，指定概念层如何中介预测，包括严格瓶颈以及松弛或混合设计；</span><span lang=\"EN-US\">(3) </span><span>概念干预，研究人类或外部系统如何在推理时与概念交互以纠正或指导推理；</span><span lang=\"EN-US\">(4) </span><span>概念评估，评估以可解释性为导向的属性，如忠实度、稀疏性和干预下的鲁棒性。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>重要的是，越来越多的工作跨越了这些维度，例如在一个架构中联合集成概念获取、决策和干预。因此，我们依次回顾每个维度并强调代表性方法。</span></p>\n<p><img alt=\"image\" class=\"lazyload\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></p>\n<p style=\"text-align: center;\"><span>表</span><span lang=\"EN-US\">1 </span><span>涵盖概念获取、决策、干预和评估的</span><span lang=\"EN-US\">CBM</span><span>方法的统一分类体系。概念来源：人工标注</span><span lang=\"EN-US\">(manual)</span><span>、词典</span><span lang=\"EN-US\">(dict.)</span><span>、视觉原型</span><span lang=\"EN-US\">(proto.)</span><span>、大型语言模型</span><span lang=\"EN-US\">(LLM)</span><span>；概念空间粒度</span><span lang=\"EN-US\">(Gran.)</span><span>：概念与视觉证据对齐的空间层级：全局</span><span lang=\"EN-US\">(global)</span><span>、局部</span><span lang=\"EN-US\">(local)</span><span>或两者；概念监督</span><span lang=\"EN-US\">(Sup.)</span><span>：强</span><span lang=\"EN-US\">(strong)</span><span>、弱</span><span lang=\"EN-US\">(weak)</span><span>、无监督</span><span lang=\"EN-US\">(unsup.)</span><span>；概念瓶颈层</span><span lang=\"EN-US\">(Bottleneck)</span><span>：</span><span>硬</span><span lang=\"EN-US\">(hard)</span><span>、软</span><span lang=\"EN-US\">(soft)</span><span>、混合</span><span lang=\"EN-US\">(hybrid)</span><span>；概念相关性建模</span><span lang=\"EN-US\">(corr.)</span><span>：独立</span><span lang=\"EN-US\">(indep.)</span><span>、联合</span><span lang=\"EN-US\">(joint)</span><span>、图</span><span lang=\"EN-US\">(graph)</span><span>、层级</span><span lang=\"EN-US\">(hier.)</span><span>、因果</span><span lang=\"EN-US\">(causal)</span><span>；干预机制：数值调整</span><span lang=\"EN-US\">(scalar)</span><span>、参数化</span><span lang=\"EN-US\">(param.)</span><span>、掩码</span><span lang=\"EN-US\">/</span><span>注意力编辑</span><span lang=\"EN-US\">(spatial)</span><span>、自然语言对话</span><span lang=\"EN-US\">(lang.)</span><span>；干预策略：随机</span><span lang=\"EN-US\">/</span><span>人工选择</span><span lang=\"EN-US\">(vanilla)</span><span>、模型驱动建议</span><span lang=\"EN-US\">(active)</span><span>、修正传播</span><span lang=\"EN-US\">(prop.)</span><span>；代码：</span><span lang=\"EN-US\">Link</span><span>（链接）、</span><span lang=\"EN-US\">– (</span><span>未发布</span><span lang=\"EN-US\">)</span><span>。</span></p>\n<h1><span lang=\"EN-US\">3 </span><span>概念获取</span></h1>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念构建是概念瓶颈模型的基础，定义了输入和预测之间的人类可解释接口。概念构建已经从完全的人工标注演变为基于词典的挖掘、</span><span lang=\"EN-US\">LLM/VLM</span><span>引导的生成和视觉原型发现。总体趋势是在提高概念质量、可扩展性和视觉接地的同时减少人工监督。本节回顾四种代表性范式及其核心机制。</span></p>\n<h2><span lang=\"EN-US\">3.1 </span><span>基于人工标注的概念</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>人工标注通过依赖专家定义和显式标记的概念（如视觉属性或临床发现）提供了最强的语义接地。这种范式确保了高可解释性以及与领域知识的清晰对齐，但从根本上受到标注成本、不完全的概念覆盖和标签噪声的限制。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>该范式最近的工作侧重于在不完美监督下提高鲁棒性和灵活性。一个研究方向是通过修改训练目标来解决噪声或缺失标注的问题，例如</span><span lang=\"EN-US\">CPO<sup>[17]</sup></span><span>，它引入基于偏好的优化来联合稳定概念预测和下游性能。一个互补的方向是对标注概念间的依赖关系进行建模，以减轻泄漏和表达能力的限制。</span><span lang=\"EN-US\">Havasi</span><span>等人</span><sup><span lang=\"EN-US\">[18]</span></sup><span>用潜在侧信道变量增强了人工标注的概念，并采用自回归预测器来捕捉概念间的结构。为了支持实际部署，</span><span lang=\"EN-US\">Editable CBMs<sup>[19]</sup></span><span>支持使用基于影响函数的更新对概念定义和数据集标签进行事后修正，而无需完全重新训练。尽管有这些进展，人工标注仍然难以扩展到复杂领域，且其对预定义概念集的依赖限制了对不断变化任务的适应性。</span></p>\n<h2><span lang=\"EN-US\">3.2 </span><span>基于词典的概念构建</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>基于词典的方法用从预定义词汇表（如形容词列表或医学描述符）中的可扩展发现取代了特定任务的人工定义。通过将概念空间限制为语言上有意义的单元，与全自动生成相比，这些方法提供了更好的可控性和透明度。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>代表性方法侧重于从大型词库中过滤出视觉接地的概念。</span><span lang=\"EN-US\">V2C-CBM<sup>[20]</sup></span><span>构建了一个</span><span lang=\"EN-US\">n-gram</span><span>候选池，并使用基于</span><span lang=\"EN-US\">CLIP</span><span>的视觉</span><span lang=\"EN-US\">-</span><span>语言相似度来移除与视觉无关或接地较弱的条目，产生紧凑且可解释的概念集。</span><span lang=\"EN-US\">OpenCBM<sup>[9]</sup></span><span>进一步将可训练的视觉特征与</span><span lang=\"EN-US\">CLIP</span><span>嵌入对齐，并引入了发现缺失概念的机制，从而提高了超出初始词典的覆盖范围。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>虽然基于词典的构建降低了标注成本并限制了幻觉风险，但其表达能力仍然受到预定义词汇表的限制，可能无法捕捉细粒度或特定任务的语义。</span></p>\n<h2><span lang=\"EN-US\">3.3 </span><span lang=\"EN-US\">LLM/VLM</span><span>引导的概念生成</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span lang=\"EN-US\">LLM</span><span>和</span><span lang=\"EN-US\">VLM</span><span>引导的概念生成通过利用大型语言模型产生多样化、高层次且通常是分层的语义，大幅扩展了概念空间。这种范式提供了强大的可扩展性和灵活性，但也引入了与幻觉、弱视觉接地和语义不稳定性相关的新挑战。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>最近的工作通过结构、对齐和统计正则化来解决这些问题。</span><span lang=\"EN-US\">Med-MICN<sup>[21]</sup></span><span>将概念组织成多层级结构，并采用门控机制来控制抽象程度。视觉</span><span lang=\"EN-US\">-</span><span>语言一致性约束</span><sup><span lang=\"EN-US\">[22, 23, 24]</span></sup><span>和提示级对齐策略</span><sup><span lang=\"EN-US\">[25]</span></sup><span>进一步提高了接地的可靠性。从监督角度来看，</span><span lang=\"EN-US\">Label-free CBMs</span><span>和</span><span lang=\"EN-US\">FCBM<sup>[26, 27]</sup></span><span>证明，只要施加了强大的视觉正则化，通过将</span><span lang=\"EN-US\">LLM</span><span>与</span><span lang=\"EN-US\">CLIP</span><span>结合，可以在没有显式标注的情况下诱导出可用概念。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>总体而言，</span><span lang=\"EN-US\">LLM/VLM</span><span>引导的概念获取显著降低了人类监督要求，但其可解释性最终取决于语言先验的可靠性和接地约束的有效性。</span></p>\n<h2><span lang=\"EN-US\">3.4 </span><span>基于视觉原型的概念发现</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>基于视觉原型的方法通过提取代表性区域、部件或解缠组件，直接从图像空间学习概念，产生强大的视觉接地和空间定位。这种范式对于医学成像和其他形态学证据对决策至关重要的领域特别有吸引力。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>基于原型的发现面临固有的挑战，包括不稳定的语义命名、显著性驱动的虚假相关性以及原型粒度与覆盖范围之间的权衡。最近的工作探索了几种设计模式来减轻这些限制。面向解缠的方法，如</span><span lang=\"EN-US\">HU-MCD</span><span>、</span><span lang=\"EN-US\">LCBM<sup>[28, 29]</sup></span><span>，学习多维概念因子以减少语义未指明性。面向部署的方法</span><sup><span lang=\"EN-US\">[30, 31]</span></sup><span>专注于从预训练模型中提取原型词典以实现事后可解释性。面向交互的设计</span><sup><span lang=\"EN-US\">[32, 33]</span></sup><span>显式地将原型与空间区域关联，支持区域级的检查和干预。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>尽管接地保真度有所提高，但在数据集之间实现视觉原型的稳定语义对齐和可扩展重用仍然是一个开放的挑战。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>总结。</span></strong><span lang=\"EN-US\"> CBMs</span><span>中的概念获取涵盖了从专家定义的语义到自动发现的范围，反映了语义精度、监督成本和鲁棒性之间的权衡，并激发了整合语言和视觉线索的混合设计。</span></p>\n<h1><span lang=\"EN-US\">4 </span><span>基于概念的决策</span></h1>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念瓶颈模型</span><span lang=\"EN-US\">(CBMs)</span><span>通过可解释概念层中介预测来提高可解释性，使验证和干预成为可能。除了概念构建，</span><span lang=\"EN-US\">CBM</span><span>架构也在不断演变以增强灵活性和可部署性。本节回顾三个关键方向：瓶颈设计、概念监督和模型适应，强调了从硬性流程向自适应、与人类对齐的决策转变。</span></p>\n<h2><span lang=\"EN-US\">4.1 </span><span>瓶颈设计</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span lang=\"EN-US\">CBMs</span><span>的一项核心创新在于概念瓶颈如何通过中介输入和输出之间的信息流来管理决策过程。早期的设计强制执行严格的流程，即所有预测信号必须通过离散的、可解释的概念层，从而最大化透明度和可干预性。然而，这种刚性结构往往限制了表示能力，并在不完美的概念监督下损害性能。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>为了解决这些限制，引入了软瓶颈</span><sup><span lang=\"EN-US\">[33, 34]</span></sup><span>，实现了连续的概念激活以及概念和任务预测器的联合优化。随后的变体通过残差通路</span><sup><span lang=\"EN-US\">[49]</span></sup><span>和结构化依赖关系放宽了严格瓶颈，包括自回归序列</span><sup><span lang=\"EN-US\">[15]</span></sup><span>、图先验</span><sup><span lang=\"EN-US\">[61, 52]</span></sup><span>和随机潜变量</span><sup><span lang=\"EN-US\">[57]</span></sup><span>。</span><span lang=\"EN-US\">Energy-based CBMs<sup>[60]</sup></span><span>进一步在联合概率框架内统一了预测和干预。更近期的工作将更高层次的结构注入瓶颈，例如用于鲁棒干预的因果图</span><sup><span lang=\"EN-US\">[6]</span></sup><span>，用于减轻概念</span><span lang=\"EN-US\">-</span><span>标签失真的概念解耦</span><sup><span lang=\"EN-US\">[64]</span></sup><span>，以及用于在复杂设置中超越线性聚合的可微逻辑组合</span><sup><span lang=\"EN-US\">[58, 13]</span></sup><span>。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>这些进展共同反映了严格瓶颈向更具表达力和结构化架构的逐渐放松，使</span><span lang=\"EN-US\">CBMs</span><span>能够在复杂的现实环境中更好地平衡可解释性和预测能力，同时保留概念层作为解释和干预的对齐语义接口。</span></p>\n<h2><span lang=\"EN-US\">4.2 </span><span>概念监督</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念监督定义了</span><span lang=\"EN-US\">CBMs</span><span>的中间语义层是如何构建和接地的。虽然早期模型依赖于具有强领域语义的人工标注概念</span><sup><span lang=\"EN-US\">[5]</span></sup><span>，但这种监督通常成本高昂、稀疏或特定于领域。因此，最近的研究寻求可扩展到更广泛任务且同时保留语义可解释性的替代方案。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span lang=\"EN-US\">Post-hoc CBMs<sup>[44]</sup></span><span>通过在冻结的编码器之上学习概念预测器来绕过架构约束，无需重新训练即可恢复概念可解释性。基于探测的方法</span><sup><span lang=\"EN-US\">[45]</span></sup><span>和原型发现方法</span><sup><span lang=\"EN-US\">[32, 30]</span></sup><span>使用稀疏正则化或基于部件的分解从预训练特征中提取可解释单元。利用视觉</span><span lang=\"EN-US\">-</span><span>语言模型，基于</span><span lang=\"EN-US\">CLIP</span><span>的流程</span><sup><span lang=\"EN-US\">[46, 26, 47]</span></sup><span>将概念标签与文本提示对齐，实现开放词汇监督。</span><span lang=\"EN-US\">LLM</span><span>引导的策略</span><sup><span lang=\"EN-US\">[21, 22, 48, 49]</span></sup><span>更进一步，通过生成分层或特定任务的概念，尽管视觉接地和幻觉方面的挑战仍然存在。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>这些方法将概念监督从直接标注转变为基于对齐和提示的归纳，实现了可扩展和自适应的概念构建。这种演变保留了</span><span lang=\"EN-US\">CBMs</span><span>与人类对齐的语义，同时提高了跨任务和领域的灵活性。</span></p>\n<h2><span lang=\"EN-US\">4.3 </span><span>模型适应</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>为了使</span><span lang=\"EN-US\">CBMs</span><span>在现实世界和不断变化的环境中具有实用价值，它们必须支持动态适应。具有固定概念的静态流程在任务需求转变、新概念出现或需要用户修正时难以更新。因此，模型适应已成为可操作</span><span lang=\"EN-US\">CBMs</span><span>的一个关键前沿。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span lang=\"EN-US\">Editable CBMs<sup>[19]</sup></span><span>引入了基于影响函数的参数更新，允许对概念、实例或标签预测进行局部更改，而无需完全重新训练。这允许在保留模型整体结构的同时进行细粒度的修正和数据集更新。概率能量模型</span><sup><span lang=\"EN-US\">[13]</span></sup><span>通过将推理公式化为输入、概念和输出上的能量最小化来支持测试时修正。更具交互性的是，最近的方法如</span><span lang=\"EN-US\">Chat-CBM<sup>[50]</sup></span><span>和</span><span lang=\"EN-US\">SALF<sup>[51]</sup></span><span>扩展了模型接口以支持自然语言和基于空间区域的干预，使非技术用户也能进行适应。无需训练的测试时适应</span><sup><span lang=\"EN-US\">[52, 53]</span></sup><span>进一步使</span><span lang=\"EN-US\">CBMs</span><span>能够仅使用少量图像级标签处理概念级分布偏移，而无需重新训练或概念监督。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>这些进展标志着从静态可解释性向动态、人在回路决策的转变。适应性</span><span lang=\"EN-US\">CBMs</span><span>保留了其核心语义瓶颈，同时允许部署后编辑、修正和协作，扩展了其在现实世界高风险系统中的生存能力。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>总结。</span></strong><span lang=\"EN-US\"> CBMs</span><span>中的基于概念的决策已从严格的瓶颈管道演变为更灵活的架构，更好地平衡了可解释性和性能。瓶颈设计、监督和模型适应方面的进步将概念层定位为用于预测和干预的动态接口。</span></p>\n<h1><span lang=\"EN-US\">5 </span><span>概念干预</span></h1>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念干预通过修正中间概念实现人类与</span><span lang=\"EN-US\">AI</span><span>的协作，将</span><span lang=\"EN-US\">CBMs</span><span>从被动的可解释模型转变为交互式系统。在</span><span lang=\"EN-US\">Koh</span><span>等人</span><sup><span lang=\"EN-US\">[5]</span></sup><span>的开创性工作中，干预被公式化为允许用户修改概念激活值的测试时编辑。虽然具有基础意义，但这种范式依赖于强独立性假设，产生高昂的人力成本，并支持有限的交互模式。</span></p>\n<h2><span lang=\"EN-US\">5.1 </span><span>概念相关性</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>早期的干预机制通常假设概念是独立的，这意味着修改一个概念不会影响其他概念。这种假设在现实世界设置中通常是不现实的，因为概念在复杂决策任务中实际上表现出很强的语义和统计依赖性。为了解决这一限制，相关性感知的干预方法显式地对概念间关系进行建模并据此传播修正。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span lang=\"EN-US\">Havasi</span><span>等人</span><sup><span lang=\"EN-US\">[18]</span></sup><span>提出了自回归概念预测器，允许对前概念的干预通过序列依赖建模影响后续预测。</span><span lang=\"EN-US\">Stochastic CBMs<sup>[39]</sup></span><span>通过将概念</span><span lang=\"EN-US\">Logits</span><span>建模为多元正态分布，进一步提高了效率，实现了通过学习到的协方差结构即时传播单个干预。</span><span lang=\"EN-US\">Energy-Based CBMs<sup>[13]</sup></span><span>采用输入、概念和输出上的联合能量公式，允许概念修正通过能量最小化进行传播。</span><span lang=\"EN-US\">Singhi</span><span>等人</span><sup><span lang=\"EN-US\">[38]</span></sup><span>引入了事后概念重对齐模块</span><span lang=\"EN-US\">(CIRM)</span><span>，根据学习到的统计相关性更新未干预的概念。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>虽然这些方法显著提高了干预的一致性和效率，但它们关键地依赖于学习到的相关性的质量。如果捕捉到虚假依赖关系，干预可能会传播错误而不是修正，这突显了一个重要的开放挑战。</span></p>\n<h2><span lang=\"EN-US\">5.2 </span><span>概念定位</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>确定干预哪些概念仍然是实际部署中的主要瓶颈，因为人工选择是劳动密集的且对认知具有高要求的。为了减轻这一负担，最近的工作已转向模型引导的定位策略，优先考虑高影响力的干预目标。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>一些方法将定位内化到训练或模型设计中。</span><span lang=\"EN-US\">Espinosa Zarlenga</span><span>等人</span><sup><span lang=\"EN-US\">[10]</span></sup><span>引入了感知干预的训练，在学习过程中模拟测试时干预，使模型能够推荐需要修正的概念。</span><span lang=\"EN-US\">Evidential CEM(evi-CEM)<sup>[54]</sup></span><span>将概念预测建模为</span><span lang=\"EN-US\">Beta</span><span>分布，利用认知不确定性指导对模糊概念的干预。其他方法依赖于事后分析或辅助结构。</span><span lang=\"EN-US\">Shin</span><span>等人</span><sup><span lang=\"EN-US\">[14]</span></sup><span>提出了基于不确定性的概念潜力</span><span lang=\"EN-US\">(UCP)</span><span>和概念预测损失</span><span lang=\"EN-US\">(LCP)</span><span>等指标来对干预优先级进行排序。</span><span lang=\"EN-US\">Shen</span><span>等人</span><sup><span lang=\"EN-US\">[55]</span></sup><span>进一步引入了</span><span lang=\"EN-US\">FIGS-BD</span><span>，将预测器蒸馏为贪婪求和树，以识别具有高贡献方差的概念组。</span></p>\n<h2><span lang=\"EN-US\">5.3 </span><span>概念交互</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>除了选择和修正概念值之外，最近的工作越来越多地探索更丰富的交互模式，这些模式对人类用户来说更直观</span><sup><span lang=\"EN-US\">[56]</span></sup><span>。</span><span lang=\"EN-US\">Chat-CBM<sup>[50]</sup></span><span>用大语言模型替换基于概念的预测器，允许用户通过自然语言对话进行干预。虽然这在实践中提高了可用性，但可能会削弱传统</span><span lang=\"EN-US\">CBMs</span><span>的严格可解释性保证。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>在视觉领域，</span><span lang=\"EN-US\">Concept-based Similarity Reasoning (CSR)<sup>[33]</sup></span><span>通过允许用户绘制边界框来引导模型注意力，实现了空间交互。然而，这种空间引导并没有显式地解耦不同概念间的干预。</span><span lang=\"EN-US\">Spatially-Aware and Label-Free (SALF) CBM<sup>[51]</sup></span><span>通过利用空间概念图解决了这一限制，使用户能够指定感兴趣的区域并选择性地调节该区域内单个概念的激活。除了判别模型，概念瓶颈生成模型将概念级交互扩展到</span><span lang=\"EN-US\">GANs</span><span>、</span><span lang=\"EN-US\">VAEs</span><span>和扩散模型，实现了生成的可解释引导和调试</span><sup><span lang=\"EN-US\">[57]</span></sup><span>。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>总结。</span></strong><span>概念干预将</span><span lang=\"EN-US\">CBMs</span><span>从静态解释模型转变为交互式的人在回路系统。通过建模概念相关性、实现干预目标的有效定位以及支持更丰富的交互模式，最近的进展提高了概念级修正的有效性和可用性。</span></p>\n<h1><span lang=\"EN-US\">6 </span><span>概念瓶颈模型的评估</span></h1>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念瓶颈模型引入了可解释和可干预的中间概念，这要求评估协议超越下游任务性能。因此，现有的评估框架评估三个核心维度：概念忠实度、概念稀疏性和可干预性，从而捕捉语义有效性、认知易处理性和人在回路的可控性。</span></p>\n<h2><span lang=\"EN-US\">6.1 </span><span>概念忠实度</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念忠实度衡量学习到的瓶颈表示是否与人类可理解的语义对齐，这是实践中可靠解释和有效干预的前提。评估策略已从直接标签匹配演变为对内在结构和外部接地的更严格评估。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>标签对齐。</span></strong><span>一种常见的方法是使用标准指标（如准确率和</span><span lang=\"EN-US\">F1</span><span>分数）评估预测概念与真实标注之间的一致性。然而，在高维和稀疏概念空间中，这些指标可能由真负样本主导。为了减轻这种偏差，最近的工作强调使用</span><span lang=\"EN-US\">Jaccard</span><span>相似度</span><sup><span lang=\"EN-US\">[58]</span></sup><span>和概念存在度量</span><span lang=\"EN-US\">(CEM)<sup>[59]</sup></span><span>等指标进行主动概念评估，这些指标侧重于被正向激活的概念。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>内在表示质量。</span></strong><span>除了标签匹配外，有效的概念表示应具有连贯的拓扑结构和明显的可分离性。在基于嵌入的模型中，概念对齐分数</span><span lang=\"EN-US\">(CAS)<sup>[10]</sup></span><span>解决了的前者的评估，验证高维空间中的局部邻域是否保持语义同质性。作为补充，为了在基于标量的瓶颈中解决的后者的评估，</span><span lang=\"EN-US\">Gap<sup>[60, 61]</sup></span><span>通过测量正负激活分布之间的散度来评估潜在空间的可分性，从而量化表示的内在判别质量。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>语义和视觉接地。</span></strong><span>为了检测虚假相关性，进一步的评估将学习到的概念与外部语义或视觉参考对齐。语义接地指标，包括</span><span lang=\"EN-US\">CGIM<sup>[59]</sup></span><span>、</span><span lang=\"EN-US\">Similarity<sup> [36]</sup></span><span>和概念纯度</span><sup><span lang=\"EN-US\">[11]</span></sup><span>，评估与人类定义的重要性或与规范嵌入的一致性。视觉接地指标，如概念可信度分数</span><sup><span lang=\"EN-US\">[32]</span></sup><span>和</span><span lang=\"EN-US\">CLM<sup>[59]</sup></span><span>，量化概念激活与标注区域之间的空间对齐。虽然在实践中有效，但这些方法通常需要昂贵的标注；基于</span><span lang=\"EN-US\">VLM</span><span>的替代方案如概念</span><span lang=\"EN-US\">-</span><span>图像相关性</span><sup><span lang=\"EN-US\">[11]</span></sup><span>减少了监督，但引入了潜在的幻觉风险。</span></p>\n<h2><span lang=\"EN-US\">6.2 </span><span>概念稀疏性</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念稀疏性评估</span><span lang=\"EN-US\">CBMs</span><span>是否依赖于一组最小且认知上可管理的概念。过度密集的瓶颈破坏了可解释性并增加了信息泄漏的风险。稀疏度</span><sup><span lang=\"EN-US\">[58]</span></sup><span>和有效概念数</span><span lang=\"EN-US\">(NEC)<sup>[62]</sup></span><span>等指标量化了激活密度和推理复杂性。然而，稀疏性必须与任务性能联合考虑。为了捕捉这种权衡，概念利用效率</span><span lang=\"EN-US\">(CUE)<sup>[36]</sup></span><span>和概念高效准确率</span><span lang=\"EN-US\">(CEA)<sup>[63]</sup></span><span>等复合指标在预测准确率与概念使用之间进行平衡，其中</span><span lang=\"EN-US\">CEA</span><span>提供了一个有界的、文本不变的公式以进行稳健比较。</span></p>\n<h2><span lang=\"EN-US\">6.3 </span><span>可干预性</span></h2>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>可干预性反映了</span><span lang=\"EN-US\">CBMs</span><span>在推理时支持有效人类修正的能力。此类别的评估协议评估模型预测对概念级修正的响应能力以及干预策略的效率。测试时干预</span><span lang=\"EN-US\">(TTI)</span><span>曲线</span><sup><span lang=\"EN-US\">[5]</span></sup><span>及其曲线下面积变体</span><sup><span lang=\"EN-US\">[64]</span></sup><span>被广泛用于总结在增加干预预算下的性能增益。扩展这一分析，最近的工作检查了相关概念间的修正传播</span><sup><span lang=\"EN-US\">[39]</span></sup><span>，并通过经验用户时间</span><sup><span lang=\"EN-US\">[60]</span></sup><span>或理论复杂性</span><sup><span lang=\"EN-US\">[14, 65]</span></sup><span>显式地建模干预成本。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>总结。</span></strong><span>评估</span><span lang=\"EN-US\">CBMs</span><span>需要超越下游准确率的整体协议。概念忠实度确保语义有效性，概念稀疏性保持认知易处理性，可干预性量化人在回路修正的有效性和成本。这些维度共同构成了评估</span><span lang=\"EN-US\">CBMs</span><span>可解释性、可靠性和实用性的原则基础。</span></p>\n<h1><span lang=\"EN-US\">7 </span><span>基准</span></h1>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>表</span><span lang=\"EN-US\">2</span><span>总结了用于评估一般和医疗领域概念瓶颈模型</span><span lang=\"EN-US\">(CBMs)</span><span>的代表性基准。这些数据集在概念标注来源、监督粒度和对概念级干预的支持方面存在显著差异，这反过来决定了</span><span lang=\"EN-US\">CBMs</span><span>的哪些方面（如忠实度、可扩展性或可干预性）可以得到可靠评估。与深度模型不同，</span><span lang=\"EN-US\">CBMs</span><span>依赖于基准设计来验证概念忠实度和干预有效性。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>一般领域数据集。</span></strong><span>一般领域基准在视觉和文本识别任务上评估</span><span lang=\"EN-US\">CBMs</span><span>，包括细粒度分类、零样本学习和</span><span lang=\"EN-US\">NLP</span><span>应用。一个关键区别在于概念监督。具有人工标注概念的数据集，如</span><span lang=\"EN-US\">CUB-200-2011</span><span>和</span><span lang=\"EN-US\">AWA2</span><span>，支持概念评估和测试时干预，使概念修正的因果分析成为可能。相比之下，依赖</span><span lang=\"EN-US\">LLM</span><span>或</span><span lang=\"EN-US\">VLM</span><span>生成概念的大规模基准评估可扩展性和开放词汇能力，但缺乏经过验证的真实值，因此无法支持基于干预的评估。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24.1pt; line-height: 20pt;\"><strong><span>医疗领域数据集。</span></strong><span>医疗基准强调临床上有意义的概念和安全关键的可解释性，使其特别适合评估概念忠实度和干预有效性。具有专家标注、有序或连续概念的数据集显式支持干预实验，并能够验证现实临床设置中人在回路的修正。依赖自动生成概念的大型医疗数据集提高了覆盖范围和可扩展性，但缺乏经过验证的概念标注通常限制了它们进行严格干预和因果分析的适用性。</span></p>\n<p><img alt=\"image\" class=\"lazyload\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></p>\n<p style=\"text-align: center;\">&nbsp;<span>表</span><span lang=\"EN-US\">2 </span><span>通用和医疗领域中评估概念瓶颈模型</span><span lang=\"EN-US\">(CBMs)</span><span>的代表性基准</span></p>\n<h1><span lang=\"EN-US\">8 </span><span>总结与未来方向</span></h1>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span>概念瓶颈模型通过将预测建立在人类可理解的概念之上，为可解释和可控的决策提供了一个原则性框架。本综述统一了涵盖概念获取、决策、干预和评估全流程的</span><span lang=\"EN-US\">CBM</span><span>研究，特别强调了高风险医疗应用。尽管取得了实质性进展，但仍存在若干基本挑战。</span></p>\n<p class=\"MsoNormal\" style=\"text-indent: 24pt; line-height: 20pt;\"><span lang=\"EN-US\">CBMs</span><span>的未来研究方向包括：</span><strong><span lang=\"EN-US\">(1) </span></strong><strong><span>自适应概念获取。</span></strong><span>未来的</span><span lang=\"EN-US\">CBMs</span><span>应超越固定的瓶颈设计，转向自适应和结构化的概念表示，如分层、组合或稀疏激活的瓶颈。这种设计更好地反映了人类语义的多层次和任务依赖性质，同时提高了效率。</span><span> <strong><span lang=\"EN-US\">(2) </span></strong></span><strong><span>忠实可靠的概念。</span></strong><span>确保学习到的概念忠实地对应其预期语义仍然是一个核心挑战，特别是在弱监督下或概念源自</span><span lang=\"EN-US\">LLMs</span><span>或</span><span lang=\"EN-US\">VLMs</span><span>时。有前景的方向包括不确定性感知建模、减轻虚假相关性的因果表示学习，以及检测或纠正不稳定概念的机制。</span><span> <strong><span lang=\"EN-US\">(3) </span></strong></span><strong><span>结构化概念干预。</span></strong><span>除了直接的概念替换，未来的工作应探索结构化干预策略，考虑概念间的依赖关系并识别决策关键子集。支持部分或软干预对于减少工作量并在安全关键领域实现可扩展的专家在环部署至关重要。</span><span> <strong><span lang=\"EN-US\">(4) </span></strong></span><strong><span>鲁棒的概念评估。</span></strong><span>鲁棒评估对于可靠部署仍然是必要的。未来的研究应审查概念表示和干预效果如何在跨领域和偏移中泛化，以及基于概念的推理如何支持不确定性估计、分布外检测和感知干预的评估协议。</span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">&nbsp;</span></p>\n<h1 align=\"center\" style=\"text-align: center;\"><span>参考文献</span></h1>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[1] Andre Esteva, Alexandre Robicquet, et al. A guide to deep learning in healthcare. In Nat. Med., 2019. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[2] Berthine Nyunga Mpinda, Mehran Hosseinzadeh, et al. Towards multi-label concept bottleneck models in medical imaging: An exploratory survey. In Medical Imaging with Deep Learning-Validation Papers, 2026. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[3] Ahmet Murat Ozbayoglu, Mehmet Ugur Gudelek, et al. Deep learning for financial applications: A survey. In Appl. Soft Comput., 2020. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[4] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. In Artif. Intell, 2019.</span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[5] Pang Wei Koh, Thao Nguyen, et al. Concept bottleneck models. In ICML, 2020. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[6] Ramprasaath R Selvaraju, Michael Cogswell, et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[7] Alec Radford, Jong Wook Kim, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.</span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[8] Yue Yang, Artemis Panagopoulou, et al. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. In CVPR, 2023. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[9] Andong Tan, Fengtao Zhou, et al. Explain via any concept: Concept bottleneck model with open vocabulary concepts. In ECCV, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[10] Mateo Espinosa Zarlenga, Pietro Barbiero, et al. Concept embedding models: Beyond the accuracy-explainability trade-off. In NeurIPS, 2022. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[11] Yang Liu, Tianwei Zhang, et al. Hybrid concept bottleneck models. In CVPR, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[12] Eunji Kim, Dahuin Jung, et al. Probabilistic concept bottleneck models. In ICML, 2023. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[13] Xinyue Xu, Yi Qin, et al. Energy-based concept bottleneck models: Unifying prediction, concept intervention, and probabilistic interpretations. In ICLR. 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[14] Sungbin Shin, Yohan Jo, et al. A closer look at the intervention procedure of concept bottleneck models. In ICML, 2023. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[15] Sujin Jeon, Inwoo Hwang, et al. Locality-aware concept bottleneck model. In UniReps. 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[16] Junlin Hou, Sicen Liu, et al. Self-explainable ai for medical image analysis: A survey and new outlooks. In arXiv preprint arXiv:2410.02331, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[17] Emiliano Penaloza, Tianyue H Zhang, et al. Addressing concept mislabeling in concept bottleneck models through preference optimization. In ICML, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[18] Marton Havasi, Sonali Parbhoo, et al. Addressing leakage in concept bottleneck models. In NeurIPS, 2022. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[19] Lijie Hu, Chenyang Ren, et al. Editable concept bottleneck models. In ICML, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[20] Hangzhou He, Lei Zhu, et al. V2c-cbm: Building concept bottlenecks with vision-to-concept tokenizer. In AAAI, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[21] Lijie Hu, Songning Lai, et al. Towards multi-dimensional explanation alignment for medical classification. In NeurIPS, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[22] Songning Lai, Lijie Hu, et al. Faithful vision-language interpretation via concept bottleneck models. In ICLR, 2023. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[23] Sukrut Rao, Sweta Mahajan, et al. Discover-then-name: Task-agnostic concept bottlenecks via automated concept discovery. In ECCV, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[24] Simon Schrodi, Julian Schur, et al. Selective concept bottlenecks without predefined concepts. In TMLR, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[25] Yequan Bie, Luyang Luo, et al. Xcoop: Explainable prompt learning for computer-aided diagnosis via concept-guided context optimization. In MICCAI, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[26] Tuomas Oikarinen, Subhro Das, et al. Label-free concept bottleneck models. In ICLR. 2023. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[27] Xingbo Du, Qiantong Dou, et al. Flexible concept bottleneck model. In AAAI, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[28] Arne Grobrügge, Niklas Kühl, et al. Towards human-understandable multi-dimensional concept discovery. In CVPR, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[29] Sujin Jeon, Inwoo Hwang, et al. Locality-aware concept bottleneck model. In UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[30] Andong Tan, ZHOU Fengtao, et al. Post-hoc part-prototype networks. In ICML, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[31] Mohammad Amin Choukali, Mehdi Chehel Amirani, et al. Pseudo-class part prototype networks for interpretable breast cancer classification. In Sci. Rep, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[32] Qihan Huang, Jie Song, et al. On the concept trustworthiness in concept bottleneck models. In AAAI, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[33] Ta Duc Huy, Sen Kim Tran, et al. Interactive medical image analysis with concept-based similarity reasoning. In CVPR, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[34] Anita Mahinpei, Justin Clark, et al. Promises and pitfalls of black-box concept learning models. In arXiv preprint arXiv:2106.13314, 2021. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[35] Andrei Margeloiu, Matthew Ashman, et al. Do concept bottleneck models learn as intended? In ICLR Workshop on Responsible AI, 2021. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[36] Chenming Shang, Shiji Zhou, et al. Incremental residual concept bottleneck models. In CVPR. 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[37] Haotian Xu, Tsui-Wei Weng, et al. Graph concept bottleneck models. In arXiv preprint arXiv:2508.14255, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[38] Nishad Singhi, Jae Myung Kim, et al. Improving intervention efficacy via concept realignment in concept bottleneck models. In ECCV, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[39] Moritz Vandenhirtz, Sonia Laguna, et al. Stochastic concept bottleneck models. In NeurIPS, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[40] Giovanni De Felice, Arianna Casanova, et al. Causally reliable concept bottleneck models. In ICLR 2025 Workshop: XAI4Science: From Understanding Model Behavior to Discovering New Scientific Knowledge, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[41] Rui Zhang, Xingbo Du, et al. The decoupling concept bottleneck model. In TPAMI, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[42] Deepika SN Vemuri, Gautham Bellamkonda, et al. Logiccbms: Logic-enhanced concept-based learning. In WACV, 2026. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[43] Yibo Gao, Hangqi Zhou, et al. Learning concept-driven logical rules for interpretable and generalizable medical image classification. In MICCAI, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[44] Mert Yuksekgonul, Maggie Wang, et al. Post-hoc concept bottleneck models. In ICLR, 2023. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[45] Andrei Semenov, Vladimir Ivanov, et al. Sparse concept bottleneck models: Gumbel tricks in contrastive learning. In arXiv preprint arXiv:2404.03323, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[46] Rémi Kazmierczak, Eloïse Berthier, et al. Clip-qda: An explainable concept bottleneck model. In TMLR, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[47] Jiakai Lin, Jinchang Zhang, et al. Graph integrated multimodal concept bottleneck model. In arXiv preprint arXiv:2510.00701, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[48] Yunhe Gao, Difei Gu, et al. Aligning human knowledge with visual concepts towards explainable medical image classification. In MICCAI, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[49] Chunjiang Wang, Kun Zhang, et al. Mvp-cbm: Multi-layer visual preference-enhanced concept bottleneck model for explainable medical image classification. In IJCAI, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[50] Hangzhou He, Lei Zhu, et al. Chat-cbm: Towards interactive concept bottleneck models with frozen large language models. In arXiv preprint arXiv:2509.17522, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[51] Itay Benou and Tammy Riklin Raviv. Show and tell: Visually explainable deep neural nets via spatially-aware concept bottleneck models. In CVPR, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[52] Townim F Chowdhury, Vu Minh Hieu Phan, et al. Adacbm: An adaptive concept bottleneck model for explainable and accurate diagnosis. In MICCAI, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[53] Hangzhou He, Jiachen Tang, et al. Training-free test-time improvement for explainable medical image classification. In MICCAI, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[54] Yibo Gao, Zheyao Gao, et al. Evidential concept embedding models: Towards reliable concept explanations for skin disease diagnosis. In MICCAI, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[55] Matthew Shen, Aliyah R Hsu, et al. Adaptive test-time intervention for concept bottleneck models. In ICLR 2025 Workshop Building Trust, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[56] David Steinmann, Wolfgang Stammer, et al. Learning to intervene on concept bottlenecks. In ICML, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[57] Aya Abdelsalam Ismail, Julius Adebayo, et al. Concept bottleneck generative models. In ICLR, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[58] Konstantinos P Panousis, Dino Ienco, et al. Coarse-to-fine concept bottleneck models. In NeurIPS, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[59] Halil Ibrahim AYSEL, Xiaohao Cai, et al. Concept-based explainable artificial intelligence: Metrics and benchmarks. In Available at SSRN 5210908. 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[60] Nesta Midavaine, Gregory Hok Tjoan Go, et al. [re] on the reproducibility of post-hoc concept bottleneck models. In TMLR, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[61] Seonghwan Park, Jueun Mun, et al. An analysis of concept bottleneck models: Measuring, understanding, and mitigating the impact of noisy annotations. In NeurIPS, 2025. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[62] Divyansh Srivastava, Ge Yan, et al. Vlg-cbm: Training concept bottleneck models with vision-language guidance. In NeurIPS, 2024. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[63] Delong Zhao, Qiang Huang, et al. Partially shared concept bottleneck models. In AAAI, 2026. </span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[64] Mateo Espinosa Zarlenga, Katie Collins, et al. Learning to receive help: Intervention-aware concept embedding models. In NeurIPS, 2023.</span></p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">[65] Andrea Pugnana, Riccardo Massidda, et al. Deferring concept bottleneck models: Learning to defer interventions to inaccurate experts. In NeurIPS. 2025.</span></p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"line-height: 20pt;\"><span lang=\"EN-US\">&nbsp;</span></p>\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-08 19:27</span>&nbsp;\n<a href=\"https://www.cnblogs.com/lemonzhang\">kkzhang</a>&nbsp;\n阅读(<span id=\"post_view_count\">12</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "GitHub Pages 技术文档站点搭建实践指南",
      "link": "https://www.cnblogs.com/GlenTt/p/19592379",
      "published": "",
      "description": "<h2>\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/GlenTt/p/19592379\" id=\"cb_post_title_url\" title=\"发布于 2026-02-08 18:56\">\n    <span>GitHub Pages 技术文档站点搭建实践指南</span>\n    \n\n</a>\n\n\t\t</h2>\n\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"github-pages-技术文档站点搭建实践指南\">GitHub Pages 技术文档站点搭建实践指南</h1>\n<h2 id=\"1-开发者的实际需求\">1. 开发者的实际需求</h2>\n<p>作为开发者，我们经常需要将技术笔记、项目文档或学习成果以网站形式对外展示。这种展示方式相比简单的代码仓库浏览具有明显优势，包括统一的导航结构、专业的视觉呈现、便捷的搜索功能以及更好的阅读体验。本文将详细介绍如何使用 MkDocs 和 GitHub Pages 构建这样一个技术文档网站，特别是如何正确处理 Jupyter Notebook 文件的展示。例如：<a href=\"https://gritjw.github.io/hot_100/\" rel=\"noopener nofollow\" target=\"_blank\">https://gritjw.github.io/hot_100/</a></p>\n<h2 id=\"2-两种展示方式的本质区别\">2. 两种展示方式的本质区别</h2>\n<p>在 GitHub 上展示内容有两种截然不同的方式，理解它们的区别是后续工作的基础。</p>\n<p>第一种是直接利用 GitHub 仓库的文件预览功能。当你将 Markdown 文件或 Jupyter Notebook 文件推送到仓库后，GitHub 会自动渲染这些文件的内容。访问者可以在仓库界面点击文件名查看格式化后的内容，包括代码、文本、数学公式和图表输出。这种方式无需任何配置，但本质上仍然是文件浏览而非网站体验。访问者看到的是仓库的目录树结构，缺少统一的首页、导航菜单和主题样式。</p>\n<p>第二种是使用静态站点生成器构建完整的网站，通过 GitHub Pages 托管。这种方式下，访问者访问的是一个具有完整网站特征的页面，拥有自定义的首页、侧边栏导航、全文搜索和统一的视觉风格。但 GitHub Pages 本身只是一个静态文件托管服务，它不会自动将你的 Markdown 或 Notebook 文件转换成网页。要实现这种展示效果，必须先使用 MkDocs、Jupyter Book 或 Sphinx 等工具将源文件构建成 HTML 页面，然后再部署到 GitHub Pages。</p>\n<p>两种方式的选择取决于具体需求。如果只是快速分享几个笔记文件，且不需要精心组织的结构，第一种方式完全足够。但如果目标是构建一个专业的文档站点，为读者提供良好的浏览体验，或者需要长期维护一个知识库，那么投入时间搭建 MkDocs 站点是值得的。</p>\n<h2 id=\"3-mkdocs-完整搭建流程\">3. MkDocs 完整搭建流程</h2>\n<p>以下是从零开始搭建支持 Jupyter Notebook 的 MkDocs 文档站点的完整流程。</p>\n<h3 id=\"31-环境准备\">3.1 环境准备</h3>\n<p>首先确保系统已安装 Python 3.7 或更高版本。然后安装必要的依赖包，包括 MkDocs 核心库、Material 主题和 Jupyter 支持插件。</p>\n<pre><code class=\"language-bash\">pip install mkdocs mkdocs-material mkdocs-jupyter\n</code></pre>\n<p>Material 主题是目前最流行的 MkDocs 主题之一，提供了现代化的界面设计和丰富的自定义选项。mkdocs-jupyter 插件则使 MkDocs 能够将 Jupyter Notebook 文件转换为网页。</p>\n<h3 id=\"32-项目结构初始化\">3.2 项目结构初始化</h3>\n<p>在本地克隆或创建 GitHub 仓库后，建立以下目录结构。</p>\n<pre><code>my-tech-notes/\n├── mkdocs.yml\n├── docs/\n│   ├── index.md\n│   ├── notebooks/\n│   │   ├── algorithm.ipynb\n│   │   └── data_analysis.ipynb\n│   └── images/\n│       └── diagram.png\n└── .gitignore\n</code></pre>\n<p>所有文档源文件统一放在 docs 目录下。index.md 将作为网站首页，notebooks 子目录存放 Jupyter Notebook 文件，images 目录存放图片资源。这种组织方式便于后续管理和扩展。</p>\n<h3 id=\"33-配置文件编写\">3.3 配置文件编写</h3>\n<p>在项目根目录创建 mkdocs.yml 配置文件，这是 MkDocs 的核心配置。以下是一个完整的配置示例。</p>\n<pre><code class=\"language-yaml\">site_name: 我的技术文档\nsite_url: https://username.github.io/repo-name/\nsite_description: 算法与数据分析学习笔记\nsite_author: Your Name\n\ntheme:\n  name: material\n  language: zh\n  palette:\n    - scheme: default\n      primary: indigo\n      toggle:\n        icon: material/brightness-7\n        name: 切换至深色模式\n    - scheme: slate\n      primary: indigo\n      toggle:\n        icon: material/brightness-4\n        name: 切换至浅色模式\n  features:\n    - navigation.tabs\n    - navigation.sections\n    - navigation.top\n    - search.highlight\n    - search.share\n    - search.suggest\n\nplugins:\n  - search:\n      lang:\n        - zh\n        - en\n  - mkdocs-jupyter:\n      include_source: true\n      execute: false\n      allow_errors: false\n\nnav:\n  - 首页: index.md\n  - 算法笔记:\n      - 双指针: notebooks/two_pointers.ipynb\n      - 动态规划: notebooks/dynamic_programming.ipynb\n  - 数据分析:\n      - Pandas 基础: notebooks/pandas_basics.ipynb\n\nmarkdown_extensions:\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.superfences\n  - pymdownx.arithmatex:\n      generic: true\n\nextra_javascript:\n  - https://polyfill.io/v3/polyfill.min.js?features=es6\n  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n</code></pre>\n<p>配置中有几个关键参数需要说明。site_url 必须设置为正确的 GitHub Pages 地址，格式为 <code>https://用户名.github.io/仓库名/</code>。theme 部分配置了 Material 主题的外观和功能特性，包括深浅色模式切换和导航增强功能。</p>\n<p>plugins 部分的 mkdocs-jupyter 插件配置尤为重要。execute 参数设置为 false 表示在构建网站时不执行 Notebook 中的代码，而是直接使用 Notebook 文件中已保存的输出。这样可以避免构建时间过长，也避免了在 CI 环境中可能遇到的依赖问题。include_source 设置为 true 则会在生成的网页中包含源代码的下载链接。</p>\n<p>nav 部分定义了网站的导航结构。你可以直接在这里引用 .ipynb 文件，mkdocs-jupyter 插件会自动处理转换。markdown_extensions 和 extra_javascript 部分则启用了代码高亮和数学公式渲染支持，这对技术文档尤为重要。</p>\n<h3 id=\"34-首页内容准备\">3.4 首页内容准备</h3>\n<p>在 docs/index.md 中编写网站首页内容，这将是访问者看到的第一个页面。</p>\n<pre><code class=\"language-markdown\"># 欢迎来到我的技术笔记\n\n本站记录了我在算法学习和数据分析过程中的笔记和思考。\n\n## 主要内容\n\n本站包含以下主题的技术笔记：\n\n- **算法专题**：包括双指针、动态规划等经典算法的分析与实现\n- **数据分析**：使用 Python 进行数据处理和可视化的实践经验\n\n## 快速开始\n\n建议从左侧导航栏选择感兴趣的主题开始阅读。每篇笔记都包含详细的代码示例和解释说明。\n</code></pre>\n<h3 id=\"35-本地预览测试\">3.5 本地预览测试</h3>\n<p>在项目根目录运行以下命令启动本地开发服务器。</p>\n<pre><code class=\"language-bash\">mkdocs serve\n</code></pre>\n<p>该命令会在本地 8000 端口启动一个实时预览服务器。在浏览器访问 <a href=\"http://127.0.0.1:8000\" rel=\"noopener nofollow\" target=\"_blank\">http://127.0.0.1:8000</a> 即可查看网站效果。当你修改 docs 目录下的任何文件或 mkdocs.yml 配置时，浏览器会自动刷新显示最新内容。这个实时预览功能使得文档编写和调试变得非常高效。</p>\n<p>在预览过程中，重点检查以下几个方面。首先确认 Notebook 文件是否正确渲染，包括代码块、输出结果和图表。其次检查导航菜单是否符合预期，所有链接是否有效。如果使用了数学公式，需要确认 MathJax 是否正常加载和渲染。发现问题时可以实时修改配置文件或源文件，直到达到满意的效果。</p>\n<h3 id=\"36-部署到-github-pages\">3.6 部署到 GitHub Pages</h3>\n<p>确认本地预览效果无误后，执行部署命令。</p>\n<pre><code class=\"language-bash\">python -m mkdocs gh-deploy --clean\n</code></pre>\n<p>这个命令会自动完成构建和部署的全过程。具体来说，MkDocs 首先会读取 mkdocs.yml 配置和 docs 目录下的所有源文件，将它们编译成静态 HTML 页面，生成到临时的 site 目录中。然后使用 ghp-import 工具将 site 目录的内容推送到仓库的 gh-pages 分支。--clean 参数确保在构建前清理旧的临时文件。</p>\n<p><strong>部署完成后，访问 GitHub 仓库的 Settings 页面，在 Pages 部分确认 Source 设置为 gh-pages 分支。通常这个设置会自动完成，但首次部署时最好手动检查一下。GitHub Pages 的更新可能需要几分钟时间才能生效，之后就可以通过 <code>https://用户名.github.io/仓库名/</code> 访问网站了</strong>。</p>\n<h2 id=\"4-两个分支的职责分离\">4. 两个分支的职责分离</h2>\n<p>理解 MkDocs 工作流程的关键在于认识到 main 分支和 gh-pages 分支有着完全不同的职责。</p>\n<p>main 分支存储的是文档的源文件，包括 Jupyter Notebook 文件、Markdown 文件、图片资源以及 mkdocs.yml 配置文件。这些是人类可读和可编辑的内容，是文档的真正来源。当你修改笔记内容、添加新的文档或调整网站配置时，都是在 main 分支上进行操作并提交。</p>\n<p>gh-pages 分支则完全不同，它存储的是机器生成的静态网页文件，包括 HTML、CSS 和 JavaScript 文件。这些文件是 MkDocs 根据 main 分支的源文件自动构建生成的，供浏览器访问使用。访问者通过 GitHub Pages 看到的网站内容就来自这个分支。</p>\n<p>这种设计的核心理念是将源代码和构建产物严格分离。如果将两者混合在同一分支，Git 的提交历史中就会充斥着大量自动生成的 HTML 代码变更，使得代码审查变得困难，也会让版本历史变得混乱。通过分离两个分支，main 分支的历史保持清晰，只记录文档内容的实质性变更，而 gh-pages 分支完全由自动化工具管理，不需要人工干预。</p>\n<p>当你执行 <code>mkdocs gh-deploy</code> 命令时，它只会操作 gh-pages 分支，不会也不应该触碰 main 分支。这不是 bug，而是刻意的设计选择。理解这一点，就能明白为什么文档更新需要两个独立的步骤，一个是提交源文件到 main 分支，另一个是发布构建产物到 gh-pages 分支。</p>\n<h2 id=\"5-日常维护工作流程\">5. 日常维护工作流程</h2>\n<p>在完成初始搭建后，日常更新文档的流程变得非常规范。每次需要更新内容时，按照以下步骤操作即可。</p>\n<pre><code class=\"language-bash\"># 步骤 1：切换到 main 分支并拉取最新代码\ngit checkout main\ngit pull origin main\n\n# 步骤 2：修改或新增文档内容\n# 在 docs/notebooks/ 目录下编辑 .ipynb 文件\n# 或在 docs/ 目录下编辑 .md 文件\n# 如果添加了新文件，记得更新 mkdocs.yml 中的 nav 配置\n\n# 步骤 3：本地预览确认效果\nmkdocs serve\n# 在浏览器中检查修改是否正确，按 Ctrl+C 停止服务器\n\n# 步骤 4：提交源文件到 main 分支\ngit add .\ngit commit -m \"更新算法笔记内容\"\ngit push origin main\n\n# 步骤 5：构建并发布网站到 gh-pages 分支\npython -m mkdocs gh-deploy --clean\n</code></pre>\n<p>这五个步骤构成了完整的更新循环。步骤 1 到 3 是准备和验证阶段，确保你的修改是正确的。步骤 4 将源文件的变更推送到 GitHub 仓库，完成版本控制。步骤 5 则将更新后的内容构建成网站并发布。</p>\n<p>需要强调的是，步骤 4 和步骤 5 虽然通常连续执行，但它们在概念上是完全独立的。<code>git push origin main</code> 是在保存你的工作成果到源代码仓库，而 <code>mkdocs gh-deploy</code> 是在发布网站到公开访问的地址。前者操作 main 分支，后者操作 gh-pages 分支，两者互不干扰。</p>\n<p>如果团队协作或希望进一步自动化流程，可以配置 GitHub Actions 工作流，在每次推送到 main 分支时自动触发 MkDocs 构建和部署。这样就可以省略手动执行步骤 5 的操作，实现完全自动化的发布流程。</p>\n<h2 id=\"6-总结与建议\">6. 总结与建议</h2>\n<p>通过 MkDocs 和 GitHub Pages 搭建技术文档站点是一个一次性投入、长期受益的过程。虽然初始配置需要理解一些概念和工具，但一旦建立起标准化的工作流程，后续的维护成本很低，而带来的专业性和可用性提升是显著的。</p>\n<p>核心要点在于理解源文件和构建产物的分离原则。main 分支管理可编辑的源内容，gh-pages 分支托管生成的网站文件，两者各司其职。日常工作中只需要关注 main 分支的内容更新，构建和部署由 MkDocs 自动处理。</p>\n<p>对于刚开始使用这套系统的开发者，建议先用简单的测试内容完整走通一遍流程，熟悉每个步骤的作用。在实际使用过程中，可以根据需要逐步调整主题配置、添加插件功能或优化导航结构。MkDocs 的生态系统非常丰富，有大量插件可以扩展功能，包括支持更多文档格式、增强搜索能力或添加评论系统等。</p>\n<p>最后需要提醒的是，Notebook 文件在提交前最好保留必要的输出结果，因为 MkDocs 默认不会重新执行代码。如果某些输出文件过大导致 Git 仓库体积增长过快，可以考虑使用 Git LFS 管理大文件，或者在 CI 环境中配置代码执行并缓存输出结果。这些优化可以在基础流程稳定后逐步实施。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t<p class=\"postfoot\">\n\t\t\tposted on \n<span id=\"post-date\">2026-02-08 18:56</span>&nbsp;\n<a href=\"https://www.cnblogs.com/GlenTt\">GlenTt</a>&nbsp;\n阅读(<span id=\"post_view_count\">15</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</p>"
    },
    {
      "title": "这世界就是个巨大的草台班子-你的飞牛nas中招了吗",
      "link": "https://www.cnblogs.com/bugshare/p/19591372",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/bugshare/p/19591372\" id=\"cb_post_title_url\" title=\"发布于 2026-02-08 12:50\">\n    <span>这世界就是个巨大的草台班子-你的飞牛nas中招了吗</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>本来我是真的不太想写这篇文章。<br />\n一方面，这事已经发酵挺久了，官方也算是出了修复版本；<br />\n另一方面——说句实话，写起来真的心疼：<br />\n我的照片、我的资料、我的备份，可能现在已经不仅属于我了...😭</p>\n<p>结果现在回头一想：<br />\n<strong>还是有必要把这次惨重的教训记录下吧，吃一堑,长一智。</strong>=</p>\n<p>最近，国产私有云系统 <strong>飞牛 NAS（fnOS）</strong> 被曝出存在<strong>严重安全漏洞</strong>。<br />\n不少用户反馈：</p>\n<ul>\n<li>设备出现异常访问</li>\n<li>数据存在被读取风险</li>\n<li>甚至还有人发现被植入了不明程序</li>\n</ul>\n<p>这已经不是“某个功能不好用”，<br />\n也不是“偶尔崩一下”的问题了。</p>\n<p><strong>这是一次实打实，直接冲着用户数据来的系统级安全事故。</strong></p>\n<p>更让人难受的是：<br />\n<strong>一开始，官方对这个漏洞的态度，并不重视。</strong><br />\n<img alt=\"feiniu.jpg\" class=\"lazyload\" /></p>\n<hr />\n<h1 id=\"一飞牛-nas-为啥会翻这么大的车\">一、飞牛 NAS 为啥会翻这么大的车？</h1>\n<h2 id=\"1️⃣-先说背景nas-正在变成家庭服务器\">1️⃣ 先说背景：NAS 正在变成“家庭服务器”</h2>\n<p>飞牛私有云 <strong>fnOS</strong>，本质上是一套基于 Debian Linux 深度定制的 NAS 操作系统。<br />\n目标用户很明确：</p>\n<ul>\n<li>家庭用户</li>\n<li>小团队</li>\n<li>把闲置 PC / 服务器当私有云用的人</li>\n</ul>\n<p>文件存储、影视库、远程访问、应用中心……<br />\n<strong>该有的都有，而且不少人是直接暴露在公网用的。</strong></p>\n<p>说白了：</p>\n<blockquote>\n<p><strong>现在的 NAS，本质就是一台 7×24 小服务器。</strong></p>\n</blockquote>\n<p>但问题也在这。</p>\n<hr />\n<h2 id=\"2️⃣-真正的根因典型致命的路径穿越漏洞\">2️⃣ 真正的根因：典型致命的路径穿越漏洞</h2>\n<p>这次翻车的核心原因，其实一点都不花哨。</p>\n<p>问题出在 <strong>Web 管理服务对路径的处理上</strong>。</p>\n<p>说人话就是一句话：</p>\n<blockquote>\n<p><strong>后台没把 <code>../</code> 这种路径跳转给拦住。</strong></p>\n</blockquote>\n<p>结果就是——<br />\n攻击者可以构造特殊请求：</p>\n<ul>\n<li>绕过目录限制</li>\n<li>想读哪就读哪</li>\n<li>系统文件、配置文件，直接暴露</li>\n</ul>\n<p>这种漏洞在安全圈有个名字，叫：</p>\n<blockquote>\n<p><strong>Path Traversal（路径穿越）</strong></p>\n</blockquote>\n<p>它真正恐怖的地方在于：</p>\n<ul>\n<li>❌ 不用登录</li>\n<li>❌ 不要账号</li>\n<li>❌ 不用爆破</li>\n<li>❌ 不需要你点任何链接</li>\n</ul>\n<p><strong>只要你的 NAS 在公网，扫到就能打。</strong></p>\n<hr />\n<h1 id=\"二这个漏洞是怎么被利用的\">二、这个漏洞是怎么被利用的？</h1>\n<h2 id=\"-复现原理真的很低级但就这么致命\">🔍 复现原理（真的很“低级”，但就这么致命）</h2>\n<p>正常情况下，Web 只允许你访问类似这种资源：</p>\n<pre><code class=\"language-http\">/app-center-static/xxx/icon.png\n</code></pre>\n<p>但如果后端不校验路径，<br />\n攻击者就可以这么玩：</p>\n<pre><code class=\"language-http\">http://[ip]:[port]/app-center-static/serviceicon/myapp/%7B0%7D/?size=../../../../vol1/1000/a/\n</code></pre>\n<p><img alt=\"PixPin_2026-02-08_01-06-03.png\" class=\"lazyload\" /></p>\n<p>甚至直接读系统文件：</p>\n<pre><code class=\"language-http\">http://[ip]:[port]/app-center-static/serviceicon/myapp/%7B0%7D/?size=../../../../etc/passwd\n</code></pre>\n<p><img alt=\"PixPin_2026-02-08_01-00-52.png\" class=\"lazyload\" /></p>\n<p>效果是什么？</p>\n<blockquote>\n<p><strong>表面上是请求“应用图标”，<br />\n实际上读的是你 NAS 里的真实文件。</strong></p>\n</blockquote>\n<p>这不是黑科技，<br />\n这是<strong>基础路径权限没控制好。</strong></p>\n<hr />\n<h2 id=\"-更可怕的读文件只是开始\">🔗 更可怕的：读文件只是开始</h2>\n<p>很多人看到“只能读文件”，会下意识松一口气。</p>\n<p>但现实是：<br />\n<strong>路径穿越，几乎从来不是终点。</strong></p>\n<p>一旦能读到这些东西：</p>\n<ul>\n<li>系统配置</li>\n<li>用户信息</li>\n<li>Token / Key</li>\n<li>Web 服务路径</li>\n</ul>\n<p>接下来能干什么？</p>\n<ul>\n<li>认证绕过</li>\n<li>写入恶意文件</li>\n<li>执行命令</li>\n<li>长期控制设备</li>\n</ul>\n<p>这也正好对应了一些用户的真实反馈：</p>\n<blockquote>\n<p>CPU 被吃满<br />\n带宽异常<br />\nNAS 像“不是自己的了”</p>\n</blockquote>\n<hr />\n<h1 id=\"三这事对普通家用用户到底有多严重\">三、这事对“普通家用用户”到底有多严重？</h1>\n<p>我知道，肯定有人会想：</p>\n<blockquote>\n<p>“我就家里放个 NAS，又不是公司服务器。”</p>\n</blockquote>\n<p>但现实刚好相反。</p>\n<p><strong>NAS 里的数据，往往比服务器更私密。</strong></p>\n<h2 id=\"️-最直接的风险包括\">⚠️ 最直接的风险包括：</h2>\n<ul>\n<li>📂 照片、视频、文档被读走</li>\n<li>🔐 系统账号、配置泄露</li>\n<li>🪙 被偷偷塞挖矿、木马</li>\n<li>🌐 成为攻击别人的跳板</li>\n<li>❌ 系统被改，升级、恢复全翻车</li>\n</ul>\n<p>最可怕的一点是：</p>\n<blockquote>\n<p><strong>绝大多数用户，根本不知道自己有没有中招。</strong></p>\n</blockquote>\n<hr />\n<h1 id=\"四官方后来修了但问题真的结束了吗\">四、官方后来修了，但问题真的结束了吗？</h1>\n<h2 id=\"-客观说一句补丁是有的也确实修了\">✅ 客观说一句：补丁是有的，也确实修了</h2>\n<p>飞牛后来发布了多个版本更新，主要做了这些事：</p>\n<ul>\n<li>严格校验路径参数</li>\n<li>修复静态资源访问逻辑</li>\n<li>增加异常请求拦截</li>\n</ul>\n<p><strong>从纯技术角度讲，补丁是有效的。</strong></p>\n<hr />\n<h2 id=\"️-但真正的问题不只是有没有补丁\">⚠️ 但真正的问题，不只是“有没有补丁”</h2>\n<p>这次争议的核心，其实在这：</p>\n<ul>\n<li>漏洞曝光时，已经有大量设备裸奔在公网</li>\n<li>很多用户根本不知道 NAS 不该这么用</li>\n<li>安全风险提示不直观</li>\n<li>默认配置对新手并不友好</li>\n</ul>\n<p><strong>安全不是写完代码就结束了。</strong></p>\n<p><img alt=\"PixPin_2026-02-08_01-30-12.png\" class=\"lazyload\" /></p>\n<p><img alt=\"154902xzluhy0ttttsbeyg.png\" class=\"lazyload\" /></p>\n<hr />\n<h1 id=\"五如果你在用飞牛现在请务必做这几件事\">五、如果你在用飞牛，现在请务必做这几件事</h1>\n<h2 id=\"-1️⃣-立刻确认有没有暴露在公网\">🛑 1️⃣ 立刻确认：有没有暴露在公网</h2>\n<p>自查这几项：</p>\n<ul>\n<li>端口映射</li>\n<li>官方中继</li>\n<li>管理后台公网可访问</li>\n</ul>\n<p><strong>只要有一个是：建议立刻关。</strong></p>\n<hr />\n<h2 id=\"-2️⃣-立刻升级到最新-fnos\">🔄 2️⃣ 立刻升级到最新 fnOS</h2>\n<p>别观望，别等等。</p>\n<blockquote>\n<p><strong>安全漏洞，从来不等人。</strong></p>\n</blockquote>\n<hr />\n<h2 id=\"-3️⃣-检查有没有不对劲\">🔍 3️⃣ 检查有没有“不对劲”</h2>\n<p>重点看：</p>\n<ul>\n<li>CPU / 内存是否异常</li>\n<li>有没有不认识的进程</li>\n<li>启动项有没有被动过</li>\n<li>Web 日志里有没有奇怪请求</li>\n</ul>\n<p>如果你已经开始不放心了：</p>\n<blockquote>\n<p><strong>备份 → 重装 → 再恢复<br />\n比任何“心理安慰”都管用。</strong></p>\n</blockquote>\n<hr />\n<h1 id=\"六比修漏洞更重要的以后-nas-应该怎么用\">六、比修漏洞更重要的：以后 NAS 应该怎么用</h1>\n<p>这次事，说到底不只是飞牛的问题。</p>\n<h2 id=\"-一个更安全的-nas-使用习惯\">✅ 一个更安全的 NAS 使用习惯</h2>\n<ul>\n<li>❌ 别把管理端口直接丢公网</li>\n<li>❌ SSH 不用就关</li>\n<li>✅ 用 VPN（WireGuard / Tailscale）</li>\n<li>✅ 管理和数据访问分开</li>\n<li>✅ 养成升级习惯</li>\n<li>✅ 多看看安全公告</li>\n</ul>\n<p>一句话送给所有 NAS 用户：</p>\n<blockquote>\n<p><strong>NAS 要按“服务器”的标准对待，<br />\n而不是当个路由器插件。</strong></p>\n</blockquote>\n<p><img alt=\"PixPin_2026-02-08_01-24-15.png\" class=\"lazyload\" /></p>\n<p><img alt=\"PixPin_2026-02-08_01-26-40.png\" class=\"lazyload\" /></p>\n<hr />\n<h1 id=\"七最后说一句这不是终点而是一记警钟\">七、最后说一句：这不是终点，而是一记警钟</h1>\n<p>飞牛 NAS 的这次漏洞，并不罕见。</p>\n<p>真正值得警惕的是：</p>\n<ul>\n<li>私有云越来越复杂</li>\n<li>很多产品功能多样化上去了，安全设计却明显滞后</li>\n<li>用户被迫承担了本不该承担的安全成本</li>\n</ul>\n<p>希望这次之后：</p>\n<ul>\n<li>用户能对公网访问多一分警惕</li>\n<li>厂商能把安全当成第一优先级</li>\n<li>国产 NAS 生态，能少一点“草台班子”</li>\n</ul>\n<blockquote>\n<p><strong>数据一旦泄露，<br />\n是没有任何补丁能帮你修回来的。</strong></p>\n</blockquote>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-08 12:50</span>&nbsp;\n<a href=\"https://www.cnblogs.com/bugshare\">BugShare</a>&nbsp;\n阅读(<span id=\"post_view_count\">21</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "从“千问送奶茶”看AI Agent落地：火爆、崩塌与进化方向",
      "link": "https://www.cnblogs.com/ChenAI-TGF/p/19590175",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ChenAI-TGF/p/19590175\" id=\"cb_post_title_url\" title=\"发布于 2026-02-08 01:56\">\n    <span>从“千问送奶茶”看AI Agent落地：火爆、崩塌与进化方向</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        阿里通义千问APP在2026年春节期间推出\"30亿免单送奶茶\"活动，通过AI Agent技术实现\"一句话点单\"的便捷体验，3小时内订单突破百万。活动成功验证了AI从聊天工具向\"主动办事助手\"的转型，但也暴露了系统在高并发下的技术短板：API网关崩溃、数据库过载和GPU显存溢出等问题。该活动展现了阿里在大模型技术、生态整合（高德、支付宝等）和成本控制（自研芯片）方面的独特优势，为AI Agent的商业化落地提供了重要参考，同时也揭示了工程化能力仍需突\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>@</p><div class=\"toc\"><div class=\"toc-container-header\">目录</div><ul><li><a href=\"#前言\" rel=\"noopener nofollow\">前言</a></li><li><a href=\"#2026年2月6日全民薅羊毛热潮下的狂欢与崩塌\" rel=\"noopener nofollow\">2026年2月6日：全民薅羊毛热潮下的狂欢与崩塌</a></li><li><a href=\"#原理解析千问是怎么通过一句话点奶茶的又为什么崩溃了\" rel=\"noopener nofollow\">原理解析：千问是怎么通过一句话点奶茶的？又为什么崩溃了？</a></li><li><a href=\"#为什么千问服务器会崩溃三重技术瓶颈被流量击穿\" rel=\"noopener nofollow\">为什么千问服务器会崩溃？三重技术瓶颈被流量击穿</a></li><li><a href=\"#为什么只有阿里能做千文送奶茶三大核心壁垒不可复制\" rel=\"noopener nofollow\">为什么只有阿里能做“千文送奶茶”？三大核心壁垒不可复制</a></li><li><a href=\"#改进方向ai-agent的进化之路从能办事到办好事\" rel=\"noopener nofollow\">改进方向：AI Agent的进化之路——从“能办事”到“办好事”</a></li><li><a href=\"#结语ai-agent落地始于场景成于细节\" rel=\"noopener nofollow\">结语：AI Agent落地，始于场景，成于细节</a></li></ul></div><p></p>\n<h1 id=\"前言\">前言</h1>\n<p>2026年春节期间，阿里通义千问APP推出的“<strong>30亿免单送奶茶</strong>”活动，意外引爆全民参与热潮，也成为AI Agent技术从实验室走向大众生活的一次标志性试炼。近年来，大模型技术飞速迭代，从19年OpenAI的GPT系列、Gemini3到国内的Deepseek，通义千问、文心一言，AI的核心能力已从“自然语言理解与生成”逐步转向“自主决策与任务执行”，但行业始终面临着一个世界性难题：<strong>用户仅停留在“聊天娱乐”场景</strong>、<strong>无法形成从交互到变现的商业闭环</strong>。</p>\n<p>阿里此次以“<strong>奶茶免单</strong>”为切入点，本质上是以免费的旗号，通过高频需求的消费场景，打通对话式AI与实际使用之间的应用壁垒，让AI从“被动应答的工具”升级为“<strong>主动办事的助手</strong>”。</p>\n<p>诚然，这场声势浩大的活动展现了AI Agent落地的巨大<strong>潜力</strong>，但同时也暴露了技术、工程化层面的诸多<strong>短板</strong>，成为值得整个AI行业深思的典型案例。本文将基于近期相关报道与技术细节，全面拆解“千文送奶茶”的现象、原理，并探讨AI Agent未来的优化方向与技术壁垒。</p>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<h1 id=\"2026年2月6日全民薅羊毛热潮下的狂欢与崩塌\">2026年2月6日：全民薅羊毛热潮下的狂欢与崩塌</h1>\n<p><strong>一、千文点奶茶：全民级的AI消费狂欢</strong><br />\n“千文送奶茶”活动自2026年2月6日上线以来，凭借“无套路、真免单”的特点迅速刷屏全网。活动核心规则简单易懂：用户更新通义千问APP后，只需通过语音或文字发出“帮我点一杯奶茶”的指令，即可领取25元无门槛免单券并且千文AI会直接帮助用户选择好奶茶并下单，覆盖全国30多万家茶饮门店，包括瑞幸、蜜雪冰城、奈雪的茶、茶百道等主流品牌，单人最高可领取525元免单额度。</p>\n<p>不同于以往互联网平台“满减、拉人头”的套路，此次活动真正实现了“一句话办事”的便捷体验，这也直接推动了活动的爆发式增长。据官方数据及媒体报道，活动上线3小时订单量即突破百万，4小时突破200万单，9小时内总订单量更是飙升至1000万单，刷新了全球AI购物的纪录；与此同时，千问APP成功登顶应用商店免费榜总榜，下载量短期内暴涨，实现了用户量的跨越式增长。</p>\n<p>从社交媒体反馈来看，大量用户分享了自己的“薅羊毛”经历，有人实现“1分钱喝奶茶”，有人批量下单分享给亲友，相关话题多次冲上热搜，形成了全民参与、全民讨论的热潮。这场狂欢的背后，是用户对“AI能办事”的新鲜感与认可，也是大众对AI Agent落地场景的首次大规模体验。</p>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<p><strong>二、点单流程：极简交互背后的全链路自动化</strong><br />\n千文点奶茶的核心优势的在于“极简交互+全流程自动化”，用户无需跳转其他APP、无需手动填写地址、无需比价凑单，仅需一句话即可完成从需求提出到支付核销的全流程，具体操作步骤可拆解为3步：</p>\n<ol>\n<li>\n<p>需求发起：用户通过千问APP的语音或文字交互框，发出点单指令，既可以是简单的“帮我点一杯奶茶”，也可以是复杂的定制化需求，如“1杯霸王茶姬，少冰、要少糖”；</p>\n</li>\n<li>\n<p>AI自主处理：千问大模型自动解析用户意图，将“少糖”“冰饮”等模糊描述转化为标准化参数（如“5分糖”“冰度正常”），同时拆解批量订单、口味偏好等隐性需求，若信息不完整（如未说明地址），会通过多轮对话追问用户；</p>\n</li>\n<li>\n<p>全流程闭环：AI自动调用用户淘宝或支付宝的常用地址（或通过高德地图实时定位），基于地理位置和用户历史偏好，筛选3公里内评分4.8以上的合作门店，生成包含优惠信息的订单方案，用户点击“支付宝付款”后，通过首次授权的账号完成面容、指纹或密码核身，即可在千问APP内完成支付，无需跳转其他应用，支付完成后自动生成核销码，用户可直接到店取餐或等待外卖送达。</p>\n</li>\n</ol>\n<p>官方数据显示，这种“AI付”模式将传统点单的操作步骤减少了90%以上，真正实现了“说一句，就办好”，也是其能够快速吸引全民参与的核心原因。</p>\n<p><strong>三、服务器崩溃：流量洪峰下的技术失守</strong></p>\n<p>就在全民狂欢的同时，千问APP的服务器却不堪重负，陷入崩溃状态。活动上线后不久，大量用户反馈：活动页面加载失败、点击无响应，频繁弹出“系统开小差了”的提示；部分用户领到的免单卡延迟到账，邀请好友的奖励出现“被吞”情况；还有用户发出点单指令后，AI长时间无响应，无法完成订单创建。</p>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<p>据技术媒体拆解，此次崩溃的核心原因是瞬时流量远超服务器承载上限。千问APP日常每秒请求数（QPS）约为1万，而活动峰值时，QPS直接冲上80万，是平时的80倍以上，远超服务器理论承载的24万QPS上限。活动专属页面的局部拥堵，让“千问崩了”冲上热搜，影响了大量用户的参与体验。</p>\n<p>在卡顿发生后10分钟内，千文官方通过微博和APP弹窗发布回应，承认“活动太火爆导致拥堵”，并承诺“正在紧急加资源扩容”。随后，技术团队火速增配200余台服务器，优化系统架构，优先保障核心下单链路；同时，官方将所有25元免单卡的有效期从原定的3天延长至2月23日，引导用户错峰参与，分流瞬时压力，并开通24小时专属客服通道，针对订单异常、奖励丢失等问题进行核实补发，逐步缓解了用户的不满情绪。</p>\n<h1 id=\"原理解析千问是怎么通过一句话点奶茶的又为什么崩溃了\">原理解析：千问是怎么通过一句话点奶茶的？又为什么崩溃了？</h1>\n<p>千文点奶茶看似简单的“一句话办事”，背后是通义千问大模型的自然语言处理能力与阿里全生态资源的深度整合，其技术逻辑可拆解为“意图解析—资源调用—闭环执行”三个核心环节，形成了完整的AI Agent任务执行链路：</p>\n<p><strong>一、意图解析：Qwen-Plus大模型的核心能力支撑</strong><br />\n用户发出的点单指令，首先由千问内置的Qwen-Plus大模型进行处理，这一步的核心是“精准理解模糊需求、拆解隐性需求”。<strong>Qwen-Plus</strong>大模型具备强大的上下文理解与多轮对话能力，能够实现：</p>\n<ul>\n<li>\n<p><strong>模糊需求标准化</strong>：将用户口中的“少糖”“微冰”“半糖去冰”等模糊描述，转化为外卖平台、茶饮门店可识别的标准化参数（如“5分糖”“冰度50%”“无冰”），避免因需求模糊导致订单出错；</p>\n</li>\n<li>\n<p><strong>隐性需求拆解</strong>：能够从用户的指令中，拆解出批量订单、口味偏好、配送方式等隐性需求，例如用户说“帮我和同事点奶茶，我要珍珠的，他们随便”，AI会自动拆解为“多杯订单+用户本人珍珠奶茶+其他同事随机口味”，并追问同事人数、是否有忌口等补充信息；</p>\n</li>\n<li>\n<p><strong>上下文连贯记忆</strong>：支持多轮对话的上下文衔接，例如用户先发出“点一杯珍珠奶茶”，后续补充“加椰果，少糖”，AI能够连贯识别为“修改原有订单的配料和甜度”，而非重新创建新订单，提升交互体验。</p>\n</li>\n</ul>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<p><strong>二、资源调用：阿里生态的“超级连接器”作用</strong><br />\n在解析用户意图后，千问Agent将扮演“超级连接器”的角色，调用阿里生态内的多平台资源，完成订单创建、支付、定位等一系列操作，这也是其能够实现“<strong>全流程自动化”</strong>的核心支撑，具体涉及三大生态资源：</p>\n<ul>\n<li>\n<p><strong>定位与地址资源</strong>：调用高德地图的实时定位接口，获取用户当前位置（精度±5米），同时联动淘宝、支付宝的常用地址库，自动填充配送地址，无需用户手动输入；</p>\n</li>\n<li>\n<p><strong>商家与订单资源</strong>：对接淘宝闪购平台的茶饮门店数据库，筛选用户周边3公里内、评分4.8以上的合作门店，获取门店库存、产品价格、优惠活动等实时信息，生成最优订单方案；</p>\n</li>\n<li>\n<p><strong>支付资源</strong>：联动支付宝的支付接口，实现“AI付”模式，用户首次授权后，可直接在千问APP内完成面容、指纹或密码核身，无需跳转支付宝，形成“交互—下单—支付”的闭环。</p>\n</li>\n</ul>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<p><strong>三、闭环执行：任务拆解与多链路协同</strong><br />\n千问将“点奶茶”这一复杂任务，拆解为“意图识别—地址获取—商家筛选—订单生成—支付核销”5个细分步骤，每个步骤由对应的模块独立执行，同时通过分布式系统实现多链路协同，确保整个流程顺畅高效。例如，在用户发出指令的同时，AI同步启动地址获取与商家筛选，无需等待前一个步骤完成，大幅缩短了订单创建时间；支付完成后，自动获取取餐码码，并同步发送到用户。</p>\n<h1 id=\"为什么千问服务器会崩溃三重技术瓶颈被流量击穿\">为什么千问服务器会崩溃？三重技术瓶颈被流量击穿</h1>\n<table>\n<thead>\n<tr>\n<th>指标</th>\n<th>数值</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>活动上线3小时订单量</td>\n<td>100万单</td>\n<td>刷新全球AI购物纪录</td>\n</tr>\n<tr>\n<td>活动上线4小时订单量</td>\n<td>200万单</td>\n<td>-</td>\n</tr>\n<tr>\n<td>活动上线9小时订单量</td>\n<td>1000万单</td>\n<td>-</td>\n</tr>\n<tr>\n<td>日常QPS</td>\n<td>1万次/秒</td>\n<td>千问APP常规请求量</td>\n</tr>\n<tr>\n<td>活动峰值QPS</td>\n<td>80万次/秒</td>\n<td>日常的80倍</td>\n</tr>\n</tbody>\n</table>\n<p>此次千问服务器崩溃，并非单一环节的故障，而是AI Agent首次面对“全民级流量”时，接入层、业务层、AI推理层三重技术瓶颈同时被击穿的结果，本质上是“工程化能力”未能匹配“场景落地需求”的体现：</p>\n<p><strong>一、接入层瓶颈：API网关扛不住高频并发</strong><br />\n接入层是用户请求进入系统的第一道关卡，负责接收用户指令、分发请求。此次活动中，80万QPS的瞬时流量直接导致API网关瘫痪，内存被网络栈完全占满，线程模型崩溃，大量用户请求无法被正常接收和分发，出现“页面加载失败、点击无响应”的情况。核心原因是对流量预判不足，未针对极端场景配置足够的网关扩容能力，且未设置完善的流量限流与分流机制。</p>\n<p><strong>二、业务层瓶颈：数据缓存与数据库承压过载</strong><br />\n业务层负责订单生成、优惠核销、用户权益发放等核心操作，依赖数据库和缓存的支撑。此次流量洪峰中，缓存被击穿，大量用户请求直接穿透缓存，访问数据库，导致数据库连接池被打满，分布式事务锁疯狂竞争，出现免单卡“被吞”、订单异常、权益延迟到账等问题。此外，业务层的订单处理逻辑未针对高频并发场景优化，单订单处理耗时过长，进一步加剧了系统拥堵。</p>\n<p><strong>三、AI推理层瓶颈：GPU显存溢出，推理效率骤降</strong><br />\nAI推理层是千问解析用户意图的核心，依赖GPU的算力支撑。此次活动中，大量用户同时发出点单指令，导致GPU显存溢出，Pod批量重启，单Pod吞吐仅为预期的1/3，用户发出的点单指令无法被及时解析，出现“AI长时间无响应”的情况。尽管阿里通过自研芯片和Qwen-MoE 2.0混合专家模型，大幅降低了推理成本，但面对80万QPS的瞬时推理请求，仍显算力不足，暴露了AI推理层的弹性扩容能力短板。</p>\n<h1 id=\"为什么只有阿里能做千文送奶茶三大核心壁垒不可复制\">为什么只有阿里能做“千文送奶茶”？三大核心壁垒不可复制</h1>\n<p>“千文送奶茶”看似是一场简单的补贴活动，但背后需要大模型技术、生态资源、成本控制三大核心能力的支撑，这也是目前国内其他科技公司难以复制的壁垒，而阿里恰好同时具备这三大优势：</p>\n<p><strong>一、算力成本壁垒：自研芯片+全栈架构，实现成本可控</strong><br />\nAI Agent的大规模落地，核心前提是“<strong>算力成本可控</strong>”。过去，大模型的训练与推理成本极高，单用户交互成本居高不下，大规模免费活动根本无法持续。而阿里通过一年的技术迭代，将算力成本降至行业极致：一方面，平头哥自研真武810E AI芯片，配合“通云哥”全栈架构，将GPU用量降低82%；另一方面，Qwen-MoE 2.0混合专家模型的推理成本较上一代下降60%，支持高并发处理；再加上动态调度、冷热分层、Serverless架构的优化，同样算力可支撑5倍用户量，单用户交互成本降至分厘级。这种成本控制能力，是阿里敢于投入30亿开展免单活动的核心底气，也是其他厂商难以企及的优势——多数厂商依赖第三方GPU芯片，算力成本无法实现如此大幅度的下降。</p>\n<p><strong>二、生态闭环壁垒：全链路资源协同，实现“AI办事”闭环</strong><br />\n“千文送奶茶”的核心是“一句话办事”，而这需要“意图解析—商家对接—支付履约”的全链路协同，这恰恰是阿里生态的独特优势。阿里旗下拥有<strong>通义千问</strong>（大模型）、<strong>淘宝闪购</strong>（履约资源）、<strong>支付宝</strong>（支付资源）、<strong>高德地图</strong>（定位资源）等全链路生态产品，能够实现数据互通、接口联动，无需依赖第三方平台。例如，千问可以直接调用淘宝的门店数据库、支付宝的支付接口，无需经过第三方授权，大幅提升了订单处理效率，也避免了第三方接口调用带来的稳定性风险。而国内其他科技公司，要么缺乏大模型技术，要么缺乏完整的消费生态，要么无法实现生态内资源的深度协同，难以实现“<strong>全流程自动化点单</strong>”——比如腾讯有微信生态和支付资源，但缺乏足够的茶饮商家资源和自研大模型的大规模落地能力；百度有文心一言大模型，但缺乏消费生态的支撑，无法实现“下单—支付”的闭环。</p>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<p><strong>三、技术落地壁垒：大模型+工程化能力，实现规模化应用</strong><br />\nAI Agent的落地，不仅需要强大的大模型技术，更需要<strong>成熟的工程化能力</strong>，能够应对<strong>高并发、高可用、高稳定</strong>的场景需求。阿里在电商、支付领域积累了多年的工程化经验，能够支撑双11等大规模并发场景的稳定运行，这种经验也被迁移到千问的落地中——尽管此次出现了服务器崩溃，但技术团队能够快速响应、紧急扩容，在短时间内缓解问题，体现了成熟的工程化应对能力。此外，千问大模型经过多年的迭代，在自然语言理解、任务拆解、多轮对话等方面的能力已趋于成熟，能够精准解析用户的点单需求，避免因意图理解错误导致订单异常，这也是其能够实现“<strong>一句话点单</strong>”的核心技术支撑。</p>\n<table>\n<thead>\n<tr>\n<th>能力维度</th>\n<th>阿里巴巴（通义千问）</th>\n<th>腾讯（元宝AI）</th>\n<th>百度（文心一言）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>大模型意图解析能力</td>\n<td>★★★★★（Qwen-Plus支撑）</td>\n<td>★★★☆☆（混元大模型）</td>\n<td>★★★★☆（文心4.0）</td>\n</tr>\n<tr>\n<td>自研AI芯片/算力成本控制</td>\n<td>★★★★★（真武810E）</td>\n<td>★★☆☆☆（无自研芯片）</td>\n<td>★★★☆☆（昆仑芯）</td>\n</tr>\n<tr>\n<td>茶饮商家资源覆盖</td>\n<td>★★★★★（30万+门店）</td>\n<td>★★☆☆☆（少量合作门店）</td>\n<td>★☆☆☆☆（无核心商家资源）</td>\n</tr>\n<tr>\n<td>支付闭环能力</td>\n<td>★★★★★（支付宝直连）</td>\n<td>★★★★★（微信支付）</td>\n<td>★☆☆☆☆（无自有支付）</td>\n</tr>\n<tr>\n<td>定位/地址资源协同</td>\n<td>★★★★★（高德地图）</td>\n<td>★★★★☆（腾讯地图）</td>\n<td>★★★☆☆（百度地图）</td>\n</tr>\n<tr>\n<td>高并发工程化经验</td>\n<td>★★★★★（双11技术沉淀）</td>\n<td>★★★★☆（微信红包场景）</td>\n<td>★★☆☆☆（缺乏大规模消费场景）</td>\n</tr>\n<tr>\n<td>全流程自动化落地成熟度</td>\n<td>★★★★★（已商用）</td>\n<td>★★☆☆☆（仅demo阶段）</td>\n<td>★☆☆☆☆（无落地场景）</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"改进方向ai-agent的进化之路从能办事到办好事\">改进方向：AI Agent的进化之路——从“能办事”到“办好事”</h1>\n<p>“千文送奶茶”的火爆与崩溃，给整个AI Agent行业上了生动的一课：AI Agent要实现真正的规模化落地，不仅要解决“能办事”的问题，更要解决“办好事”的问题——即提升用户体验，考虑用户决策过程中的各类隐性需求，提供更精准、更贴心的服务。结合此次活动暴露的问题，以及AI Agent的技术发展趋势，千问及同类AI Agent在点单场景中，可从以下方面进行改进。</p>\n<p><strong>展示门店与外卖实时情况，辅助用户决策</strong></p>\n<p><strong>核心需求：展示外卖门店出餐拥堵与履约负荷，前置提示外卖等待风险</strong></p>\n<p>用户点外卖奶茶时，真正影响体验的是门店出餐积压、骑手运力不足、配送链路拥堵导致的长时间等待与超时送达。当前千问仅基于距离、评分、价格筛选门店，未整合外卖全链路的实时出餐状态、订单负荷、运力匹配度，导致用户下单后才发现门店爆单出餐慢、区域无骑手、配送超时，大幅降低用户满意度与复购意愿。<strong>并且给奶茶店面和外卖骑手带来巨大的压力</strong>。</p>\n<p>外卖履约的核心瓶颈是 <strong>“商家出餐效率 + 区域运力供给 + 路况配送效率”</strong> 的三重动态平衡：高峰期热门茶饮店订单积压可超千杯，出餐时长从 10 分钟拉长至 40 分钟以上；同时区域骑手供不应求，取餐等待、配送绕路进一步加剧时效延误。千问作为 AI Agent，必须在下单前向用户透明展示这些履约风险，辅助用户决策是否下单、是否更换门店。</p>\n<h1 id=\"结语ai-agent落地始于场景成于细节\">结语：AI Agent落地，始于场景，成于细节</h1>\n<p>“千文送奶茶”活动无疑是AI Agent落地的一次成功尝试——它用全民可感知的方式，证明了AI从“能聊天”到“能办事”的可行性，也跑通了“大模型+生态”的商业化闭环，为整个行业提供了宝贵的参考经验。但同时，服务器崩溃、用户体验不足等问题，也暴露了AI Agent在工程化能力、场景细节优化等方面的短板。</p>\n<p>从“能办事”到“办好事”，是AI Agent未来的核心进化方向。对于千问而言，此次活动的改进空间，恰恰是其提升核心竞争力的关键——通过整合门店排队数据、外卖运力数据，优化ETA算法，提供个性化替代推荐，不仅能提升用户体验，更能进一步巩固其“生态+技术”的核心壁垒。而对于整个AI行业而言，“千文送奶茶”的启示在于：AI Agent的落地，从来不是单纯的技术竞赛，而是技术、生态、工程化、用户体验的综合比拼。</p>\n<p>未来，随着算力成本的进一步降低、多源数据协同能力的提升、算法精准度的优化，AI Agent将逐步渗透到外卖、购物、出行等更多高频刚需场景，真正成为人们生活中的“全能助手”。而“千文送奶茶”，不过是这场AI革命的一个起点。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-08 01:56</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ChenAI-TGF\">TTGF</a>&nbsp;\n阅读(<span id=\"post_view_count\">262</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "开始记录一个普通煤矿工人的一生，是否改变，留作记录",
      "link": "https://www.cnblogs.com/yuanjinwen/p/19589825",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/yuanjinwen/p/19589825\" id=\"cb_post_title_url\" title=\"发布于 2026-02-07 22:03\">\n    <span>开始记录一个普通煤矿工人的一生，是否改变，留作记录</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p><strong>&nbsp; &nbsp; &nbsp; &nbsp; 我是一名普通的煤矿工人</strong>，从事煤矿安检工作，是煤矿井下安全检查工，属于特种工种。我会记录我的一生和我学习的<strong>各种技能，这是开篇，愿我这个34岁的普通人，能给像我一样的普通人一个人生历程，提前知道一个普通人的一生都做了什么，能不能改变人生，让少走点弯路，也许这也是这也是一个惆怅迷茫者的一生</strong>。</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp; 1992年我出生在一个普通的农村家庭，那时候生活很苦，但是我的爸爸妈妈很爱我，所以我没怎么受过苦。我有一个哥哥，我很爱他，我有时候甚至在想，即使我为他付出生命也可以，也许有人觉得很傻，也许这就是血浓于水吧。爸爸妈妈现在住在我的楼上，是个顶楼，是大哥的努力才让他们居住到了城市。我的妈妈身体大致健康，就是咳嗽，已经好几年了，看了好多医生，没有根治，从那个时候我就开始觉的找个好医生太难了，现在我有了孩子，加上我身体也是亚健康，妻子也是瘦瘦的，这种感觉让我更加强烈。我的哥哥嫂子表面是很健康，只是哥哥做了甲状腺手术，让人很担心，他们一家说实话我不是很了解，可是我打心底希望他们好好的，很多时候心里会默默祈祷，对于读书多的人可能觉得这是迷信，我觉的不是，即使是，也寄存了我的渴望。我的侄子上了初中，侄女还是小学，我的女儿两岁了，看着他们是越看越可爱。今天在借住的办公室，吸了几根烟，想学习，又不知从何学起，手机也看的厌烦，而且很空虚，所以打开了电脑，打算大体记录下我的一生，忘记说了，我的老婆是个很可爱也很爱我的人，我想陪她到老，我知道这很困难，越是长大，越觉得健康到老，或者是活到老真的好不容易，我很爱她，待在单位时总是很想她，可能是陕北种老婆孩子热炕头的思想根深蒂固，也可能是我真的好想老婆和我的女儿。</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp; 我34岁了，总觉的一事无成，也没有赚到钱，一个月的工资也不是很够开支，博客园是个大家庭，我希望用我的一生，给中华儿女，尤其是年轻人一份一份人生体验，也是给我自己我老婆我的孩子一份答卷，我的父母大致不会看到了，他们能会用手机就已经很厉害了，我的哥哥嫂子侄子侄女估计也是不会看到，他们应该不会关注到博客园，我也是有一段编程经历才用的博客园。</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp; 我的小学是我们村里的小学读的，5、6年级到了乡里的，初中是我们孟家湾初级中学读得多，因为小学没学好，刚上初中也是年级倒数的，后来初三要分班了，我的班主任也是我的数学老师把我分到了一班，我一直以为我进不去快班的。快班里面都是学习好的，我心里总是羡慕他们，我就心里不服输，直到初三毕业中考的时候到达了年级第十六名，也如愿以偿的到了市里的一中。高考到了西安工程大学，大学四年里没学下啥东西，应该不是智商问题，是自己不自律，浪费了我的四年青春吧，毕业后，因为想有工资高的工作，去西安传智培训了程序员，培训完没有找工作，自己又自学了几个月后才开始找的工作，去了西安软通动力，也比较幸运，分到了我们组长的组里，组长将我们一起来的几个人，放到了上海华为线，由上海华为的刘挺剑带我，他是中科大的，也是从这个时候我体会到了碾压，知道了差距，感觉他的逻辑特别清晰，也是从那个时候知道985大学生是真的厉害。后来我辞职了，到了内蒙古鄂托克旗的鄂尔多斯集团，也遇到了第一个我认为大佬级别的人才，我的部长田继军，这个人的知识储备是相当强，有一次他让我写报告，我去了他的办公室，然后他点上烟，他说我写，在写的过程中，一些专业的东西是我第一次知道，还有很多是我完全想不到的，可惜的是，我也辞职了，去了煤矿，从事监测监控工，也因为我吃不下苦，这份工作也没有让我坚持住，知道2023年，我来的一家新开的煤矿，我觉的我应该坚持下去，所以做到了现在，可是我对现状不满意，想争取到一个更好的职位，我想大部分人都是这样的。今天真的是想了好多，觉的应该记录一下我的一生，一个普通人的一生。</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp; 今天是2026年2月7日，早上6点多起床，到了安监科办公室，接了井下安检同事的工作汇报，本来不应该是我接的，但是技术员去开调度会，打电话让我接。之后开完班前会，快八点我们下井了，我去的地方是我们矿第一个综采工作面，处于安装阶段，在安装刮板输送机、转载机和采煤机和超前支架，也就是我们所说的三机，我们是早班，一个班就安装了两节转载机加高槽，和一个超前支架，一个班就这么结束了。 下班坐在办公室，想学习了，也以此督促自己改变一下我平凡的一生。三方面知识，煤矿知识、编程知识、还没想好的知识。有点贪心啊。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-07 22:03</span>&nbsp;\n<a href=\"https://www.cnblogs.com/yuanjinwen\">幽静_Loneness</a>&nbsp;\n阅读(<span id=\"post_view_count\">203</span>)&nbsp;\n评论(<span id=\"post_comment_count\">3</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI 白嫖代码：中小型开发组织的开源困境与破局之道 —— Blazor WASM 与 MWGA 如何帮助中小团队在 AI 时代破局",
      "link": "https://www.cnblogs.com/xdesigner/p/19589317",
      "published": "",
      "description": "<h2>\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xdesigner/p/19589317\" id=\"cb_post_title_url\" title=\"发布于 2026-02-07 17:19\">\n    <span>AI 白嫖代码：中小型开发组织的开源困境与破局之道 —— Blazor WASM 与 MWGA 如何帮助中小团队在 AI 时代破局</span>\n    \n\n</a>\n\n\t\t</h2>\n\t\t<div class=\"postText\">    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        在 AI 编程普及的当下，大模型”无授权复用、无反馈回报”的开源代码”白嫖”模式，给抗风险能力较弱的中小型开发组织带来严峻挑战。同时，中小组织拥抱 AI 辅助编程时，又面临 JS 等弱类型语言易滋生 AI”幻觉代码”、隐藏 bug 难排查的问题。开源行为与技术选型的双重调整，成为中小组织破局的关键。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>引言</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在 AI 编程普及的当下，大模型”无授权复用、无反馈回报”的开源代码”白嫖”模式，给抗风险能力较弱的中小型开发组织带来严峻挑战。同时，中小组织拥抱 AI 辅助编程时，又面临 JS 等弱类型语言易滋生 AI”幻觉代码”、隐藏 bug 难排查的问题。开源行为与技术选型的双重调整，成为中小组织破局的关键。</span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>一、核心冲击：开源动力衰减</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>AI 白嫖的核心冲击是开源动力衰减。中小团队往往投入数月心血打磨核心算法与工具代码，这些成果被 AI 一键抓取整合后，既无商业回报，还可能遭竞争对手复刻。这种”付出与回报失衡”，让曾经秉持”技术普惠”的开发者从”无保留开放”转向”谨慎观望”，开源行为迎来结构性调整。</span></p>\n</div>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>立项阶段，中小组织提前划分”闭源核心区 + 开源外围区”，商业壁垒模块严格闭源，仅开放无核心价值的工具类代码；协议选择也从宽松的 MIT、Apache 转向强约束的 AGPLv3 或定制化协议，明确”禁止 AI 训练复用”条款，从规则层面筑牢防护线。</span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n</div>\n<div class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>二、技术栈重构：核心应对手段</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>技术栈重构成为核心应对手段，微软 Blazor WebAssembly（Blazor WASM）凭借”防白嫖 + 降幻觉”的双重优势，成为中小组织的优选，而其本质也是安全性与开发效率的精准权衡。</span></p>\n</div>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Blazor WASM 将 .NET 代码编译为 Wasm 字节码，其中虽包含 IL 中间代码，存在被反编译的可能，但远非”易破解”：IL 代码经混淆压缩后，逆向需突破”IL 反编译 + Wasm 指令还原”双重关卡，相较于明文 JS 的零门槛抓取，破解成本大幅提升，足以抵御绝大多数 AI 白嫖和初级破解工具，完全匹配中小组织的安全需求。</span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n</div>\n<div class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>三、MWGA：降低 Blazor WASM 门槛的关键助力</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>而 MWGA 工具的出现，进一步降低了中小组织拥抱 Blazor WASM 的门槛，成为关键助力。作为 WinForms 程序向 Blazor WASM 迁移的高效工具，MWGA 能将含 GDI+ 绘图功能的传统项目代码修改量控制在 10% 以下，甚至零修改即可完成迁移，7 万行级别的复杂项目也仅需调整不足 1% 的代码。</span></p>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>这让中小组织无需投入大量人力重写核心逻辑，即可快速将成熟的 C# 业务代码转化为 Wasm 格式，既保留了 C# 强类型的防幻觉优势，又借助 Wasm 实现核心代码防护，完美解决”老项目现代化”与”防 AI 白嫖”的双重需求。</span></div>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>更重要的是，MWGA 支持”一份代码双端生成”，可同时编译为桌面 EXE 与 Web 端 Wasm 文件，无需维护两套代码库，大幅降低跨平台开发与维护成本，让中小团队以极低投入获得双端部署能力。其零 Blazor 前端基础要求的特性，让原有 C# 开发团队无需学习新技术栈即可上手，避免了额外的人才培养或招聘成本，完全适配中小组织资源有限的现状。</span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n</div>\n<div class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>四、C# 强类型：为 AI 辅助编程保驾护航</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>更关键的是，C# 强类型特性为 AI 辅助编程保驾护航。JS 作为弱类型语言，变量类型模糊，AI 易生成逻辑矛盾却语法合法的”幻觉代码”，bug 运行时才暴露，排查成本极高；而 C# 要求明确变量类型，编译阶段即可校验类型匹配、方法调用等错误，即便 AI 生成有漏洞的代码，也会被编译器快速拦截，大幅降低隐藏 bug 风险。</span></p>\n</div>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>搭配 NuGet 生态的加密库，可形成”代码防护 + 通信加密 + AI 幻觉拦截”三重屏障，进一步强化安全防线。</span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n</div>\n<div class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>五、理性开源生态互动</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在开源生态互动中，中小组织行为更趋理性：发布代码时明确 AI 使用授权范围，优先参与有 AI 使用规范的社区，或联合组建防护联盟推动协议升级与维权；同时探索”开源回馈”模式，要求 AI 公司使用代码后捐赠资金或贡献优化成果，构建”开源 - 复用 - 反哺”的良性循环。</span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n</div>\n<div class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>六、总结：破局之道</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>AI 白嫖倒逼中小组织摆脱”盲目开源”，聚焦核心算法、场景优化等 AI 难以替代的高端领域，推动开源生态向高质量进化。</span></p>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>对于中小组织而言，无需因噎废食，Blazor WASM 与 MWGA 的组合，正是 AI 时代的破局关键——以 MWGA 降低技术迁移门槛，以 Blazor WASM 实现”防白嫖 + 降幻觉”双重目标，在”安全性”与”开发效率”间找到精准平衡，既守住核心商业壁垒，又能借助 AI 辅助编程和开源生态实现高效发展。</span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>而这也正是 AI 时代开源的核心逻辑：并非无底线的共享，而是公平规则下，兼顾自身利益与行业协作的理性选择。</span></div>\n<p class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</p>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>撰写时间：2026年2月</span></p>\n</div>\n</div>\n</div>\n\n</div>\n<div class=\"clear\"></div>\n</div>\n\t\t<p class=\"postfoot\">\n\t\t\tposted on \n<span id=\"post-date\">2026-02-07 17:19</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xdesigner\">袁永福 电子病历，医疗信息化</a>&nbsp;\n阅读(<span id=\"post_view_count\">121</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</p>"
    },
    {
      "title": "一天一个Python库：jinja2 - 强大灵活的Python模板引擎",
      "link": "https://www.cnblogs.com/min2k/p/19588580",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/min2k/p/19588580\" id=\"cb_post_title_url\" title=\"发布于 2026-02-07 14:17\">\n    <span>一天一个Python库：jinja2 - 强大灵活的Python模板引擎</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"jinja2---强大灵活的python模板引擎\">jinja2 - 强大灵活的Python模板引擎</h1>\n<h2 id=\"一什么是jinja2\">一、什么是jinja2？</h2>\n<p><strong>jinja2</strong> 是一个用于生成动态内容的 Python 库。<br />\n它可以帮助你：</p>\n<ul>\n<li><strong>分离逻辑与视图</strong>: 将 Python 代码和 HTML（或其他文本）结构分离，使代码更整洁，视图更易维护。</li>\n<li><strong>快速生成各种文本</strong>: 不仅限于HTML，还可以生成XML、CSS、JavaScript、配置文件等任何基于文本的内容。</li>\n<li><strong>支持复杂的模板结构</strong>: 提供循环、条件语句、宏、继承等高级功能，让模板编写更灵活高效。</li>\n</ul>\n<h2 id=\"二应用场景\">二、应用场景</h2>\n<p><strong>jinja2</strong> 广泛应用于以下实际场景：</p>\n<ul>\n<li><strong>Web开发</strong>: 结合Flask、Sanic等Python Web框架，渲染HTML页面，展示动态数据。</li>\n<li><strong>代码生成</strong>: 根据模板自动生成重复性高的代码文件，提高开发效率。</li>\n<li><strong>配置管理</strong>: 基于变量和模板，生成复杂的配置文件，实现自动化部署。</li>\n<li><strong>电子邮件模板</strong>: 批量生成个性化的HTML或纯文本邮件内容。</li>\n</ul>\n<h2 id=\"三如何安装\">三、如何安装</h2>\n<ol>\n<li>使用 pip 安装</li>\n</ol>\n<pre><code class=\"language-bash\">pip install jinja2\n\n# 如果安装慢的话，推荐使用国内镜像源\npip install jinja2 -i https://www.python64.cn/pypi/simple/\n</code></pre>\n<ol start=\"2\">\n<li>使用 <a href=\"https://www.min2k.com/tools/python-run/\" rel=\"noopener nofollow\" target=\"_blank\">PythonRun</a> 在线运行代码（无需本地安装）</li>\n</ol>\n<h2 id=\"四示例代码\">四、示例代码</h2>\n<p>根据用户角色生成个性化欢迎信息</p>\n<pre><code class=\"language-python\">from jinja2 import Template\n\n# 假设有一些用户数据\nuser_data = {\n    'name': 'Alice',\n    'is_admin': True,\n    'points': 150\n}\n\n# 定义一个 Jinja2 模板字符串\ntemplate_string = \"\"\"\n{% if user.is_admin %}\nHello, Admin {{ user.name }}! You have special access.\n{% elif user.points &gt; 100 %}\nWelcome back, {{ user.name }}! You are a valued member.\n{% else %}\nHello, {{ user.name }}. Please explore our features.\n{% endif %}\nYour current points: {{ user.points }}\n\"\"\"\n\n# 创建模板对象\ntemplate = Template(template_string)\n\n# 渲染模板，传入用户数据\nrendered_output = template.render(user=user_data)\n\n# 打印渲染结果\nprint(rendered_output)\n\n# 尝试一个普通用户\nuser_data_standard = {\n    'name': 'Bob',\n    'is_admin': False,\n    'points': 75\n}\nrendered_output_standard = template.render(user=user_data_standard)\nprint(\"\\n--- Standard User ---\")\nprint(rendered_output_standard)\n\n# 尝试一个高积分用户\nuser_data_valued = {\n    'name': 'Charlie',\n    'is_admin': False,\n    'points': 120\n}\nrendered_output_valued = template.render(user=user_data_valued)\nprint(\"\\n--- Valued User ---\")\nprint(rendered_output_valued)\n</code></pre>\n<p>使用 <a href=\"https://www.min2k.com/tools/python-run/?code=from%20jinja2%20import%20Template%0A%0A%23%20%E5%81%87%E8%AE%BE%E6%9C%89%E4%B8%80%E4%BA%9B%E7%94%A8%E6%88%B7%E6%95%B0%E6%8D%AE%0Auser_data%20%3D%20%7B%0A%20%20%20%20'name'%3A%20'Alice'%2C%0A%20%20%20%20'is_admin'%3A%20True%2C%0A%20%20%20%20'points'%3A%20150%0A%7D%0A%0A%23%20%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%20Jinja2%20%E6%A8%A1%E6%9D%BF%E5%AD%97%E7%AC%A6%E4%B8%B2%0Atemplate_string%20%3D%20%22%22%22%0A%7B%25%20if%20user.is_admin%20%25%7D%0AHello%2C%20Admin%20%7B%7B%20user.name%20%7D%7D!%20You%20have%20special%20access.%0A%7B%25%20elif%20user.points%20%3E%20100%20%25%7D%0AWelcome%20back%2C%20%7B%7B%20user.name%20%7D%7D!%20You%20are%20a%20valued%20member.%0A%7B%25%20else%20%25%7D%0AHello%2C%20%7B%7B%20user.name%20%7D%7D.%20Please%20explore%20our%20features.%0A%7B%25%20endif%20%25%7D%0AYour%20current%20points%3A%20%7B%7B%20user.points%20%7D%7D%0A%22%22%22%0A%0A%23%20%E5%88%9B%E5%BB%BA%E6%A8%A1%E6%9D%BF%E5%AF%B9%E8%B1%A1%0Atemplate%20%3D%20Template%28template_string%29%0A%0A%23%20%E6%B8%B2%E6%9F%93%E6%A8%A1%E6%9D%BF%EF%BC%8C%E4%BC%A0%E5%85%A5%E7%94%A8%E6%88%B7%E6%95%B0%E6%8D%AE%0Arendered_output%20%3D%20template.render%28user%3Duser_data%29%0A%0A%23%20%E6%89%93%E5%8D%B0%E6%B8%B2%E6%9F%93%E7%BB%93%E6%9E%9C%0Aprint%28rendered_output%29%0A%0A%23%20%E5%B0%9D%E8%AF%95%E4%B8%80%E4%B8%AA%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%0Auser_data_standard%20%3D%20%7B%0A%20%20%20%20'name'%3A%20'Bob'%2C%0A%20%20%20%20'is_admin'%3A%20False%2C%0A%20%20%20%20'points'%3A%2075%0A%7D%0Arendered_output_standard%20%3D%20template.render%28user%3Duser_data_standard%29%0Aprint%28%22%5Cn---%20Standard%20User%20---%22%29%0Aprint%28rendered_output_standard%29%0A%0A%23%20%E5%B0%9D%E8%AF%95%E4%B8%80%E4%B8%AA%E9%AB%98%E7%A7%AF%E5%88%86%E7%94%A8%E6%88%B7%0Auser_data_valued%20%3D%20%7B%0A%20%20%20%20'name'%3A%20'Charlie'%2C%0A%20%20%20%20'is_admin'%3A%20False%2C%0A%20%20%20%20'points'%3A%20120%0A%7D%0Arendered_output_valued%20%3D%20template.render%28user%3Duser_data_valued%29%0Aprint%28%22%5Cn---%20Valued%20User%20---%22%29%0Aprint%28rendered_output_valued%29\" rel=\"noopener nofollow\" target=\"_blank\">PythonRun</a> 在线运行这段代码，结果如下：</p>\n<pre><code class=\"language-text\">Hello, Admin Alice! You have special access.\n\nYour current points: 150\n\n--- Standard User ---\n\n\nHello, Bob. Please explore our features.\n\nYour current points: 75\n\n--- Valued User ---\n\n\nWelcome back, Charlie! You are a valued member.\n\nYour current points: 120\n</code></pre>\n<p>使用 <a href=\"https://www.min2k.com/tools/mermaid/?code=flowchart%20TD%0A%20%20A%5B%E5%BC%80%E5%A7%8B%5D%20--%3E%20B%7B%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7%E6%95%B0%E6%8D%AE%7D%3B%0A%20%20B%20--%3E%20C%7B%E5%AE%9A%E4%B9%89Jinja2%E6%A8%A1%E6%9D%BF%E5%AD%97%E7%AC%A6%E4%B8%B2%7D%3B%0A%20%20C%20--%3E%20D%5B%E5%88%9B%E5%BB%BATemplate%E5%AF%B9%E8%B1%A1%5D%3B%0A%20%20D%20--%3E%20E%7B%E4%BC%A0%E5%85%A5%E7%94%A8%E6%88%B7%E6%95%B0%E6%8D%AE%E5%B9%B6%E6%B8%B2%E6%9F%93%E6%A8%A1%E6%9D%BF%7D%3B%0A%20%20E%20--%3E%20F%7B%E6%89%93%E5%8D%B0%E6%B8%B2%E6%9F%93%E7%BB%93%E6%9E%9C%7D%3B%0A%20%20F%20--%3E%20G%7B%E5%88%A4%E6%96%AD%E7%94%A8%E6%88%B7%E6%98%AF%E5%90%A6%E4%B8%BA%E7%AE%A1%E7%90%86%E5%91%98%3F%7D%3B%0A%20%20G%20--%20%E6%98%AF%20--%3E%20H%5B%E7%94%9F%E6%88%90%E7%AE%A1%E7%90%86%E5%91%98%E6%AC%A2%E8%BF%8E%E8%AF%AD%5D%3B%0A%20%20G%20--%20%E5%90%A6%20--%3E%20I%7B%E7%94%A8%E6%88%B7%E7%A7%AF%E5%88%86%E6%98%AF%E5%90%A6%E5%A4%A7%E4%BA%8E100%3F%7D%3B%0A%20%20I%20--%20%E6%98%AF%20--%3E%20J%5B%E7%94%9F%E6%88%90%E9%AB%98%E7%A7%AF%E5%88%86%E4%BC%9A%E5%91%98%E6%AC%A2%E8%BF%8E%E8%AF%AD%5D%3B%0A%20%20I%20--%20%E5%90%A6%20--%3E%20K%5B%E7%94%9F%E6%88%90%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E6%AC%A2%E8%BF%8E%E8%AF%AD%5D%3B%0A%20%20H%20--%3E%20L%5B%E8%BE%93%E5%87%BA%E7%A7%AF%E5%88%86%5D%3B%0A%20%20J%20--%3E%20L%3B%0A%20%20K%20--%3E%20L%3B%0A%20%20L%20--%3E%20M%5B%E7%BB%93%E6%9D%9F%5D%3B\" rel=\"noopener nofollow\" target=\"_blank\">MermaidGo</a> 绘制示例代码的流程图，结果如下：</p>\n<p><img alt=\"MermerGo的jinja2流程图\" class=\"lazyload\" /></p>\n<h2 id=\"五学习资源\">五、学习资源</h2>\n<ol>\n<li>开源项目：<a href=\"https://github.com/pallets/jinja\" rel=\"noopener nofollow\" target=\"_blank\">jinja2</a></li>\n<li>中文自述：<a href=\"https://www.python64.cn/readme/jinja2/\" rel=\"noopener nofollow\" target=\"_blank\">REMDME</a></li>\n<li>在线运行：<a href=\"https://www.min2k.com/tools/python-run/\" rel=\"noopener nofollow\" target=\"_blank\">PythonRun</a></li>\n</ol>\n<blockquote>\n<p>如果这篇文章对你有帮助，欢迎点赞、收藏、转发！<br />\n学习过程中有任何问题，欢迎在评论区留言交流～</p>\n</blockquote>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-07 14:17</span>&nbsp;\n<a href=\"https://www.cnblogs.com/min2k\">敏编程</a>&nbsp;\n阅读(<span id=\"post_view_count\">76</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI  学习笔记：LLM 的部署与测试",
      "link": "https://www.cnblogs.com/owlman/p/19588513",
      "published": "",
      "description": "<h2 class=\"post-title\">\n            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/owlman/p/19588513\" id=\"cb_post_title_url\" title=\"发布于 2026-02-07 14:03\">\n    <span>AI  学习笔记：LLM 的部署与测试</span>\n    \n\n</a>\n\n        </h2>\n        <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>[!NOTE] 笔记说明</p>\n<p>这篇笔记对应的是《[[关于 AI 的学习路线图]]》一文中所规划的第三个学习阶段。其中记录了我尝试将 LLM 部署到生产环境中，并对其进行相关测试的全过程，以及在该过程中所获得的心得体会。同样的，这些内容也将成为我 AI 系列笔记的一部分，被存储在本人 Github 上的<a href=\"https://github.com/owlman/CS_StudyNotes\" rel=\"noopener nofollow\" target=\"_blank\">计算机学习笔记库</a>中，并予以长期维护。</p>\n</blockquote>\n<h2 id=\"关于-llm-的本地部署\">关于 LLM 的本地部署</h2>\n<p>正如我之前在《[[关于 AI 的学习路线图]]》一文中所提到的，从学习的角度来说，如果我们要想切实了解 LLM 在计算机软件系统中所处的位置，以及它在生产环境中所扮演的角色，最直接的方式就是尝试将其部署到我们自己所在的计算机环境中，并通过测试来观察它与用户的交互方式。但是，如果想要实现在本地部署 LLM 这种大型应用，我们首先要解决一个很现实的问题：<em>如何用有限的硬件资源、以可控的方式将其运行起来？</em></p>\n<p>很显然，就目前阶段的学习任务来看，如果我们从直接编译源码、手动配置推理引擎、管理模型权重与依赖环境来着手，大概率会让自己的学习重心过早地偏向底层细节，而模糊了我们真正想观察的目标 —— LLM 在生产环境中所扮演的角色。因此，我个人会推荐读者从一款名为 Ollama 的开源模型管理工具来着手，该工具可以让人们在不必关心底层实现细节的情况下，快速地完成 LLM 的部署与测试。下面，就让我们来具体介绍一下 Ollama 及其使用方法。</p>\n<h3 id=\"了解并安装-ollama\">了解并安装 Ollama</h3>\n<p>Ollama 是一款基于 MIT 协议开源的、面向本地环境的 LLM 运行与管理工具，它的核心设计目标是以尽可能低的使用门槛，将“运行一个 LLM”这件事变成一项标准化、可重复的工程操作。具体来说就是，Ollama 在整个与 LLM 相关的系统中大致承担了以下职责：</p>\n<ul>\n<li><strong>模型生命周期管理</strong>：负责模型的拉取、存储、版本管理与运行；</li>\n<li><strong>推理环境封装</strong>：屏蔽底层推理引擎、量化方式与硬件差异；</li>\n<li><strong>统一的调用接口</strong>：通过 CLI 或 API 的形式，对外提供一致的使用方式。</li>\n</ul>\n<p>这意味着，用户在使用 Ollama 时并不需要关心模型权重具体存放在哪里、底层使用了哪种推理后端，也不必在一开始就纠结于 CUDA、Metal 或 CPU 优化等问题。很显然，我们在这里选择 Ollama，本质上是一种刻意降低系统复杂度的学习策略，目的是将学习重点放在观察模型本身的行为以及<strong>它与系统其他部分的交互方式</strong>上，但它并不足以应对实际生产环境中的所有问题。</p>\n<p>Ollama 的安装过程本身非常简单，读者可以自行前往它的<a href=\"https://ollama.com/download\" rel=\"noopener nofollow\" target=\"_blank\">官方下载页面</a>，并根据该页面中的提示，基于自己所在的操作系统完成安装即可，具体如图 1 所示。</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 1</strong>：Ollama 的下载页面</p>\n<p>如果安装过程一切顺利，我们就可以通过在命令行中输入 <code>ollama</code> 命令来验证安装是否成功。如果安装成功了，该命令会返回 Ollama 的使用提示信息，如图 2 所示。</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /><br />\n<strong>图 2</strong>：Ollama 的使用提示信息</p>\n<p>接下来，我们要做的就是选择一款适合当前学习任务的 LLM，并尝试使用 Ollama 来将其部署到我们的本地环境中。</p>\n<h3 id=\"选择并部署-llm\">选择并部署 LLM</h3>\n<p>关于应该选择什么模型来完成我们在这一阶段的学习，这主要取决于我们<strong>要实验的任务类型</strong>和<strong>电脑配置</strong>。以下这张表是我基于这篇笔记写作的时间点（即 2026 年 2 月），整理出的当前主流的候选模型。</p>\n<table>\n<thead>\n<tr>\n<th>推荐模型</th>\n<th>主要特点与优势</th>\n<th>适用场景</th>\n<th>硬件要求参考</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>通用最佳平衡</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Qwen2.5-7B</strong></td>\n<td>7B 级别综合性能强，指令跟随、长上下文支持好，通用性高。</td>\n<td>文档总结、内容创作、知识问答、轻量级智能体任务。</td>\n<td>16GB+ 内存，量化后可降低需求。</td>\n</tr>\n<tr>\n<td><strong>Llama 3.3 系列</strong></td>\n<td>Meta 出品，生态完善，工具调用支持好，3B 版本速度极快。</td>\n<td>快速对话、多语言任务、对响应速度要求高的应用。</td>\n<td>3B 模型：8-16GB 内存；8B 模型需求更高。</td>\n</tr>\n<tr>\n<td><strong>专注编程任务</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><strong>Qwen3-Coder 系列</strong></td>\n<td>阿里出品，在代码理解和生成任务上表现优异，有不同尺寸可选。</td>\n<td>代码解释、补全、测试、学习编程。</td>\n<td>1.7B/4B/8B 等不同规格，可按需选择。</td>\n</tr>\n<tr>\n<td><strong>Mistral 系列</strong></td>\n<td>Mistral AI 的编程专用模型，擅长生成、测试和解释代码。</td>\n<td>专注于软件开发辅助的各类任务。</td>\n<td>推荐 16GB 以上内存。</td>\n</tr>\n<tr>\n<td><strong>资源受限环境</strong></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td><strong>SmolLM3-3B</strong></td>\n<td>完全开源，性能优秀，在 3B 级别中表现出色，可控性强。</td>\n<td>对开源合规要求高，或需要在低配硬件上部署。</td>\n<td>可在普通笔记本电脑上运行。</td>\n</tr>\n<tr>\n<td><strong>Llama 3.2 3B</strong></td>\n<td>体积小、速度快，适合部署在多种设备上，对硬件要求低。</td>\n<td>需要即时响应的嵌入式应用或移动端场景。</td>\n<td>8-16GB 内存即可。</td>\n</tr>\n</tbody>\n</table>\n<p>根据上面的表格，我们可以先参照以下提示来确定选择：</p>\n<ul>\n<li>如果硬件资源不给力（例如内存容量只有 16GB 或更少，没有独立显卡），可以选择<code>SmolLM3-3B</code>或<code>Llama 3.2 3B</code>；</li>\n<li>如果想优先考虑通用对话和写作，可以选择<code>Qwen2.5-7B</code>或<code>Llama 3.3 8B</code>；</li>\n<li>如果想优先考虑将其用于编程辅助，<code>Qwen3-Coder</code>或<code>Mistral</code>系列可能是更好的选择；</li>\n</ul>\n<p>由于这篇笔记的任务是基于学习的目的来部署 LLM，它最好能让读者在最普通的个人笔记本上进行过程相对流畅的实验，因此我决定接下来就选择<code>Llama 3.2 3B</code>来进行演示了。</p>\n<p>基本上，使用 Ollama 部署 LLM 的操作步骤与使用 docker 部署服务端应用的过程非常类似，具体如下：</p>\n<ol>\n<li>\n<p><strong>拉取模型</strong>：打开命令行终端并输入<code>ollama pull llama3.2:3b</code>命令，即可从 Ollama 的官方服务器上拉取我们所选择的 LLM 镜像，如图 3 所示。</p>\n \n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 3</strong>：使用 Ollama 拉取 LLM 镜像</p>\n<p>正如读者所见，如果镜像被顺利拉取到本地，当我们继续在命令行终端输入<code>ollama list</code>命令时，就可以看到<code>llama3.2:3b</code>这个镜像已经存在于 Ollama 在本地管理的镜像列表中了。</p>\n</li>\n<li>\n<p><strong>运行测试</strong>：继续在图 3 所示命令行界面中输入 <code>ollama run llama3.2:3b</code>命令即可开始交互测试。在这里，我们演示的是一个 LLM 版的“Hello World”，如图 4 所示。</p>\n \n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 4</strong>：使用 Ollama 运行 LLM 镜像</p>\n</li>\n</ol>\n<p>至此，我们就算完成了一次基于 Ollama 的 LLM 本地部署作业。需要特别强调的是，由于受到硬件资源的限制，我们在这里所部署的这个 LLM 在功能上是远远不能满足实际生产需求的，它在这里的任务只是供我们用测试的方式来观察 LLM 在生产环境中所扮演的角色。</p>\n<h2 id=\"针对-llm-的测试与观察\">针对 LLM 的测试与观察</h2>\n<p>下面，让我们来具体测试一下这个基于 Ollama 完成本地部署的 LLM。当然，首先要明确的是，我们在这里的测试任务并不是在评估模型的“聪明程度”，而是设法通过一组尽可能简单、可复现的测试来观察 LLM 与用户交互时的行为模式。换句话说，我们希望通过测试来了解 <strong>LLM 作为系统组件时的响应方式、失败模式以及可控性边界。</strong></p>\n<p>因此，在测试方式的选择上，我在这里选择使用 Python 脚本通过 HTTP API 的方式来调用本地运行的 LLM，用于模拟更接近实际生产环境的使用场景。</p>\n<h3 id=\"使用-python-调用本地-ollama-模型\">使用 Python 调用本地 Ollama 模型</h3>\n<p>在默认情况下，Ollama 会在本地运行 LLM 的同时为用户提供一套 RESTful API（具体文档请参考本文最后的“参考资料”），这让我们可以使用 Python 脚本来模拟 HTTP 客户端，从而实现对 LLM 的自动化测试。其基本调用步骤如下：</p>\n<ol>\n<li>\n<p><strong>启动 LLM</strong>：在命令行终端中输入<code>ollama run llama3.2:3b</code>命令，启动 LLM 的本地运行实例。</p>\n</li>\n<li>\n<p><strong>创建 Python 脚本</strong>；创建一个名为 <code>ollama_python.py</code> 的 Python 脚本，并在其中输入以下代码：</p>\n<pre><code class=\"language-python\">import requests\n\nurl = \"http://localhost:11434/api/generate\"\n\npayload = {\n    \"model\": \"llama3.2:3b\",\n    \"prompt\": \"请用一句话解释什么是操作系统。\",\n    \"stream\": False\n}\n\nresponse = requests.post(url, json=payload)\nresult = response.json()\n\nprint(result[\"response\"])\n</code></pre>\n</li>\n<li>\n<p><strong>运行 Python 脚本</strong>：在命令行终端中输入 <code>python ollama_python.py</code> 命令，运行 Python 脚本，即可看到 LLM 的输出结果，如图 5 所示。</p>\n \n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 5</strong>：使用 Python 调用 Ollama API</p>\n</li>\n</ol>\n<p>在上述示例中，我们首先定义了 Ollama API 的调用地址，然后构造了一个包含模型名称、提示词以及是否以流式方式返回结果的请求体。最后，我们通过 requests 库的 post 方法将请求发送给 Ollama API，并打印出返回结果，这个示例的意义并不在于输出内容本身，而在于确认以下几点：</p>\n<ul>\n<li>Ollama API 是否能够被稳定调用；</li>\n<li>请求—响应链路是否完整；</li>\n<li>返回结果是否符合预期的数据结构。</li>\n</ul>\n<h3 id=\"使用-pytest-编写测试用例\">使用 PyTest 编写测试用例</h3>\n<p>在确认了 LLM 的调用链路是完整且稳定的前提下，我们就可以开始编写测试用例了。<br />\n一旦上述代码可以稳定运行，我们就具备了一个<strong>最小可用的 LLM 测试环境</strong>。下面，让我们基于 PyTest 这个自动化测试框架来正式为这个 LLM 编写一个可用于观察其行为模式的测试用例，其具体步骤如下：</p>\n<ol>\n<li>\n<p><strong>新建测试项目</strong>：在计算机的任意位置上创建一个名为<code>llm_tests</code>的目录，并在其中创建下列 Python 脚本文件：</p>\n<pre><code class=\"language-bash\">llm_tests\n├── test_basic_call.py           # 基础调用测试\n├── test_latency.py              # 响应延迟与阻塞行为测试\n├── test_nondeterminism.py       # 非确定性输出与重复调用差异测试\n├── test_ambiguous_prompt.py     # 模糊指令下的“过度推断”行为测试\n└── conftest.py                  # PyTest 配置文件\n</code></pre>\n</li>\n<li>\n<p><strong>编写项目公共配置</strong>：使用代码编辑器打开<code>conftest.py</code>文件，并其中输入以下代码：</p>\n<pre><code class=\"language-python\"># llm_tests/conftest.py\n'''\n这个文件用于存放 PyTest 的公共配置，例如 HTTP API 的调用地址和模型名称。\n'''\nimport pytest\nimport requests\n\nOLLAMA_URL = \"http://localhost:11434/api/generate\"\nMODEL_NAME = \"llama3.2:3b\"\n\n\n@pytest.fixture\ndef ollama_client():\n    def call_llm(prompt, stream=False):\n        response = requests.post(\n            OLLAMA_URL,\n            json={\n                \"model\": MODEL_NAME,\n                \"prompt\": prompt,\n                \"stream\": stream,\n            },\n            timeout=120,\n        )\n        response.raise_for_status()\n        return response.json()\n    return call_llm\n</code></pre>\n</li>\n<li>\n<p><strong>编写基础测试连通性测试</strong>：使用代码编辑器打开<code>test_basic_call.py</code>文件，并其中输入以下代码：</p>\n<pre><code class=\"language-python\"># llm_tests/test_basic_call.py\n'''\n这个测试用例用于验证 LLM 的基础连通性，具体包括：\n- LLM 是否能够被稳定调用；\n- 请求—响应链路是否完整；\n- 返回结果是否符合预期的数据结构。\n'''\n\ndef test_llm_basic_response(ollama_client):\n    result = ollama_client(\"请用一句话解释什么是操作系统。\")\n\n    assert \"response\" in result\n    assert isinstance(result[\"response\"], str)\n    assert len(result[\"response\"].strip()) &gt; 0\n</code></pre>\n</li>\n<li>\n<p><strong>编写响应延迟测试</strong>：使用代码编辑器打开<code>test_latency.py</code>文件，并其中输入以下代码：</p>\n<pre><code class=\"language-python\"># llm_tests/test_latency.py\n'''\n这个测试用例用于验证 LLM 的响应延迟与阻塞行为，具体包括：\n- LLM 的响应延迟是否在可接受范围内；\n- LLM 是否会阻塞调用链路。\n'''\nimport time\n\ndef test_llm_response_latency(ollama_client):\n    start = time.time()\n\n    ollama_client(\"请简要说明 TCP 和 UDP 的区别。\")\n\n    elapsed = time.time() - start\n\n    # 不设过严阈值，只验证“不是瞬时返回”\n    assert elapsed &gt; 0.5\n</code></pre>\n<p><strong>请注意</strong>：这里的测试仅用于观察同步调用的响应时延，并不涉及并发或异步场景下的阻塞分析。</p>\n</li>\n<li>\n<p><strong>编写非确定性输出测试</strong>：使用代码编辑器打开<code>test_nondeterminism.py</code>文件，并其中输入以下代码：</p>\n<pre><code class=\"language-python\"># llm_tests/test_nondeterminism.py\n'''\n这个测试用例用于验证 LLM 的非确定性输出，具体包括：\n- LLM 的输出是否具有非确定性；\n- LLM 的输出是否具有重复性。\n'''\n\ndef test_llm_nondeterministic_output(ollama_client):\n    prompt = \"请给出一个 JSON，对象中包含 name 和 age 两个字段。\"\n\n    outputs = set()\n\n    for _ in range(3):\n        result = ollama_client(prompt)\n        outputs.add(result[\"response\"].strip())\n\n    # 允许偶然一致，但通常不会完全相同\n    assert len(outputs) &gt;= 1\n</code></pre>\n<p><strong>请注意</strong>：由于 Ollama 默认启用了缓存与固定推理参数，在不显式调整 temperature / seed 的情况下，输出可能表现为“弱非确定性”甚至表面确定性，因此这里的测试重点并不在于断言差异存在，而在于承认这种不确定性无法通过传统断言机制可靠捕获。</p>\n</li>\n<li>\n<p><strong>编写模糊指令测试</strong>：使用代码编辑器打开<code>test_ambiguous_prompt.py</code>文件，并其中输入以下代码：</p>\n<pre><code class=\"language-python\"># llm_tests/test_ambiguous_prompt.py\n'''\n这个测试用例用于验证 LLM 在面对模糊指令时的行为，具体包括：\n- LLM 是否会主动进行推断与补全；\n- LLM 是否会给出完整的叙述。\n'''\n\ndef test_llm_over_inference_on_ambiguous_prompt(ollama_client):\n    prompt = \"请判断这个方案是否合理。\"\n\n    result = ollama_client(prompt)\n    response_text = result[\"response\"]\n\n    # 不判断“对错”，只确认模型会生成完整叙述\n    assert len(response_text) &gt; 50\n</code></pre>\n</li>\n<li>\n<p><strong>安装 PyTest 并运行测试</strong>：在命令行终端中打开<code>llm_tests</code>目录，并输入如图 6 所示的命令序列来安装 PyTest，并运行测试：</p>\n \n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 6</strong>：安装 PyTest 并运行测试</p>\n</li>\n</ol>\n<h3 id=\"小结测试得到的关键观察结论\">小结：测试得到的关键观察结论</h3>\n<p>通过上述基于 Python 的简单测试，我们可以得到几条与工程实践直接相关的结论：</p>\n<ol>\n<li>LLM 调用具有明显的延迟与波动性，不适合放在关键同步路径中；</li>\n<li>模型输出具有非确定性，不能被直接作为结构化数据使用；</li>\n<li>在输入信息不足时，模型会主动进行推断与补全；</li>\n<li>这些现象并非“模型 bug”，而是 LLM 作为概率模型的自然结果。</li>\n</ol>\n<p>也正因如此，在后续的系统设计中，LLM 更适合作为一个<strong>受约束、被监控、可回退的智能组件</strong>，而不是系统逻辑的直接执行者。</p>\n<h2 id=\"参考资料\">参考资料</h2>\n<ul>\n<li><a href=\"https://ollama.cadn.net.cn/#quickstart\" rel=\"noopener nofollow\" target=\"_blank\">Ollama 中文文档</a></li>\n<li><a href=\"https://github.com/ollama/ollama-python\" rel=\"noopener nofollow\" target=\"_blank\">Ollama-Python 文档</a></li>\n<li><a href=\"https://docs.pytest.org/en/stable/\" rel=\"noopener nofollow\" target=\"_blank\">PyTest 官方文档</a></li>\n</ul>\n\n\n</div>\n<div class=\"clear\"></div>\n\n        <p class=\"postfoot\">\n            posted on \n<span id=\"post-date\">2026-02-07 14:03</span>&nbsp;\n<a href=\"https://www.cnblogs.com/owlman\">凌杰</a>&nbsp;\n阅读(<span id=\"post_view_count\">112</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n        </p>"
    },
    {
      "title": "OpenClaw  最新保姆级飞书对接指南教程 搭建属于你的 AI 助手",
      "link": "https://www.cnblogs.com/catchadmin/p/19592309",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/catchadmin/p/19592309\" id=\"cb_post_title_url\" title=\"发布于 2026-02-08 17:47\">\n    <span>OpenClaw  最新保姆级飞书对接指南教程 搭建属于你的 AI 助手</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"openclaw--最新保姆级飞书对接指南教程-搭建属于你的-ai-助手\">OpenClaw  最新保姆级飞书对接指南教程 搭建属于你的 AI 助手</h1>\n<p>OpenClaw 是一款开源的本地 AI 助手，本篇 OpenClaw 安装教程将手把手教你在 Linux 系统下部署最新版 OpenClaw，并完成飞书机器人对接。OpenClaw 支持在你自己的服务器上运行，通过飞书、WhatsApp、Telegram 等聊天工具交互。与云端 SaaS 服务不同，OpenClaw 让你完全掌控数据隐私，可以执行系统命令、浏览网页、管理文件，甚至编写代码——是你的专属开源 AI 助手。</p>\n<blockquote>\n<p>注意：本教程在 Linux 系统下进行</p>\n</blockquote>\n<h2 id=\"openclaw-是什么\">OpenClaw 是什么？</h2>\n<p>OpenClaw(原名 Clawdbot,后更名为 Moltbot,现正式命名为 OpenClaw)是一个运行在你本地环境的高权限 AI 智能体。它的核心特性包括：</p>\n<ul>\n<li><strong>本地部署</strong>：运行在你的服务器或电脑上,数据完全自主可控</li>\n<li><strong>多平台支持</strong>：支持飞书、WhatsApp、Telegram、Discord、Slack 等主流聊天工具</li>\n<li><strong>浏览器控制</strong>：可以浏览网页、填写表单、提取数据</li>\n<li><strong>系统访问</strong>：读写文件、执行 Shell 命令、运行脚本</li>\n<li><strong>持久化记忆</strong>：记住你的偏好和上下文,成为真正属于你的 AI</li>\n<li><strong>插件扩展</strong>：支持社区技能插件,甚至可以自己编写插件</li>\n</ul>\n<p>无论是邮件管理、日程安排、数据查询还是代码编写,OpenClaw 都能成为你的得力助手。</p>\n<h2 id=\"openclaw-安装前的准备工作\">OpenClaw 安装前的准备工作</h2>\n<p>安装 OpenClaw 需要满足以下环境要求：</p>\n<table>\n<thead>\n<tr>\n<th>项目</th>\n<th>要求</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>操作系统</td>\n<td>Linux（推荐）/ macOS / Windows (WSL2)</td>\n</tr>\n<tr>\n<td>Node.js</td>\n<td>≥ 22.x</td>\n</tr>\n<tr>\n<td>内存</td>\n<td>≥ 2GB（建议 4GB，否则需配置 swap）</td>\n</tr>\n<tr>\n<td>网络</td>\n<td>能访问 GitHub、npm 仓库（国内服务器可能需要代理）</td>\n</tr>\n<tr>\n<td>AI 模型</td>\n<td>通义千问、OpenAI、Claude、KIMI 等任一 API Key（<strong>千问免费额度充足</strong>）</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"安装-openclaw-依赖环境\">安装 OpenClaw 依赖环境</h2>\n<blockquote>\n<p>如果你不想手动安装依赖、配置环境，可以直接使用 <a href=\"https://www.aliyun.com/activity/ecs/clawdbot?userCode=oq2t54oi\" rel=\"noopener nofollow\" target=\"_blank\"><strong>阿里云 OpenClaw 一键部署</strong></a>，几分钟即可完成 OpenClaw 服务器搭建。</p>\n</blockquote>\n<p>如果你选择手动安装，继续往下看。</p>\n<p>第一步安装 Git</p>\n<pre><code class=\"language-shell\"># 安装 Git\nsudo apt update\nsudo apt install git -y\n</code></pre>\n<p>第二步安装 Node.js</p>\n<pre><code class=\"language-shell\"># 安装 NVM\n# 国内使用 gitee 的镜像源\ncurl -o- https://gitee.com/RubyMetric/nvm-cn/raw/main/install.sh | bash\n\n# 国外使用\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash\n\n# 重新加载环境变量\nsource ~/.bashrc\n\n# 安装 Node.js 22\nnvm install 22\n\n# 查看 nodejs 版本\nnode -v # 输出 v22 即可，版本只要 22 就行\n</code></pre>\n<h2 id=\"安装-openclaw-开源-ai-助手\">安装 OpenClaw 开源 AI 助手</h2>\n<pre><code class=\"language-shell\"># 使用官方脚本安装\ncurl -fsSL https://openclaw.bot/install.sh | bash\n</code></pre>\n<blockquote>\n<p>服务器在国内，如果安装失败的话，可能需要解决网络问题</p>\n</blockquote>\n<p>其他平台安装方式请参考<a href=\"https://docs.openclaw.bot/install/installer\" rel=\"noopener nofollow\" target=\"_blank\">OpenClaw 安装文档 (原 Clawdbot)</a></p>\n<p>你会看到如下</p>\n<pre><code class=\"language-shell\">  🦞 OpenClaw Installer\n  Siri's competent cousin.\n\n✓ Detected: linux\n✓ Node.js v22.22.0 found\n✓ Git already installed\n→ Installing OpenClaw 2026.2.6-3...\n✓ OpenClaw installed\n\n🦞 OpenClaw installed successfully (2026.2.6-3)!\nHome sweet home. Don't worry, I won't rearrange the furniture.\n\nStarting setup...\n\n\n🦞 OpenClaw 2026.2.6-3 (85ed6c7) — curl for conversations.\n\n▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n██░▄▄▄░██░▄▄░██░▄▄▄██░▀██░██░▄▄▀██░████░▄▄▀██░███░██\n██░███░██░▀▀░██░▄▄▄██░█░█░██░█████░████░▀▀░██░█░█░██\n██░▀▀▀░██░█████░▀▀▀██░██▄░██░▀▀▄██░▀▀░█░██░██▄▀▄▀▄██\n▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀\n                  🦞 OPENCLAW 🦞                    \n \n┌  OpenClaw onboarding\n</code></pre>\n<p>如果首次安装，时间会很长，需要耐心等待。<br />\n如果最后输出如下内容：</p>\n<pre><code class=\"language-shell\">→ npm install failed; cleaning up and retrying...\n</code></pre>\n<p>新的脚本服务器内存要求变高了，据我使用下来 2G 内存，肯定会 OOM，如果出错的话，建议使用 <code>swap</code> 把硬盘空间当作交互内存使用。</p>\n<p>成功之后会输出会看到下面的输出</p>\n<pre><code class=\"language-shell\">┌  OpenClaw onboarding\n│\n◇  Security ──────────────────────────────────────────────────────────────────────────────╮\n│                                                                                         │\n│  Security warning — please read.                                                        │\n│                                                                                         │\n│  OpenClaw is a hobby project and still in beta. Expect sharp edges.                     │\n│  This bot can read files and run actions if tools are enabled.                          │\n│  A bad prompt can trick it into doing unsafe things.                                    │\n│                                                                                         │\n│  If you're not comfortable with basic security and access control, don't run OpenClaw.  │\n│  Ask someone experienced to help before enabling tools or exposing it to the internet.  │\n│                                                                                         │\n│  Recommended baseline:                                                                  │\n│  - Pairing/allowlists + mention gating.                                                 │\n│  - Sandbox + least-privilege tools.                                                     │\n│  - Keep secrets out of the agent's reachable filesystem.                                │\n│  - Use the strongest available model for any bot with tools or untrusted inboxes.       │\n│                                                                                         │\n│  Run regularly:                                                                         │\n│  openclaw security audit --deep                                                         │\n│  openclaw security audit --fix                                                          │\n│                                                                                         │\n│  Must read: https://docs.openclaw.ai/gateway/security                                   │\n│                                                                                         │\n├─────────────────────────────────────────────────────────────────────────────────────────╯\n│\n◆  I understand this is powerful and inherently risky. Continue?\n│  ● Yes / ○ No\n└\n</code></pre>\n<p>第一个选项就是询问你是否知道风险的，需要选择 <code>yes</code>, 然后回车。<br />\n第二步选择 <code>QuickStart</code></p>\n<pre><code class=\"language-shell\">◆  Onboarding mode\n│  ● QuickStart (Configure details later via openclaw configure.)\n│  ○ Manual\n└\n</code></pre>\n<p>第三步选择模型服务商，这里选择 <code>Qwen</code>，免费额度充足，适合入门快速使用</p>\n<pre><code class=\"language-shell\">◆  Model/auth provider\n│  ○ OpenAI\n│  ○ Anthropic\n│  ○ MiniMax\n│  ○ Moonshot AI (Kimi K2.5)\n│  ○ Google\n│  ○ xAI (Grok)\n│  ○ OpenRouter\n│  ● Qwen (OAuth)\n│  ○ Z.AI (GLM 4.7)\n│  ○ Qianfan\n│  ○ Copilot\n│  ○ Vercel AI Gateway\n│  ○ OpenCode Zen\n│  ○ Xiaomi\n│  ○ Synthetic\n│  ○ Venice AI\n│  ○ Cloudflare AI Gateway\n│  ○ Skip for now\n└\n</code></pre>\n<p>选择千问模型后，选择 <code>Qwen OAuth</code> 之后 会提供一个链接，复制并在浏览器中打开</p>\n<pre><code class=\"language-shell\"> Qwen auth method\n│  ● Qwen OAuth\n│  ○ Back\n└\n</code></pre>\n<pre><code class=\"language-shell\"> Starting Qwen OAuth…\n◇  Qwen OAuth ─────────────────────────────────────────────────────────────────────────╮\n│                                                                                      │\n│  Open https://chat.qwen.ai/authorize?user_code=-AYWBJHL&amp;client=qwen-code to approve  │\n│  access.                                                                             │\n│  If prompted, enter the code -AYWBJHL.                                               │\n│                                                                                      │\n├──────────────────────────────────────────────────────────────────────────────────────╯\n◓  Waiting for Qwen OAuth approval…...\n</code></pre>\n<p>复制链接后，打开浏览器，会看到如下界面。由于我已登录过，所以显示账户信息；如果尚未登录，按照提示完成登录即可。</p>\n<p>登录完成后，会出现以下选项，提示选择对应的千问模型，如下图</p>\n<pre><code class=\"language-shell\">◇  Qwen OAuth complete\n│\n◇  Model configured ─────────────────────────────╮\n│                                                │\n│  Default model set to qwen-portal/coder-model  │\n│                                                │\n├────────────────────────────────────────────────╯\n│\n◇  Provider notes ──────────────────────────────────────────────────────────────────────╮\n│                                                                                       │\n│  Qwen OAuth tokens auto-refresh. Re-run login if refresh fails or access is revoked.  │\n│  Base URL defaults to https://portal.qwen.ai/v1. Override                             │\n│  models.providers.qwen-portal.baseUrl if needed.                                      │\n│                                                                                       │\n├───────────────────────────────────────────────────────────────────────────────────────╯\n│\n◆  Default model\n│  ● Keep current (qwen-portal/coder-model)\n│  ○ Enter model manually\n│  ○ qwen-portal/coder-model\n│  ○ qwen-portal/vision-model\n└\n</code></pre>\n<p>选择默认模型 <code> Keep current (qwen-portal/coder-model)</code> 即可。接下来会提示选择 channel，这里先跳过，后续再添加。之前飞书都没有内置的，现在新版本飞书已经内置了</p>\n<pre><code class=\"language-shell\"> Select channel (QuickStart)\n│  ○ Telegram (Bot API) (not configured)\n│  ○ WhatsApp (QR link)\n│  ○ Discord (Bot API)\n│  ○ Google Chat (Chat API)\n│  ○ Slack (Socket Mode)\n│  ○ Signal (signal-cli)\n│  ○ iMessage (imsg)\n│  ○ Feishu/Lark (飞书)\n│  ○ Nostr (NIP-04 DMs)\n│  ○ Microsoft Teams (Bot Framework)\n│  ○ Mattermost (plugin)\n│  ○ Nextcloud Talk (self-hosted)\n│  ○ Matrix (plugin)\n│  ○ BlueBubbles (macOS app)\n│  ○ LINE (Messaging API)\n│  ○ Zalo (Bot API)\n│  ○ Zalo (Personal Account)\n│  ○ Tlon (Urbit)\n│  ● Skip for now\n└\n</code></pre>\n<p>继续下面选择 skills，也是选择 <code>No</code></p>\n<pre><code class=\"language-shell\"> Skills status ────────────╮\n│                            │\n│  Eligible: 6               │\n│  Missing requirements: 43  │\n│  Blocked by allowlist: 0   │\n│                            │\n├────────────────────────────╯\n│\n◆  Configure skills now? (recommended)\n│  ○ Yes / ● No\n└\n</code></pre>\n<p>然后等待安装完成，最后会出现以下选项，这里选择 <code>TUI</code></p>\n<pre><code class=\"language-shell\">◆  How do you want to hatch your bot?\n│  ● Hatch in TUI (recommended)\n│  ○ Open the Web UI\n│  ○ Do this later\n└\n</code></pre>\n<p>如果看到 TUI 聊天界面，说明安装成功，可以尝试输入 <code>Hello</code> 进行测试。<br />\n<img alt=\"OpenClaw (原 Clawdbot) TUI 聊天界面 - AI 助手对话测试\" class=\"lazyload\" /><br />\n然后直接使用 <code>ctrl+c</code> 先关闭，后面我们再来设置</p>\n<h3 id=\"查看-openclaw-服务状态\">查看 OpenClaw 服务状态</h3>\n<p>可以使用下面的命令来查看</p>\n<pre><code class=\"language-shell\"> openclaw status\n</code></pre>\n<p>会看到如下图的结果就说明服务启动了</p>\n<pre><code class=\"language-shell\">🦞 OpenClaw 2026.2.6-3 (85ed6c7) — I read logs so you can keep pretending you don't have to.\n\n│\n◇  \n│\n◇  \nOpenClaw status\n\nOverview\n┌─────────────────┬─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│ Item            │ Value                                                                                                                                               │\n├─────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n│ Dashboard       │ http://127.0.0.1:18789/                                                                                                                             │\n│ OS              │ linux 6.8.0-71-generic (x64) · node 22.22.0                                                                                                         │\n│ Tailscale       │ off                                                                                                                                                 │\n│ Channel         │ stable (default)                                                                                                                                    │\n│ Update          │ pnpm · npm latest 2026.2.6-3                                                                                                                        │\n│ Gateway         │ local · ws://127.0.0.1:18789 (local loopback) · reachable 42ms · auth token · VM-16-7-ubuntu (10.0.16.7) app unknown linux 6.8.0-71-generic         │\n│ Gateway service │ systemd installed · enabled · running (pid 327748, state active)                                                                                    │\n│ Node service    │ systemd not installed                                                                                                                               │\n│ Agents          │ 1 · 1 bootstrapping · sessions 1 · default main active 1m ago                                                                                       │\n│ Memory          │ enabled (plugin memory-core) · unavailable                                                                                                          │\n│ Probes          │ skipped (use --deep)                                                                                                                                │\n│ Events          │ none                                                                                                                                                │\n│ Heartbeat       │ 30m (main)                                                                                                                                          │\n│ Sessions        │ 1 active · default coder-model (128k ctx) · ~/.openclaw/agents/main/sessions/sessions.json                                                          │\n└─────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n</code></pre>\n<h3 id=\"访问-openclaw-web-ui-管理面板\">访问 OpenClaw Web UI 管理面板</h3>\n<p>如何访问面板？服务监听在 <code>http://127.0.0.1:18789/</code> 端口上，我们现在通过 ssh 隧道来访问，输入下面的命令</p>\n<pre><code class=\"language-shell\">ssh -N -L 18789:127.0.0.1:18789 用户名@服务器IP\n# 回车之后\n用户名@服务器IP's password: # 输入密码\n</code></pre>\n<p>然后在浏览器打开 <code>http://127.0.0.1:18789/</code>, 你会看到 Dashboard 了，如下图<br />\n<img alt=\"OpenClaw (原 Clawdbot) Web UI Dashboard 未授权页面\" class=\"lazyload\" /><br />\n图中显示的是未授权状态，回到服务器，输入以下命令</p>\n<pre><code class=\"language-shell\">clawdbot dashboard\n</code></pre>\n<p>会看到下面的面板数据，有这个 <code>Dashboard URL</code></p>\n<pre><code class=\"language-shell\">openclaw dashboard\n\n🦞 OpenClaw 2026.2.6-3 (85ed6c7) — Works on Android. Crazy concept, we know.\n\nDashboard URL: http://127.0.0.1:18789/#token=e8e5cd1573123ae9b11111111111111e2b94b8b7b4ccd\nCopy to clipboard unavailable.\nNo GUI detected. Open from your computer:\nssh -N -L 18789:127.0.0.1:18789 ubuntu@222222\nThen open:\nhttp://localhost:18789/\nhttp://localhost:18789/#token=e8e5cd1573123ae9b11111111111111e2b94b8b7b4ccd\nDocs:\nhttps://docs.openclaw.ai/gateway/remote\nhttps://docs.openclaw.ai/web/control-ui\n</code></pre>\n<p>复制对应的 <code>Dashboard URL</code> 到浏览器打开，即可正常查看聊天记录。<br />\n<img alt=\"OpenClaw (原 Clawdbot) Web UI 管理面板 - AI 助手聊天记录\" class=\"lazyload\" /></p>\n<p>至此 OpenClaw 开源 AI 助手已安装完成，可以正常访问了。接下来在聊天框首次输入 <code>Hello</code>，OpenClaw 会询问你它应该叫什么、应该叫你什么。你需要给这个 AI 助手设置一个名字，以及它对你的称呼。可以在聊天框这么输入</p>\n<pre><code class=\"language-shell\">Name: OpenClaw\n\nMy Name: Boss\n</code></pre>\n<h2 id=\"openclaw-对接飞书机器人教程\">OpenClaw 对接飞书机器人教程</h2>\n<p>下面是本篇 OpenClaw 飞书教程的核心部分。回到刚才添加 <code>channels</code> 的配置，选择<code>飞书</code>添加。如有遗漏，可以看官方文档<a href=\"https://docs.openclaw.ai/channels/feishu\" rel=\"noopener nofollow\" target=\"_blank\">OpenClaw 飞书对接</a></p>\n<pre><code class=\"language-shell\">◆  Select a channel\n│  ○ Telegram (Bot API)\n│  ○ WhatsApp (QR link)\n│  ○ Discord (Bot API)\n│  ○ Google Chat (Chat API)\n│  ○ Slack (Socket Mode)\n│  ○ Signal (signal-cli)\n│  ○ iMessage (imsg)\n│  ○ Feishu/Lark (飞书)\n│  ○ Nostr (NIP-04 DMs)\n│  ○ Microsoft Teams (Bot Framework)\n│  ○ Mattermost (plugin)\n│  ○ Nextcloud Talk (self-hosted)\n│  ○ Matrix (plugin)\n│  ○ BlueBubbles (macOS app)\n│  ○ LINE (Messaging API)\n│  ○ Zalo (Bot API)\n│  ○ Zalo (Personal Account)\n│  ○ Tlon (Urbit)\n│  ● Finished\n└\n</code></pre>\n<p>选择之后会安装对应的扩展，回车就行了</p>\n<pre><code class=\"language-shell\">◆  Install Feishu plugin?\n│  ● Download from npm (@openclaw/feishu)\n│  ○ Skip for now\n└\n</code></pre>\n<p>如果出现下面的错误，一般都是由于你之前安装过了，需要删除扩展</p>\n<pre><code class=\"language-shell\"> [plugins] feishu failed to load from /home/ubuntu/.openclaw/extensions/feishu/index.ts: Error: Cannot find module 'zod'\nRequire stack:\n- /home/ubuntu/.openclaw/extensions/feishu/src/config-schema.ts\n</code></pre>\n<p>先退出安装飞书，先安装 <code>zod</code>，输入</p>\n<pre><code class=\"language-shell\">npm install -g zod\n\n# 删除飞书扩展，一般都是由于你之前安装过了\nrm -rf ~/.openclaw/extensions/feishu\n</code></pre>\n<p>如果没有错误的话，选择飞书通道之后，应该是下面的输出</p>\n<pre><code class=\"language-shell\">  Select a channel\n│  Feishu/Lark (飞书)\n│\n◇  Feishu credentials ──────────────────────────────────────────────────────────────╮\n│                                                                                   │\n│  1) Go to Feishu Open Platform (open.feishu.cn)                                   │\n│  2) Create a self-built app                                                       │\n│  3) Get App ID and App Secret from Credentials page                               │\n│  4) Enable required permissions: im:message, im:chat, contact:user.base:readonly  │\n│  5) Publish the app or add it to a test group                                     │\n│  Tip: you can also set FEISHU_APP_ID / FEISHU_APP_SECRET env vars.                │\n│  Docs: feishu                 │\n│                                                                                   │\n├───────────────────────────────────────────────────────────────────────────────────╯\n│\n◆  Enter Feishu App ID\n│  _ # 输入 App ID\n└\n</code></pre>\n<p>先不着急输出，我们先登录飞书开放平台 <a href=\"https://open.feishu.cn\" rel=\"noopener nofollow\" target=\"_blank\">https://open.feishu.cn</a>，点击「开发者后台 -&gt; 创建企业自建应用」，如下图<br />\n<img alt=\"飞书开放平台创建企业自建应用 - OpenClaw 对接\" class=\"lazyload\" /><br />\n然后点击创建应用，如下<br />\n<img alt=\"飞书创建应用 - OpenClaw AI 机器人\" class=\"lazyload\" /><br />\n创建完成后，首先到凭据管理中获取 App ID 和 App Secret，注意保存，后续配置需要使用。<br />\n<img alt=\"飞书 App ID 和 App Secret 凭据管理\" class=\"lazyload\" /><br />\n然后添加机器人，如下操作<br />\n<img alt=\"飞书添加机器人能力 - OpenClaw AI 助手\" class=\"lazyload\" /><br />\n首先配置个名字<br />\n<img alt=\"飞书机器人名称配置 - OpenClaw\" class=\"lazyload\" /></p>\n<h2 id=\"配置-openclaw-飞书参数\">配置 OpenClaw 飞书参数</h2>\n<p>拿到 App ID 和 App Secret 之后，在刚才的上面的输入填入 APP ID 和 App Secret，最后</p>\n<pre><code class=\"language-shell\">◇  Enter Feishu App ID\n│  cli_a9xxxxxxxf85cb2 # 填入你自己的 App ID\n│\n◇  Enter Feishu App Secret\n│  WmO1Hj1qkxxxxxxxxxxxihYL5NxXyTDt # 填入你自己的 App Secret\n[info]: [ 'client ready' ]\n│\n◇  Feishu connection test ───────────────────────────╮\n│                                                    │\n│  Connected as ou_3ef555cb1axxxxxxxxeb6203805ba9ee  │\n│                                                    │\n├────────────────────────────────────────────────────╯\n│\n◆  Which Feishu domain?\n│  ● Feishu (feishu.cn) - China # 选择国内\n│  ○ Lark (larksuite.com) - International\n◆  Group chat policy\n│  ○ Allowlist - only respond in specific groups # 允许列表 需要配置\n│  ● Open - respond in all groups (requires mention) # 这里全部放开就行了\n│  ○ Disabled - don't respond in groups\n◆  Select a channel\n│  ○ Telegram (Bot API)\n│  ○ WhatsApp (QR link)\n│  ○ Discord (Bot API)\n│  ○ Google Chat (Chat API)\n│  ○ Slack (Socket Mode)\n│  ○ Signal (signal-cli)\n│  ○ iMessage (imsg)\n│  ○ Feishu/Lark (飞书)\n│  ○ Nostr (NIP-04 DMs)\n│  ○ Microsoft Teams (Bot Framework)\n│  ○ Mattermost (plugin)\n│  ○ Nextcloud Talk (self-hosted)\n│  ○ Matrix (plugin)\n│  ○ BlueBubbles (macOS app)\n│  ○ LINE (Messaging API)\n│  ○ Zalo (Bot API)\n│  ○ Zalo (Personal Account)\n│  ○ Tlon (Urbit)\n│  ● Finished (Done) # 选择完成\n</code></pre>\n<p>完成之后会继续让你选择访问策略</p>\n<pre><code class=\"language-shell\">◇  Configure DM access policies now? (default: pairing) #\n│  Yes\n│\n◇  Feishu DM access ─────────────────────────────────────────────────────────────────────────╮\n│                                                                                            │\n│  Default: pairing (unknown DMs get a pairing code).                                        │\n│  Approve: openclaw pairing approve feishu &lt;code&gt;                                           │\n│  Allowlist DMs: channels.feishu.dmPolicy=\"allowlist\" + channels.feishu.allowFrom entries.  │\n│  Public DMs: channels.feishu.dmPolicy=\"open\" + channels.feishu.allowFrom includes \"*\".     │\n│  Multi-user DMs: set session.dmScope=\"per-channel-peer\" (or \"per-account-channel-peer\"     │\n│  for multi-account channels) to isolate sessions.                                          │\n│  Docs: start/pairing                     │\n│                                                                                            │\n├────────────────────────────────────────────────────────────────────────────────────────────╯\n│\n◇  Feishu DM policy\n│  Open (public inbound DMs) # 公开\n│\n◇  Add display names for these accounts? (optional)\n│  No # 不需要\n│\n└  Channels updated.\n</code></pre>\n<p>你可以通过 <code>~/.openclaw/openclaw.json</code> 查看对应的 channel 配置，最后配置如下</p>\n<pre><code class=\"language-json\">{\n    \"channels\": {\n    \"feishu\": {\n      \"enabled\": true,\n      \"appId\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n      \"appSecret\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n      \"domain\": \"feishu\",\n      \"groupPolicy\": \"open\",\n      \"dmPolicy\": \"open\",\n      \"allowFrom\": [\n        \"*\"\n      ]\n    }\n  }\n}\n</code></pre>\n<p>配置完成之后，重启</p>\n<pre><code class=\"language-shell\">openclaw gateway restart\n</code></pre>\n<p>重启完成后回到飞书，找到「事件和回调」，选择长连接模式，如下图<br />\n<img alt=\"飞书事件和回调配置 - OpenClaw 长连接模式\" class=\"lazyload\" /><br />\n如果配置成功，说明连接已建立。继续下面的配置，添加事件，选择「接收消息」事件<br />\n<img alt=\"飞书添加接收消息事件 - OpenClaw AI 助手\" class=\"lazyload\" /><br />\n事件添加完成之后，还需要开通权限，有以下权限全部勾选</p>\n<table>\n<thead>\n<tr>\n<th>权限</th>\n<th>Scope（范围）</th>\n<th>Description（说明）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>contact:user.base:readonly</td>\n<td>用户信息</td>\n<td>获取基础用户信息</td>\n</tr>\n<tr>\n<td>im:message</td>\n<td>消息 全部勾选</td>\n<td>发送和接收消息</td>\n</tr>\n</tbody>\n</table>\n<p>如下图<br />\n<img alt=\"飞书权限配置 - OpenClaw 用户信息权限\" class=\"lazyload\" /></p>\n<p><img alt=\"飞书消息权限配置 - OpenClaw AI 机器人\" class=\"lazyload\" /></p>\n<p>以上步骤全部完成后，即可与机器人对话。但在此之前需要先创建一个版本<br />\n<img alt=\"飞书应用版本发布 - OpenClaw AI 助手上线\" class=\"lazyload\" /></p>\n<blockquote>\n<p>注意：每次修改配置后都需要重新发布版本，建议全部配置完成后再统一发布。</p>\n</blockquote>\n<p>发布完成后，回到飞书客户端，可以看到应用已上线，点击打开应用<br />\n<img alt=\"飞书应用发布成功 - OpenClaw AI 机器人\" class=\"lazyload\" /><br />\n向机器人发送 <code>Hello</code>，即可收到 Moltbot 的回复<br />\n<img alt=\"飞书 OpenClaw AI 助手回复测试成功\" class=\"lazyload\" /></p>\n<h2 id=\"openclaw-常用命令速查\">OpenClaw 常用命令速查</h2>\n<p>安装完成后，以下是日常使用中最常用的 OpenClaw 命令：</p>\n<table>\n<thead>\n<tr>\n<th>命令</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>openclaw status</code></td>\n<td>查看 OpenClaw 运行状态</td>\n</tr>\n<tr>\n<td><code>openclaw onboard</code></td>\n<td>重新进入配置向导</td>\n</tr>\n<tr>\n<td><code>openclaw gateway start</code></td>\n<td>启动服务</td>\n</tr>\n<tr>\n<td><code>openclaw gateway stop</code></td>\n<td>停止服务</td>\n</tr>\n<tr>\n<td><code>openclaw gateway restart</code></td>\n<td>重启服务</td>\n</tr>\n<tr>\n<td><code>openclaw update</code></td>\n<td>更新到最新版本</td>\n</tr>\n<tr>\n<td><code>openclaw health</code></td>\n<td>健康检查</td>\n</tr>\n<tr>\n<td><code>openclaw doctor</code></td>\n<td>诊断问题</td>\n</tr>\n<tr>\n<td><code>openclaw dashboard</code></td>\n<td>获取 Web UI 访问链接</td>\n</tr>\n<tr>\n<td><code>openclaw security audit --deep</code></td>\n<td>安全审计</td>\n</tr>\n<tr>\n<td><code>openclaw uninstall</code></td>\n<td>卸载 OpenClaw</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"openclaw-成本说明与免费模型推荐\">OpenClaw 成本说明与免费模型推荐</h2>\n<p>OpenClaw 本身完全免费开源，主要成本来自两个方面：</p>\n<p><strong>服务器成本</strong>：一台最低配置的云服务器即可</p>\n<p><strong>AI 模型 API 调用费用</strong>：各模型服务商的免费额度和计费模式不同，以下是常见选择：</p>\n<table>\n<thead>\n<tr>\n<th>模型服务商</th>\n<th>免费额度</th>\n<th>适合场景</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>通义千问（Qwen）</td>\n<td>免费额度充足</td>\n<td>本教程推荐，入门首选</td>\n</tr>\n<tr>\n<td>小米 MiMo</td>\n<td>有免费试用额度</td>\n<td>成本敏感用户</td>\n</tr>\n<tr>\n<td>KIMI (Moonshot)</td>\n<td>有免费额度</td>\n<td>中文理解能力强</td>\n</tr>\n<tr>\n<td>GLM 4.7 (Z.AI)</td>\n<td>有免费额度</td>\n<td>性价比高</td>\n</tr>\n<tr>\n<td>OpenAI GPT</td>\n<td>付费</td>\n<td>英文场景最佳</td>\n</tr>\n<tr>\n<td>Anthropic Claude</td>\n<td>付费</td>\n<td>代码能力最强</td>\n</tr>\n</tbody>\n</table>\n<p>对于刚接触 OpenClaw 的用户，建议先用通义千问的免费额度体验，熟练后再根据实际需求选择其他模型。</p>\n<h2 id=\"总结\">总结</h2>\n<p>本篇 OpenClaw 安装教程从环境准备、OpenClaw 部署、飞书机器人对接到权限配置，完整走完了一个最新版 OpenClaw 开源 AI 助手的搭建流程。如果你按照这篇 OpenClaw 飞书教程完成了所有步骤，现在应该已经可以在飞书中和你的 OpenClaw 助手正常对话了。</p>\n<h2 id=\"openclaw-常见问题-faq\">OpenClaw 常见问题 FAQ</h2>\n<h3 id=\"openclaw-和-clawdbotmoltbot-是什么关系\">OpenClaw 和 Clawdbot、Moltbot 是什么关系？</h3>\n<p>OpenClaw 是该项目的最新正式名称。项目最初叫 Clawdbot，后因商标问题更名为 Moltbot，最终在 2025 年 1 月正式定名为 OpenClaw。三者是同一个项目的不同阶段命名。</p>\n<h3 id=\"openclaw-支持哪些-ai-模型\">OpenClaw 支持哪些 AI 模型？</h3>\n<p>OpenClaw 支持多种 AI 模型服务商，包括 Anthropic Claude、OpenAI GPT、通义千问（Qwen）、KIMI、小米 MiMo 等。本教程使用通义千问是因为其免费额度充足，适合入门学习。</p>\n<h3 id=\"为什么安装时提示-npm-install-failed\">为什么安装时提示 npm install failed？</h3>\n<p>这通常是服务器内存不足导致的。新版本脚本对内存要求较高，2G 内存可能会出现 OOM（内存溢出）。建议配置 swap 交换空间，将硬盘空间作为虚拟内存使用。</p>\n<h3 id=\"openclaw-可以在-windows-或-macos-上运行吗\">OpenClaw 可以在 Windows 或 macOS 上运行吗？</h3>\n<p>可以。OpenClaw 支持 Mac、Windows 和 Linux 系统。本教程以 Linux 为例，其他系统的安装方式可参考<a href=\"https://docs.openclaw.ai/\" rel=\"noopener nofollow\" target=\"_blank\">官方文档</a>。</p>\n<h3 id=\"飞书机器人配置后无法收到消息怎么办\">飞书机器人配置后无法收到消息怎么办？</h3>\n<p>请检查以下几点：</p>\n<ol>\n<li>确认飞书通道已正确安装（新版 OpenClaw 已内置飞书支持，安装时选择 Feishu/Lark 即可）</li>\n<li>检查 App ID 和 App Secret 配置是否正确</li>\n<li>确认已开通「接收消息」事件权限</li>\n<li>检查长连接模式是否配置成功</li>\n<li>确保应用版本已发布</li>\n<li>使用 <code>openclaw gateway restart</code> 重启服务后再试</li>\n</ol>\n<h3 id=\"openclaw-数据安全吗\">OpenClaw 数据安全吗？</h3>\n<p>OpenClaw 运行在你自己的服务器上，所有数据都在本地存储，不会上传到第三方云端。但由于它具有系统级权限，建议在独立的服务器上部署，避免在生产环境或重要数据的机器上运行。</p>\n<h3 id=\"除了飞书openclaw-还支持哪些平台\">除了飞书，OpenClaw 还支持哪些平台？</h3>\n<p>OpenClaw 支持多个聊天平台，包括 WhatsApp、Telegram、Discord、Slack、Microsoft Teams、Signal、iMessage、Google Chat、Twitch 等。每个平台需要安装对应的插件。</p>\n<h3 id=\"openclaw-可以做什么\">OpenClaw 可以做什么？</h3>\n<p>OpenClaw 不只是一个聊天机器人，它能真正在你的服务器上执行操作。以下是一些典型使用场景：</p>\n<ul>\n<li><strong>文件整理</strong>：“帮我把上周下载的文件按类型分类”，它会直接操作文件系统完成分类</li>\n<li><strong>网页摘要</strong>：发一个 URL 给它，它能自动打开网页、提取内容并生成摘要</li>\n<li><strong>代码编写</strong>：“写一个 Python 脚本批量重命名文件”，它能写完代码还能直接在服务器上运行</li>\n<li><strong>数据查询</strong>：连接本地数据库查询数据，并把结果发回飞书</li>\n<li><strong>日程管理</strong>：定时提醒、晴间简报、邮件自动回复</li>\n<li><strong>系统运维</strong>：执行 Shell 命令、监控服务器状态、自动化脚本</li>\n</ul>\n<p>简单说，OpenClaw 是一个 7×24 小时在线的 AI 助手，你睡觉时它还能继续干活。</p>\n<h3 id=\"如何更新-openclaw-到最新版本\">如何更新 OpenClaw 到最新版本？</h3>\n<p>使用以下命令更新：</p>\n<pre><code class=\"language-shell\">openclaw update\n</code></pre>\n<h3 id=\"openclaw-命令和-clawdbot-命令有什么区别\">OpenClaw 命令和 clawdbot 命令有什么区别？</h3>\n<p>OpenClaw 更名后，官方推荐使用 <code>openclaw</code> 命令，但为了兼容性，<code>clawdbot</code> 命令仍然可用。两者功能完全相同，建议新用户直接使用 <code>openclaw</code> 命令。</p>\n<h3 id=\"提示-openclaw-命令找不到怎么办\">提示 openclaw 命令找不到怎么办？</h3>\n<p>这通常是环境变量未加载导致的。尝试以下步骤：</p>\n<ol>\n<li>关闭当前终端窗口，重新打开</li>\n<li>执行 <code>source ~/.bashrc</code> 重新加载环境变量</li>\n<li>如果还不行，执行 <code>openclaw doctor</code> 检查问题</li>\n<li>实在无法解决，尝试重启服务器</li>\n</ol>\n<h3 id=\"openclaw-安装卡住不动怎么办\">OpenClaw 安装卡住不动怎么办？</h3>\n<ol>\n<li>按 <code>Ctrl + C</code> 中断当前操作</li>\n<li>执行 <code>openclaw doctor</code> 检查问题</li>\n<li>如提示网络问题，检查服务器网络是否能访问 GitHub 和 npm</li>\n<li>尝试重新运行 <code>openclaw onboard</code></li>\n</ol>\n<h3 id=\"端口-18789-被占用怎么办\">端口 18789 被占用怎么办？</h3>\n<p>使用其他端口启动服务：</p>\n<pre><code class=\"language-shell\">openclaw gateway --port 18790\n</code></pre>\n<h3 id=\"如何配置-swap-解决内存不足\">如何配置 swap 解决内存不足？</h3>\n<p>如果服务器内存不足 2GB，可以配置 swap 交换空间：</p>\n<pre><code class=\"language-shell\"># 创建 2G 的 swap 文件\nsudo fallocate -l 2G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n\n# 设置开机自动启用\necho '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-08 17:47</span>&nbsp;\n<a href=\"https://www.cnblogs.com/catchadmin\">JaguarJack</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "字符串哈希",
      "link": "https://www.cnblogs.com/zheyutao/p/19587765/StringHash",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/zheyutao/p/19587765/StringHash\" id=\"cb_post_title_url\" title=\"发布于 2026-02-08 17:20\">\n    <span>字符串哈希</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        从定义可以看出，字符串 Hash 函数的实质是：把每个不同的字符串转化为不同的整数，希望 $O(1)$ 判断两个字符串是否相等。\n\n然而事实上，经常会出现两个不同的字符串映射到相同的 Hash 值的现象，称为 **哈希冲突**。由此引出哈希函数两条最重要的性质。\n\n......\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"引入\">引入</h2>\n<blockquote>\n<p><strong>定义：</strong><br />\n把字符串映射到整数的函数 <span class=\"math inline\">\\(f\\)</span>，称 <span class=\"math inline\">\\(f\\)</span> 为 <strong>Hash 函数</strong>。</p>\n</blockquote>\n<p>从定义可以看出，字符串 Hash 函数的实质是：把每个不同的字符串转化为不同的整数，希望 <span class=\"math inline\">\\(O(1)\\)</span> 判断两个字符串是否相等。</p>\n<p>然而事实上，经常会出现两个不同的字符串映射到相同的 Hash 值的现象，称为 <strong>哈希冲突</strong>。由此引出哈希函数两条最重要的性质。</p>\n<blockquote>\n<p><strong>性质：</strong><br />\n在 Hash 函数值不一样时，两个字符串一定不一样；<br />\n在 Hash 函数值一样时，两个字符串不一定一样。</p>\n</blockquote>\n<p>对于一个长度为 <span class=\"math inline\">\\(l\\)</span> 的字符串 <span class=\"math inline\">\\(s\\)</span>，定义其多项式 Hash 函数为：$$f(s) = \\sum_{i=1}^l s[i] \\times p^{l-i} \\pmod M$$</p>\n<p>可以类比 <span class=\"math inline\">\\(p\\)</span> 进制数来帮助理解。例如字符串 <span class=\"math inline\">\\(xyz\\)</span>，其哈希函数值为 <span class=\"math inline\">\\(xp^2+yp+z\\)</span>。下文中的 Hash 函数均采用这种定义方式。</p>\n<h3 id=\"生日悖论\">生日悖论</h3>\n<p>考虑这样一个问题：多少个人里有两个生日相同的人的概率有 50% 呢？答案是反直觉的 23 个人。</p>\n<p><strong>证明：</strong> 设房间里共有 <span class=\"math inline\">\\(n\\)</span> 个人，排除闰年 <span class=\"math inline\">\\(366\\)</span> 天的情况。第一个人的生日是 <span class=\"math inline\">\\(365\\)</span> 选 <span class=\"math inline\">\\(365\\)</span>，第二个人的生日是 <span class=\"math inline\">\\(365\\)</span> 选 <span class=\"math inline\">\\(364\\)</span>，第三个人的生日是 <span class=\"math inline\">\\(365\\)</span> 选 <span class=\"math inline\">\\(363\\)</span>。以此类推，第 <span class=\"math inline\">\\(n\\)</span> 个人的生日是 <span class=\"math inline\">\\(365\\)</span> 选 <span class=\"math inline\">\\(365 - n + 1\\)</span>。</p>\n<p></p><div class=\"math display\">\\[P(所有人生日不同) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{365 - n + 1}{365} = \\frac{365!}{365^n(365-n)!} \n\\]</div><p></p><p></p><div class=\"math display\">\\[P(至少有两人生日相同) = 1 - P(所有人生日不同) \n\\]</div><p></p><blockquote>\n<p><strong>说明：</strong><br />\n当元素的个数增多时，哈希冲突的概率会以很快的速度增长。</p>\n</blockquote>\n<p>再考虑模数 <span class=\"math inline\">\\(M\\)</span> 应满足什么条件。因为质数不与其他数字存在公因数，可以减少因取模操作带来的周期性冲突，所以通常选取足够大的质数来作为模数 <span class=\"math inline\">\\(M\\)</span>。</p>\n<h2 id=\"方法\">方法</h2>\n<p>对于读入的字符串，习惯于在前加一个空格符调整下标，一般直接采用其 ASCII 码，用数组预处理基数的幂。</p>\n<h3 id=\"自然溢出法\">自然溢出法</h3>\n<p>顾名思义，利用无符号长整形 <code>unsigned long long</code>自然溢出的特性。若数据超出 ull 的存储范围，则结果自然 <span class=\"math inline\">\\(\\pmod {2^{64}}-1\\)</span>。Hash 公式如下：</p>\n<pre><code>typedef unsigned long long ull;\null hs[N];\nhs[i] = hs[i] * p + s[i];\n</code></pre>\n<p>其中，基数 <span class=\"math inline\">\\(p\\)</span> 是一个较大的质数，如 <span class=\"math inline\">\\(233\\)</span>，<span class=\"math inline\">\\(271\\)</span>，<span class=\"math inline\">\\(2333\\)</span> 等，否则唯一性也难以保证。</p>\n<blockquote>\n<p><strong>例题：</strong><br />\n<a href=\"https://www.luogu.com.cn/problem/P2852\" rel=\"noopener nofollow\" target=\"_blank\">产奶模式</a></p>\n</blockquote>\n<p>题目要求出现了至少 <span class=\"math inline\">\\(k\\)</span> 次的最大连续子段的长度。</p>\n<p>若最终答案为 <span class=\"math inline\">\\(m\\)</span>。则这些连续子段去掉末尾元素后仍然相同，即：序列中长为 <span class=\"math inline\">\\(m-1\\)</span> 的相同连续子段也会出现至少 <span class=\"math inline\">\\(k\\)</span> 次。可见，连续子段长度为 <span class=\"math inline\">\\(0\\)</span> ~ <span class=\"math inline\">\\(m\\)</span> 时都可行，长度为 <span class=\"math inline\">\\((m+1)\\)</span> ~ <span class=\"math inline\">\\(n\\)</span> 时都不可行。答案满足单调性，考虑二分答案。</p>\n<p>预处理序列前缀 Hash，在 <code>check</code> 函数里，遍历所有长度为 <span class=\"math inline\">\\(mid\\)</span> 的连续子段，对其 Hash 值出现的次数计数。由于 Hash 值较大，不能使用桶数组，可使用 <code>map</code> 计数。时间复杂度 <span class=\"math inline\">\\(O(n \\log ^ 2 n)\\)</span>。代码如下：</p>\n<pre><code>#include &lt;iostream&gt;\n#include &lt;map&gt;\nusing namespace std;\ntypedef unsigned long long ull;\nconst int N = 2e4 + 8, p = 233;\null ppow[N], hs[N];\nint n, k, a[N];\null geths(int l, int r) {\n    return hs[r] - hs[l - 1] * ppow[r - l + 1];\n}\nbool check(int mid) {\n    map&lt;ull, ull&gt; mp;\n    for (int l = 1; l + mid - 1 &lt;= n; l++) {\n        ull h = geths(l, l + mid - 1);\n        if (++mp[h] &gt;= k) return true;\n    }\n    return false;\n}\nint main() {\n    ppow[0] = 1;\n    for (int i = 1; i &lt; N; i++) ppow[i] = ppow[i - 1] * p;\n    cin &gt;&gt; n &gt;&gt; k;\n    for (int i = 1; i &lt;= n; i++) {\n        cin &gt;&gt; a[i];\n        hs[i] = hs[i - 1] * p + a[i];\n    }\n    int l = 0, r = n, mid, ans;\n    while (l &lt;= r) {\n        int mid = l + r &gt;&gt; 1;\n        if (check(mid)) l = mid + 1, ans = mid;\n        else r = mid - 1;\n    }\n    cout &lt;&lt; ans;\n    return 0;\n}\n</code></pre>\n<h3 id=\"単-hash-法\">単 Hash 法</h3>\n<blockquote>\n<p><strong>注意：</strong> 単 Hash 法在模数较小的时候，唯一性难以保证。</p>\n</blockquote>\n<p>相当于没有了自动取模特性的自然溢出法，唯一不同就是需要手动加上取模。Hash 公式如下：</p>\n<pre><code>hs[i] = (hs[i - 1] * p + s[i]) % mod;\n</code></pre>\n<p>其中 <span class=\"math inline\">\\(p &lt; mod\\)</span> 且 <span class=\"math inline\">\\(p\\)</span> 与 <span class=\"math inline\">\\(mod\\)</span> 是足够大的质数。</p>\n<h3 id=\"双-hash-法\">双 Hash 法</h3>\n<p>很稳很安全的 Hash 方法。对比単 Hash 法，双 Hash 法用两个不同的基数 <span class=\"math inline\">\\(p\\)</span> 进行两次取模 <span class=\"math inline\">\\(mod\\)</span> 操作。Hash 公式如下：</p>\n<pre><code>hs1[i] = (hs1[i] * p + s[i]) % mod1;\nhs2[i] = (hs2[i] * p + s[i]) % mod2;\n</code></pre>\n<p>判断是否相同时，用一对 Hash 函数值来比较。只要有一个不匹配就说明字符串不相同。<code>cmp</code> 函数如下：</p>\n<pre><code>bool cmp(string s, string t) {\n    return geths1(s) == geths1(t) &amp;&amp; geths2(s) == geths2(t);\n}\n</code></pre>\n<h3 id=\"获取子串的-hash\">获取子串的 Hash</h3>\n<p>已知字符串 <span class=\"math inline\">\\(S\\)</span> 的 Hash 值 <span class=\"math inline\">\\(hs[i]\\)</span>，且 <span class=\"math inline\">\\(|S| = n\\)</span>。它的子串 <span class=\"math inline\">\\(S[l..r]\\)</span>，其中 <span class=\"math inline\">\\(1 \\leq l \\leq r \\leq n\\)</span>，对应的 Hash 值为：</p>\n<pre><code>((hs[r] - hs[l - 1] * ppow[r - l + 1]) % mod + mod) % mod\n</code></pre>\n<p>推导过程类似于前缀和中的区间和，感兴趣的读者请自行研究。</p>\n<h2 id=\"应用\">应用</h2>\n<h3 id=\"字符串匹配\">字符串匹配</h3>\n<blockquote>\n<p><strong>例题：</strong><br />\n给出两个字符串 <span class=\"math inline\">\\(S\\)</span> 和 <span class=\"math inline\">\\(T\\)</span>，求 <span class=\"math inline\">\\(T\\)</span> 在 <span class=\"math inline\">\\(S\\)</span> 中出现的次数。不同位置出现的 <span class=\"math inline\">\\(T\\)</span> 可重叠。</p>\n</blockquote>\n<p>求出模式串 <span class=\"math inline\">\\(T\\)</span> 的 Hash 值后，求出文本串 <span class=\"math inline\">\\(S\\)</span> 中每个长度为 <span class=\"math inline\">\\(T\\)</span> 长度的字串的 Hash 值，分别于 <span class=\"math inline\">\\(T\\)</span> 的 Hash 值比较即可。代码如下：</p>\n<pre><code>#include &lt;iostream&gt;\nusing namespace std;\ntypedef long long ll;\nconst int p = 233, mod = 1e9 + 7, N = 1e6 + 8;\nstring s, t;\nll ppow[N], hsh[N], ths;\nll get_hash(int l, int r) {\n    return ((hsh[r] - hsh[l - 1] * ppow[r - l + 1]) % mod + mod) % mod;\n}\nint main() {\n\tppow[0] = 1;\n\tfor (int i = 1; i &lt; N; i++) ppow[i] = ppow[i - 1] * p % mod;\n\tcin &gt;&gt; s &gt;&gt; t;\n\tint slen = s.size(), tlen = t.size();\n\ts = ' ' + s;\n\tt = ' ' + t;\n\tfor (int i = 1; i &lt;= slen; i++) hsh[i] = (hsh[i - 1] * p + s[i]) % mod;\n\tfor (int i = 1; i &lt;= tlen; i++) ths = (ths * p + t[i]) % mod;\n    int ans = 0;\n    for (int l = 1; l + tlen - 1 &lt;= slen; l++)\n        if (get_hash(l, l + tlen - 1) == ths) ans++;\n    cout &lt;&lt; ans;\n    return 0;\n}\n</code></pre>\n<h3 id=\"最长回文子串\">最长回文子串</h3>\n<blockquote>\n<p><strong>例题：</strong><br />\n<a href=\"https://www.luogu.com.cn/problem/P3501\" rel=\"noopener nofollow\" target=\"_blank\">反对称串</a></p>\n</blockquote>\n<p>问题：给定一个 <span class=\"math inline\">\\(0/1\\)</span> 序列，求其异或意义下的回文子串的数量。</p>\n<p>由题意得，回文子串的长度必须为偶数，否则因为对称中心一定改变，所以该子串一定不是回文子串。</p>\n<p>我们不妨枚举它的回文中心，尽可能地扩展它的回文半径。因为回文半径具有单调性，所以考虑二分答案。至于判断回文中心两侧是否相等，可以预处理正着的 Hash 值和倒着的 Hash 值，在 <code>check</code> 函数中比较即可。代码如下：</p>\n<pre><code>#include &lt;iostream&gt;\nusing namespace std;\ntypedef unsigned long long ull;\nconst int N = 1e6 + 8, p = 131;\nint n;\nstring s;\null ppow[N], shs[2][N];\null get_hash(ull h[], int l, int r) {\n    return h[r] - h[l - 1] * ppow[r - l + 1];\n}\nbool check(int l, int r) {\n    return l &lt;= r &amp;&amp; get_hash(shs[0], l, r) == get_hash(shs[1], n - r + 1, n - l + 1);\n}\nint main() {\n    ppow[0] = 1;\n    for (int i = 1; i &lt; N; i++) ppow[i] = ppow[i - 1] * p;\n    cin &gt;&gt; n &gt;&gt; s;\n    s = ' ' + s;\n    for (int i = 1; i &lt;= n; i++) {\n        shs[0][i] = shs[0][i - 1] * p + s[i];\n        shs[1][i] = shs[1][i - 1] * p + (s[n - i + 1] == '0' ? '1' : '0'); // 异或的同时倒着存Hash值\n    }\n    int ans = 0;\n    for (int i = 1; i &lt; n; i++) {\n        int l = 1, r = min(i, n - i), mid, res = 0; // 二分答案回文半径\n        while (l &lt;= r) {\n            mid = (l + r) &gt;&gt; 1;\n            if (check(i - mid + 1, i + mid)) l = mid + 1, res = mid;\n            else r = mid - 1;\n        }\n        ans += res;\n    }\n    cout &lt;&lt; ans;\n    return 0;\n}\n</code></pre>\n<p>如果上面的二分答案只改动 <span class=\"math inline\">\\(l\\)</span> 的值为 <span class=\"math inline\">\\(0\\)</span>，那么回文半径单峰而不单调，与二分答案要求严格单调性相违背。但若在此基础上，将 <span class=\"math inline\">\\(mid\\)</span> 向上取整，二分边界 <span class=\"math inline\">\\([l, r)\\)</span> 左闭右开，用 <span class=\"math inline\">\\(l\\)</span> 存答案，结果仍然正确。代码如下：</p>\n<pre><code>        int l = 0, r = min(i, n - i), mid;\n        while (l &lt; r) {\n            mid = (l + r + 1) &gt;&gt; 1;\n            if (check(i - mid + 1, i + mid)) l = mid;\n            else r = mid - 1;\n        }\n        ans += l;\n</code></pre>\n<h3 id=\"最短循环节\">最短循环节</h3>\n<blockquote>\n<p><strong>例题：</strong><br />\n<a href=\"https://www.luogu.com.cn/problem/P3538\" rel=\"noopener nofollow\" target=\"_blank\">糟糕的诗</a></p>\n</blockquote>\n<p>问题：给定一个由小写英文字母组成的长度为 <span class=\"math inline\">\\(L\\)</span> 的字符串 <span class=\"math inline\">\\(S\\)</span>，有 <span class=\"math inline\">\\(q\\)</span> 个询问，每次询问给定 <span class=\"math inline\">\\(S\\)</span> 的一个子串，求其最短循环节。若字符串 <span class=\"math inline\">\\(A\\)</span> 能够由字符串 B 重复若干次得到，则称字符串 <span class=\"math inline\">\\(B\\)</span> 是字符串 <span class=\"math inline\">\\(A\\)</span> 的一个循环节。</p>\n<p>仔细思考，我们能得出以下几个很重要的结论：</p>\n<ol>\n<li>若 <span class=\"math inline\">\\(n\\)</span> 是循环节的长度，则 <span class=\"math inline\">\\(hs(l + n, r) = hs(l, r - n)\\)</span>。这说明我们能够在 <span class=\"math inline\">\\(O(1)\\)</span> 的时间复杂度内判断是否为循环节。</li>\n</ol>\n<p><img alt=\"循环节\" src=\"https://img2024.cnblogs.com/blog/3684275/202602/3684275-20260208162031249-1965114259.png\" /></p>\n<ol start=\"2\">\n<li>\n<p>循环节的长度 <span class=\"math inline\">\\(n\\)</span> 是总长 <span class=\"math inline\">\\(L\\)</span> 的因数。</p>\n</li>\n<li>\n<p>若 <span class=\"math inline\">\\(n\\)</span> 是一个循环节的长度，<span class=\"math inline\">\\(k\\)</span> 是循环次数，则 <span class=\"math inline\">\\(k \\times n\\)</span> 也是一个循环节。这说明：先把 <span class=\"math inline\">\\(n\\)</span> 分解质因数，得到循环节的因子和循环次数的因子，从 <span class=\"math inline\">\\(n\\)</span> 开始试除总长 <span class=\"math inline\">\\(L\\)</span>，将循环次数的因子除尽，最后得到的就是最小循环节的长度。</p>\n</li>\n</ol>\n<p>分解质因数用欧拉筛，时间复杂度为 <span class=\"math inline\">\\(O(q \\sqrt L)\\)</span>，常数较大加快读。代码如下：</p>\n<pre><code>#include &lt;iostream&gt;\nusing namespace std;\ntypedef unsigned long long ull;\null read() {\n    ull num = 0;\n    char ch = getchar();\n    while (ch &lt; '0' || ch &gt; '9') ch = getchar();\n    while (ch &gt;= '0' &amp;&amp; ch &lt;= '9') {\n        num = (num &lt;&lt; 1) + (num &lt;&lt; 3) + ch - '0';\n        ch = getchar();\n    }\n    return num;\n}\nconst int N = 5e5 + 8, p = 233;\nint n, q, pcnt, pri[N], mnp[N];\nbool notpri[N];\nstring s;\null ppow[N], shs[N];\null geths(int l, int r) {\n    return shs[r] - shs[l - 1] * ppow[r - l + 1];\n}\nvoid init() {\n    notpri[0] = notpri[1] = true; // 欧拉筛\n    for (int i = 2; i &lt; N; i++) {\n        if (!notpri[i]) pri[++pcnt] = i, mnp[i] = i;\n        for (int j = 1; j &lt;= pcnt &amp;&amp; i * pri[j] &lt; N; j++) {\n            mnp[pri[j] * i] = pri[j];\n            notpri[pri[j] * i] = true;\n            if (i % pri[j] == 0) break;\n        }\n    }\n    ppow[0] = 1;\n    for (int i = 1; i &lt; N; i++) ppow[i] = ppow[i - 1] * p;\n}\nbool check(int l, int r, int len) {\n    return geths(l, r - len) == geths(l + len, r);\n}\nint main() {\n    init();\n    n = read();\n    cin &gt;&gt; s;\n    s = ' ' + s;\n    q = read();\n    for (int i = 1; i &lt;= n; i++) shs[i] = shs[i - 1] * p + s[i];\n    while (q--) {\n        int l = read(), r = read(), len = r - l + 1, rmn = r - l + 1, fac;\n        while (rmn &gt; 1) {\n            for (fac = mnp[rmn]; rmn &gt; 1 &amp;&amp; check(l, r, len / fac); fac = mnp[rmn]) {\n                len /= fac; // len 表示循环节长度\n                rmn /= fac; // rmn 表示该子串剩余长度\n            }\n            while (rmn &gt; 1 &amp;&amp; rmn % fac == 0) rmn /= fac;\n        }\n        cout &lt;&lt; len &lt;&lt; '\\n';\n    }\n    return 0;\n}\n</code></pre>\n<h3 id=\"哈希表\">哈希表</h3>\n<blockquote>\n<p><strong>例题：</strong><br />\n<a href=\"https://www.luogu.com.cn/problem/P4305\" rel=\"noopener nofollow\" target=\"_blank\">不重复数字</a></p>\n</blockquote>\n<p>问题：给定 <span class=\"math inline\">\\(n\\)</span> 个数，要求把其中重复的去掉，只保留第一次出现的数。</p>\n<p>哈希表就是解决这个问题的数据结构。其内部采用“链地址法”解决哈希冲突。拓展一个哈希表的重要属性：负载因子 <span class=\"math inline\">\\(\\alpha\\)</span>。</p>\n<p></p><div class=\"math display\">\\[\\alpha = \\frac{已有元素个数}{桶数}\n\\]</div><p></p><p>一般认为，当 <span class=\"math inline\">\\(\\alpha\\)</span> 在 <span class=\"math inline\">\\(0.75\\)</span> 左右时，哈希表的性能优秀。</p>\n<p>代码上采用类似于链式前向星的写法封装结构体手写哈希表。事实上，STL库提供的 <code>unordered_map</code> 更为常用。</p>\n<pre><code>#include &lt;iostream&gt;\n#include &lt;cstring&gt;\nusing namespace std;\nconst int N = 6e4 + 8;\nstruct hash_table {\n    int sz, head[N], nxt[N], val[N];\n    int geths(int x) {\n        return (x % N + N) % N;\n    }\n    void init() {\n        sz = 0;\n        memset(head, 0, sizeof(head));\n    }\n    int find(int x) {\n        int h = geths(x);\n        for (int i = head[h]; i; i = nxt[i])\n            if (val[i] == x) return i;\n        return 0;\n    }\n    int insert(int x) {\n        if (find(x)) return 0;\n        int h = geths(x);\n        nxt[++sz] = head[h];\n        val[sz] = x;\n        head[h] = sz;\n        return sz;\n    }\n} ht;\nint main() {\n    ios::sync_with_stdio(0);\n    cin.tie(0); cout.tie(0);\n    int T, n;\n    cin &gt;&gt; T;\n    while (T--) {\n        cin &gt;&gt; n;\n        ht.init();\n        for (int i = 1, x; i &lt;= n; i++) {\n            cin &gt;&gt; x;\n            if (ht.insert(x)) cout &lt;&lt; x &lt;&lt; ' ';\n        }\n        cout &lt;&lt; '\\n';\n    }\n    return 0;\n}\n</code></pre>\n<h3 id=\"确定字符串中子字符串的个数\">确定字符串中子字符串的个数</h3>\n<blockquote>\n<p><strong>例题：</strong><br />\n<a href=\"https://www.luogu.com.cn/problem/P3498\" rel=\"noopener nofollow\" target=\"_blank\">Beads 项链</a></p>\n</blockquote>\n<p>问题：给定长为 <span class=\"math inline\">\\(n\\)</span> 的序列，将它划分为每段长度都为 <span class=\"math inline\">\\(k\\)</span> 的子串，求最多可以得到的不同子串的个数、取到最优解时不同的 <span class=\"math inline\">\\(k\\)</span> 值和 <span class=\"math inline\">\\(k\\)</span> 的个数。子串可反转。</p>\n<p>正向、反向做两遍 Hash 求子串的 Hash 值。利用 <code>set</code> 数据结构自动去重。每次选择 Hash 值更小（大）的情况插入即可。代码如下。</p>\n<pre><code>#include &lt;iostream&gt;\n#include &lt;set&gt;\n#include &lt;vector&gt;\nusing namespace std;\ntypedef unsigned long long ull;\nconst int N = 2e5 + 8, p = 233333;\null hs[2][N], ppow[N];\nint n, a[N];\null geths(ull h[], int l, int r) {\n    return h[r] - h[l - 1] * ppow[r - l + 1];\n}\nint main() {\n    ppow[0] = 1;\n    for (int i = 1; i &lt; N; i++) ppow[i] = ppow[i - 1] * p;\n    cin &gt;&gt; n;\n    for (int i = 1; i &lt;= n; i++) cin &gt;&gt; a[i];\n    for (int i = 1; i &lt;= n; i++) { // 正向、反向 Hash\n        hs[0][i] = hs[0][i - 1] * p + a[i];\n        hs[1][i] = hs[1][i - 1] * p + a[n - i + 1];\n    }\n    int ans = 0;\n    vector&lt;int&gt; dk;\n    for (int k = 1; k &lt;= n; k++) {\n        if (n / k &lt; ans) break; // 最优性特判\n        set&lt;ull&gt; st; // 自动去重\n        for (int l = 1; l + k - 1 &lt;= n; l += k) { // r = l + k - 1\n            int h1 = geths(hs[0], l, l + k - 1), h2 = geths(hs[1], n - l - k + 2, n - l + 1);\n            st.insert(min(h1, h2)); // 统一选择 Hash 值更小的子串\n        }\n        if (st.size() &gt; ans) {\n            dk.clear();\n            dk.push_back(k);\n            ans = st.size();\n        } else if (st.size() == ans)\n            dk.push_back(k);\n    }\n    cout &lt;&lt; ans &lt;&lt; ' ' &lt;&lt; dk.size() &lt;&lt; '\\n';\n    for (int k : dk) cout &lt;&lt; k &lt;&lt; ' ';\n    return 0;\n}\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-08 17:20</span>&nbsp;\n<a href=\"https://www.cnblogs.com/zheyutao\">zheyutao</a>&nbsp;\n阅读(<span id=\"post_view_count\">4</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}