{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "OpenEuler 20.03æž„å»ºzabbix8.0 rpmåŒ",
      "link": "https://www.cnblogs.com/virtualzzf/p/19617629",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/virtualzzf/p/19617629\" id=\"cb_post_title_url\" title=\"发布于 2026-02-15 10:46\">\n    <span>OpenEuler 20.03构建zabbix8.0 rpm包</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        OpenEuler 20.03自行构建zabbix8.0 rpm包\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"一说明\">一、说明</h1>\n<h2 id=\"为什么要自己构建\">为什么要自己构建？</h2>\n<p>由于centos从7版本之后改为stream，工作环境由centos转向OpenEuler。zabbix官网上有各大主流操作系统预编译的rpm包，但是Openeuler相对小众，自然没有制作好的包。即使是centos系统，7版本也过于陈旧了，从zabbix 6.0开始，centos 7已经不提供server的rpm包了，只剩下proxy和agent，到了7.0版本，连proxy都没有了。学会自己创建rpm包，以备操作系统环境发生改变是非常有必要的。</p>\n<h2 id=\"为什么不直接源代码编译\">为什么不直接源代码编译</h2>\n<ol>\n<li>由于采用的是sever-proxy-agent的多层架构，server只有一台，但是proxy有几十台，agent更是上千，每一台都用源代码编译工作量大大增加。</li>\n<li>源代码编译的软件，在一些例如配置文件、启停命令上与rpm版本有差异，如果混布增加了运维复杂度。</li>\n</ol>\n<h2 id=\"有没有预编译好的rpm包\">有没有预编译好的rpm包</h2>\n<p>在OpenEuler的官方社区的软件中心，有社区成员自行构建的rpm包，可以尝试找找有无符合自己要求的版本。<br />\n<img alt=\"image\" class=\"lazyload\" /></p>\n<h1 id=\"二准备工作\">二、准备工作</h1>\n<h2 id=\"21-添加repo源\">2.1 添加repo源</h2>\n<p>如果OpenEuler缺少默认的repo源，需要自己添加<br />\n在/etc/yum.repos.d/openEuler_x86_64.repo中添加如下内容：</p>\n<pre><code>[OS]\nname=openEuler-$releasever - OS\nbaseurl=https://repo.openeuler.openatom.cn/openEuler-20.03-LTS-SP4/OS/$basearch/\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.openeuler.openatom.cn/openEuler-20.03-LTS-SP4/OS/$basearch/RPM-GPG-KEY-openEuler\n</code></pre>\n<p>另外再添加everything的源，可以提供更多的包。</p>\n<pre><code>dnf config-manager --add-repo https://repo.openeuler.org/openEuler-20.03-LTS/everything/x86_64\n</code></pre>\n<p>使用<code>dnf clean all &amp;&amp; dnf makecache</code>命令更新。</p>\n<h2 id=\"22-准备构建rpm包环境\">2.2 准备构建rpm包环境</h2>\n<p>之前的文章里已经介绍了构建rpm包的基本方法，这里不再赘述。root用户下运行命令如下：</p>\n<pre><code>dnf install -y rpm-build\ndnf install -y rpmdevtools\nrpmdev-setuptree\n</code></pre>\n<p>下载srpm包（ <a href=\"http://repo.zabbix.com/zabbix/7.0/rhel/8/SRPMS/zabbix-7.0.23-release1.el8.src.rpm\" rel=\"noopener nofollow\" target=\"_blank\">http://repo.zabbix.com/zabbix/7.0/rhel/8/SRPMS/zabbix-7.0.23-release1.el8.src.rpm</a> ） ，这里以rhel8版本的srpm文件为例：</p>\n<pre><code>rpm -ivh zabbix-7.0.23-release1.el8.src.rpm\n</code></pre>\n<p>此时，在/root/rpmbuild目录下的SOURCES目录下会产生源代码压缩包、补丁和配置文件，SPECS目录会产生spec文件。但是此spec文件是Centos8版本的，与OpenEuler不完全契合，需要修改一下。</p>\n<h1 id=\"三安装依赖包\">三、安装依赖包</h1>\n<h2 id=\"31-buildrequires要求的依赖包\">3.1 BuildRequires要求的依赖包</h2>\n<table>\n<thead>\n<tr>\n<th>依赖包</th>\n<th>要求的版本</th>\n<th>dnf安装的版本</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>make</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>mariadb-connector-c-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>postgresql-devel</td>\n<td>&gt;= 12.0</td>\n<td>10.5</td>\n</tr>\n<tr>\n<td>sqlite-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>net-snmp-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>openldap-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>unixODBC-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>curl-devel</td>\n<td>&gt;= 7.13.1</td>\n<td>7.66.0</td>\n</tr>\n<tr>\n<td>OpenIPMI-devel</td>\n<td>&gt;= 2</td>\n<td>2.0.29</td>\n</tr>\n<tr>\n<td>libssh-devel</td>\n<td>&gt;= 0.9.0</td>\n<td>0.9.4</td>\n</tr>\n<tr>\n<td>java-devel（java-1.8.0-openjdk-devel）</td>\n<td>&gt;= 1.6.0</td>\n<td>1.8.0.392.b08</td>\n</tr>\n<tr>\n<td>libxml2-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>libevent-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>pcre2-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>openssl-devel</td>\n<td>&gt;= 1.0.1</td>\n<td>1.1.1f</td>\n</tr>\n<tr>\n<td>systemd</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>policycoreutils-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>selinux-policy-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>c-ares-devel</td>\n<td>&gt;= 1.19.0</td>\n<td>1.16.1</td>\n</tr>\n<tr>\n<td>安装全部依赖：</td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<pre><code>dnf install -y make mariadb-connector-c-devel postgresql-devel sqlite-devel net-snmp-devel openldap-devel unixODBC-devel curl-devel OpenIPMI-devel libssh-devel java-1.8.0-openjdk-devel libxml2-devel libevent-devel pcre2-devel openssl-devel systemd policycoreutils-devel selinux-policy-devel c-ares-devel\n</code></pre>\n<h2 id=\"32-其他依赖包\">3.2 其他依赖包</h2>\n<p>zabbix agent2是使用GO语言编写的，并且使用的语法对版本还有要求，OpenEuler 20.03默认repo源的版本为1.15，需要安装一个较新版本的。<br />\n首先下载golang的压缩包并解压</p>\n<pre><code>tar -C /usr/local -xzf go1.24.8.linux-amd64.tar.gz\n</code></pre>\n<p>配置PATH变量并生效</p>\n<pre><code>tee /etc/profile.d/go.sh &lt;&lt;EOL\nexport GO_HOME=/usr/local/go\nexport PATH=\\$PATH:\\$GO_HOME/bin\nEOL\nsource /etc/profile\n</code></pre>\n<p>但是实际上agent2和web_service<strong>并未构建成功</strong>，见4.3小节</p>\n<h1 id=\"四修改spec文件\">四、修改spec文件</h1>\n<p>修改好的spec文件见：<a href=\"https://files.cnblogs.com/files/blogs/745793/zabbix.zip?t=1771123185&amp;download=true\" target=\"_blank\">https://files.cnblogs.com/files/blogs/745793/zabbix.zip?t=1771123185&amp;download=true</a></p>\n<h2 id=\"41-删除rhel和amzn宏\">4.1 删除%{rhel}和%{?amzn}宏</h2>\n<p>%{rhel}和%{?amzn}两个宏分别标识了redhat和amazon系linux的大版本号，用于构建时一些配置方式的选择。由于这两个宏在OpenEuler中为空，在spec文件中会被全局定义为0，直接使用会影响构建，需要全部进行处理。<br />\n与OpenEuler相对接近的是Centos8，把%{?rhel}当做“8”处理，%{?amzn}直接删除。<br />\n示例1：</p>\n<pre><code>%if ( 0%{?rhel} &gt;= 7 &amp;&amp; 0%{?amzn} == 0 ) || 0%{?amzn} &gt;= 2023\n%{!?build_agent2: %global build_agent2 1}\n%endif\n</code></pre>\n<p>由于08 &gt;= 7，直接修改为</p>\n<pre><code>%{!?build_agent2: %global build_agent2 1}\n</code></pre>\n<p>示例2：</p>\n<pre><code>%if 0%{rhel} &gt;= 9 || 0%{?amzn} &gt;= 2023\nBuildRequires: selinux-policy-devel\nBuildRequires: c-ares-devel &gt;= 1.19.0\n%endif\n</code></pre>\n<p>由于不满足 08 &gt;= 9 ，直接删除</p>\n<h2 id=\"42-修改buildrequires版本要求\">4.2 修改BuildRequires版本要求</h2>\n<p>官网repo源的postgresql-devel版本不达标，直接进行构建会报错。<br />\npostgresql官网没有OpenEuler的预编译rpm包，想要满足要求必须自行从源代码进行编译。<br />\n本文仅为演示，将<code>postgresql-devel &gt;= 12.0</code>修改为<code>postgresql-devel</code></p>\n<h2 id=\"43-去除agent2和web_service\">4.3 去除agent2和web_service</h2>\n<p>agent3和web_service都使用了GO语言，由于网络问题导致两者的创建会出错，直接删除以下内容：</p>\n<pre><code>%ifarch x86_64 aarch64\n%if ( 0%{?rhel} &gt;= 7 &amp;&amp; 0%{?amzn} == 0 ) || 0%{?amzn} &gt;= 2023\n%{!?build_agent2: %global build_agent2 1}\n%endif\n%if 0%{?rhel} &gt;= 8 || 0%{?amzn} &gt;= 2023\n%{!?build_web_service: %global build_web_service 1}\n%endif\n%endif\n</code></pre>\n<p>本次构建不包括两者，想要解决可能必须使用魔法了</p>\n<h1 id=\"五构建\">五、构建</h1>\n<p>使用<code>rpmbuild -bb zabbix.spec</code>命令进行构建，需要比较长的时间。<br />\n完成后在/root/rpmbuild/RPMS目录下就会生成编译好的rpm包。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-15 10:46</span>&nbsp;\n<a href=\"https://www.cnblogs.com/virtualzzf\">virtualzzf</a>&nbsp;\n阅读(<span id=\"post_view_count\">16</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "为什么现代 C++ 库都用 PIMPL？一场关于封装、依赖与安全的演进",
      "link": "https://www.cnblogs.com/charlee44/p/19616660",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/charlee44/p/19616660\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 21:27\">\n    <span>为什么现代 C++ 库都用 PIMPL？一场关于封装、依赖与安全的演进</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        系统阐述了在 C++ 工程中如何通过 PIMPL 惯用法，在坚守 RAII 资源安全的前提下，有效解耦头文件依赖、提升编译效率并保持接口简洁。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>在 C++ 的工程实践中，如何在保证资源安全管理的同时，又避免头文件污染和不必要的编译依赖？这个问题贯穿了现代 C++ 库设计的核心。本文将沿着一条清晰的技术演进路径，探讨从 RAII 封装出发，历经值语义、裸指针、智能指针等阶段，最终走向 PIMPL（Pointer to Implementation） 这一成熟且优雅的解决方案。</p>\n</blockquote>\n<h1 id=\"1-raii资源管理的基石\">1. RAII——资源管理的基石</h1>\n<p>C++ 的核心哲学之一是 RAII（Resource Acquisition Is Initialization）：资源（内存、文件句柄、网络连接等）的生命周期应由对象的构造与析构自动管理。例如：</p>\n<pre><code class=\"language-cpp\">class FileHandle {\n    FILE* fp;\npublic:\n    FileHandle(const char* path) : fp(fopen(path, \"r\")) {}\n    ~FileHandle() { if (fp) fclose(fp); }\n};\n</code></pre>\n<p>RAII 让资源管理变得安全：利用类对象的生命周期，在构造函数中申请资源，在析构函数中释放资源。如果这个类对象是基于栈的值对象，那么就可以自动实现资源的管理。因此，在现代 C++ 中，相比传统的指针语义，更加提倡使用基于 RAII 的值语义。</p>\n<h1 id=\"2-值语义的诱惑与代价\">2. 值语义的诱惑与代价</h1>\n<p>但是，当我们把这种思想用于封装复杂组件（如 ONNX 模型会话、数据库连接池）时，问题出现了。理想情况下，我们希望像使用 std::string 一样，用“值语义”操作一个封装对象：</p>\n<pre><code class=\"language-cpp\">class Embedder {\n    Ort::Session session; // 值成员\npublic:\n    std::vector&lt;float&gt; embed(const std::string&amp; text);\n};\n</code></pre>\n<p>这看起来非常简洁、高效、符合现代 C++ 风格。但也有另外一个问题：破坏了封装，导致不必要的环境依赖。最直观的问题就是 <code>Ort::Session</code> 的完整定义必须出现在头文件中，这意味着使用者必须包含 onnxruntime ，而这个头文件可能重达数 MB ，依赖数十个系统库。这就会造成如下问题：</p>\n<ul>\n<li>编译时间暴增，微小的改动都需要编译很长的时间。</li>\n<li>头文件耦合严重，调用者使用不方便，甚至造成环境污染。</li>\n<li>ABI 极其脆弱，内部改动导致所有用户重编译。</li>\n</ul>\n<h1 id=\"3-指针语义的回退\">3. 指针语义的回退</h1>\n<p>为了解耦，一个比较好的办法就是使用前置声明 + 指针语义：</p>\n<pre><code class=\"language-cpp\">// header\nclass SessionImpl; // 前置声明\nclass Embedder {\n    SessionImpl* pimpl;\npublic:\n    Embedder();\n    ~Embedder(); // 必须手动 delete\n};\n</code></pre>\n<p>这样做确实切断了编译依赖，但也引入了新的问题。那就是需要按照 RAII 原则写好构造函数和析构函数。而一旦要写析构函数，也往往意味着需要写另外四个特殊的成员函数：</p>\n<ol>\n<li>拷贝构造函数（Copy Constructor）</li>\n<li>拷贝赋值运算符（Copy Assignment Operator）</li>\n<li>移动构造函数（Move Constructor）</li>\n<li>移动赋值运算符（Move Assignment Operator）</li>\n</ol>\n<p>这样做要写非常多的样板代码，而且也很容易出问题。为了封装牺牲安全，得不偿失。</p>\n<h1 id=\"4-使用智能指针\">4. 使用智能指针</h1>\n<p>使用裸指针又麻烦又不安全，那么就可以使用 C++11 引入的智能指针：std::unique_ptr 和 std::shared_ptr；智能指针同样是基于 RAII 的：</p>\n<pre><code class=\"language-cpp\">class SessionImpl;\nclass Embedder {\n    std::unique_ptr&lt;SessionImpl&gt; pimpl;\n};\n</code></pre>\n<p>这里为什么使用 <code>std::unique_ptr</code> 而不使用 <code>std::shared_ptr</code> 呢？其实也可以，不过在现代 C++ 中，更推荐使用 <code>std::unique_ptr</code> 。<code>std::shared_ptr</code> 是用来共享资源的所有权，会对引用资源进行计数，但是有可能会造成相互循环引用造成不能释放资源的问题；而<code>std::unique_ptr</code> 则表示独占资源的所有权，不仅开销更低（无引用计数），也更加安全（只能通过 <code>std::move</code> 转移所有权 ）。</p>\n<p>不过有一点需要注意：<code>std::unique_ptr</code> 和 <code>std::shared_ptr</code> 在处理不完整类型（incomplete type）时的行为截然不同。具体来说，当在头文件中使用前置声明（如 <code>class Impl;</code>）并用智能指针持有它时，<code>Impl</code> 是一个不完整类型。</p>\n<ul>\n<li><code>std::shared_ptr</code> 可以安全地在头文件中默认析构，因为它在构造时（通常在 <code>.cpp</code> 文件中）会捕获一个完整的删除器（deleter），即使析构发生在头文件上下文中，也能正确调用 <code>delete</code>。</li>\n<li>而 <code>std::unique_ptr</code> 的删除器是其类型的一部分（通常是默认的 <code>std::default_delete&lt;Impl&gt;</code>），它要求在析构点（即类的析构函数被实例化的地方）<code>Impl</code> 必须是完整类型。如果在头文件中写 <code>~Embedder() = default;</code>，此时 <code>Impl</code> 仍是不完整的，编译器可能不会报错，但会导致未定义行为（通常是链接失败或运行时崩溃）。</li>\n</ul>\n<p>因此，使用 <code>std::unique_ptr&lt;Impl&gt;</code> 时，必须将主类的析构函数定义移到 <code>.cpp</code> 文件中，确保 <code>Impl</code> 已被完整定义：</p>\n<pre><code class=\"language-cpp\">// Embedder.cpp\nclass Embedder::Impl {\n    // 完整定义...\n};\n\nEmbedder::~Embedder() = default; // ✅ 此时 Impl 完整，安全析构\n</code></pre>\n<h1 id=\"5-封装与效率的平衡pimpl\">5. 封装与效率的平衡：PIMPL</h1>\n<p>使用智能指针虽然好，但是总归是比不上值语义方便。当类中只有一个需要隐藏的成员还好，如果有很多个需要隐藏的成员，每一个都写前置声明，并用智能指针来管理，那就实在太繁琐了。并且，从编程品味上来说，C++ 智能指针的写法说不上优雅：智能指针是由传染性的，当满屏都是 <code>std::shared_ptr</code> 或者 <code>std::unique_ptr</code> 的时候，实在很影响阅读性。</p>\n<p>另外，作为对外的接口，最好是提供像 Java / C# 那样的接口，C++ 的纯虚函类也行，隐藏掉所有的细节，包括私有函数和数据成员。这样有非常多的好处：</p>\n<ol>\n<li>最小化依赖环境，提升编译速度。</li>\n<li>调用者使用方便，不会污染环境。</li>\n<li>ABI 稳定，可以只更新库而不用更新整个程序。</li>\n</ol>\n<p>那么要怎么进行优化呢？很简单，我们可以实现一个名为 <code>Impl</code> 的类中类 ，使用<code>std::unique_ptr</code>进行管理。<code>Impl</code> 是实现在 cpp 中的，可以将一切实现的细节，比说私有函数和数据成员，都放在这个 <code>Impl</code> 中。更重要的是，<code>Impl</code> 中的数据成员完全可以使用值类型！如下所示：</p>\n<pre><code class=\"language-cpp\">// 头文件\nclass Embedder {\n    class Impl;\n    std::unique_ptr&lt;Impl&gt; impl;\npublic:\n    Embedder(const std::string&amp; model);\n    ~Embedder(); // 声明但不在头文件定义！\n    std::vector&lt;float&gt; embed(std::string_view text) const;\n};\n</code></pre>\n<pre><code class=\"language-cpp\">// 源文件\nclass Embedder::Impl {\n    Ort::Session session;\n    hf::Tokenizer tokenizer;\n    int64_t dim;\npublic:\n    Impl(const std::string&amp; path, const hf::Tokenizer&amp; tok) \n        : session(...), tokenizer(tok) { /* init */ }\n    std::vector&lt;float&gt; embed(std::string_view text) const { /* ... */ }\n};\n\nEmbedder::Embedder(const std::string&amp; path) \n    : impl(std::make_unique&lt;Impl&gt;(path, global_tokenizer)) {}\n\nEmbedder::~Embedder() = default; // 此时 Impl 完整，安全！\n</code></pre>\n<p>这个实现，就是所谓的 PIMPL（Pointer to IMPLementation）惯用法，也常被称作 “编译防火墙”（Compilation Firewall） 或 “Opaque Pointer” 模式。不得不说，这种 PIMPL 设计模式确实精妙——它在安全性、封装性、编译效率与接口简洁性之间取得了近乎完美的平衡，既坚守了 RAII 的资源管理原则，又有效隔离了实现细节，堪称现代 C++ 工程实践中“高内聚、低耦合”的典范。</p>\n<h1 id=\"6-没有银弹只有权衡\">6. 没有银弹，只有权衡</h1>\n<p>PIMPL 使用了前置声明。是否使用前置声明一直是 C++ 中比较争议的一点，Qt 遵循前置声明的原则实现了非常强大、优雅且高效的 C++ 运行时框架。Google 则经历了从推荐使用前置声明到不推荐使用前置声明的转变。个人认为，PIMPL 解决的就是 C++ 中两个重要原则矛盾的问题：</p>\n<ul>\n<li>推荐使用值语义，但是会引入更多环境依赖</li>\n<li>封装需要尽可能隐藏不必要的细节</li>\n</ul>\n<p>如果两者只能选择其中一个，那么还是尽量使用值语义的原则更加重要，毕竟这涉及到安全问题，而资源管理的安全问题贯穿 C++ 程序的始终。事实上，如果不是提供对外接口，或者实现比较小，那么直接使用值语义即可（第2节中的内容）——值语义永远是最简洁安全的实现。</p>\n<p>另外，如果实现 C++20 Modules ，那么就不必要使用 PIMPL 了，完全可以回归值语义实现，因为 C++20 Modules 在语言层面已经实现了 PIMPL 的诸多优点。</p>\n<h1 id=\"7-示例代码\">7. 示例代码</h1>\n<p>最后放出笔者自己实现的基于 PIMPL 的嵌入器的完整代码供读者参考：</p>\n<pre><code class=\"language-cpp\">// BgeOnnxEmbedder.h\n#pragma once\n\n#include &lt;memory&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n\nnamespace embedding {\n\nnamespace hf {\nclass Tokenizer;\n}\n\nclass BgeOnnxEmbedder {\n public:\n  explicit BgeOnnxEmbedder(const std::string&amp; modelPath,\n                           const hf::Tokenizer&amp; tokenizer);\n  ~BgeOnnxEmbedder();\n\n  const int64_t&amp; EmbeddingDim() const;\n\n  std::vector&lt;float&gt; Embed(const std::string&amp; text) const;\n\n private:\n  class Impl;  // 前向声明\n  std::unique_ptr&lt;Impl&gt; impl;\n};\n\n}  // namespace embedding\n</code></pre>\n<pre><code class=\"language-cpp\">//BgeOnnxEmbedder.cpp\n#include \"BgeOnnxEmbedder.h\"\n\n#include &lt;onnxruntime_cxx_api.h&gt;\n\n#include \"HfTokenizer.h\"\n#include \"Util/StringEncode.h\"\n\nnamespace embedding {\n\nclass BgeOnnxEmbedder::Impl {\n public:\n  Ort::Env&amp; GetOrtEnv() {\n    static Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"BgeOnnxEmbedder\");\n    return env;\n  }\n\n  const int64_t&amp; EmbeddingDim() const { return embeddingDim; }\n\n  explicit Impl(const std::string&amp; modelPath, const hf::Tokenizer&amp; tokenizer)\n      : session{GetOrtEnv(),\n#ifdef _WIN32\n                util::StringEncode::Utf8StringToWideString(modelPath).c_str(),\n#else\n                modelPath.c_str(),\n#endif\n                Ort::SessionOptions()},\n        memInfo{Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU)},\n        tokenizer(tokenizer),\n        embeddingDim(0) {\n\n    //\n    const auto&amp; outputInfo = session.GetOutputTypeInfo(0);\n    const auto&amp; tensorInfo = outputInfo.GetTensorTypeAndShapeInfo();\n    const auto&amp; shape = tensorInfo.GetShape();\n\n    // 假设输出是 [batch, seq, dim] 或 [batch, dim]\n    // 我们取最后一个非 -1 的维度\n    for (auto it = shape.rbegin(); it != shape.rend(); ++it) {\n      if (*it != -1) {\n        embeddingDim = *it;\n        break;\n      }\n    }\n\n    if (embeddingDim == 0) {\n      throw std::runtime_error(\n          \"Failed to infer embedding dimension from ONNX model.\");\n    }\n  }\n\n  std::vector&lt;float&gt; Embed(const std::string&amp; text) const {\n    hf::Tokenizer::ResultPtr result = tokenizer.Encode(text);\n    if (!result) {\n      throw std::runtime_error(\"tokenizer_encode failed\");\n    }\n\n    // 定义张量维度\n    int64_t seqLen = static_cast&lt;int64_t&gt;(result-&gt;length);\n    std::vector&lt;int64_t&gt; inputShape = {1, seqLen};\n    size_t dataByteCount = sizeof(int64_t) * seqLen;\n\n    Ort::Value inputIdsTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;input_ids, dataByteCount, inputShape.data(),\n        inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    Ort::Value attentionMaskTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;attention_mask, dataByteCount,\n        inputShape.data(), inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    Ort::Value tokenTypeIdsTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;token_type_ids, dataByteCount,\n        inputShape.data(), inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    // 输入名必须与模型定义一致\n    const char* inputNames[] = {\"input_ids\", \"attention_mask\",\n                                \"token_type_ids\"};\n    const char* outputNames[] = {\"last_hidden_state\"};\n\n    // 把三个输入张量放进数组\n    std::vector&lt;Ort::Value&gt; inputs;\n    inputs.push_back(std::move(inputIdsTensor));\n    inputs.push_back(std::move(attentionMaskTensor));\n    inputs.push_back(std::move(tokenTypeIdsTensor));\n\n    // 执行推理\n    auto outputs = session.Run(Ort::RunOptions(),  // 运行选项（通常 nullptr）\n                               inputNames,         // 输入名数组\n                               inputs.data(),  // 输入张量数组\n                               inputs.size(),  // 输入数量（3）\n                               outputNames,    // 输出名数组\n                               1               // 输出数量（1）\n    );\n\n    // 获取输出信息\n    auto&amp; output_tensor = outputs[0];\n    auto output_shape = output_tensor.GetTensorTypeAndShapeInfo().GetShape();\n    if (output_shape.size() != 3 || output_shape[0] != 1) {\n      throw std::runtime_error(\"Unexpected output shape\");\n    }\n\n    // 获取输出张量的原始 float 指针\n    const float* outputData = outputs[0].GetTensorData&lt;float&gt;();\n\n    // 提取 [CLS] token 的 embedding（第0个token）\n    int64_t hiddenSize = output_shape[2];\n    std::vector&lt;float&gt; embedding(outputData, outputData + hiddenSize);\n\n    // L2 归一化（BGE 要求）\n    float norm = 0.0f;\n    for (float v : embedding) norm += v * v;\n    norm = std::sqrt(norm);\n    if (norm &gt; 1e-8) {\n      for (float&amp; v : embedding) v /= norm;\n    }\n\n    return embedding;\n  }\n\n private:\n  mutable Ort::Session session;\n  Ort::MemoryInfo memInfo;\n  const hf::Tokenizer&amp; tokenizer;\n  int64_t embeddingDim;\n};\n\nBgeOnnxEmbedder::BgeOnnxEmbedder(const std::string&amp; modelPath,\n                                 const hf::Tokenizer&amp; tokenizer)\n    : impl(std::make_unique&lt;Impl&gt;(modelPath, tokenizer)) {}\n\nBgeOnnxEmbedder::~BgeOnnxEmbedder() = default;  // 此时 Impl 已定义，可安全析构\n\nconst int64_t&amp; BgeOnnxEmbedder::EmbeddingDim() const {\n  return impl-&gt;EmbeddingDim();\n}\n\nstd::vector&lt;float&gt; BgeOnnxEmbedder::Embed(const std::string&amp; text) const {\n  return impl-&gt;Embed(text);\n}\n\n}  // namespace embedding\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 21:27</span>&nbsp;\n<a href=\"https://www.cnblogs.com/charlee44\">charlee44</a>&nbsp;\n阅读(<span id=\"post_view_count\">42</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI编程时代普通本科计算机毕业生的出路",
      "link": "https://www.cnblogs.com/xdesigner/p/19615230",
      "published": "",
      "description": "<h2>\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xdesigner/p/19615230\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 13:36\">\n    <span>AI编程时代普通本科计算机毕业生的出路</span>\n    \n\n</a>\n\n\t\t</h2>\n\t\t<div class=\"postText\">    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        AI编程工具快速普及，正在彻底改写整个程序员行业的格局与就业逻辑。对国内数百万一本、二本普通计算机专业的毕业生来说，就业环境已经变得异常残酷，那么出路在哪里？\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p><span style=\"font-size: 18pt;\">AI<span>编程时代</span>普通本科计算机毕业生的出路</span></p>\n<p><span style=\"font-size: x-large;\">袁永福 2026-2-14</span></p>\n<p><span style=\"font-size: 18pt;\">&nbsp; AI<span>编程工具快速普及，正在彻底改写整个程序员行业的格局与就业逻辑。对于985，211重点大学计算机专业毕业生来说还能应付。但对数百万一本、二本普通计算机专业的毕业生来说，环境已经变得异常残酷：</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>程序员岗位总量快速收缩，互联网大厂招人数量大幅下降；</span><span style=\"font-family: Calibri;\">ToB&nbsp;</span><span>程序员岗位虽然也在收紧，招人规模有所减少，但依然保留着真实的就业机会。 而很多学生选择的全职考公、考研、考编，对大多数人来说早已不是靠谱出路，而是一块逃避现实、变相啃老的遮羞布。面对大势，最清醒、优先级最高的选择非常明确：拥抱</span><span style=\"font-family: Calibri;\">AI</span><span>而不是抵制</span><span style=\"font-family: Calibri;\">AI</span><span>，充分利用</span><span style=\"font-family: Calibri;\">AI</span><span>带来的技术平权，优先进入</span><span style=\"font-family: Calibri;\">ToB</span><span>领域，扎扎实实与一个长期行业深度绑定，这才是普通计算机毕业生最现实、最可持续的就业路线。</span></span></p>\n<p><span style=\"font-size: 18pt;\">&nbsp; 从全球行业数据来看，估计<span>当前全球程序员总量约</span>2700<span>万人，其中&nbsp;</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>程序员和</span><span style=\"font-family: Calibri;\">ToB&nbsp;</span><span>程序员大约各占一半</span><span>。过去十几年，互联网高速扩张，</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>领域一直是吸纳程序员最多的方向。</span></span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 但在</span>AI<span>与行业周期的双重冲击下，这个格局正在快速反转。</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>程序员的岗位正在大幅减少。 互联网大厂、电商、社交、工具类产品，大量工作集中在界面开发、接口实现、业务逻辑拼接等重复性内容，而这正是</span><span style=\"font-family: Calibri;\">AI</span><span>最擅长替代的部分。再加上行业降本增效，企业不再靠堆人头做规模，而是用资深工程师配合</span><span style=\"font-family: Calibri;\">AI</span><span>工具完成工作，</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>整体岗位数量快速下降。大家常说的“</span><span style=\"font-family: Calibri;\">30</span><span>岁危机”“</span><span style=\"font-family: Calibri;\">35</span><span>岁退休”，几乎全部来自</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>行业，因为这里拼成本、拼速度、拼年轻化，普通学历开发者几乎没有长期竞争力。</span></span></p>\n<p><span style=\"font-size: 18pt;\">&nbsp; ToB&nbsp;<span>程序员的处境则完全不同。 虽然受经济环境影响，企业招新人的数量也在下降，入口比以前更窄，但岗位并没有消失。</span><span style=\"font-family: Calibri;\">ToB&nbsp;</span><span>面向企业、政府、医疗、金融、工业、能源等垂直行业，核心价值在于业务理解、行业规则、系统稳定性和复杂项目落地能力。</span><span style=\"font-family: Calibri;\">AI</span><span>可以辅助编码，但无法替代行业经验与真实项目积累。在</span><span style=\"font-family: Calibri;\">ToB</span><span>领域，</span><span style=\"font-family: Calibri;\">35</span><span>岁不是危机，而是黄金年龄，经验越丰富越不可替代。</span></span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 根据预测，</span>5<span>年后全球程序员结构将出现颠覆性变化：</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>程序员会从现在的</span><span style=\"font-family: Calibri;\">1800</span><span>万减到1400万左右</span><span>，总量接近腰斩；而&nbsp;</span><span style=\"font-family: Calibri;\">ToB&nbsp;</span><span>程序员会从1300</span><span>万稳步增长到1700</span><span>万左右。</span><span>这组数据，为普通计算机毕业生指明了最确定的方向。</span></span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 可现实中，大量一本、二本学生一毕业就放弃就业，全职备考。不工作、不实习、无收入，完全依靠家庭支持，看上去是在努力奋斗，本质却非常残酷：考研极度内卷，考公录取率不足</span>3%<span>，全职备考本质上就是用极低概率的梦想，掩盖自己不敢进入社会的事实，成为一块体面又合理的啃老遮羞布。 几年</span>后不出意外的考不上，技能退化、经验空白、年龄变大，最终既没上岸，也没工作，两头落空。是一种对自己对家人不负责任的行为。</span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 面对</span>AI<span>编程的大趋势，正确的态度不是恐惧和抵制，而是主动拥抱。</span><span style=\"font-family: Calibri;\">AI</span><span>带来了前所未有的技术平权：过去需要长期训练才能具备的编码能力，现在借助</span><span style=\"font-family: Calibri;\">AI</span><span>工具可以快速上手。这对基础一般、学历普通的学生来说，是巨大的机会——你不需要成为顶尖高手，只要会用</span><span style=\"font-family: Calibri;\">AI</span><span>、能落地、懂业务，就能在</span><span style=\"font-family: Calibri;\">To B</span><span>领域站稳脚跟</span>&nbsp;</span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 因此，普通本科计算机毕业生最高优先级的选择，不是死磕</span>To C<span>大厂，不是赌命考公考研，而是放低姿态、接受合理起薪，优先进入</span><span style=\"font-family: Calibri;\">To B</span><span>行业。不必执着于大厂光环，不必纠结于起点高低，先入行、先积累。选择医疗</span><span style=\"font-family: Calibri;\">IT</span><span>、政务信息化、工业软件、金融后台等长期稳定的方向，在一个行业扎根</span><span style=\"font-family: Calibri;\">3</span><span>—</span><span style=\"font-family: Calibri;\">5</span><span>年，把自己和行业深度绑定，用业务壁垒建立</span><span style=\"font-family: Calibri;\">AI</span><span>无法替代的竞争力。</span></span></p>\n<p><span style=\"font-size: 18pt;\">&nbsp; AI<span>时代没有捷径，只有最现实的生存策略。</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>在收缩，考公考研是概率陷阱，全职备考只是</span>啃老<span>遮羞布。拥抱</span>AI<span>、用好技术平权、坚定入局</span><span style=\"font-family: Calibri;\">ToB</span><span>、深耕垂直行业，才是普通计算机毕业生在时代变局中，最清醒、相对最务实，走得最远的出路。</span></span></p>\n\n</div>\n<div class=\"clear\"></div>\n</div>\n\t\t<p class=\"postfoot\">\n\t\t\tposted on \n<span id=\"post-date\">2026-02-14 13:36</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xdesigner\">袁永福 电子病历，医疗信息化</a>&nbsp;\n阅读(<span id=\"post_view_count\">228</span>)&nbsp;\n评论(<span id=\"post_comment_count\">2</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</p>"
    },
    {
      "title": "Flask - 常见应用部署方案",
      "link": "https://www.cnblogs.com/XY-Heruo/p/19615176",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/XY-Heruo/p/19615176\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 13:19\">\n    <span>Flask - 常见应用部署方案</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        常见Flask应用部署方案\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"前言\">前言</h2>\n<p>开发调试阶段，运行 Flask 的方式多直接使用 <code>app.run()</code>，但 Flask 内置的 WSGI Server 的性能并不高。对于生产环境，一般使用 <code>gunicorn</code>。如果老项目并不需要多高的性能，而且用了很多单进程内的共享变量，使用 gunicorn 会影响不同会话间的通信，那么也可以试试直接用 <code>gevent</code>。</p>\n<p>在 Docker 流行之前，生产环境部署 Flask 项目多使用 virtualenv + gunicorn + supervisor。Docker 流行之后，部署方式就换成了 gunicorn + Docker。如果没有容器编排服务，后端服务前面一般还会有个 nginx 做代理。如果使用 Kubernetes，一般会使用 service + ingress（或 istio 等）。</p>\n<h2 id=\"运行方式\">运行方式</h2>\n<h3 id=\"flask-内置-wsgi-server\">Flask 内置 WSGI Server</h3>\n<p>开发阶段一般使用这种运行方式。</p>\n<pre><code class=\"language-python\"># main.py\nfrom flask import Flask\nfrom time import sleep\n\napp = Flask(__name__)\n\n@app.get(\"/test\")\ndef get_test():\n    sleep(0.1)\n    return \"ok\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=10000)\n</code></pre>\n<p>运行:</p>\n<pre><code class=\"language-bash\">python main.py\n</code></pre>\n<h3 id=\"gevent\">gevent</h3>\n<p>使用 gevent 运行 Flask，需要先安装 gevent</p>\n<pre><code class=\"language-bash\">python -m pip install -U gevent\n</code></pre>\n<p>代码需要稍作修改。</p>\n<p>需要注意 <code>monkey.patch_all()</code> 一定要写在入口代码文件的最开头部分，这样 monkey patch 才能生效。</p>\n<pre><code class=\"language-python\"># main.py\nfrom gevent import monkey\nmonkey.patch_all()\nimport time\n\nfrom flask import Flask\nfrom gevent.pywsgi import WSGIServer\n\n\napp = Flask(__name__)\n\n\n@app.get(\"/test\")\ndef get_test():\n    time.sleep(0.1)\n    return \"ok\"\n\n\nif __name__ == \"__main__\":\n    server = WSGIServer((\"0.0.0.0\", 10000), app)\n    server.serve_forever()\n</code></pre>\n<p>运行</p>\n<pre><code class=\"language-bash\">python main.py\n</code></pre>\n<h3 id=\"gunicorn--gevent\">gunicorn + gevent</h3>\n<blockquote>\n<p>如果现有项目大量使用单进程内的内存级共享变量，贸然使用 gunicorn 多 worker 模式可能会导致数据访问不一致的问题。</p>\n</blockquote>\n<p>同样需要先安装依赖。</p>\n<pre><code class=\"language-bash\">python -m pip install -U gunicorn gevent\n</code></pre>\n<p>不同于单独使用 gevent，这种方式不需要修改代码，gunicorn 会自动注入 gevent 的 monkey patch。</p>\n<p>gunicorn 可以在命令行配置启动参数，但个人一般习惯在 gunicorn 的配置文件内配置启动参数，这样可以动态设置一些配置，而且可以修改日志格式。</p>\n<p><code>gunicorn.conf.py</code> 的配置示例如下：</p>\n<pre><code class=\"language-python\"># Gunicorn 配置文件\nfrom pathlib import Path\nfrom multiprocessing import cpu_count\nimport gunicorn.glogging\nfrom datetime import datetime\n\nclass CustomLogger(gunicorn.glogging.Logger):\n    def atoms(self, resp, req, environ, request_time):\n        \"\"\"\n        重写 atoms 方法来自定义日志占位符\n        \"\"\"\n        # 获取默认的所有占位符数据\n        atoms = super().atoms(resp, req, environ, request_time)\n        \n        # 自定义 't' (时间戳) 的格式\n        now = datetime.now().astimezone()\n        atoms['t'] = now.isoformat(timespec=\"seconds\")\n        \n        return atoms\n    \n\n# 预加载应用代码\npreload_app = True\n\n# 工作进程数量：通常是 CPU 核心数的 2 倍加 1\n# workers = int(cpu_count() * 2 + 1)\nworkers = 4\n\n# 使用 gevent 异步 worker 类型，适合 I/O 密集型应用\n# 注意：gevent worker 不使用 threads 参数，而是使用协程进行并发处理\nworker_class = \"gevent\"\n\n# 每个 gevent worker 可处理的最大并发连接数\nworker_connections = 2000\n\n# 绑定地址和端口\nbind = \"127.0.0.1:10001\"\n\n# 进程名称\nproc_name = \"flask-dev\"\n\n# PID 文件路径\npidfile = str(Path(__file__).parent / \"tmp\" / \"gunicorn.pid\")\n\nlogger_class = CustomLogger\naccess_log_format = (\n    '{\"@timestamp\": \"%(t)s\", '\n    '\"remote_addr\": \"%(h)s\", '\n    '\"protocol\": \"%(H)s\", '\n    '\"host\": \"%({host}i)s\", '\n    '\"request_method\": \"%(m)s\", '\n    '\"request_path\": \"%(U)s\", '\n    '\"status_code\": %(s)s, '\n    '\"response_length\": %(b)s, '\n    '\"referer\": \"%(f)s\", '\n    '\"user_agent\": \"%(a)s\", '\n    '\"x_tracking_id\": \"%({x-tracking-id}i)s\", '\n    '\"request_time\": %(L)s}'\n)\n\n# 访问日志路径\naccesslog = str(Path(__file__).parent / \"logs\" / \"access.log\")\n\n# 错误日志路径\nerrorlog = str(Path(__file__).parent / \"logs\" / \"error.log\")\n\n# 日志级别\nloglevel = \"debug\"\n</code></pre>\n<p>运行。gunicorn 的默认配置文件名就是 <code>gunicorn.conf.py</code>，如果文件名不同，可以使用 <code>-c</code> 参数来指定。</p>\n<pre><code class=\"language-bash\">gunicorn main:app\n</code></pre>\n<h2 id=\"传统进程管理实现自动启动\">传统进程管理：实现自动启动</h2>\n<p>在传统服务器部署时，常见的进程守护方式有：</p>\n<ol>\n<li>配置 crontab + shell 脚本。定时检查进程在不在，不在就启动。</li>\n<li>配置 supervisor。</li>\n<li>配置 systemd。</li>\n</ol>\n<p>由于 <code>supervisor</code> 需要单独安装，而本着能用自带工具就用自带工具、能少装就少装的原则，个人一般不会使用 supervisor，因此本文不会涉及如何使用 supervisor。</p>\n<p>在服务器部署时，一般也会为项目单独创建 Python 虚拟环境。</p>\n<pre><code class=\"language-bash\"># 使用 Python 内置的 venv，在当前目录创建 Python 虚拟环境目录 .venv\npython3 -m venv .venv\nsource .venv/bin/activate\npython -m pip install -r ./requirements.txt\n\n# 如果使用uv, 直接uv sync 即可\n</code></pre>\n<h3 id=\"crontab--shell-脚本-不推荐生产环境\">crontab + shell 脚本 (不推荐生产环境)</h3>\n<p>刚入行的时候对 systemd 不熟悉，经常用 crontab + shell 脚本来守护进程，现在想想这种方式并不合适，比较考验 shell 脚本的编写水平，需要考虑方方面面</p>\n<ul>\n<li>首先要确保用户级 crontab 启用，有些生产环境会禁用用户级的 crontab，而且也不允许随便配置系统级的 crontab。</li>\n<li>crontab 是分钟级的，服务停止时间可能要一分钟。</li>\n<li>如果有控制台日志，需要手动处理日志重定向，还有日志文件轮转问题。</li>\n<li>如果 ulimit 不高，还得控制 ulimit。</li>\n<li>经常出现僵尸进程，shell 脚本来要写一堆状态检查的逻辑。</li>\n</ul>\n<p>如果只需要简单用用，也可以提供个示例</p>\n<pre><code class=\"language-bash\">#!/bin/bash\n\n# 环境配置\nexport FLASK_ENV=\"production\"\nexport DATABASE_URL=\"postgresql://user:pass@localhost:5432/mydb\"\nexport REDIS_URL=\"redis://localhost:6379/0\"\n\nscript_dir=$(cd $(dirname $0) &amp;&amp; pwd)\napp_name=\"gunicorn\"  # 实际进程名是 gunicorn，不是 Flask app\nwsgi_module=\"wsgi:app\"  # 替换 WSGI 入口\nsocket_path=\"${script_dir}/myapp.sock\"  # Unix Socket 路径（避免 /run 重启丢失）\nlog_file=\"${script_dir}/app.log\"\npid_file=\"${script_dir}/gunicorn.pid\"   # 用 PID 文件控制\n\n# 进程检测\nis_running() {\n    if [ -f \"$pid_file\" ]; then\n        pid=$(cat \"$pid_file\")\n        if ps -p \"$pid\" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; grep -q \"gunicorn.*${wsgi_module}\" /proc/\"$pid\"/cmdline 2&gt;/dev/null; then\n            echo \"Gunicorn (PID: $pid) is running\"\n            return 0\n        else\n            rm -f \"$pid_file\"  # 清理失效 PID\n            echo \"Stale PID file found, cleaned up\"\n            return 1\n        fi\n    else\n        # 备用检测：通过 socket 文件 + 进程名\n        if [ -S \"$socket_path\" ] &amp;&amp; pgrep -f \"gunicorn.*${wsgi_module}\" &gt; /dev/null 2&gt;&amp;1; then\n            echo \"Gunicorn is running (detected by socket)\"\n            return 0\n        fi\n        echo \"Gunicorn is not running\"\n        return 1\n    fi\n}\n\n# 启动应用\nstart_app() {\n    is_running\n    if [ $? -eq 0 ]; then\n        echo \"Already running, skip start\"\n        return 0\n    fi\n\n    echo \"Starting Gunicorn at $(date)\"\n    echo \"Socket: $socket_path\"\n    echo \"Log: $log_file\"\n\n    # 确保 socket 目录存在\n    mkdir -p \"$(dirname \"$socket_path\")\"\n\n    # 启动命令（关键：不加 --daemon，用 nohup 托管）\n    cd \"$script_dir\" || exit 1\n    # 生成 PID 文件\n    nohup \"$script_dir/venv/bin/gunicorn\" \\\n        --workers 3 \\\n        --bind \"unix:$socket_path\" \\\n        --pid \"$pid_file\" \\\n        --access-logfile \"$log_file\" \\\n        --error-logfile \"$log_file\" \\\n        --log-level info \\\n        \"$wsgi_module\" &gt; /dev/null 2&gt;&amp;1 &amp;\n\n    # 等待启动完成\n    sleep 2\n    if is_running; then\n        echo \"✓ Start success (PID: $(cat \"$pid_file\" 2&gt;/dev/null))\"\n        return 0\n    else\n        echo \"✗ Start failed, check $log_file\"\n        return 1\n    fi\n}\n\n# 停止应用\nstop_app() {\n    is_running\n    if [ $? -eq 1 ]; then\n        echo \"Not running, skip stop\"\n        return 0\n    fi\n\n    pid=$(cat \"$pid_file\" 2&gt;/dev/null)\n    echo \"Stopping Gunicorn (PID: $pid) gracefully...\"\n\n    # 先发 SIGTERM（优雅停止）\n    kill -15 \"$pid\" 2&gt;/dev/null || true\n    sleep 5\n\n    # 检查是否还在运行\n    if ps -p \"$pid\" &gt; /dev/null 2&gt;&amp;1; then\n        echo \"Still running after 5s, force killing...\"\n        kill -9 \"$pid\" 2&gt;/dev/null || true\n        sleep 2\n    fi\n\n    # 清理残留\n    rm -f \"$pid_file\" \"$socket_path\"\n    echo \"✓ Stopped\"\n}\n\n# 重启应用\nrestart_app() {\n    echo \"Restarting Gunicorn...\"\n    stop_app\n    sleep 1\n    start_app\n}\n\n# 入口函数\nmain() {\n    # 检查 Gunicorn 是否存在\n    if [ ! -f \"$script_dir/venv/bin/gunicorn\" ]; then\n        echo \"ERROR: Gunicorn not found at $script_dir/venv/bin/gunicorn\"\n        echo \"Hint: Did you activate virtualenv? (source venv/bin/activate)\"\n        exit 1\n    fi\n\n    local action=${1:-start}  # 默认动作：start\n\n    case \"$action\" in\n        start)\n            start_app\n            ;;\n        stop)\n            stop_app\n            ;;\n        restart)\n            restart_app\n            ;;\n        status)\n            is_running\n            ;;\n        cron-check)\n            # 专为 crontab 设计：只检查+重启，不输出干扰日志\n            if ! is_running &gt; /dev/null 2&gt;&amp;1; then\n                echo \"[$(date '+%F %T')] CRON: Gunicorn down, auto-restarting...\" &gt;&gt; \"$log_file\"\n                start_app &gt;&gt; \"$log_file\" 2&gt;&amp;1\n            fi\n            ;;\n        *)\n            echo \"Usage: $0 {start|stop|restart|status|cron-check}\"\n            echo \"  cron-check: Silent mode for crontab (logs to app.log only)\"\n            exit 1\n            ;;\n    esac\n}\n\nmain \"$@\"\n</code></pre>\n<p>手动运行测试</p>\n<pre><code class=\"language-bash\">bash app_ctl.sh start\n</code></pre>\n<p>配置 crontab</p>\n<pre><code># 编辑当前用户 crontab\ncrontab -e\n\n# 添加以下行（每分钟检查一次）\n* * * * * /opt/myflaskapp/app_ctl.sh cron-check &gt;/dev/null 2&gt;&amp;1\n</code></pre>\n<p>配置logrotate</p>\n<pre><code># /etc/logrotate.d/myflaskapp\n/opt/myflaskapp/app.log {\n    daily\n    rotate 7\n    compress\n    delaycompress\n    missingok\n    notifempty\n    copytruncate  # 避免 Gunicorn 丢失文件句柄\n}\n</code></pre>\n<h3 id=\"systemd-推荐生产环境使用\">systemd (推荐生产环境使用)</h3>\n<ol>\n<li>创建 systemd 服务文件</li>\n</ol>\n<pre><code class=\"language-bash\">sudo vim /etc/systemd/system/myflaskapp.service\n</code></pre>\n<ol start=\"2\">\n<li>示例如下</li>\n</ol>\n<pre><code class=\"language-ini\">[Unit]\nDescription=Gunicorn instance for Flask App\nAfter=network.target\n\n[Service]\nUser=www-data\nGroup=www-data\nWorkingDirectory=/path/to/your/app\nEnvironment=\"PATH=/path/to/venv/bin\"\nExecStart=/path/to/venv/bin/gunicorn \\\n          --workers 4 \\\n          --bind unix:/run/myapp.sock \\\n          --access-logfile - \\\n          --error-logfile - \\\n          wsgi:app\n\n# 禁止添加 --daemon！systemd 需直接监控主进程\nRestart=on-failure        # 仅异常退出时重启（非0状态码、被信号杀死等）\nRestartSec=5s             # 重启前等待5秒\nStartLimitInterval=60s    # 60秒内\nStartLimitBurst=5         # 最多重启5次，防雪崩\nTimeoutStopSec=30         # 停止时等待30秒（优雅关闭）\n\n# 安全加固\nPrivateTmp=true\nNoNewPrivileges=true\nProtectSystem=strict\nReadWritePaths=/run /var/log/myapp\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<ol start=\"3\">\n<li>设置开机自启并启动服务</li>\n</ol>\n<pre><code class=\"language-bash\">sudo systemctl daemon-reload\nsudo systemctl enable myflaskapp    # 开机自启\nsudo systemctl start myflaskapp\n</code></pre>\n<p>可以试试用<code>kill -9</code>停止后端服务进程，观察能否被重新拉起。</p>\n<blockquote>\n<p>注意，<code>kill -15</code>算是正常停止，不算异常退出。</p>\n</blockquote>\n<h2 id=\"docker-部署方案\">Docker 部署方案</h2>\n<ol>\n<li>Dockerfile。Python 项目通常不需要多阶段构建，单阶段即可。</li>\n</ol>\n<pre><code class=\"language-dockerfile\">FROM python:3.11-slim-bookworm\n\n# 安全加固\n## 创建非 root 用户（避免使用 nobody，权限太受限）\nRUN useradd -m -u 1000 appuser &amp;&amp; \\\n    # 安装运行时必需的系统库（非编译工具）\n    apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n        libgomp1 \\\n        libpq5 \\\n        libsqlite3-0 \\\n        &amp;&amp; rm -rf /var/lib/apt/lists/* \\\n        &amp;&amp; apt-get autoremove -y \\\n        &amp;&amp; apt-get clean\n\n# Python 优化\nENV PYTHONUNBUFFERED=1 \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1\n\nWORKDIR /app\n\n# 利用 Docker 层缓存：先复制 requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --prefer-binary -r requirements.txt \\\n    # 清理 pip 缓存（虽然 --no-cache-dir 已禁用，但保险起见）\n    &amp;&amp; rm -rf /root/.cache\n\n# 应用代码\nCOPY --chown=appuser:appuser . .\n\n# 使用非root用户运行\nUSER appuser\n\n# 启动\nEXPOSE 8000\nCMD [\"gunicorn\", \"--config\", \"config/gunicorn.conf.py\", \"wsgi:app\"]\n</code></pre>\n<ol start=\"2\">\n<li>编写 docker-compose.yaml</li>\n</ol>\n<pre><code class=\"language-yaml\">services:\n  web:\n    image: myflaskapp:latest\n    container_name: flask_web\n    # 端口映射\n    ## 如果 nginx 也使用 Docker 部署，而且使用同一个网络配置，则可以不做端口映射\n    ports:\n      - \"8000:8000\"\n    # 环境变量\n    environment:\n      - FLASK_ENV=production\n      - DATABASE_URL=postgresql://user:pass@db:5432/mydb\n      - REDIS_URL=redis://redis:6379/0\n    # 健康检查\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s      # 每 30 秒检查一次\n      timeout: 5s        # 超时 5 秒\n      start_period: 15s  # 启动后 15 秒开始检查（给应用初始化时间）\n      retries: 3         # 失败重试 3 次后标记 unhealthy\n    \n    # 自动重启策略\n    restart: unless-stopped  # always / on-failure / unless-stopped\n    \n    # 资源限制\n    deploy:\n      resources:\n        limits:\n          cpus: '2'        # 最多 2 个 CPU\n          memory: 1G       # 最多 1GB 内存\n        reservations:\n          cpus: '0.5'      # 保留 0.5 个 CPU\n          memory: 256M     # 保留 256MB 内存\n    \n    # ulimit 限制（防资源滥用）\n    ulimits:\n      nproc: 65535       # 最大进程数\n      nofile:\n        soft: 65535      # 打开文件数软限制\n        hard: 65535      # 打开文件数硬限制\n      core: 0            # 禁止 core dump\n    \n    # 安全加固\n    security_opt:\n      - no-new-privileges:true  # 禁止提权\n    \n    # 只读文件系统（除 /tmp 外）\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid,size=100m\n    \n    # 卷挂载（日志、临时文件）\n    volumes:\n      - ./logs:/app/logs:rw\n      # - ./static:/app/static:ro  # 静态文件（可选）\n    \n    # 网络\n    networks:\n      - app-network\n        \n# 网络配置\nnetworks:\n  app-network:\n    driver: bridge\n\n# 卷配置\nvolumes:\n  db_data:\n    driver: local\n  redis_data:\n    driver: local\n</code></pre>\n<h2 id=\"kubernetes-部署方案\">Kubernetes 部署方案</h2>\n<h3 id=\"deployment\">Deployment</h3>\n<pre><code class=\"language-yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-app\n  namespace: default\n  labels:\n    app: flask-app\n    tier: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: flask-app\n  template:\n    metadata:\n      labels:\n        app: flask-app\n        tier: backend\n    spec:\n      securityContext:\n        runAsNonRoot: true      # 禁止 root 运行\n        runAsUser: 1000         # 使用非 root 用户\n        runAsGroup: 1000\n        fsGroup: 1000\n        seccompProfile:\n          type: RuntimeDefault  # 启用 seccomp 安全策略\n      containers:\n      - name: flask-app\n        image: myregistry.com/myflaskapp:1.0.0\n        imagePullPolicy: IfNotPresent  # 生产环境建议用 Always\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        env:\n        - name: FLASK_ENV\n          value: \"production\"\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: flask-app-secrets\n              key: database-url\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: flask-app-secrets\n              key: redis-url\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: flask-app-secrets\n              key: secret-key\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"   # 超过会 OOM Kill\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n            scheme: HTTP\n          initialDelaySeconds: 30  # 启动后 30 秒开始检查\n          periodSeconds: 10        # 每 10 秒检查一次\n          timeoutSeconds: 3        # 超时 3 秒\n          successThreshold: 1\n          failureThreshold: 3      # 失败 3 次后重启容器\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n            scheme: HTTP\n          initialDelaySeconds: 10  # 启动后 10 秒开始检查\n          periodSeconds: 5         # 每 5 秒检查一次\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 3      # 失败 3 次后从 Service 移除\n        startupProbe:\n          httpGet:\n            path: /health\n            port: 8000\n            scheme: HTTP\n          failureThreshold: 30     # 最多重试 30 次\n          periodSeconds: 5         # 每 5 秒一次，共 150 秒容忍慢启动\n          timeoutSeconds: 3\n        securityContext:\n          allowPrivilegeEscalation: false  # 禁止提权\n          readOnlyRootFilesystem: true     # 根文件系统只读\n          capabilities:\n            drop:\n            - ALL                          # 删除所有 Linux capabilities\n          privileged: false\n        volumeMounts:\n        - name: tmp-volume\n          mountPath: /tmp\n        - name: config-volume\n          mountPath: /app/config\n          readOnly: true\n      imagePullSecrets:\n      - name: registry-secret  # 如果使用私有镜像仓库\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - flask-app\n              topologyKey: kubernetes.io/hostname  # 避免所有 Pod 调度到同一节点\n      volumes:\n      - name: tmp-volume\n        emptyDir:\n          medium: Memory  # 使用内存卷，更快\n          sizeLimit: 100Mi\n      - name: config-volume\n        configMap:\n          name: flask-app-config\n</code></pre>\n<h3 id=\"service\">Service</h3>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: flask-app-service\n  namespace: default\n  labels:\n    app: flask-app\n    tier: backend\nspec:\n  type: ClusterIP\n  selector:\n    app: flask-app\n  ports:\n  - name: http\n    port: 80        # Service 端口\n    targetPort: 8000  # Pod 端口\n    protocol: TCP\n</code></pre>\n<h3 id=\"ingress-nginx\">ingress-nginx</h3>\n<pre><code class=\"language-yaml\">apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: flask-app-ingress\n  namespace: default\n  annotations:\n    # ==================== Nginx 配置 ====================\n    kubernetes.io/ingress.class: \"nginx\"\n    \n    # 启用 HTTPS 重定向\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    \n    # 限流（每秒 10 个请求，突发 20）\n    nginx.ingress.kubernetes.io/limit-rps: \"10\"\n    nginx.ingress.kubernetes.io/limit-burst-multiplier: \"2\"\n    \n    # 客户端真实 IP\n    nginx.ingress.kubernetes.io/enable-real-ip: \"true\"\n    nginx.ingress.kubernetes.io/proxy-real-ip-cidr: \"0.0.0.0/0\"\n    \n    # 连接超时\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"60\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"60\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"60\"\n    \n    # 缓冲区大小\n    nginx.ingress.kubernetes.io/proxy-buffering: \"on\"\n    nginx.ingress.kubernetes.io/proxy-buffer-size: \"16k\"\n    nginx.ingress.kubernetes.io/proxy-buffers-number: \"4\"\n    \n    # Gzip 压缩\n    nginx.ingress.kubernetes.io/enable-gzip: \"true\"\n    nginx.ingress.kubernetes.io/gzip-level: \"6\"\n    nginx.ingress.kubernetes.io/gzip-min-length: \"1024\"\n    nginx.ingress.kubernetes.io/gzip-types: \"text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript\"\n    \n    # 安全头\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      add_header X-Frame-Options \"SAMEORIGIN\" always;\n      add_header X-Content-Type-Options \"nosniff\" always;\n      add_header X-XSS-Protection \"1; mode=block\" always;\n      add_header Referrer-Policy \"strict-origin-when-cross-origin\" always;\n    \n    # 认证\n    # nginx.ingress.kubernetes.io/auth-type: basic\n    # nginx.ingress.kubernetes.io/auth-secret: flask-app-basic-auth\n    # nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\n    \n    # 自定义错误页面\n    # nginx.ingress.kubernetes.io/custom-http-errors: \"404,500,502,503,504\"\n    # nginx.ingress.kubernetes.io/default-backend: custom-error-pages\n    \n    # 重写目标\n    # nginx.ingress.kubernetes.io/rewrite-target: /$1\n    \n    # WAF（如果安装了 ModSecurity）\n    # nginx.ingress.kubernetes.io/enable-modsecurity: \"true\"\n    # nginx.ingress.kubernetes.io/modsecurity-snippet: |\n    #   SecRuleEngine On\n    #   SecRequestBodyAccess On\n\nspec:\n  tls:\n  - hosts:\n    - flask.example.com\n    secretName: flask-app-tls-secret  # TLS 证书 Secret\n\n  rules:\n  - host: flask.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: flask-app-service\n            port:\n              number: 80\n</code></pre>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/XY-Heruo/\" target=\"_blank\">花酒锄作田</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/XY-Heruo/p/19615176\" target=\"_blank\">https://www.cnblogs.com/XY-Heruo/p/19615176</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 13:19</span>&nbsp;\n<a href=\"https://www.cnblogs.com/XY-Heruo\">花酒锄作田</a>&nbsp;\n阅读(<span id=\"post_view_count\">69</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "“Fatal error: require(): Failed opening required...” 以及如何彻底避免它再次出现",
      "link": "https://www.cnblogs.com/catchadmin/p/19614589",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/catchadmin/p/19614589\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 09:22\">\n    <span>“Fatal error: require(): Failed opening required...” 以及如何彻底避免它再次出现</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"fatal-error-require-failed-opening-required-以及如何彻底避免它再次出现\">“Fatal error: require(): Failed opening required...” 以及如何彻底避免它再次出现</h1>\n<p>凌晨两点，值班告警响了。生产环境 API 开始报 500，而且只出现在新扩容的节点上。你打开日志，熟悉又刺眼的报错跳了出来：</p>\n<p>本地一切正常，测试环境也没问题。但在云原生部署这种“环境随时变化”的现实里，一个看起来不起眼的路径差异，就足以把服务直接打趴。</p>\n<p>这并不是什么“新手失误”，而是很多人对 PHP 最基础能力——文件加载机制——理解不够深入导致的系统性问题。</p>\n<p>早期 PHP 时代，我们把 <code>include</code> 和 <code>require</code> 当积木用来拼页面。到了 PHP 8.2+、Composer、容器化微服务的今天，这组函数仍然在引擎核心位置。但现实中，很多开发者依旧把它们当成“设完就不用管”的工具。</p>\n<p>如果你想从“写脚本”走向“做稳定系统”，就必须搞清楚：当一个文件被加载进另一个文件时，底层到底发生了什么。</p>\n<p>这篇文章会从运行机制、线上常见坑和工程实践三层，讲清楚怎样把 PHP 文件加载写到足够稳。</p>\n<h2 id=\"底层到底在发生什么\">底层到底在发生什么？</h2>\n<p>当你执行 <code>include 'file.php'</code>，并不是“复制粘贴代码”这么简单。PHP 实际上会让当前执行流程暂停，切换到目标文件，把它编译为操作码，再在当前作用域里执行。</p>\n<h3 id=\"文件加载的四种形式\">文件加载的四种形式</h3>\n<p>PHP 有四种主加载方式，它们不是语法糖，而是行为差异：</p>\n<ul>\n<li><code>include</code>：温和模式。文件不存在时抛 <code>Warning</code>，脚本继续执行。</li>\n<li><code>require</code>：强制模式。文件不存在时直接致命错误并中断执行。</li>\n<li><code>include_once</code> / <code>require_once</code>：在前两者基础上增加“是否已加载”检查，避免重复声明。</li>\n</ul>\n<p>理解这个差异非常关键：在现代业务系统里，很多核心依赖一旦缺失，不应该“带伤继续跑”。</p>\n<h3 id=\"一个更实用的心智模型作用域注入器\">一个更实用的心智模型：作用域注入器</h3>\n<p>可以把文件加载理解成“作用域注入器”：</p>\n<ul>\n<li>在函数内部 <code>include</code>，被加载文件里定义的变量只在该函数作用域可见。</li>\n<li>在脚本顶层 <code>include</code>，变量会进入全局作用域。</li>\n</ul>\n<p>另外，很多人误判性能瓶颈。真正重的通常不是代码执行本身，而是文件状态检查（stat 调用）：</p>\n<p>每次 <code>include</code>，PHP 都要向操作系统确认：文件是否存在、权限是否可读、最后修改时间等。在高并发 API 中，这个动作每秒成千上万次时，开销会非常明显。</p>\n<h2 id=\"php-是如何解析路径的\">PHP 是如何解析路径的</h2>\n<p>当你写 <code>include 'utils.php';</code> 这种相对路径时，PHP 会依次尝试：</p>\n<ul>\n<li>当前脚本目录</li>\n<li><code>php.ini</code> 中 <code>include_path</code> 指定的目录</li>\n<li>当前工作目录（cwd）</li>\n</ul>\n<p>问题就出在这里：它有环境依赖。</p>\n<p>比如你的命令行任务进程工作目录是 <code>/var/www/</code>，而 Web 进程工作目录是 <code>/var/www/public/</code>，同一行相对路径代码可能一个能跑、一个直接崩。</p>\n<h2 id=\"最容易把线上搞崩的-5-类错误\">最容易把线上搞崩的 5 类错误</h2>\n<p>这些是我在遗留项目重构里反复见到的高频问题。</p>\n<h3 id=\"相对路径陷阱\">相对路径陷阱</h3>\n<p><strong>错误写法</strong>：<code>include 'includes/header.php';</code></p>\n<p><strong>为什么会发生</strong>：本地启动目录刚好是项目根目录，所以一直“看起来正常”。</p>\n<p><strong>线上后果</strong>：一旦被子目录调用、被定时任务调用，或者入口目录变了，路径上下文就变了。这是“我本地没问题”类事故的头号来源。</p>\n<h3 id=\"_once-的性能税\"><code>_once</code> 的性能税</h3>\n<p><strong>错误写法</strong>：在高频循环里大量使用 <code>require_once</code>。</p>\n<p><strong>为什么会发生</strong>：担心 <code>Cannot redeclare class</code> 之类的重复声明。</p>\n<p><strong>线上后果</strong>：每次 <code>_once</code> 都会触发已加载表检查。PHP 8 虽然优化了很多，但它依然比直 <code>require</code> 慢。依赖关系清晰的模块化系统，不该长期依赖引擎“二次确认”。</p>\n<h3 id=\"用--把报错静音\">用 <code>@</code> 把报错静音</h3>\n<p><strong>错误写法</strong>：<code>@include 'optional_config.php';</code></p>\n<p><strong>为什么会发生</strong>：想省掉 <code>if (file_exists(...))</code> 的显式判断。</p>\n<p><strong>线上后果</strong>：你把真正问题藏起来了。文件读取失败可能不是“文件不存在”，而是权限不对（如 <code>chmod</code>）。报错被吃掉后，排障时间会从 5 分钟拉到几小时。</p>\n<h3 id=\"动态-include-引发路径穿越\">动态 include 引发路径穿越</h3>\n<p><strong>错误写法</strong>：<code>include $_GET['page'] . '.php';</code></p>\n<p><strong>为什么会发生</strong>：图省事做“动态路由”。</p>\n<p><strong>线上后果</strong>：严重安全风险。攻击者可构造 <code>../../../../etc/passwd</code>，或利用 <code>php://filter/...</code> 读取敏感配置。即使关闭远程 URL 加载，本地文件同样会被攻击。</p>\n<h3 id=\"加载带副作用的文件\">加载带副作用的文件</h3>\n<p><strong>错误写法</strong>：一个文件既定义类，又直接执行逻辑（输出 HTML、连数据库等）。</p>\n<p><strong>为什么会发生</strong>：历史代码里职责边界没分清。</p>\n<p><strong>线上后果</strong>：测试几乎没法写。你只是想测试类定义，却被迫触发数据库连接和页面输出。</p>\n<h2 id=\"正确做法php-8\">正确做法（PHP 8+）</h2>\n<p>在现代项目里，类加载通常由 Composer + PSR-4 自动加载处理，<code>include</code>/<code>require</code> 更多用于配置、模板和少量模块逻辑。</p>\n<p>但即便如此，也建议守住下面三条。</p>\n<h3 id=\"始终使用绝对锚点路径\">始终使用绝对锚点路径</h3>\n<p>把路径固定在已知根上。<code>__DIR__</code> 永远指向“当前文件所在目录”，不会随工作目录变化。</p>\n<p><strong>错误示例（脆弱）</strong></p>\n<pre><code class=\"language-php\">&lt;?php\n// 如果从 public/ 目录启动，这里可能失败\nrequire 'config/settings.php';\n</code></pre>\n<p><strong>正确示例（稳定）</strong></p>\n<pre><code class=\"language-php\">&lt;?php\n// 无论从哪里调用，都能稳定解析\nrequire __DIR__ . '/config/settings.php';\n</code></pre>\n<h3 id=\"善用加载返回值\">善用加载返回值</h3>\n<p>这是 PHP 里经常被忽略但非常实用的能力：被加载文件可以 <code>return</code> 值。</p>\n<p><code>config.php</code></p>\n<pre><code class=\"language-php\">&lt;?php\nreturn [\n    'db' =&gt; [\n        'host' =&gt; '127.0.0.1',\n        'pass' =&gt; $_ENV['DB_PASS'] ?? 'root',\n    ],\n    'debug' =&gt; false,\n];\n</code></pre>\n<p><code>app.php</code></p>\n<pre><code class=\"language-php\">&lt;?php\n$config = require __DIR__ . '/config.php';\n// $config 是局部变量，不污染全局\n</code></pre>\n<h3 id=\"关键组件要做防御式加载\">关键组件要做防御式加载</h3>\n<p>对于必须存在的文件，不要依赖默认报错，自己把预期写清楚。</p>\n<pre><code class=\"language-php\">&lt;?php\n$templatePath = __DIR__ . '/views/header.php';\nif (!file_exists($templatePath)) {\n    throw new \\RuntimeException(\"关键视图组件缺失: {$templatePath}\");\n}\nrequire $templatePath;\n</code></pre>\n<h2 id=\"生产环境注意点扩缩容与安全\">生产环境注意点：扩缩容与安全</h2>\n<p>当系统从单机走到容器集群或函数计算，文件加载不再只是代码细节，而是基础设施问题。</p>\n<h3 id=\"安全路径穿越防护\">安全：路径穿越防护</h3>\n<p>很多“PHP 不安全”的印象，本质是加载策略不安全。</p>\n<ul>\n<li><strong>白名单（Allow-list）</strong>：绝不直接信任用户输入拼路径。</li>\n<li><strong><code>basename()</code></strong>：确实需要用输入值时，先做路径片段清洗，拦截 <code>../</code> 穿越。</li>\n<li><strong><code>open_basedir</code></strong>：在 <code>php.ini</code> 限制 PHP 可访问路径范围，防止越界读取。</li>\n</ul>\n<h3 id=\"性能opcache-是基础设施而不是可选项\">性能：OPcache 是基础设施而不是可选项</h3>\n<p>生产环境应开启 OPcache。它会把预编译后的字节码放内存，避免每次请求重复解析文件。</p>\n<p><strong>部署提示</strong>：在高并发集群中可以考虑 <code>opcache.validate_timestamps=0</code>，换取更快加载速度；但这意味着每次发布都必须做平滑重载，否则代码更新不会生效。</p>\n<h3 id=\"可观测性失败必须可追踪\">可观测性：失败必须可追踪</h3>\n<p>文件加载失败不应只留下一个“白屏”或 500。</p>\n<ul>\n<li><strong>可追踪信息</strong>：日志至少要包含 <code>include_path</code> 与 <code>cwd</code>。</li>\n<li><strong>监控策略</strong>：对 <code>E_COMPILE_ERROR</code> 做专门告警，这类问题通常与发布或环境差异有关，需优先回滚。</li>\n</ul>\n<h3 id=\"部署形态差异容器-vs-函数计算\">部署形态差异（容器 vs 函数计算）</h3>\n<p>容器镜像里文件路径通常固定可预测；函数计算环境常见只读文件系统、目录映射变化。统一使用 <code>__DIR__</code> 能显著降低环境差异带来的路径问题。</p>\n<h2 id=\"真实事故空配置幽灵\">真实事故：\"空配置\"幽灵</h2>\n<p>我曾参与排查过一个支付业务事故：后台任务随机失败。问题根因是他们用 <code>include</code> 加载环境配置。</p>\n<p>某次发布脚本漏拷了生产配置文件。因为是 <code>include</code>，进程没有崩，业务继续跑，只是拿到一个空的 <code>$config</code>。</p>\n<p>结果是任务带着空 API 密钥连续运行了 6 小时，造成大量交易失败。</p>\n<p>如果当时使用的是 <code>require</code>，任务会第一时间中断并触发告警，损失会小得多。</p>\n<p>一句话：<strong>没有它系统就不能活，那就必须 <code>require</code>。</strong></p>\n<h2 id=\"排障清单看到-failed-opening-required-时直接照做\">排障清单（看到 Failed opening required 时直接照做）</h2>\n<ol>\n<li>\n<p><strong>打印绝对路径</strong>：<br />\n<code>var_dump(realpath(__DIR__ . '/your-file.php'));</code><br />\n若返回 <code>false</code>，说明文件根本不在你以为的位置。</p>\n</li>\n<li>\n<p><strong>确认运行身份</strong>：<br />\n<code>echo exec('whoami');</code><br />\n看当前系统用户是否有读权限。</p>\n</li>\n<li>\n<p><strong>排查隐藏语法错误</strong>：<br />\n某些文件不是“不存在”，而是语法错误导致加载失败。<br />\n用命令行执行：<code>php -l filename.php</code>。</p>\n</li>\n<li>\n<p><strong>检查 PHP 开始标签</strong>：<br />\n文件应以 <code>&lt;?php</code> 开头。若短标签关闭而你写了 <code>&lt;?</code>，后续可能出现各种诡异问题（如 header 已发送）。</p>\n</li>\n</ol>\n<h2 id=\"更专业的加载封装示例\">更专业的加载封装示例</h2>\n<p>不要长期依赖裸 <code>var_dump</code>。建议用结构化日志和统一包装。</p>\n<pre><code class=\"language-php\">&lt;?php\n/**\n * 带可观测性的文件加载器\n * 开发环境要“响亮失败”，生产环境可控降级。\n */\nfunction load_component(string $filePath, array $context = []): mixed\n{\n    $absolutePath = realpath($filePath);\n    if (!$absolutePath || !file_exists($absolutePath)) {\n        error_log(sprintf(\n            \"[FileLoader] Failure: %s | CWD: %s | User: %s\",\n            $filePath,\n            getcwd(),\n            get_current_user()\n        ));\n\n        if (getenv('APP_DEBUG') === 'true') {\n            throw new \\Exception(\"组件不存在: {$filePath}\");\n        }\n\n        return null; // 生产环境按约定降级\n    }\n\n    extract($context);\n    return require $absolutePath;\n}\n</code></pre>\n<h2 id=\"常见问题\">常见问题</h2>\n<h3 id=\"qrequire_once-一定比-require-更好吗\">Q：<code>require_once</code> 一定比 <code>require</code> 更好吗？</h3>\n<p>不一定。<code>require_once</code> 更像是组织不清晰时的安全网。依赖关系明确、自动加载健全时，<code>require</code> 更直接、性能更好。</p>\n<h3 id=\"q可以根据数据库值动态-include-文件吗\">Q：可以根据数据库值动态 include 文件吗？</h3>\n<p>可以，但必须非常谨慎。推荐白名单映射：数据库只存 ID，代码里把 ID 映射到固定路径，不要把路径原文存进数据库后直接加载。</p>\n<h3 id=\"q加载大文件会拖慢应用吗\">Q：加载大文件会拖慢应用吗？</h3>\n<p>开启 OPcache 后，首次之后基本没有“解析”成本；但文件中的业务逻辑仍要执行，依旧消耗 CPU 和内存。文件内容要聚焦，避免把大量无关逻辑塞在一起。</p>\n<h3 id=\"q模板文件适合用-include-吗\">Q：模板文件适合用 <code>include</code> 吗？</h3>\n<p>小项目可以。中大型系统建议使用成熟模板方案，能在安全性和复用性上更稳。</p>\n<h2 id=\"结语\">结语</h2>\n<p>把 <code>include</code> 和 <code>require</code> 用好，不只是语法问题，而是工程能力问题。</p>\n<p>你的代码运行在操作系统、权限模型、缓存机制和部署流水线共同构成的环境里。只理解“本地能跑”，远远不够。</p>\n<h3 id=\"最佳实践小结\">最佳实践小结</h3>\n<ul>\n<li><strong>快速失败</strong>：关键依赖统一使用 <code>require</code>。</li>\n<li><strong>路径绝对化</strong>：避免相对路径，优先 <code>__DIR__</code>。</li>\n<li><strong>作用域收敛</strong>：用 <code>return</code> 返回配置，避免全局变量污染。</li>\n<li><strong>失败可观测</strong>：把加载失败当成一类关键系统事件处理。</li>\n</ul>\n<h3 id=\"你的下一步\">你的下一步</h3>\n<p>现在就打开项目，全局搜索 <code>include</code> / <code>require</code>：</p>\n<p>凡是不以 <code>__DIR__</code> 或统一根路径常量开头的，今天就改。</p>\n<p>这一步做完，你的生产环境就会少一类高概率事故。<br />\n<a href=\"https://catchadmin.com/post/2026-02/fatal-error-require-failed-opening-required\" rel=\"noopener nofollow\" target=\"_blank\">Fatal error: require(): Failed opening required...”—以及如何彻底避免它再次出现</a></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 09:22</span>&nbsp;\n<a href=\"https://www.cnblogs.com/catchadmin\">JaguarJack</a>&nbsp;\n阅读(<span id=\"post_view_count\">40</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "[拆解LangChain执行引擎] ManagedValue——一种特殊的只读虚拟通道",
      "link": "https://www.cnblogs.com/jaydenai/p/19614333/managed-value",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jaydenai/p/19614333/managed-value\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 07:57\">\n    <span>[拆解LangChain执行引擎] ManagedValue——一种特殊的只读虚拟通道</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        我们一直在强调Pregel对象的状态是通过`Channel`维护和传递的，其实承载传递状态功能的组件除了Channel，还有  `ManagedValue`，我们可以将ManagedValue视为虚拟Channel。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>我们一直在强调Pregel对象的状态是通过<code>Channel</code>维护和传递的，其实承载传递状态功能的组件除了Channel，还有  <code>ManagedValue</code>。我们可以将ManagedValue视为虚拟Channel，Node不仅采用与读取Channel完全一样的方式读取ManagedValue，而且注册的ManagedValue也直接存放在Pregel的channels字段中。</p>\n<p>如果我们仔细查看Pregel类的定义，可以看出其<code>channels</code>字段返回一个字典，字典的值的类型联合了BaseChannel和<code>ManagedValueSpec</code>两种类型，前者是Channel的基类，后者就是<code>ManagedValue</code>类的别名。</p>\n<pre><code class=\"language-python\">class Pregel(\n    PregelProtocol[StateT, ContextT, InputT, OutputT],\n    Generic[StateT, ContextT, InputT, OutputT]): \n    channels : dict[str, BaseChannel | ManagedValueSpec]\n\nManagedValueSpec = type[ManagedValue]\n</code></pre>\n<p>如果说Channel存储的是的业务状态，那么ManagedValue传递的就是Pregel这个执行引擎的运行时状态。一般来说，ManagedValue自身不负责存储状态，其提供的值可以实时计算得出，所以它不参与基于Checkpoint的持久化。从如下所示的代码片段可以看出，ManagedValue仅仅定义了一个唯一的静态抽象方法<code>get</code>返回对应的值，由于作为输入的<code>PregelScratchpad</code>对象提供的信息有限，所以ManagedValue能够发挥的空间其实很有限，在大部分情况下用不到它。</p>\n<pre><code class=\"language-python\">class ManagedValue(ABC, Generic[V]):\n    @staticmethod\n    @abstractmethod\n    def get(scratchpad: PregelScratchpad) -&gt; V: ...\n</code></pre>\n<h2 id=\"1-pregelscratchpad\">1. PregelScratchpad</h2>\n<p>ManagedValue提供的值是通过其get方法根据PregelScratchpad对象计算所得。当确定后续待执行的Node后，引擎会为每个Node创建一个任务，每个任务都会附加一个PregelScratchpad对象。PregelScratchpad的<code>step</code>和<code>stop</code>字段就返回当前Superstep的序号和针对迭代的限制（最大超步数），其它字段与持久化有关。</p>\n<pre><code class=\"language-python\">@dataclasses.dataclass(**_DC_KWARGS)\nclass PregelScratchpad:\n    step : int\n    stop : int\n    call_counter : Callable[[], int]\n    interrupt_counter : Callable[[], int]\n    get_null_resume\t: Callable[[bool], Any]\n    resume : list[Any]\n    subgraph_counter\t: Callable[[], int]\t\n</code></pre>\n<p>PregelScratchpad的<code>call_counter</code>、<code>interrupt_counter</code>和<code>subgraph_counter</code>字段以闭包的形式返回一个计数器。<code>call_counter</code>计数器用于为当前Superstep内产生的所有任务分配唯一的内部序列号。</p>\n<h3 id=\"11-resume-value和中断计数器\">1.1 Resume Value和中断计数器</h3>\n<p><code>interrupt_counter</code>、<code>get_null_resume</code>和<code>resume</code>字段与Pregel基于 “中断（Interrupt）/恢复（Resume）” 的执行方式有关。假设Pregel的对应一个需要人工介入的多级审批流程，在每次需要以人工介入的方式收集审批者决定的时候，流程进入一个中断，当前的状态被持久化。当审批决定给出后，流程以 “恢复” 的形式开始执行，中断时持久化的快照被提取出来 “恢复现场” ，审批决定以Resume Value的形式提供给引擎。为了匹配多个中断点与对应的Resume Value，后者会按照顺序被持久化，并在恢复执行的时候连同当前提供的Resume Value一并填充到PregelScratchpad的<code>resume</code>列表中。</p>\n<p>恢复执行做不到在中断点出开始执行，它总是<code>从头执行</code>Node的处理函数，所以定义 <code>幂等Node</code> 应该成为Agent编程的 “金科玉律”。由于PregelScratchpad的resume字段会按照中断的顺序存放Resume Value，所以在恢复执行的时候，每遇到一个中断，引擎可以利用<code>interrupt_counter</code>字段返回的计数器作为位置索引从resume列表中将匹配的Resume Value提取出来。如果提取的Resume Value为None，或者计数器返回的索引越界，<code>get_null_resume</code>字段提供的回调就会执行。这个回调函数具有一个bool类型的参数is_called，调用时该参数被设置为True，表示该中断确实被触发了，但没有对应的数据。这会消耗掉这个中断位，确保流程不至于永远得不到恢复。</p>\n<h3 id=\"12-子图调用计数器\">1.2 子图调用计数器</h3>\n<p>如果说<code>interrupt_counter</code>计数器旨在解决每次中断与提供的Resume Value的匹配问题，那么<code>subgraph_counter</code>计数器解决的每次“子图调用”与对应Pregel实例的匹配问题。如果站在“图”的视角，每个Pregel对象就是由多个Node组成的图，而Pregel也可以作为一个Node出现在另一个Pregel构建的图中，两个Pregel之间就称为了“父子”关系，子Pregel构建的图就是“子图”，针对它的调用就是子图调用。</p>\n<p>虽然在同一个图中，每个Pregel会独自完成自身的持久化。在恢复执行场景中，引擎会率先加载作为“根”的Pregel对应的Checkpoint来恢复现场。当遇到“子图”形式调用另一个Pregel时，引擎会加载对应的Checkpoint来恢复子图在中断那个时间点的状态。现在问题来了：在子Pregel众多持久化的Checkpoint中，怎么知道该加载哪一个呢？</p>\n<p>这个问题本质上是如何解决作为子图执行的Pregel在执行持久化时，如何将生成的Checkpoint与当前执行上下文进行匹配的问题，这个问题是利用<code>Checkpoint命名空间</code>来解决的。Node是以任务的形式被执行的，每个任务具有唯一的ID，并且在恢复时保持不变，如果命名空间由执行链路上每个任务的<code>节点名称+任务ID</code>组成，那么子图的Checkpoint就能利用此命名空间关联起来。</p>\n<p>但是问题还是没有完全解决，如果同一个任务涉及针对<code>同一子图的多次调用</code>，如命名空间只包含基于任务的执行路径，此时两个子图会共享相同的命名空间，具体对应哪个Checkpoint依然无法解决。因此若涉及同一个Node针对同一个Pregel对象的多次调用，持久化这个Pregel的Checkpoint的命名空间还应该包含<code>调用顺序</code>。</p>\n<p>Checkpoint的命名空间的规则可以通过如下这个演示实例来证实。如代码片段所示，我们创建了一个由单一Node组成的Pregel对象（sub_graph），命名为 “baz” 的Node在执行的时候会从当前的RunnableConfig配置中提取并输出当前的Checkpoint命名空间。</p>\n<pre><code class=\"language-python\">from langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.channels import LastValue\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.pregel._write import ChannelWrite, ChannelWriteTupleEntry\nfrom langgraph.types import RunnableConfig\nfrom typing import Any\n\ndef handle(args:dict[str,Any], config:RunnableConfig)-&gt;None:\n    print(config[\"configurable\"][\"checkpoint_ns\"])\nsub_node = (NodeBuilder()\n    .subscribe_to(\"start\")\n    .do(handle))\nsub_graph = Pregel(\n    nodes={\"baz\": sub_node},\n    channels={\n        \"start\": LastValue(None),\n    },\n    input_channels=[\"start\"],\n    output_channels=[])\n\ndef handle1(args:dict[str,Any])-&gt;None:\n    sub_graph.invoke(input={\"start\": None})\n\ndef handle2(args:dict[str,Any])-&gt;str:    \n    sub_graph.invoke(input={\"start\": None})    \n    sub_graph.invoke(input={\"start\": None})\n\nfoo = (NodeBuilder()\n    .subscribe_to(\"foo\")\n    .do(handle1)\n    .write_to(bar=None))\nbar = (NodeBuilder()\n    .subscribe_to(\"bar\")\n    .do(handle2))\n\ngraph = Pregel(\n    nodes={\"foo\": foo, \"bar\": bar},\n    channels={\n        \"foo\": LastValue(None),\n        \"bar\": LastValue(str),\n    },\n    input_channels=[\"foo\"],\n    output_channels=[],\n    checkpointer= InMemorySaver())\n\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\ngraph.invoke(input={\"foo\": None}, config=config)\n</code></pre>\n<p>在另一个Pregel中，我们为它设置了两个先后执行的Node（foo和bar），前者调用sub_graph一次，后者调用两次。针对三次调用，sub_graph为自身持久化设置的Checkpoint命名会以如下的形式输出，可以看出命名空间同时体现了调用链路和次序。</p>\n<pre><code>foo:36817c76-c3f7-643f-7924-0d29b39f469a|baz:311cc911-96a0-56b6-225b-28e4cece7cd9\nbar:97be6a71-1b71-7364-e691-a122cfef1a92|baz:789287de-869f-42b8-dd03-7518820daaa6\nbar:97be6a71-1b71-7364-e691-a122cfef1a92|1|baz:dd1ddd1b-fc62-b46a-c2ec-6a1d8344b793\n</code></pre>\n<p>基于Pregel“中断/恢复”的执行方式，让我们对<code>Pregel实例</code>会有特别的理解。我们习惯了将一个通过调用某个类构造函数创建的对象视为该类型的一个实例，但是在Node的处理函数中，即使针对<code>同一Pregel实例</code>的连续两次调用都有可能出现中断，一旦恢复执行，后一个实例就有可能使根据另一个Checkpoint的状态创建的，它自然也就不是原来的那个实例了。在不断的“中断/恢复”执行流程中，所谓<code>Pregel实例</code>有时候表示成<code>对应的Checkpoint</code>可能更准确。</p>\n<p>对于同一个节点任务来说，如果涉及针对同一个<code>子Pregel</code>的多次调用，从第二次调用开始，对方持久化生成的Checkpoint会将<code>调用次序</code>包含在命名空间中。与之相对的，在恢复执行的时候，也需要根据当前的执行上下文提供包含此序号的命名空间采用加载对应的Checkpoint，并最终恢复对应的Pregel对象，PregelScratchpad的subgraph_counter字段返回的计数器就是为了提供这个序号。</p>\n<h2 id=\"2-两个原生的managedvalue\">2. 两个原生的ManagedValue</h2>\n<p>由于ManagedValue所能提供的值是根据PregelScratchpad计算生成，而后者可用的唯有表示当前和最大Superstep序号的<code>step</code>和<code>stop</code>字段，所以我们采用ManagedValue的应用场景其实很窄。我从只找到如下两个原生的ManagedValue类型，它们都定义在langgraph.managed.is_last_step这个包中。其中一个<code>IsLastStepManager</code>用于判断是否为最后一个Superstep，而<code>RemainingStepsManager</code>则用来确定余下的Superstep数。具体的实现非常简单，仅仅是针对PregelScratchpad的<code>step</code>和<code>stop</code>字段的简单运算而已。</p>\n<pre><code class=\"language-python\">class IsLastStepManager(ManagedValue[bool]):\n    @staticmethod\n    def get(scratchpad: PregelScratchpad) -&gt; bool:\n        return scratchpad.step == scratchpad.stop - 1\n\nclass RemainingStepsManager(ManagedValue[int]):\n    @staticmethod\n    def get(scratchpad: PregelScratchpad) -&gt; int:\n        return scratchpad.stop - scratchpad.step\n</code></pre>\n<p>由于ManagedValue属于一个<code>计算属性</code>，所以它只能作为Node的输入。它可以被视为一种虚拟的Channel，Node针对ManagedValue和常规Channel的读取方式完全一致。在创建Pregel对象时，所用到的ManagedValue需要在<code>channels</code>字段中显式声明，但是不能将其添加到输入和输出Channel列表中。</p>\n<p>如下的实例演示了RemainingStepsManager的使用方式，创建的Pregel由两个先后执行的Node构成（foo和bar），它们会将命名为<code>remaining_steps</code>的ManagedValue作为输入，并将其分别输出到<code>remaining_steps_after_foo</code>和<code>remaining_steps_after_bar</code>这两个Channel中，分别表示在这两个Node完成执行后所剩的Superstep数。</p>\n<pre><code class=\"language-python\">from langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.managed.is_last_step import RemainingStepsManager\nfrom langgraph.channels import LastValue\n\nfoo = (NodeBuilder()\n       .subscribe_to(\"foo\")\n       .read_from(\"remaining_steps\")\n       .do(lambda args: args[\"remaining_steps\"])\n       .write_to(remaining_steps_after_foo= lambda args:args, bar=None))\n\nbar = (NodeBuilder()\n       .subscribe_to(\"bar\")\n       .read_from(\"remaining_steps\")\n       .do(lambda args: args[\"remaining_steps\"])\n       .write_to(\"remaining_steps_after_bar\"))\n\napp = Pregel(\n    nodes={\"foo\":foo, \"bar\":bar},\n    channels={\n        \"foo\": LastValue(None),\n        \"bar\":LastValue(None),\n        \"remaining_steps_after_foo\": LastValue(int),\n        \"remaining_steps_after_bar\":LastValue(int),\n        \"remaining_steps\": RemainingStepsManager, \n    },\n    input_channels=[\"foo\"],\n    output_channels=[\"remaining_steps_after_foo\", \"remaining_steps_after_bar\"])\n\nconfig = {\"recursion_limit\": 10}\nresult = app.invoke({\"foo\":None}, config=config)\nassert result[\"remaining_steps_after_foo\"] == 10\nassert result[\"remaining_steps_after_bar\"] == 9\n</code></pre>\n<p>在根据两个Node创建Pregel对象时，我们将针对命名为<code>remaining_steps</code>的ManagedValue的声明添加到channels字段中，对应的类型被设置为RemainingStepsManager。由于在调用Pregel对象时利用RunnableConfig配置将Superstep迭代限制为10，所以先后执行的两个Node后剩余步数分别为10和9。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 07:57</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jaydenai\">JaydenAI</a>&nbsp;\n阅读(<span id=\"post_view_count\">35</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "2026年互联网行业十大热门话题：AI狂飙与技术平权的十字路口",
      "link": "https://www.cnblogs.com/nf01/p/19613794",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/nf01/p/19613794\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 23:11\">\n    <span>2026年互联网行业十大热门话题：AI狂飙与技术平权的十字路口</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"当大模型从工具进化为伙伴我们正站在互联网历史的转折点上\">当大模型从\"工具\"进化为\"伙伴\"，我们正站在互联网历史的转折点上</h1>\n<hr />\n<blockquote>\n<p><em>写在前面：2026年的互联网行业，热闹得让人有些应接不暇。从AI Agent的全面爆发，到国产大模型的百家争鸣，从视频生成的狂飙突进，到AI硬件的悄然崛起——这可能是近十年来技术变革最密集的一年。本文5000余字，带你系统性梳理当下互联网行业最热门的话题，探究热潮背后的逻辑，并尝试回答一个根本问题：这一次的技术浪潮，究竟会把我们带向何方？</em></p>\n</blockquote>\n<hr />\n<h2 id=\"一ai-agent从回答问题到代替操作\">一、AI Agent：从\"回答问题\"到\"代替操作\"</h2>\n<p>如果要用一个词来形容2026年互联网行业的主题，<strong>AI Agent（智能体）</strong> 绝对当之无愧。</p>\n<p>还记得2023年ChatGPT横空出世时，我们惊叹于它能\"回答问题\"；2024年，我们开始习惯它能\"生成内容\"；而到了2026年，行业焦点已经完全转向——<strong>AI能替我\"干活\"了吗？</strong></p>\n<p>答案是：不仅能，而且正在加速渗透。</p>\n<h3 id=\"11-什么是ai-agent简单来说\">1.1 什么是AI Agent？简单来说</h3>\n<p>传统AI助手（比如Siri、小爱同学）是\"你问一句，它答一句\"的交互模式。而AI Agent则具备了<strong>规划、工具使用、记忆和自主决策</strong>四大能力。你可以给它一个目标（比如\"帮我订一张下周去上海的机票，要靠窗座位，价格在1500元以内\"），它会自己分析需求、搜索信息、比较选项、完成操作——全程不需要你再动手。</p>\n<h3 id=\"12-巨头都在布局\">1.2 巨头都在布局</h3>\n<p>2026年初，OpenAI发布了GPT-5，其核心卖点就是<strong>Agent模式</strong>的全面升级。用户只需给出一个高层次指令，GPT-5就能自主拆解任务、调用各种工具（浏览器、文档、代码编译器等）、分步执行，最终交付完整结果。</p>\n<blockquote>\n<p><em>\"我们希望让AI成为你的'数字员工'，而不仅仅是'数字百科全书'。\"</em> —— OpenAI CEO Sam Altman</p>\n</blockquote>\n<p>国内这边，百度的\"文心一言\"、字节的\"豆包\"、阿里的\"通义千问\"都在快速迭代Agent能力。百度更是将Agent概念融入了搜索和地图产品，喊出了\"搜索即服务\"的口号。</p>\n<h3 id=\"13-为什么2026年是agent元年\">1.3 为什么2026年是Agent元年？</h3>\n<p><strong>三个条件的成熟：</strong></p>\n<ol>\n<li><strong>大模型能力跃升</strong>：推理成本大幅下降，模型响应速度加快，多模态能力增强</li>\n<li><strong>工具生态完善</strong>：浏览器自动化、API接口、文件系统操作等基础设施日趋成熟</li>\n<li><strong>用户需求觉醒</strong>：996浪潮下 everyone都在寻找\"效率解药\"，老板们渴望AI帮自己打工</li>\n</ol>\n<p><strong>一句话总结：</strong> 2026年，AI不再只是\"应答机\"，而是开始真正\"干活\"了。这一转变的影响，可能比当年移动互联网的诞生还要深远。</p>\n<hr />\n<h2 id=\"二国产大模型百家争鸣与淘汰赛并行\">二、国产大模型：百家争鸣与淘汰赛并行</h2>\n<h3 id=\"21-百模大战进入下半场\">2.1 \"百模大战\"进入下半场</h3>\n<p>2024年，国内大模型赛道可以用\"疯狂\"来形容——据不完全统计，那一年冒出了超过200个大模型。但到了2026年，剧情急转直下：<strong>活下来的不超过20家，其他的都成了\"炮灰\"。</strong></p>\n<p>这不是危言耸听。2025年下半年开始，投资人变得异常冷静。那些\"PPT融资\"、Demo漂亮但落地困难的公司，要么被迫合并，要么悄然退场。幸存者有两类：</p>\n<ul>\n<li><strong>有场景的</strong>：字节（豆包+飞书）、阿里（通义+钉钉）、百度（文心+搜索）</li>\n<li><strong>有资金的</strong>：智谱AI、MiniMax、阶跃星辰</li>\n</ul>\n<h3 id=\"22-deepseek带来的冲击波\">2.2 DeepSeek带来的冲击波</h3>\n<p>2025年末，一匹黑马杀出重围——<strong>DeepSeek</strong>。</p>\n<p>这个由国内团队打造的大模型，在多项基准测试中逼近GPT-4水平，但推理成本只有对方的1/10。更重要的是，DeepSeek选择了<strong>开源</strong>路线，开放了模型权重和技术报告。</p>\n<blockquote>\n<p><em>\"我们希望让每一家中国公司都用得起顶级AI。\"</em> —— DeepSeek创始人</p>\n</blockquote>\n<p>此举直接点燃了行业价格战。各大厂商被迫跟进，API调用价格一降再降。有人说DeepSeek是\"行业公敌\"，也有人说它是\"普惠先锋\"。无论如何，<strong>2026年，大模型的\"平权时代\"真的来了。</strong></p>\n<h3 id=\"23-应用层的机会在哪里\">2.3 应用层的机会在哪里？</h3>\n<p>底层模型的竞争趋于白热化，但投资人真正关心的，是<strong>应用层</strong>的爆发。</p>\n<p>目前来看，落地最快的赛道包括：</p>\n<ul>\n<li><strong>AI写作/办公</strong>：WPS AI、Notion AI、秘塔写作猫</li>\n<li><strong>AI编程</strong>：Cursor、Devin（硅谷明星产品）、国内各种\"AI程序员\"</li>\n<li><strong>AI教育</strong>：作业帮AI、猿辅导AI、小猿AI</li>\n<li><strong>AI营销</strong>：各种AI生成文案、图片、视频的工具</li>\n</ul>\n<p><strong>结论：与其卷模型，不如卷应用。</strong> 2026年，这句话正在成为行业共识。</p>\n<hr />\n<h2 id=\"三视频生成sora不再是唯一答案\">三、视频生成：Sora不再是唯一答案</h2>\n<h3 id=\"31-文字生成视频进入实用阶段\">3.1 \"文字生成视频\"进入实用阶段</h3>\n<p>2024年初，OpenAI发布Sora，演示了60秒高清视频生成能力，全世界为之震动。两年后的今天，Sora早已不是唯一选项。</p>\n<p><strong>国内玩家集体发力：</strong></p>\n<ul>\n<li><strong>快手可灵</strong>：主打\"人人可用\"，生成速度极快，已迭代到2.0版本</li>\n<li><strong>字节即梦</strong>：背靠抖音生态，一键生成短视频素材</li>\n<li><strong>MiniMax</strong>：在长视频生成方面有独特优势</li>\n<li><strong>Runway Gen-3</strong>：依然是全球创作者的首选工具</li>\n</ul>\n<h3 id=\"32-行业影响影视制作正在被重塑\">3.2 行业影响：影视制作正在被重塑</h3>\n<p>过去，一支30秒的广告片，从创意到成片可能需要一周、耗费数万元。如今，<strong>用AI生成视频素材+人工后期调整，2小时就能搞定</strong>，成本不足原来的1/10。</p>\n<p>这意味着什么？</p>\n<ul>\n<li><strong>短视频创作者</strong>：再也不用为素材发愁，AI帮你\"造\"</li>\n<li><strong>MCN机构</strong>：一人+AI = 过去一个团队的生产力</li>\n<li><strong>中小广告公司</strong>：终于能用得起\"大片级\"效果</li>\n</ul>\n<p>当然，<strong>\"AI生成视频\"目前仍有明显短板</strong>：</p>\n<ul>\n<li>物理规律把握不准（物体穿模、液体变形等问题常见）</li>\n<li>人物一致性难以维持（生成多镜头时脸会变）</li>\n<li>版权争议不断（训练数据来源问题）</li>\n</ul>\n<h3 id=\"33-2026年一个人人都是导演的时代正在降临\">3.3 2026年：一个\"人人都是导演\"的时代正在降临</h3>\n<p>当技术门槛足够低，创意将成为唯一的稀缺资源。这句话说了十年，现在终于要实现了。</p>\n<hr />\n<h2 id=\"四ai硬件除了手机和电脑还有什么\">四、AI硬件：除了手机和电脑，还有什么？</h2>\n<h3 id=\"41-ai-pin凉了但方向没错\">4.1 AI Pin：凉了，但方向没错</h3>\n<p>2024年，AI Pin（如Humane Pin、Rabbit R1）概念爆火，被认为是\"替代手机\"的下一代硬件。然而现实很残酷：<strong>AI Pin市场表现惨淡</strong>，出货量远低于预期。</p>\n<p>原因不复杂：<strong>功能手机都能做，单独硬件没必要；交互体验不够自然；续航和发热问题严重。</strong></p>\n<h3 id=\"42-ai眼镜2026年新风口\">4.2 AI眼镜：2026年新风口？</h3>\n<p>但资本没有死心。2026年，<strong>AI眼镜</strong>成为新焦点。</p>\n<p><strong>Meta与Ray-Ban合作的AI眼镜</strong>已经迭代到第三代，支持实时翻译、物体识别、语音助手等功能。扎克伯格断言：\"眼镜是AI最合适的载体，因为它离眼睛最近、离耳朵最近。\"</p>\n<p>国内玩家闻风而动：</p>\n<ul>\n<li><strong>小米</strong>：被曝正在研发AI眼镜，定位\"年轻人的第一个AI设备\"</li>\n<li><strong>字节</strong>：通过投资方式布局AR/VR硬件</li>\n<li><strong>华为</strong>：Mate系列后续可能集成AI眼镜功能</li>\n</ul>\n<h3 id=\"43-机器人从概念到上岗\">4.3 机器人：从\"概念\"到\"上岗\"</h3>\n<p>除了穿戴设备，<strong>AI机器人</strong>也在2026年加速落地。</p>\n<ul>\n<li><strong>特斯拉Optimus</strong>：已经在工厂\"打工\"，进行简单装配工作</li>\n<li><strong>波士顿动力Atlas</strong>：宣布进入商业化阶段</li>\n<li><strong>国内优必选、达闼科技</strong>：在服务机器人领域持续突破</li>\n</ul>\n<blockquote>\n<p><em>\"未来十年，每个家庭可能都会有一台机器人。\"</em> —— 马斯克预测</p>\n</blockquote>\n<p><strong>但请注意：</strong> 目前能商用的机器人主要还是\"专用型\"（扫地的扫地、搬货的搬货），\"通用型\"家庭机器人离我们还有距离。</p>\n<hr />\n<h2 id=\"五ai监管与数据安全当技术跑得比法律快\">五、AI监管与数据安全：当技术跑得比法律快</h2>\n<h3 id=\"51-全球监管加速\">5.1 全球监管加速</h3>\n<p>2026年，AI监管不再是\"纸上谈兵\"。</p>\n<ul>\n<li><strong>欧盟AI法案</strong>：2025年正式生效，对高风险AI应用实施严格监管</li>\n<li><strong>美国</strong>：各州分别立法，加州走了最前沿</li>\n<li><strong>中国</strong>：网信办发布《生成式AI管理办法》升级版，强调内容标识和版权保护</li>\n</ul>\n<h3 id=\"52-版权战争愈演愈烈\">5.2 版权战争愈演愈烈</h3>\n<p>AI训练数据的版权问题，已经从\"学术讨论\"升级为\"商业战争\"。</p>\n<ul>\n<li><strong>好莱坞 vs AI公司</strong>：多起集体诉讼正在进行中</li>\n<li><strong>文字工作者联盟</strong>：要求AI公司支付稿酬</li>\n<li><strong>音乐行业</strong>：AI生成音乐泛滥，Spotify等平台被要求下架</li>\n</ul>\n<p><strong>一个有意思的悖论：</strong></p>\n<blockquote>\n<p>AI公司说：\"我们用公开数据训练，是合理使用。\"<br />\n版权方说：\"你的'公开'没经过我同意。\"</p>\n</blockquote>\n<p>这场争论，2026年不会有答案，但一定会越来越激烈。</p>\n<h3 id=\"53-ai污染被忽视的危机\">5.3 \"AI污染\"：被忽视的危机</h3>\n<p>还有一个被低估的话题：<strong>AI生成内容的污染问题</strong>。</p>\n<p>当互联网上AI生成的内容越来越多，<strong>\"人类数据枯竭\"</strong>正在成为新的焦虑。如果模型继续用AI生成的数据训练，会导致\"模型崩溃\"（model collapse）。如何区分\"人类智慧\"和\"机器流水账\"，将成为平台和监管方的核心挑战。</p>\n<hr />\n<h2 id=\"六社交与内容平台流量重新分配\">六、社交与内容平台：流量重新分配</h2>\n<h3 id=\"61-短视频赛道的终局\">6.1 短视频赛道的终局？</h3>\n<p>2026年，短视频江湖已经趋于稳定：</p>\n<ul>\n<li><strong>抖音+TikTok</strong>：全球制霸</li>\n<li><strong>快手</strong>：下沉市场稳如老狗</li>\n<li><strong>视频号</strong>：依托微信生态持续增长</li>\n<li><strong>小红书</strong>：从\"种草社区\"进化为\"搜索入口\"，DAU屡创新高</li>\n</ul>\n<p>有趣的是，<strong>\"搜索\"正在回归</strong>。年轻人不再只依赖算法推荐，而是主动在小红书、抖音上\"搜答案\"。这让百度有点尴尬，也让内容平台看到了新的商业化空间。</p>\n<h3 id=\"62-私密社交升温\">6.2 私密社交升温</h3>\n<p>与此同时，<strong>私密社交</strong>正在复兴。</p>\n<ul>\n<li><strong>Discord</strong>：从游戏社区扩展为\"兴趣飞地\"</li>\n<li><strong>Telegram</strong>：付费功能上线，用户不减反增</li>\n<li><strong>各种\"小群\"社交</strong>：微信群、Slack Workspace、Discord服务器……</li>\n</ul>\n<p><strong>逻辑很好理解：</strong> 公共平台的噪音太大了，人们渴望\"安静的小圈子\"。</p>\n<h3 id=\"63-创作者经济20\">6.3 创作者经济2.0</h3>\n<p>\"所有平台都在抢创作者\"——这已经不是新闻，但2026年的新变化是：</p>\n<ul>\n<li><strong>平台分成比例上调</strong>：抖音、B站纷纷\"让利\"</li>\n<li><strong>创作者保险</strong>出现：平台开始为创作者提供医疗、养老保障</li>\n<li><strong>AI辅助工具普及</strong>：一个人就是一支团队</li>\n</ul>\n<p><strong>\"人人都是创作者\"的时代，内容供给严重过剩——怎么办？答案只有一个：卷质量。</strong></p>\n<hr />\n<h2 id=\"七自动驾驶落地比预期快\">七、自动驾驶：落地比预期快</h2>\n<h3 id=\"71-l3级自动驾驶2026年成真\">7.1 L3级自动驾驶：2026年成真</h3>\n<p>2026年，<strong>L3级自动驾驶</strong>（有条件自动化，在特定路段可以脱手）终于在中国落地：</p>\n<ul>\n<li><strong>华为问界</strong>：ADS 3.0版本推送，的高速NOA全国通</li>\n<li><strong>蔚来NOP+</strong>：城市NOA功能开放</li>\n<li><strong>小鹏XNGP</strong>：持续迭代，体验接近\"老司机\"</li>\n</ul>\n<p><strong>但请注意：</strong> L3仍然有严格的使用条件（高速公路、快速路、天气良好等），完全\"无人驾驶\"（L4/L5）还远。</p>\n<h3 id=\"72-商业化破局\">7.2 商业化破局</h3>\n<p>Robotaxi（无人出租车）也在加速：</p>\n<ul>\n<li><strong>百度萝卜快跑</strong>：多个城市商业化运营</li>\n<li><strong>Waymo</strong>：在美国持续扩张</li>\n<li><strong>Cruise</strong>：虽然2024年出了安全事故，但2026年已恢复运营</li>\n</ul>\n<p><strong>一个有趣的现象：</strong> 2026年很多人开始讨论\"要不要买车\"。当Robotaxi足够便宜且方便，养车的成本（停车、保险、油费/电费）似乎越来越不划算。</p>\n<hr />\n<h2 id=\"八web3与区块链冷门但没死\">八、Web3与区块链：冷门但没死</h2>\n<h3 id=\"81-比特币从泡沫到资产\">8.1 比特币：从\"泡沫\"到\"资产\"</h3>\n<p>2026年，比特币依然是最\"出圈\"的加密资产。虽然价格波动剧烈，但<strong>主流机构接纳度持续提高</strong>：</p>\n<ul>\n<li>贝莱德、富达等传统金融巨头继续布局加密资产</li>\n<li>更多的ETF产品上线</li>\n<li>甚至有国家（如萨尔瓦多）把比特币定为法定货币</li>\n</ul>\n<h3 id=\"82-web3应用仍在探索\">8.2 Web3应用：仍在探索</h3>\n<p>\"Web3去中心化互联网\"的愿景，目前来看依然遥远。<strong>DeFi（去中心化金融）、NFT、GameFi</strong> 这些概念经历过山车行情，泡沫破了一批又一批。</p>\n<p>但底层技术在进步：</p>\n<ul>\n<li><strong>Layer 2解决方案</strong>让以太坊更快、更便宜</li>\n<li><strong>ZK（零知识证明）</strong> 技术开始应用于隐私计算</li>\n<li><strong>去中心化身份（DID）</strong> 在一些垂直场景落地</li>\n</ul>\n<p><strong>总结：Web3不是凉了，而是从\"投机泡沫\"回归\"技术基建\"。</strong></p>\n<hr />\n<h2 id=\"九远程办公与协作常态化的红利\">九、远程办公与协作：常态化的红利</h2>\n<h3 id=\"91-混合办公成为标配\">9.1 混合办公成为标配</h3>\n<p>2026年，<strong>\"三天办公室+两天在家\"</strong> 已经成为大多数知识工作者的常态。</p>\n<ul>\n<li><strong>Slack、飞书、钉钉</strong>：功能越来越强大，文档、审批、会议、OKR全覆盖</li>\n<li><strong>Notion、Confluence</strong>：知识管理成为企业的\"基础设施\"</li>\n<li><strong>Figma、Miro</strong>：在线协作工具让\"异地设计\"成为可能</li>\n</ul>\n<h3 id=\"92-ai同事来了\">9.2 \"AI同事\"来了</h3>\n<p>更关键的变化是：<strong>你的同事可能不全是\"人\"了。</strong></p>\n<ul>\n<li><strong>AI会议纪要</strong>：自动生成会议摘要、待办事项</li>\n<li><strong>AI周报</strong>：自动汇总你一周的工作内容</li>\n<li><strong>AI客服</strong>：7×24小时在线，解答重复问题</li>\n</ul>\n<p><strong>2026年的职场生存法则：</strong> 要么你会用AI，要么你被会用AI的人取代。</p>\n<hr />\n<h2 id=\"十云计算与基础设施ai时代的卖铲人\">十、云计算与基础设施：AI时代的\"卖铲人\"</h2>\n<h3 id=\"101-云厂商的ai转型\">10.1 云厂商的\"AI转型\"</h3>\n<p>AI爆发带来巨大的算力需求，云厂商赚得盆满钵满。</p>\n<ul>\n<li><strong>AWS、Azure、阿里云</strong>：AI算力成为增长最快的业务线</li>\n<li><strong>GPU租赁</strong>成为新热点：A100、H100显卡一卡难求</li>\n<li><strong>边缘计算</strong>升温：AI推理正在从云端走向终端</li>\n</ul>\n<h3 id=\"102-国产替代加速\">10.2 \"国产替代\"加速</h3>\n<p>受地缘政治影响，<strong>国产芯片</strong>的需求暴涨：</p>\n<ul>\n<li><strong>华为昇腾</strong>：持续迭代，部分场景可替代英伟达</li>\n<li><strong>寒武纪、景嘉微</strong>：国产AI芯片厂商获得更多市场机会</li>\n<li><strong>算力租赁平台</strong>：帮助中小企业\"用得起\"AI</li>\n</ul>\n<p><strong>一句话：AI时代的基础设施，永远是\"硬通货\"。</strong></p>\n<hr />\n<h2 id=\"结语站在浪潮之巅我们在等待什么\">结语：站在浪潮之巅，我们在等待什么？</h2>\n<p>写到这里，你会发现2026年的互联网行业呈现出一个清晰的图景：<strong>AI从\"工具\"进化为\"伙伴\"，技术创新从\"少数人的游戏\"走向\"普惠时代\"。</strong></p>\n<p>但繁荣之下，也有隐忧：</p>\n<ul>\n<li>AI的版权、伦理、监管问题日益尖锐</li>\n<li>流量红利见顶，增量竞争转向存量博弈</li>\n<li>全球经济不确定性依然存在，资本市场趋于保守</li>\n</ul>\n<p><strong>作为一个普通用户、一个从业者、一个关注科技的人，我们应该如何看待这一切？</strong></p>\n<p>我的答案是：<strong>保持好奇，保持警惕，保持行动。</strong></p>\n<p>技术浪潮不可阻挡，但每一次浪潮都是一次重新洗牌的机会。真正值钱的不是技术本身，而是<strong>用技术解决真实问题的能力</strong>。</p>\n<blockquote>\n<p><em>\"未来已经到来，只是分布不均。\"</em> —— 科幻作家威廉·吉布森</p>\n</blockquote>\n<hr />\n<h2 id=\"-互动时间\">📢 互动时间</h2>\n<p>读完这篇文章，你最关注哪个话题？</p>\n<ul>\n<li>A. AI Agent——最期待\"AI帮我打工\"</li>\n<li>B. 国产大模型——想支持国货但不知道怎么选</li>\n<li>C. 视频生成——想试试做\"AI导演\"</li>\n<li>D. AI硬件——等待一个\"划时代\"的产品</li>\n<li>E. 自动驾驶——关心什么时候能真正\"脱手\"</li>\n<li>F. 其他——评论区聊聊</li>\n</ul>\n<p><strong>也欢迎你在评论区分享：2026年，你最想用AI帮你做什么？</strong></p>\n<blockquote>\n<p><em>如果你觉得这篇文章有帮助，欢迎点个赞、在看、转发送给朋友。你的支持是我持续输出的动力。</em></p>\n</blockquote>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 23:11</span>&nbsp;\n<a href=\"https://www.cnblogs.com/nf01\">农夫运维</a>&nbsp;\n阅读(<span id=\"post_view_count\">173</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Vue3解析学习 - handlers 模块",
      "link": "https://www.cnblogs.com/aleafshit/p/19613786",
      "published": "",
      "description": "<div class=\"postcontent\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>欢迎大家交流讨论，也欢迎提出问题或不同观点。本文基于个人对源码的理解与整理，难免存在偏差或不完整之处，如果你有更深入的见解或发现错误，期待一起探讨与修正。</p>\n<hr />\n<h1 id=\"一handlers-的核心设计目标\">一、handlers 的核心设计目标</h1>\n<p>一句话总结：</p>\n<blockquote>\n<p>不同数据结构，用不同的代理策略，做到“最小拦截 + 精确触发”。</p>\n</blockquote>\n<p>响应式系统本质上是两件事：</p>\n<ol>\n<li>\n<p><strong>依赖收集（track）</strong>：读取时记录依赖</p>\n</li>\n<li>\n<p><strong>副作用触发（trigger）</strong>：写入时触发更新</p>\n</li>\n</ol>\n<p>但不同类型的数据：</p>\n<ul>\n<li>\n<p>变更方式不同</p>\n</li>\n<li>\n<p>可拦截能力不同</p>\n</li>\n</ul>\n<p>因此 Vue 把代理逻辑拆成三套：</p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>变更方式</th>\n<th>Proxy 能力</th>\n<th>方案</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Object</td>\n<td>属性赋值</td>\n<td>可完整拦截</td>\n<td>baseHandlers</td>\n</tr>\n<tr>\n<td>Array</td>\n<td>属性 + 方法</td>\n<td>方法无法直接拦截</td>\n<td>重写数组方法</td>\n</tr>\n<tr>\n<td>Map/Set</td>\n<td>方法驱动</td>\n<td>只能拦截 get</td>\n<td>返回重写方法</td>\n</tr>\n</tbody>\n</table>\n<p>这不是“复杂”，而是对 JavaScript 语义边界的工程妥协。</p>\n<hr />\n<h1 id=\"二object类继承体系\">二、Object：类继承体系</h1>\n<p>Object 的 handler 在 <code>baseHandlers.ts</code> 中，通过类继承组织：</p>\n<h2 id=\"1-basereactivehandler抽象基类\">1. BaseReactiveHandler（抽象基类）</h2>\n<p>这是统一入口，负责：</p>\n<ul>\n<li>\n<p>get 拦截</p>\n</li>\n<li>\n<p>ref 解包</p>\n</li>\n<li>\n<p>深/浅响应式递归</p>\n</li>\n<li>\n<p>依赖收集</p>\n</li>\n</ul>\n<p>核心职责：</p>\n<pre><code class=\"language-bash\">访问属性 → track → 返回响应式值\n</code></pre>\n<hr />\n<h2 id=\"2-mutablereactivehandler可变对象\">2. MutableReactiveHandler（可变对象）</h2>\n<p>这是最常见的 reactive 版本：</p>\n<h3 id=\"set\">set</h3>\n<p>触发两种操作类型：</p>\n<ul>\n<li>\n<p>TriggerOpTypes.ADD（新增属性）</p>\n</li>\n<li>\n<p>TriggerOpTypes.SET（修改已有属性）</p>\n</li>\n</ul>\n<p>区分 ADD / SET 的意义在于：</p>\n<pre><code class=\"language-bash\">新增属性 ≠ 修改属性\n</code></pre>\n<p>某些依赖只关心结构变化（比如 for...in、Object.keys）。</p>\n<hr />\n<h3 id=\"deleteproperty\">deleteProperty</h3>\n<p>触发：</p>\n<pre><code class=\"language-bash\">TriggerOpTypes.DELETE\n</code></pre>\n<hr />\n<h3 id=\"has\">has</h3>\n<p>用于：</p>\n<pre><code class=\"language-bash\">key in obj\n</code></pre>\n<p>依赖收集：</p>\n<pre><code class=\"language-bash\">TrackOpTypes.HAS\n</code></pre>\n<hr />\n<h3 id=\"ownkeys\">ownKeys</h3>\n<p>用于：</p>\n<pre><code class=\"language-bash\">for...in / Object.keys / Reflect.ownKeys\n</code></pre>\n<p>依赖收集：</p>\n<pre><code class=\"language-bash\">TrackOpTypes.ITERATE\n</code></pre>\n<p>这一步是很多人忽略的关键：</p>\n<blockquote>\n<p>遍历结构本身也是依赖。</p>\n</blockquote>\n<hr />\n<h2 id=\"3-readonlyreactivehandler只读\">3. ReadonlyReactiveHandler（只读）</h2>\n<p>只读版本直接阻断写操作：</p>\n<ul>\n<li>\n<p>set → 警告 + 不执行</p>\n</li>\n<li>\n<p>deleteProperty → 警告 + 不执行</p>\n</li>\n</ul>\n<p>但 <strong>读取仍然会 track</strong>。</p>\n<p>因为只读不等于“无依赖”。</p>\n<hr />\n<h1 id=\"三array方法重写策略\">三、Array：方法重写策略</h1>\n<p>数组的问题在于：</p>\n<blockquote>\n<p>大部分变更不是通过属性赋值，而是通过方法。</p>\n</blockquote>\n<p>例如：</p>\n<pre><code class=\"language-bash\">arr.push()\narr.splice()\narr.shift()\n</code></pre>\n<p>Proxy 无法直接拦截方法调用。</p>\n<p>Vue3 的方案是：</p>\n<blockquote>\n<p>在 get trap 中返回“重写版本”的数组方法。</p>\n</blockquote>\n<hr />\n<h2 id=\"arrayinstrumentationsts\">arrayInstrumentations.ts</h2>\n<p>返回“重写版本”的方法对象。</p>\n<p>当访问数组方法时：</p>\n<pre><code class=\"language-bash\">proxy.push\n</code></pre>\n<p>BaseReactiveHandler 的 get trap 会判断：</p>\n<pre><code class=\"language-bash\">是不是数组？\n是不是被重写的方法？\n</code></pre>\n<p>如果是：</p>\n<p>→ 返回”重写版本“方法</p>\n<p>这些方法内部：</p>\n<ol>\n<li>\n<p>先暂停依赖收集（避免死循环）</p>\n</li>\n<li>\n<p>调用原始数组方法</p>\n</li>\n<li>\n<p>手动 trigger</p>\n</li>\n</ol>\n<p>这就是：</p>\n<blockquote>\n<p>用函数包装模拟“方法拦截”。</p>\n</blockquote>\n<hr />\n<h1 id=\"四map--set工厂函数体系\">四、Map / Set：工厂函数体系</h1>\n<p>集合类型比数组更极端：</p>\n<blockquote>\n<p>所有变更都通过方法。</p>\n</blockquote>\n<pre><code class=\"language-bash\">map.set()\nmap.delete()\nset.add()\n</code></pre>\n<p>Proxy 能拦截的只有：</p>\n<pre><code class=\"language-bash\">get\n</code></pre>\n<p>因此 Vue 采用纯工厂函数模式，而不是类继承。</p>\n<hr />\n<h2 id=\"1-只拦截-get\">1. 只拦截 get</h2>\n<p>collectionHandlers 的策略：</p>\n<pre><code class=\"language-bash\">get → 返回重写后的方法\n</code></pre>\n<p>和数组类似，但更彻底。</p>\n<hr />\n<h2 id=\"2-四种-handler-组合\">2. 四种 handler 组合</h2>\n<p>创建导出 4 个全局代理对象：</p>\n<ol>\n<li>\n<p>mutableCollectionHandlers（深响应式）</p>\n</li>\n<li>\n<p>shallowCollectionHandlers（浅响应式）</p>\n</li>\n<li>\n<p>readonlyCollectionHandlers（只读）</p>\n</li>\n<li>\n<p>shallowReadonlyCollectionHandlers（浅只读）</p>\n</li>\n</ol>\n<p>这些不是手写的，而是通过：</p>\n<pre><code class=\"language-bash\">createInstrumentationsGetter(isReadonly, shallow)\n</code></pre>\n<p>动态生成。</p>\n<hr />\n<h2 id=\"3-createinstrumentations-核心工厂\">3. createInstrumentations 核心工厂</h2>\n<p>这个工厂函数负责：</p>\n<ul>\n<li>\n<p>构造重写后的 Map/Set 方法</p>\n</li>\n<li>\n<p>在方法内部访问原始对象</p>\n</li>\n</ul>\n<p>关键点：</p>\n<pre><code class=\"language-bash\">ReactiveFlags.RAW → 拿到原始数据\n</code></pre>\n<p>避免代理套代理导致的递归问题。</p>\n<hr />\n<h1 id=\"五track--trigger-的精细控制\">五、Track / Trigger 的精细控制</h1>\n<p>这是 Vue 3 响应式系统的灵魂改进之一。</p>\n<p>读取操作被分为：</p>\n<ul>\n<li>\n<p>GET</p>\n</li>\n<li>\n<p>HAS</p>\n</li>\n<li>\n<p>ITERATE</p>\n</li>\n</ul>\n<p>写入操作被分为：</p>\n<ul>\n<li>\n<p>SET</p>\n</li>\n<li>\n<p>ADD</p>\n</li>\n<li>\n<p>DELETE</p>\n</li>\n</ul>\n<p>它们不是简单的一对一关系。</p>\n<hr />\n<h2 id=\"精确触发的意义\">精确触发的意义</h2>\n<p>关键规则：</p>\n<blockquote>\n<p>并不是所有写操作都应该触发所有依赖。</p>\n</blockquote>\n<p>例如：</p>\n<ul>\n<li>\n<p>GET 依赖只关心 SET</p>\n</li>\n<li>\n<p>ITERATE 依赖关心 ADD / DELETE</p>\n</li>\n<li>\n<p>HAS 依赖只关心 key 是否存在</p>\n</li>\n</ul>\n<p>如果不区分：</p>\n<pre><code class=\"language-bash\">任何写操作 → 全部 effect 触发\n</code></pre>\n<p>这会导致：</p>\n<ul>\n<li>\n<p>不必要的重渲染</p>\n</li>\n<li>\n<p>性能雪崩</p>\n</li>\n<li>\n<p>副作用风暴</p>\n</li>\n</ul>\n<p>Vue 2 的响应式就属于粗粒度触发。</p>\n<p>Vue 3 的 handlers 设计本质是：</p>\n<blockquote>\n<p>把“读类型”和“写类型”建立映射关系。</p>\n</blockquote>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"itemdesc\">\n                发表于 \n<span id=\"post-date\">2026-02-13 23:02</span>&nbsp;\n<a href=\"https://www.cnblogs.com/aleafshit\">death_ray</a>&nbsp;\n阅读(<span id=\"post_view_count\">81</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n            </div>"
    },
    {
      "title": "【Azure App Service】32位 Windows App Service 最大能使用多少内存？",
      "link": "https://www.cnblogs.com/lulight/p/19613358",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/lulight/p/19613358\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 19:13\">\n    <span>【Azure App Service】32位 Windows App Service 最大能使用多少内存？</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"问题描述\">问题描述</h2>\n<p>在使用 Windows-based Azure Web App（32位）时，经常遇到以下疑问：</p>\n<ul>\n<li>进程内存上限是多少？</li>\n<li>不同托管模式下可用内存如何计算？</li>\n</ul>\n<p>本文将针对这些问题进行详细解答。<br />\n<img alt=\"image\" class=\"lazyload\" /></p>\n<hr />\n<h2 id=\"问题解答\">问题解答</h2>\n<h3 id=\"一32-位程序最大能使用多少内存\">一、32 位程序最大能使用多少内存？</h3>\n<p><strong>理论上限约为 4GB</strong></p>\n<p>32 位程序的内存地址由 32 个二进制位组成，因此理论上可以有 2³² = 4,294,967,296 种不同的内存地址。每个内存地址指向 1 Byte 的空间，所以：</p>\n<blockquote>\n<p>32 位地址空间 = 2³² Byte (4<em>1024</em>1024*1024 B) ≈ 4GB</p>\n</blockquote>\n<p><strong>为什么文档中提到 2GB？</strong></p>\n<p>Windows 默认将 4GB 虚拟地址空间划分为：</p>\n<ul>\n<li><strong>2GB 用户态</strong>：供应用程序使用</li>\n<li><strong>2GB 内核态</strong>：供操作系统使用</li>\n</ul>\n<p>因此，默认情况下单进程可用用户态内存为 <strong>2GB</strong>。这只是默认行为，并非 32 位程序的绝对上限。在某些情况下（例如启用 Large Address Aware + 特定系统配置），可以超过 2GB。</p>\n<hr />\n<h3 id=\"二in-process-与-out-of-process-模型对内存的影响\">二、In-Process 与 Out-Of-Process 模型对内存的影响</h3>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<h4 id=\"两种托管模式对比\">两种托管模式对比</h4>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>In-Process</th>\n<th>Out-of-Process</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>宿主进程</td>\n<td><code>w3wp.exe</code>（IIS 工作进程）</td>\n<td><code>dotnet.exe</code>（独立进程）</td>\n</tr>\n<tr>\n<td>进程数量</td>\n<td>应用与 IIS 共享同一进程</td>\n<td>应用运行在独立进程中</td>\n</tr>\n<tr>\n<td>内存隔离</td>\n<td>与 IIS 共享内存空间</td>\n<td>独立内存空间</td>\n</tr>\n<tr>\n<td>性能</td>\n<td>更高（无进程间通信开销）</td>\n<td>略低（需通过 HTTP 代理）</td>\n</tr>\n</tbody>\n</table>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<h4 id=\"in-process-模式内存行为\">In-Process 模式内存行为</h4>\n<ul>\n<li>应用代码直接运行在 <code>w3wp.exe</code> 进程中</li>\n<li>内存上限受 <code>w3wp.exe</code> 进程限制：\n<ul>\n<li>32 位：约 <strong>2GB</strong>（Windows 用户态默认限制）</li>\n<li>64 位：受 Sandbox 限制</li>\n</ul>\n</li>\n<li><strong>注意</strong>：应用内存 + IIS 模块内存 共同占用进程空间</li>\n</ul>\n<h4 id=\"out-of-process-模式内存行为\">Out-of-Process 模式内存行为</h4>\n<ul>\n<li>应用运行在独立的 <code>dotnet.exe</code> 进程中</li>\n<li>Kestrel 作为边缘服务器，IIS 仅作为反向代理</li>\n<li>内存上限独立计算：\n<ul>\n<li>32 位：约 <strong>4GB</strong>（可寻址上限）</li>\n<li>64 位：受 Sandbox 限制</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"azure-app-service-sandbox-限制\">Azure App Service Sandbox 限制</h4>\n<p>在 Azure App Service 中，存在一个核心限制：</p>\n<blockquote>\n<p><strong>Sandbox 限制</strong>：进程实际能获得的最大物理内存 = 机器物理内存 × 75%</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>App Service Plan</th>\n<th>物理内存</th>\n<th>64位进程可用内存（约）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>B1/S1</td>\n<td>1.75 GB</td>\n<td>~1.3 GB</td>\n</tr>\n<tr>\n<td>B2/S2</td>\n<td>3.5 GB</td>\n<td>~2.6 GB</td>\n</tr>\n<tr>\n<td>B3/S3</td>\n<td>7 GB</td>\n<td>~5.25 GB</td>\n</tr>\n<tr>\n<td>P1v2</td>\n<td>3.5 GB</td>\n<td>~2.6 GB</td>\n</tr>\n<tr>\n<td>P2v2</td>\n<td>7 GB</td>\n<td>~5.25 GB</td>\n</tr>\n<tr>\n<td>P3v2</td>\n<td>14 GB</td>\n<td>~10.5 GB</td>\n</tr>\n</tbody>\n</table>\n<p><strong>总结：</strong></p>\n<ul>\n<li><strong>32 位进程</strong>：永远无法突破约 4GB 的可寻址限制（受托管模式影响不大）</li>\n<li><strong>64 位进程</strong>：会触及 Sandbox 限制（最大约为物理内存的 75%）</li>\n</ul>\n<hr />\n<h3 id=\"三多个虚拟目录时的内存计算方式\">三、多个虚拟目录时的内存计算方式</h3>\n<p>当同一 App Service 下存在多个虚拟目录（vdir）时：</p>\n<h4 id=\"in-process-模式\">In-Process 模式</h4>\n<ul>\n<li>所有虚拟目录共享同一个 <code>w3wp.exe</code> 进程</li>\n<li>内存上限为该进程的总上限（32位约2GB，64位受Sandbox限制）</li>\n<li>各应用间无内存隔离，一个应用内存泄漏可能影响其他应用</li>\n</ul>\n<h4 id=\"out-of-process-模式\">Out-of-Process 模式</h4>\n<ul>\n<li>每个虚拟目录会生成独立的 <code>dotnet.exe</code> 进程</li>\n<li>每个进程单独计算可用内存上限：\n<ul>\n<li>32 位 → 约 4GB（实际可能更低，取决于系统配置）</li>\n<li>64 位 → 受 Sandbox 限制（机器内存 × 75%）</li>\n</ul>\n</li>\n<li>多个站点共享同一台 VM 的物理内存，进程间会相互竞争资源</li>\n<li><strong>优势</strong>：进程隔离，一个应用崩溃不影响其他应用</li>\n</ul>\n<hr />\n<h3 id=\"四scmkudu进程是否计入总内存\">四、SCM（Kudu）进程是否计入总内存？</h3>\n<p><strong>是的</strong>。SCM 进程也运行在同一台 VM 上，其内存占用会一并计入 App Service Plan 的物理内存使用量。</p>\n<p><strong>Kudu 典型内存占用</strong>：约 200MB - 500MB（视操作而定）</p>\n<hr />\n<h3 id=\"五如何监控-app-service-内存使用\">五、如何监控 App Service 内存使用？</h3>\n<h4 id=\"方法一azure-portal-指标\">方法一：Azure Portal 指标</h4>\n<p>在 Azure Portal 中查看 App Service 的 <strong>Metrics</strong>：</p>\n<ul>\n<li><code>Memory working set</code>：当前工作集内存</li>\n<li><code>Private Bytes</code>：进程私有内存</li>\n</ul>\n<h4 id=\"方法二kudu-进程管理器\">方法二：Kudu 进程管理器</h4>\n<p>访问 <code>https://&lt;your-app&gt;.scm.chinacloudsites.cn/ProcessExplorer/</code> 查看：</p>\n<ul>\n<li>各进程的内存占用详情</li>\n<li><code>w3wp.exe</code> 和 <code>dotnet.exe</code> 的实时内存状态</li>\n</ul>\n<h4 id=\"方法三application-insights\">方法三：Application Insights</h4>\n<p>启用 Application Insights 后，可监控：</p>\n<ul>\n<li>内存使用趋势</li>\n<li>内存异常告警</li>\n<li>GC 行为分析</li>\n</ul>\n<hr />\n<h2 id=\"总结\">总结</h2>\n<table>\n<thead>\n<tr>\n<th>场景</th>\n<th>内存上限</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32 位进程（理论）</td>\n<td>~4GB</td>\n</tr>\n<tr>\n<td>32 位进程（Windows 默认）</td>\n<td>~2GB</td>\n</tr>\n<tr>\n<td>64 位进程</td>\n<td>物理内存 × 75%</td>\n</tr>\n<tr>\n<td>In-Process（32位）</td>\n<td>~2GB（与IIS共享）</td>\n</tr>\n<tr>\n<td>Out-of-Process（32位）</td>\n<td>~4GB（独立进程）</td>\n</tr>\n</tbody>\n</table>\n<p><strong>建议</strong>：</p>\n<ol>\n<li>如果应用对内存需求较高，推荐使用 <strong>64 位</strong> 配置</li>\n<li>对于需要进程隔离的场景，选择 <strong>Out-of-Process</strong> 模式</li>\n<li>定期监控内存使用，避免触及上限导致应用异常</li>\n</ol>\n<hr />\n<h2 id=\"参考资料\">参考资料</h2>\n<ul>\n<li>\n<p>Virtual Address Space (Memory Management) : <a href=\"https://learn.microsoft.com/en-us/windows/win32/memory/virtual-address-space\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/windows/win32/memory/virtual-address-space</a></p>\n</li>\n<li>\n<p>.NET GC - Heap hard limit percent : <a href=\"https://learn.microsoft.com/en-us/dotnet/core/runtime-config/garbage-collector#heap-hard-limit-percent\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/dotnet/core/runtime-config/garbage-collector#heap-hard-limit-percent</a></p>\n</li>\n<li>\n<p>ASP.NET Core Module (In-Process vs Out-of-Process) : <a href=\"https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/aspnet-core-module\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/aspnet-core-module</a></p>\n</li>\n<li>\n<p>Azure App Service Sandbox : <a href=\"https://github.com/projectkudu/kudu/wiki/Azure-Web-App-sandbox\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/projectkudu/kudu/wiki/Azure-Web-App-sandbox</a></p>\n</li>\n</ul>\n\n\n</div>\n<div id=\"MySignature\">\n    <div style=\"background: #1c5f55; height: 36px; width: 618px; padding: 14px 5px 0px 3px;\">\n  <p style=\"font-weight: bold; color: white;\">当在复杂的环境中面临问题，格物之道需：浊而静之徐清，安以动之徐生。 云中，恰是如此!</p>\n</div>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 19:13</span>&nbsp;\n<a href=\"https://www.cnblogs.com/lulight\">编码者卢布</a>&nbsp;\n阅读(<span id=\"post_view_count\">50</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "[拆解LangChain执行引擎] PregelNode——无状态的功能节点",
      "link": "https://www.cnblogs.com/jaydenai/p/19617222/pregel-node",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jaydenai/p/19617222/pregel-node\" id=\"cb_post_title_url\" title=\"发布于 2026-02-15 07:37\">\n    <span>[拆解LangChain执行引擎] PregelNode——无状态的功能节点</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        一个Pregel由Node和Channel构建而成，后者保持状态并以Pub/Sub的方式驱动Node执行，Pregel中的Node是一个PregelNode对象，是一个完全无状态的功能节点。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p><code>Pregel</code>中的Node对应的类型为<code>PregelNode</code>。对于一个<code>PregelNode</code>对象来说，它最核心的部分就是绑定在它上面的一个可执行操作，它是抽象类Runnable的子类。在LangChain整个体系中，Runnable类型几乎无处不在，包括语言模型（不论是传统的LLM模型还是Chat模型）、实现RAG的Retriever和提示词模板等组件都是一个Runnable对象，实际上Pregel本身也是一个Runnable对象。LangChain的“Chain”就是由一组Runnable对象按照指定的顺序构建而成。Runnable如此重要，值得单开<a href=\"https://blog.csdn.net/jaydenai/category_13128380.html\" rel=\"noopener nofollow\" target=\"_blank\">一个系列</a>进行独立介绍，这里我们可用先将它理解成一个可执行对象，可用帮助我们执行定义Node的函数。</p>\n<pre><code class=\"language-python\">class PregelNode:\n    bound: Runnable[Any, Any]\n</code></pre>\n<h2 id=\"1-输入\">1. 输入</h2>\n<p>PregelNode的<code>channels</code>和<code>triggers</code>字段表示作为输入和触发器的Channel的名称。由于ManagedValue也可用作为输入，所以channels字段也可用包含ManagerValue的名称，这里我们统称为Channel。同一个Channel可以同时作为输入和触发器，出现在这两个字段中。</p>\n<pre><code class=\"language-python\">class PregelNode:\n    channels : str | list[str]\n    triggers : list[str]\n</code></pre>\n<p>对于单一的输入，Channel名称可以设置为字符串，也可以封装成列表，它们会影响Node处理函数的输入参数传递方式。对于前者（字符串），对应Channel的值会作为原始参数传递给Node的处理函数，后者则会封装成字典进行传递，字典的Key为Channel的名称。如下的演示实例体现了这一点。</p>\n<pre><code class=\"language-python\">from langgraph.channels import LastValue\nfrom langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.pregel._read import PregelNode\n\ndef build_node(node_name:str)-&gt;PregelNode:\n    channel_in = f\"{node_name}_in\"\n    channel_out = f\"{node_name}_out\"\n    node = (NodeBuilder()\n        .do(lambda args:args)\n        .write_to(channel_out)).build()\n   \n    node.triggers = [channel_in]\n    node.channels = channel_in if node_name == \"foo\" else [channel_in]\n    return node\n\napp = Pregel(\n    nodes= {name: build_node(name) for name in [\"foo\",\"bar\"]},\n    channels= {\n        \"foo_in\": LastValue(str),\n        \"bar_in\": LastValue(str),\n        \"foo_out\": LastValue(object),\n        \"bar_out\": LastValue(object),\n    },\n    input_channels=[\"foo_in\", \"bar_in\"],\n    output_channels=[\"foo_out\", \"bar_out\"])\n\nresult = app.invoke(input={\"foo_in\":\"foobar\", \"bar_in\":\"foobar\"})\nassert result[\"foo_out\"] == \"foobar\"\nassert result[\"bar_out\"] == {'bar_in': 'foobar'}\n</code></pre>\n<p>如果一个Channel被多个Node作为输入，只要该Channel被任一Node以列表的形式注册，引擎内部的对齐机制将统一使用字典作为所有Node处理函数的输入。比如我们按照如下的方式修改了上面的程序，是foo和bar两个Node都使用foobarChannel作为输入，该输入Channel在bar中以列表的形式进行了设置，以字符串形式设置的fooNode的处理函数的输入参数依然会编程字典。这是一个不为人知的细节。</p>\n<pre><code class=\"language-python\">from langgraph.channels import LastValue\nfrom langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.pregel._read import PregelNode\n\ndef build_node(node_name:str)-&gt;PregelNode:\n    node = (NodeBuilder()\n        .do(lambda args:args)\n        .write_to(node_name)).build()   \n    node.triggers = [\"input\"]\n    node.channels = \"input\" if node_name == \"foo\" else [\"input\"]\n    return node\n\napp = Pregel(\n    nodes= {name: build_node(name) for name in [\"foo\",\"bar\"]},\n    channels= {\n        \"input\": LastValue(str),\n        \"foo\": LastValue(object),\n        \"bar\": LastValue(object),\n    },\n    input_channels=[\"input\"],\n    output_channels=[\"foo\", \"bar\"])\n\nresult = app.invoke(input={\"input\":\"foobar\"})\nassert result[\"foo\"] == {'input': 'foobar'}\nassert result[\"bar\"] == {'input': 'foobar'}\n</code></pre>\n<h2 id=\"2-输出\">2. 输出</h2>\n<p>Node的执行结果依赖于PregelNode的writers字段返回的一组Runnable对象输出到对应的Channel，系统默认使用的是一个<code>ChannelWrite</code>。如代码片段所示，初始化ChannelWrite对象时需要提供一组<code>ChannelWriteEntry</code>或<code>ChannelWriteTupleEntry</code>来表示针对目标Channel的写入意图。另一个应用了@cached_property装饰器的flat_writers返回一组扁平化的Runnable对象以提供性能。</p>\n<pre><code class=\"language-python\">class PregelNode:\n    writers : list[Runnable]\n    @cached_property\n    def flat_writers(self) -&gt; list[Runnable]\n</code></pre>\n<pre><code class=\"language-python\">class ChannelWrite(RunnableCallable):\n    writes: list[ChannelWriteEntry | ChannelWriteTupleEntry | Send]\n    def __init__(\n        self,\n        writes: Sequence[ChannelWriteEntry | ChannelWriteTupleEntry | Send],\n        *,\n        tags: Sequence[str] | None = None,\n    )\n</code></pre>\n<p>ChannelWriteEntry的<code>channel</code>和<code>value</code>字段分别表示输出Channel名称和值。<code>skip_none</code>字段决定是否需要忽略None值，如果指定了<code>mapper</code>字段，<code>value</code>还会被它作进一步处理并最终生成输出到Channel的值。一个ChannelWriteEntry对应一个单一Channel的输出，而ChannelWriteTupleEntry可以完成针对多Channel的输出。具体来说，Node处理函数返回的对象可以绑定在它的value字段上，通过mapper提供的可执行对象映射为一个“二元组序列”，此二元组的两部分对应输出Channel的名称和值。ChannelWriteTupleEntry的static设置的三元组序列仅作静态分析用，可以忽略。</p>\n<pre><code class=\"language-python\">class ChannelWriteEntry(NamedTuple):\n    channel: str\n    value: Any = PASSTHROUGH\n    skip_none: bool = False\n    mapper: Callable | None = None\n\nclass ChannelWriteTupleEntry(NamedTuple):\n    mapper: Callable[[Any], Sequence[tuple[str, Any]] | None]\n    value: Any = PASSTHROUGH\n    static: Sequence[tuple[str, Any, str | None]] | None = None\n\nPASSTHROUGH = object()\n</code></pre>\n<p>在如下所示的演示实例中，我们创建了一个包含唯一Node的Pregel对象，并创建了一个包含单一ChannelWrite对象的列表作为该Node的writers字段。此ChannelWrite的writes列表包含两个ChannelWriteEntry和一个ChannelWriteTupleEntry，它们分别完成了针对三个Channel的输出。</p>\n<pre><code class=\"language-python\">from langgraph.channels import LastValue\nfrom langgraph.pregel import Pregel\nfrom langgraph.pregel._read import PregelNode\nfrom langgraph.pregel._write import ChannelWrite, ChannelWriteEntry, ChannelWriteTupleEntry\n\nentry1 = ChannelWriteEntry(\n    channel=\"foo\",\n    value= \"123\"\n)\nentry2 = ChannelWriteEntry(\n    channel=\"bar\",\n    value= \"456\",\n    mapper= lambda v: int(v)\n)\ntuple_entry = ChannelWriteTupleEntry(\n    value= {\"foo\":\"123\", \"bar\":\"456\"},\n    mapper= lambda v: [(\"baz\", int(v[\"foo\"]) + int(v[\"bar\"]))]\n)\nnode = PregelNode(\n    triggers=[\"start\"],\n    channels=[],\n    writers=[ChannelWrite(writes=[entry1, entry2, tuple_entry])]\n)\napp = Pregel(\n    nodes={\"body\": node},\n    channels={\n        \"start\": LastValue(None),\n        \"foo\": LastValue(str),\n        \"bar\": LastValue(int),\n        \"baz\": LastValue(int)\n    },\n    input_channels=[\"start\"],\n    output_channels=[\"foo\", \"bar\", \"baz\"],\n)\n\nresult = app.invoke(input={\"start\": None})\nassert result == {\"foo\": \"123\", \"bar\": 456, \"baz\": 579}\n</code></pre>\n<p>对于通过PregelNode对象表示的Node来说，其bound字段返回的Runnable对象用于执行处理函数，操作执行的结果由writers列表的一组Runnable对象写入相应的Channel，这两个核心工作最终会被如下这个node属性合并。对于Pregel来说，该属性在功能上基本就代表了整个Node，这也是该属性如此命名的原因。</p>\n<pre><code class=\"language-python\">class PregelNode:    \n    @cached_property\n    def node(self) -&gt; Runnable[Any, Any] | None\n</code></pre>\n<h2 id=\"3-输入映射\">3. 输入映射</h2>\n<p>Node基于Channel的输入、触发和输出分别对应<code>channels</code>、<code>triggers</code>和<code>writers</code>字段。如果所有输入Channel读取的原始输入（以字典形式表示）和提交给处理函数的参数有出入，我们还可以利用mapper字段返回的可执行对象（<code>Callable[[Any], Any]</code>）作进一步映射。</p>\n<pre><code class=\"language-python\">class PregelNode:\n    mapper \t: Callable[[Any], Any] | None\n</code></pre>\n<p>如下的演示程序通过Pregel的mapper字段设置为了一个Lambda表达式，将提供给Node处理函数处理的字典转换成元组。</p>\n<pre><code class=\"language-python\">from langgraph.channels import LastValue\nfrom langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.pregel._read import PregelNode\nfrom typing import Tuple\n\nnode: PregelNode = (NodeBuilder()\n    .subscribe_to(\"foo\",\"bar\")\n    .do(lambda args:args)\n    .write_to(\"output\")).build()    \nnode.mapper = lambda args:tuple(args.values())\n\napp = Pregel(\n    nodes={\"body\": node},\n    channels={\n        \"foo\": LastValue(str),\n        \"bar\": LastValue(str),\n        \"output\": LastValue(Tuple[str,str]),\n    },\n    input_channels=[\"foo\",\"bar\"],\n    output_channels=[\"output\"],\n)\nresult = app.invoke(input={\"foo\":\"hello\", \"bar\":\"world\"})\nassert result[\"output\"] == (\"hello\",\"world\")\n</code></pre>\n<h2 id=\"4-失败重试\">4. 失败重试</h2>\n<p>Agent中的Node可能会涉及网络传输、数据检索等会导致瞬时错误的操作，失败后自动重试机制是确保可靠性的主要手段，我们可以利用PregelNode 的<code>retry_policy</code>字段设置相应的重试策略。具体的重试策略通过如下所示的<code>RetryPolicy</code>具名元组表示。RetryPolicy的<code>max_attempts</code>和<code>initial_interval</code>分别表示最大重试次数（包含初次调用）和第一次重试前的初始等待时间，单位为秒。如果没有为Node设置针对性的重试策略，Pregel的retry_policy字段设置的重试策略将作为兜底。</p>\n<pre><code class=\"language-python\">class PregelNode:\nretry_policy : Sequence[RetryPolicy] | None\n\nclass RetryPolicy(NamedTuple):\n    initial_interval: float = 0.5\n    backoff_factor: float = 2.0\n    max_interval: float = 128.0\n    max_attempts: int = 3\n    jitter: bool = True\n    retry_on: (\n        type[Exception] | Sequence[type[Exception]] | Callable[[Exception], bool]\n    ) = default_retry_on\n\nclass Pregel(\n    PregelProtocol[StateT, ContextT, InputT, OutputT],\n    Generic[StateT, ContextT, InputT, OutputT]): \nretry_policy : Sequence[RetryPolicy] = ()\n</code></pre>\n<p>重试策略采用基于“间隔倍增”的退避机制（Back off），也就是下次重试等待时间是前一次等待的N倍，这个倍数通过<code>backoff_factor</code>字段来提供，<code>max_interval</code>字段为等待实现设置了上限。为了防止多个并发Node同时重试而产生“惊群效应”，我们可以在重试间隔中添加随机抖动，<code>jitter</code>是这一特性的开关。调用失败有很多原因，重试在任何错误场景中都有意义，我们可以利用<code>retry_on</code>字段设置为重置设置前置条件。该字段的默认值对应如下这个<code>default_retry_on</code>函数。</p>\n<pre><code class=\"language-python\">def default_retry_on(exc: Exception) -&gt; bool:\n    import httpx\n    import requests\n\n    if isinstance(exc, ConnectionError):\n        return True\n    if isinstance(exc, httpx.HTTPStatusError):\n        return 500 &lt;= exc.response.status_code &lt; 600\n    if isinstance(exc, requests.HTTPError):\n        return 500 &lt;= exc.response.status_code &lt; 600 if exc.response else True\n    if isinstance(\n        exc,\n        (\n            ValueError,\n            TypeError,\n            ArithmeticError,\n            ImportError,\n            LookupError,\n            NameError,\n            SyntaxError,\n            RuntimeError,\n            ReferenceError,\n            StopIteration,\n            StopAsyncIteration,\n            OSError,\n        ),\n    ):\n        return False\n    return True\n</code></pre>\n<p>在如下的演示程序中，我们定义了一个用于创建Pregel对象的<code>build_pregel</code>函数，该函数的<code>max_attempts</code>参数决定组成返回Pregel对象的Node采用的RetryPolicy的重试次数。Node的处理函数是一个由get_handler函数返回的闭包，该闭包在前两次执行的时候总是会排除异常。程序反映的情况是，重试次数若设置为2，调用会抛出异常；但是若设置为3，会保证成功调用。</p>\n<pre><code class=\"language-python\">from langgraph.channels import LastValue\nfrom langgraph.types import RetryPolicy\nfrom langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.pregel._read import PregelNode\nfrom typing import Any\n\ndef get_handler():\n    times = 0\n    def handle(args: dict[str, Any]) -&gt; str:\n        nonlocal times\n        times += 1\n        if times &lt; 3:\n            raise Exception(\"manually thrown error.\")\n        return \"Success\"\n    return handle\n\ndef build_node(max_attempts: int) -&gt; PregelNode:\n    node = (\n        NodeBuilder().subscribe_to(\"start\").do(get_handler()).write_to(\"output\")\n    ).build()\n    node.retry_policy = [RetryPolicy(max_attempts=max_attempts)]\n    return node\n\ndef build_pregel(max_attempts: int) -&gt; Pregel:\n    return Pregel(\n        nodes={\"body\": build_node(max_attempts)},\n        channels={\n            \"start\": LastValue(None),\n            \"output\": LastValue(str),\n        },\n        input_channels=[\"start\"],\n        output_channels=[\"output\"],\n    )\n\napp = build_pregel(max_attempts=2)\ntry:\n    app.invoke(input={\"start\": None})\n    assert False, \"Expected an exception but none was raised.\"\nexcept Exception as e:\n    assert str(e) == \"manually thrown error.\"\n\napp = build_pregel(max_attempts=3)\nresult = app.invoke(input={\"start\": None})\nassert result[\"output\"] == \"Success\"\n</code></pre>\n<h2 id=\"5-结果缓存\">5. 结果缓存</h2>\n<p>如果Node绑定一个相对耗时的计算，并且结果完全由给定的输入决定，那么针对输入对结果予以缓存无疑是改善时延的好办法。基于结果的缓存可以通过PregelNode 的<code>cache_policy</code>字段返回的缓存策略来控制。缓存策略通过<code>CachePolicy</code>类型表示，它具有<code>key_func</code>和<code>ttl</code>两个字段，前者提供一个用于解析缓存键（字符串或者字节数组）的可执行对象，后者用于设置缓存过期时间（如果没有显示设置，意味着永不过期）。</p>\n<pre><code class=\"language-python\">class PregelNode:    \n    cache_policy : CachePolicy | None\n    @cached_property\ndef input_cache_key(self) -&gt; INPUT_CACHE_KEY_TYPE\t\n\n@dataclass(**_DC_KWARGS)\nclass CachePolicy(Generic[KeyFuncT]):\n    key_func: KeyFuncT = default_cache_key  \nttl: int | None = None\n\nKeyFuncT = TypeVar(\"KeyFuncT\", bound=Callable[..., str | bytes])\nINPUT_CACHE_KEY_TYPE = tuple[Callable[..., Any], tuple[str, ...]]\n\ndef default_cache_key(*args: Any, **kwargs: Any) -&gt; str | bytes:\n    return pickle.dumps((_freeze(args), _freeze(kwargs)), protocol=5, fix_imports=False)\n</code></pre>\n<p>对结果实施缓存的前提是需要将输入的“指纹”作为缓存键，这里的缓存键根据通过<code>INPUT_CACHE_KEY_TYPE</code>定义的二元组进行计算，该二元组前半部分提供的可执行对象相当于一个哈希函数，能够将原始内容转换成“指纹”；后者提供的多元组以路径的方式唯一标识当前的节点。CachePolicy的key_func字段对应的可执行对象将会作为INPUT_CACHE_KEY_TYPE二元组的前半部分，从定义可以看出默认设置的<code>default_cache_key</code>函数会利用<code>pickle</code>以序列化的方式将原始输入转换成字节作为指纹。该指纹和二元组后半部分合并称为Node执行结果缓存项的Key。</p>\n<p>这里之所以需要强制使用字符串或者字节来表示缓存键，是因为Pregel针对缓存的实现并不限于内存存储，原则上可以与任意的内存数据库进行整合（比如redis）。缓存存储在Pregel中通过如下这个抽象基类BaseCache表示，它定义了一系列的抽象方法完成针对缓存的读取、写入和清除，我们可以通过派生此基类实现自定义的缓存存储。开发测试阶段我们经常会使用基于内存存储的InMemoryCache。如果没有对Node的缓存策略作针对性设置，在Pregel对象上设置的缓存策略将作为兜底。</p>\n<pre><code class=\"language-python\">class BaseCache(ABC, Generic[ValueT]):\n    serde: SerializerProtocol = JsonPlusSerializer(pickle_fallback=True)\n\n    def __init__(self, *, serde: SerializerProtocol | None = None) -&gt; None:\n        self.serde = serde or self.serde\n\n    @abstractmethod\n    def get(self, keys: Sequence[FullKey]) -&gt; dict[FullKey, ValueT]:\n\n    @abstractmethod\n    async def aget(self, keys: Sequence[FullKey]) -&gt; dict[FullKey, ValueT]:\n\n    @abstractmethod\n    def set(self, pairs: Mapping[FullKey, tuple[ValueT, int | None]]) -&gt; None:\n\n    @abstractmethod\n    async def aset(self, pairs: Mapping[FullKey, tuple[ValueT, int | None]]) -&gt; None:\n\n    @abstractmethod\n    def clear(self, namespaces: Sequence[Namespace] | None = None) -&gt; None:\n\n    @abstractmethod\nasync def aclear(self, namespaces: Sequence[Namespace] | None = None) -&gt; None:\n\nclass Pregel(\n    PregelProtocol[StateT, ContextT, InputT, OutputT],\n    Generic[StateT, ContextT, InputT, OutputT]): \n    cache: BaseCache | None = None\n    cache_policy: CachePolicy | None = None\n</code></pre>\n<p>在如下所示的演示程序中，对于创建的Pregel的唯一Node，它虽然具有两个输入Channel（foo和bar），但是对应的处理函数会将当前时间戳作为返回结果。我们为该Node设置了缓存策略，并将过期时间设置为30秒。</p>\n<pre><code class=\"language-python\">from langgraph.channels import LastValue\nfrom langgraph.types import CachePolicy\nfrom langgraph.pregel import Pregel,NodeBuilder\nimport datetime,time\nfrom langgraph.cache.memory import InMemoryCache \n\nnode = (NodeBuilder()\n        .subscribe_to(\"foo\",\"bar\")\n        .do(lambda _: datetime.datetime.now())\n        .write_to(\"output\")).build()\nnode.cache_policy = CachePolicy(ttl=30)\napp = Pregel(\n    nodes={\"body\": node},\n    cache=InMemoryCache(),\n    channels={\n        \"foo\": LastValue(str),\n        \"bar\": LastValue(str),\n        \"output\": LastValue(str),\n    },\n    input_channels=[\"foo\",\"bar\"],\n    output_channels=[\"output\"])\n\ninput = {\"foo\":\"abc\", \"bar\":\"xyz\"}\nresult = app.invoke(input=input)\nprint(f\"[{datetime.datetime.now()}]{input} -&gt; {result['output']}\")\n\ntime.sleep(5)\nresult = app.invoke(input=input)\nprint(f\"[{datetime.datetime.now()}]{input} -&gt; {result['output']}\")\n\ntime.sleep(5)\ninput = {\"foo\":\"xyz\", \"bar\":\"abc\"}\nresult = app.invoke(input=input)\nprint(f\"[{datetime.datetime.now()}]{input} -&gt; {result['output']}\")\n</code></pre>\n<p>我们以5秒为间隔调用了Pregel对象三次，前两次使用相同的输入（{\"foo\":\"abc\", \"bar\":\"xyz\"}）。三次调用的时间戳和输入输出会以如下的形式打印出来，我们可以清晰地看到前两次由于提供了相同的参数，所以得到了相同的结果，很明显第二次得到的是缓存的结果。</p>\n<pre><code>[2026-01-31 23:51:20.178285]{'foo': 'abc', 'bar': 'xyz'} -&gt; 2026-01-31 23:51:20.177192\n[2026-01-31 23:51:25.180527]{'foo': 'abc', 'bar': 'xyz'} -&gt; 2026-01-31 23:51:20.177192\n[2026-01-31 23:51:30.183805]{'foo': 'xyz', 'bar': 'abc'} -&gt; 2026-01-31 23:51:30.182490\n</code></pre>\n<h2 id=\"6补遗\">6.补遗</h2>\n<p>前面已经介绍了PregelNode类型的大部分核心成员，对于如下几个遗漏的成员，我们在这里作一下概况性介绍。tags使我们可以在Node上打上相应的标签，而metadata则可以在它上面附加任意的元数据。</p>\n<pre><code class=\"language-python\">class PregelNode:        \n    tags : Sequence[str] | None\n    metadata : Mapping[str, Any] | None\t\n    subgraphs : Sequence[PregelProtocol]    \n\n    def copy(self, update: dict[str, Any]) -&gt; PregelNode    \n    def invoke(\n        self,\n        input: Any,\n        config: RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) -&gt; Any    \n    async def ainvoke(\n        self,\n        input: Any,\n        config: RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) -&gt; Any\n    def stream(\n        self,\n        input: Any,\n        config: RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) -&gt; Iterator[Any]\n    async def astream(\n        self,\n        input: Any,\n        config: RunnableConfig | None = None,\n        **kwargs: Any | None,\n    ) -&gt; AsyncIterator[Any]\t\n</code></pre>\n<p>Node结合边构成了图，而图本身也可以作为一个Node参与构建一个更大的图，所以图具有一个嵌套的层级结构，一个Node可以包含一组子图，体现在<code>subgraphs</code>字段上。方法copy对返回自身的一个浅拷贝，至于四个方法（<code>invoke</code>、<code>ainvoke</code>、<code>stream</code>和<code>astream</code>）实现的两种调用模式，最终还是通过调用bound字段的Runnable对象的同名方法实现的。</p>\n<p>我们最后使用最简单的语言对Pregel做一个总结：我们可以将 PregelNode 想象成一个智能反应堆，其中<code>triggers</code>是点火装置（决定什么时候开始），<code>channels</code>是原料管道（输入数据），<code>mapper</code> 是入料加工（数据预处理），<code>bound</code> 是核心反应室（业务逻辑），<code>writers</code> 是成品输送带（更新状态）。</p>\n<h2 id=\"7-nodebuilder\">7. NodeBuilder</h2>\n<p>为了让大家对表示Node的PregelNode类型有深入地理解，在前面的演示中我们大都采用直接对其字段进行设置的方式，实际上在真正的开发中基本不会这么做，而是选择使用<code>NodeBuilder</code>来构建它，后者提供更加精简的API。</p>\n<pre><code class=\"language-python\">class NodeBuilder:\n    def subscribe_only(\n        self,\n        channel: str,\n    ) -&gt; Self\n    def subscribe_to(\n        self,\n        *channels: str,\n        read: bool = True,\n    ) -&gt; Self\n    def read_from(\n        self,\n        *channels: str,\n    ) -&gt; Self\n    def write_to(\n        self,\n        *channels: str | ChannelWriteEntry,\n        **kwargs: _WriteValue,\n    ) -&gt; Self\n    def meta(self, *tags: str, **metadata: Any) -&gt; Self \n    def add_retry_policies(self, *policies: RetryPolicy) -&gt; Self\n    def add_cache_policy(self, policy: CachePolicy) -&gt; Self \n    def build(self) -&gt; PregelNode \n</code></pre>\n<p>如果构建的Node只需定义一个Channel，我们可以调用<code>subscribe_only</code>方法，它将以字符串的（不是列表）形式赋值给channels字段。<code>subscribe_to</code>方法默认会将指定的Channel同时添加到<code>triggers</code>和<code>channels</code>列表中，如果将read参数设置为False，指定的Channel只会作为输入添加到channels列表中。read_from方法指定的仅仅是输入Channel，所以只会添加到channels列表中。</p>\n<p>如果自行构建PregelNode，针对Channel的输出会很麻烦，使用NodeBuilder的<code>write_to</code>方法就简单多了，我们只需要指定输出Channel的名称列表就可以了。如果需要多输出作更细粒度的控制，也可以指定一组<code>ChannelWriteEntry</code>对象。我们也可以利用write_to方法提供的关键字参数针对Channel的写入（此时参数名会作为输出Channel的名称），其类型_WriteValue定义如下，所以我们可以使用兼容的Lambda表达式简单快捷地完成输出。</p>\n<pre><code class=\"language-python\">_WriteValue = Callable[[Input], Output] | Any\n</code></pre>\n<p>前面介绍的打标签和附加元数据的功能可以调用NodeBuilder的<code>meta</code>方法来完成，失败重试和结果缓存策略则由<code>add_retry_policies</code>和<code>add_cache_policy</code>方法来提供。等所有设置完成之后，我们直接调用<code>build</code>方法将目标Node构建出来。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-15 07:37</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jaydenai\">JaydenAI</a>&nbsp;\n阅读(<span id=\"post_view_count\">9</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}