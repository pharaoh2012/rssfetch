{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "没有前端后，我把 MCP 做进了 Chats 1.7.0 AI 网关",
      "link": "https://www.cnblogs.com/sdcb/p/19489261/20260115-chats-170",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/sdcb/p/19489261/20260115-chats-170\" id=\"cb_post_title_url\" title=\"发布于 2026-01-16 08:45\">\n    <span>没有前端后，我把 MCP 做进了 Chats 1.7.0 AI 网关</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>这是一篇补档文章。</p>\n<p>如果你还不了解 <strong>Sdcb Chats</strong>：简单说，这是一个支持 20+ 主流模型服务商的 AI 网关。它不只能让你在一个统一界面里聚合管理所有模型，同时也兼容标准 API 协议，支持 Docker 一键部署。</p>\n<p>现在回头看，Sdcb Chats 最新版本已经到了 1.10，后续又融合了交错思考、Code Interpreter、多模态和企业级权限等“看起来更酷”的能力。但如果要问我：<strong>哪个版本是我个人开发节奏的分水岭？</strong>答案大概率还是 1.7.0。</p>\n<p>因为 1.7.0 不只是“加了个功能”，而是有三件事同时发生：</p>\n<ul>\n<li>MCP（Model Context Protocol）在 Chats 里真正落地，终于从“能聊”走向“能调用工具”，理论上能支持任何符合 MCP 的模型服务商和工具服务商，比如知识库、搜索引擎、计算引擎等；</li>\n<li>数据模型与数据库大改（破坏性变更），为后续演进把地基打牢；</li>\n<li>更关键的是：从这个版本开始，Chats 基本变成我一个人维护了——而我第一次深度尝试了 AI 的“氛围编程（Vibe Coding）”。</li>\n</ul>\n<hr />\n<h2 id=\"三个月空窗没有前端的我第一次把ai当同事\">三个月空窗：没有前端的我，第一次把AI当同事</h2>\n<p>距离上次 1.6 正式发布过去了 3 个多月。这期间，和我搭档做 Chats 前端的朋友因为有事没办法继续参与开发。没有前端开发，我一个后端在 Next.js / React 这套体系里，生产力几乎直接归零——项目一度陷入停滞。</p>\n<p>于是我第一次认真尝试把 AI 当作“副驾驶”：从页面布局、状态管理、组件拆分，到各种奇怪的 UI 边角行为（尤其是流式输出和工具调用展示），都让 AI 一起参与。</p>\n<p>可以这样说：</p>\n<ul>\n<li>Chats 1.7 之前：基本还是人类一行代码一行代码撸上去的；</li>\n<li>Chats 1.7.0 起：我开始“系统性”地 Vibe Coding，<strong>尤其是在我并不熟的 React 上，生产力提升非常明显</strong>；</li>\n<li>也从这时起，Chats 的维护者（几乎）变成了我一个人。</li>\n</ul>\n<p>所以这篇文章标题里写“感谢 AI”，不是客套，是事实。</p>\n<hr />\n<h2 id=\"170-的核心mcp-协议全面落地\">1.7.0 的核心：MCP 协议全面落地</h2>\n<p>如果你把 Chats 只当成一个“统一模型网关 + 漂亮 UI”的聊天前端，那它的上限就只是“把模型回答展示出来”。但 MCP 的出现，让“模型能做事”有了更统一、更可组合的方式。</p>\n<p>在 1.7.0 里，MCP 的落地不是停留在“能连上”，而是把整条链路打通了：</p>\n<ul>\n<li>\n<p>后端有完整的 MCP 实体与权限关系（Server、Tool、User 授权、Chat 绑定）；</p>\n</li>\n<li>\n<p>前端设置页新增 MCP 管理：新增/编辑 Server、抓取工具、分配用户；<br />\n<img alt=\"02-mcp-list\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260115213720114-1699629549.png\" /><br />\n<img alt=\"03-mcp-edit\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260115213726314-440296231.png\" /><br />\n<img alt=\"04-mcp-assign-user\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260115213729222-1808157068.png\" /></p>\n</li>\n<li>\n<p>会话侧可绑定多个 MCP Server，并在会话前校验当前用户权限；</p>\n</li>\n<li>\n<p>工具调用全程走流式输出，参数与结果能以结构化方式进入消息内容，前端也能更好地可视化展示。<br />\n<img alt=\"05-mcp-call\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260115213734256-659322865.png\" /></p>\n</li>\n</ul>\n<p>对我而言，这意味着 Chats 从“聊天 UI”升级成了“工具编排平台”的雏形：你可以给不同的 Chat Span 配置不同的工具集合，让它们在同一套对话体验里发挥作用。</p>\n<hr />\n<h2 id=\"工具调用体验不只是能用而是要看得懂\">工具调用体验：不只是能用，而是要“看得懂”</h2>\n<p>做过工具调用的人都知道：<strong>能调用是一回事，让用户看懂发生了什么是另一回事。</strong></p>\n<p>1.7.0 在工具调用的事件与消息结构上做了比较大的增强：SSE 事件更丰富、消息内容里新增了工具请求/响应的类型，前端能把“调用了什么工具、传了什么参数、拿到了什么结果”以更清晰的方式展示出来。<br />\n<img alt=\"06-sse-response-line\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260115213740193-1073025986.png\" /></p>\n<p>这件事看起来偏“体验”，但它会直接影响你是否愿意在真实业务里用工具调用：当工具一多、调用链一长，如果 UI 只是一坨 Markdown 混在一起，那基本等于不可用。</p>\n<hr />\n<h2 id=\"破坏性变更数据库与数据模型的大规模重构\">破坏性变更：数据库与数据模型的大规模重构</h2>\n<p>1.7.0 还有一个绕不开的关键词：<strong>破坏性变更</strong>。</p>\n<p>为了提升可维护性与可观测性，我在这个版本里对消息存储层做了重构（比如把 <code>Message</code> 拆分为 <code>ChatTurn/Step</code> 的分层结构），同时还伴随了用量关联、默认值约束、排序字段等一系列调整。<br />\n<img alt=\"07-db-schema\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260115213746807-893994786.png\" /></p>\n<p>这种重构的特点是：你短期会痛一次，但长期会省很多命。尤其是当你后面要持续叠加“推理/工具/多模态/审计/性能统计”这些能力时，底层结构是否清晰，决定了你是在“继续写功能”，还是在“每加一个功能都要拆一次墙”。</p>\n<hr />\n<h2 id=\"一些我很在意的细节改进\">一些我很在意的细节改进</h2>\n<p>除了 MCP 和数据库重构，1.7.0 还把不少“用起来会爽一点”的点补齐了，比如：</p>\n<ul>\n<li>模型/密钥/预设支持拖拽排序（Provider/Key/Model 的组织方式更清晰）；<br />\n<img alt=\"\" src=\"https://io.starworks.cc:88/cv-public/2026/08-drag-reordering.avif\" /></li>\n<li>聊天再生成能力增强：单条重新生成、从某条用户消息开始重新生成整段；<br />\n<img alt=\"\" src=\"https://io.starworks.cc:88/cv-public/2026/09-regenerate.avif\" /></li>\n<li>Markdown Mermaid 渲染升级：暗/亮主题适配、全屏查看、流式友好；<br />\n<img alt=\"\" src=\"https://io.starworks.cc:88/cv-public/2026/10-mermaid.avif\" /></li>\n<li>图片生成尺寸控制：在会话中指定常用尺寸；</li>\n<li>OpenAI 兼容与第三方联调增强（工具调用适配修复、登录兼容优化等）。<br />\n<img alt=\"11-image-size-control\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260115214018456-481361818.png\" /><br />\n这些看起来零碎，但它们共同指向同一个目标：<strong>把 Chats 从“功能堆叠”推向“可长期使用的产品质感”。</strong></li>\n</ul>\n<hr />\n<h2 id=\"升级与数据迁移不支持自动迁移请手动跑-sql只提供-sql-server\">升级与数据迁移：不支持自动迁移，请手动跑 SQL（只提供 SQL Server）</h2>\n<p>Sdcb Chats的数据库变更 <strong>不支持自动数据迁移</strong>。升级时你需要<strong>手动执行 SQL 迁移脚本</strong>，并且目前只提供了 <strong>SQL Server</strong> 的迁移脚本：</p>\n<ul>\n<li>1.7.0 迁移脚本：<code>src/scripts/db-migration/1.7/20250516-mcp.sql</code></li>\n</ul>\n<p>基本步骤也很朴素：</p>\n<ol>\n<li>先备份数据库；</li>\n<li>在 SQL Server 上执行上面的迁移脚本；</li>\n</ol>\n<p>如果你用的是 SQLite 或 Postgres……我建议你像我一样：把 SQL 甩给 AI，让它帮你改成 SQLite/Postgres 版本，然后你再一边跑一边修，或者如果你能接受，先删库，Chats 会在第一次启动时自动创建新表结构。</p>\n<hr />\n<h2 id=\"致谢\">致谢</h2>\n<p>1.7.0 的发布说明里，我特别感谢过社区贡献（比如<a href=\"https://github.com/sdcb/chats/pull/96\" rel=\"noopener nofollow\" target=\"_blank\">修复登录页面运行时错误的 PR #96</a>）。而在这篇补档里，我还想加一个更个人的致谢：感谢 AI。</p>\n<hr />\n<p>感谢阅读！喜欢的朋友请给我的Github项目一个star：<a href=\"https://github.com/sdcb/chats\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/sdcb/chats</a><br />\n有什么想法也欢迎在评论区留言交流，也欢迎加入我的 <strong>Chats QQ群：498452653</strong>，我们一起探索更多AI技术硬核玩法。</p>\n<p>微信群：<img alt=\"\" src=\"https://io.starworks.cc:88/cv-public/2026/chats-wxg-qr.png\" /></p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-16 08:45</span>&nbsp;\n<a href=\"https://www.cnblogs.com/sdcb\">.NET骚操作</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "别再手写 URL 解析器了：PHP 8.5 URI 扩展让 URL 处理更安全、更干净",
      "link": "https://www.cnblogs.com/catchadmin/p/19490082",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/catchadmin/p/19490082\" id=\"cb_post_title_url\" title=\"发布于 2026-01-16 07:48\">\n    <span>别再手写 URL 解析器了：PHP 8.5 URI 扩展让 URL 处理更安全、更干净</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"别再手写-url-解析器了php-85-uri-扩展让-url-处理更安全更干净\">别再手写 URL 解析器了：PHP 8.5 URI 扩展让 URL 处理更安全、更干净</h1>\n<h2 id=\"parse_url-能用但不够用\">parse_url() 能用，但不够用</h2>\n<p>多年来，PHP 开发者处理 URL 的方式大同小异：</p>\n<ul>\n<li>用 <code>parse_url()</code> 拆分各部分</li>\n<li>用 <code>rawurlencode()</code> / <code>urlencode()</code> 转义</li>\n<li>用字符串拼接重建最终 URL</li>\n<li>遇到\"奇怪\"输入时，再上几个正则</li>\n</ul>\n<p>大多数情况下这套流程能跑通。问题在于，URL 处理恰恰是那种\"在边角情况下出问题\"的领域：编码、fragment、userinfo、国际化域名、\"等价但不相同\"的 URL，以及各种只在生产日志里才冒出来的边缘场景。</p>\n<p>PHP 8.5 提供了一个内置替代方案：一个始终可用的 URI 扩展，提供 API 来按照 RFC 3986 和 WHATWG URL 标准解析、修改 URL/URI。</p>\n<p>如果\"标准\"二字让你觉得抽象，这里给出实际含义：</p>\n<ul>\n<li>拿到 URI/URL 对象，而不是数组 + 字符串拼接。</li>\n<li>用安全的组件 getter 和不可变的 <code>with*()</code> 方法。</li>\n<li>你可以选择 RFC 3986 行为（严格 URI，\"原始 vs 规范化解码\"）或浏览器风格的 WHATWG 行为（Unicode/IDNA、软错误、自动编码）。</li>\n</ul>\n<p>本文是一篇实战教程，重点讲：</p>\n<ul>\n<li>为什么手动解析容易出错，</li>\n<li>新 URI 对象怎么用，</li>\n<li>如何安全地修改/规范化，</li>\n<li>在重定向、签名链接等安全敏感场景下如何使用。</li>\n</ul>\n<p>不讲升级指南，不讲废弃清单——只讲 URI 扩展。</p>\n<p><a href=\"https://catchadmin.com/post/2026-01/php85-uri-extension-safer-cleaner-urls\" rel=\"noopener nofollow\" target=\"_blank\">原文 别再手写 URL 解析器了：PHP 8.5 URI 扩展让 URL 处理更安全、更干净</a></p>\n<h2 id=\"手动-url-解析的问题\">手动 URL 解析的问题</h2>\n<h3 id=\"parse_url-不解码而且很容易忘\">parse_url() 不解码（而且很容易忘）</h3>\n<p>PHP 的 <code>parse_url()</code> 返回各组件，但<strong>不会</strong> URL 解码它们。</p>\n<p>也就是说：</p>\n<pre><code class=\"language-php\">$u = parse_url(\"https://example.com/t%65st?name=Ali%63e#fr%61g\");\nvar_dump($u['path']);   // \"/t%65st\"\nvar_dump($u['query']);  // \"name=Ali%63e\"\nvar_dump($u['fragment']); // \"fr%61g\"\n</code></pre>\n<p>如果你在比较路径或应用路由规则时没有统一解码/规范化，可能会把等价的 URI 当成不同的来处理。</p>\n<p>更糟的是：团队往往混用：</p>\n<ul>\n<li>有些地方用解码后的值，</li>\n<li>有些地方用原始值，</li>\n<li>再加上散落在各处的临时解码逻辑。</li>\n</ul>\n<p>这种混乱很容易埋下隐蔽 bug 和安全隐患。</p>\n<h3 id=\"字符串拼接容易拼出差一点对的-url\">字符串拼接容易拼出\"差一点对\"的 URL</h3>\n<p>手动重建 URL 容易犯的错：</p>\n<ul>\n<li>漏掉 <code>?</code> 或 <code>#</code></li>\n<li>重复编码</li>\n<li>完全没编码</li>\n<li>编码错了东西（比如把整个 query string 编码而不是只编码 value）</li>\n<li>丢失或打乱参数顺序</li>\n<li>空 query/fragment 处理不当</li>\n</ul>\n<p>一个常见的\"差一点对\"函数：</p>\n<pre><code class=\"language-php\">function addQueryParam(string $url, string $key, string $value): string\n{\n    $parts = parse_url($url);\n    $query = $parts['query'] ?? '';\n    $query .= ($query === '' ? '' : '&amp;') . $key . '=' . urlencode($value);\n    $out = $parts['scheme'] . '://' . $parts['host'] . ($parts['path'] ?? '');\n    if ($query !== '') {\n        $out .= '?' . $query;\n    }\n    if (isset($parts['fragment'])) {\n        $out .= '#' . $parts['fragment'];\n    }\n    return $out;\n}\n</code></pre>\n<p>看起来没问题。直到遇到：</p>\n<ul>\n<li>没有 scheme/host 的 URL（相对 URL），</li>\n<li>带 userinfo/port 的 URL，</li>\n<li>已经编码过的值，</li>\n<li>需要 <code>rawurlencode()</code> 规则（RFC 3986）的参数，</li>\n<li>应该原样保留的 fragment。</li>\n</ul>\n<h3 id=\"等价的-url-不一定是相同的字符串\">等价的 URL 不一定是相同的字符串</h3>\n<p>下面这些可以指向同一个资源，但字符串不同：</p>\n<ul>\n<li>scheme/host 大小写不同（<code>HTTPS://EXAMPLE.com</code>）</li>\n<li>path 中 <code>%65</code> vs <code>e</code>（<code>/t%65st</code> vs <code>/test</code>）</li>\n<li>path 中的点号段（<code>/foo/../bar/</code>）</li>\n<li>默认端口（https 的 <code>:443</code>）</li>\n</ul>\n<p>如果你把 URL 当成纯字符串处理，要么：</p>\n<ul>\n<li>缓存/路由出现诡异问题，</li>\n<li>要么安全检查被绕过，因为你比较的是\"错误的表示形式\"。</li>\n</ul>\n<h3 id=\"idna国际化域名还涉及安全问题\">IDNA（国际化域名）还涉及安全问题</h3>\n<p>如果你允许用户提交 URL，国际化域名可能是合法的——但也可能造成混淆。域名可以用 Unicode 或 punycode（ASCII 形式）表示。RFC 讨论中明确指出人为风险：punycode 域名在 Unicode 渲染时可能看起来像一个熟悉的、但实际不同的域名。</p>\n<p>这不是你想用正则\"手动处理\"然后祈祷没问题的事。</p>\n<h2 id=\"uri-扩展的概念uri-对象--两套标准\">URI 扩展的概念：URI 对象 + 两套标准</h2>\n<p>PHP 8.5 的 URI 扩展提供两个主要类：</p>\n<ul>\n<li><code>Uri\\Rfc3986\\Uri</code>（RFC 3986 合规，严格 URI 规则）</li>\n<li><code>Uri\\WhatWg\\Url</code>（WHATWG URL 合规，浏览器风格的 URL 规则）</li>\n</ul>\n<p>配套的类型包括：</p>\n<ul>\n<li><code>Uri\\InvalidUriException</code></li>\n<li><code>Uri\\WhatWg\\InvalidUrlException</code></li>\n<li><code>Uri\\WhatWg\\UrlValidationError</code> 和 <code>UrlValidationErrorType</code></li>\n<li><code>Uri\\UriComparisonMode</code>（用于比较）</li>\n</ul>\n<p>一个关键设计决策：两个实现都是 readonly 且以不可变方式使用——<code>withPath()</code>、<code>withQuery()</code> 等方法返回新实例。</p>\n<p>另一个要点：这个扩展在 PHP 8.5 中始终可用，底层由以下库驱动：</p>\n<ul>\n<li>uriparser（用于 RFC 3986）</li>\n<li>Lexbor（用于 WHATWG URL）</li>\n</ul>\n<p>所以你不需要第三方库就能获得合理的 URL 处理能力。</p>\n<h2 id=\"创建和解析-uri从字符串到组件\">创建和解析 URI：从字符串到组件</h2>\n<h3 id=\"最简单的情况解析一个-http-url-并访问各部分\">最简单的情况：解析一个 HTTP URL 并访问各部分</h3>\n<p>RFC 3986：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\n\n$uri = new Uri(\"https://php.net/releases/8.5/en.php\");\n\necho $uri-&gt;getScheme(); // \"https\"\necho $uri-&gt;getHost();   // \"php.net\"\necho $uri-&gt;getPath();   // \"/releases/8.5/en.php\"\n</code></pre>\n<p>WHATWG：</p>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\n\n$url = new Url(\"https://example.com/path?query=1#frag\");\n\necho $url-&gt;getScheme();      // \"https\"\necho $url-&gt;getAsciiHost();   // \"example.com\"\necho $url-&gt;getPath();        // \"/path\"\necho $url-&gt;getQuery();       // \"query=1\"\necho $url-&gt;getFragment();    // \"frag\"\n</code></pre>\n<p>注意：WHATWG 的 getter 故意不返回分隔符（如 <code>:</code> <code>/</code> <code>?</code> <code>#</code>）。</p>\n<h3 id=\"构造函数抛异常parse-返回-null\">构造函数抛异常；parse() 返回 null</h3>\n<p>两个实现都支持两种解析风格：</p>\n<ul>\n<li>构造函数：无效时抛异常</li>\n<li><code>parse()</code>：无效时返回 null</li>\n</ul>\n<p>RFC 3986 行为：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\nuse Uri\\InvalidUriException;\n\ntry {\n    $uri = new Uri(\"not a uri\");\n} catch (InvalidUriException $e) {\n    // 拒绝输入\n}\n\n$uri = Uri::parse(\"not a uri\");\nvar_dump($uri); // null\n</code></pre>\n<p>WHATWG 行为提供更丰富的错误信息：</p>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\nuse Uri\\WhatWg\\InvalidUrlException;\n\ntry {\n    $url = new Url(\"invalid url\");\n} catch (InvalidUrlException $e) {\n    // $e-&gt;errors 包含 UrlValidationError 实例\n}\n\n$errors = [];\n$url = Url::parse(\"invalid url\", null, $errors);\nvar_dump($url);    // null\nvar_dump($errors); // UrlValidationError 数组\n</code></pre>\n<p>这种区分（硬错误 vs 软错误 vs parse-and-return-null）在 RFC 中有详细说明。</p>\n<h3 id=\"base-url-和引用解析相对--绝对\">Base URL 和引用解析（相对 → 绝对）</h3>\n<p>有了这个功能，你不再需要手动检查是否以 <code>/</code> 开头然后拼接。</p>\n<p>在构造函数或 <code>parse()</code> 中使用 base URL：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\n\n$base = new Uri(\"https://example.com\");\n$uri  = new Uri(\"/foo\", $base);\n\necho $uri-&gt;toString(); // \"https://example.com/foo\"\n</code></pre>\n<p>WHATWG 类似：</p>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\n\n$base = new Url(\"https://example.com\");\n$url  = Url::parse(\"/foo\", $base);\n\necho $url-&gt;toAsciiString(); // \"https://example.com/foo\"\n</code></pre>\n<p>扩展还提供了便捷的 <code>resolve()</code> 方法，以当前对象作为 base。</p>\n<h2 id=\"安全修改设置替换-pathqueryfragment不可变\">安全修改：设置/替换 path、query、fragment（不可变）</h2>\n<p>拿到对象后，可以用 <code>with*()</code> 方法修改组件。这些方法返回新实例，原对象保持不变。</p>\n<h3 id=\"rfc-3986withpathwithquerywithfragment\">RFC 3986：withPath()、withQuery()、withFragment()</h3>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\n\n$uri = new Uri(\"https://example.com/products?sort=asc#top\");\n\n$updated = $uri\n    -&gt;withPath(\"/products/123\")\n    -&gt;withQuery(\"sort=desc&amp;ref=home\")\n    -&gt;withFragment(\"reviews\");\n\necho $uri-&gt;toString();     // 原对象不变\necho $updated-&gt;toString(); // 修改后的\n</code></pre>\n<p>RFC 3986 提供\"原始\"和\"规范化解码\"两种 getter（下一节详述），以及 <code>toString()</code> 和 <code>toRawString()</code> 方法。</p>\n<h3 id=\"whatwg同样的思路外加-asciiunicode-字符串输出\">WHATWG：同样的思路，外加 ASCII/Unicode 字符串输出</h3>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\n\n$url = new Url(\"https://example.com/\");\n\n$new = $url\n    -&gt;withPath(\"/search\")\n    -&gt;withQuery(\"q=php+8.5\")\n    -&gt;withFragment(\"results\");\n\necho $new-&gt;toAsciiString();\necho $new-&gt;toUnicodeString();\n</code></pre>\n<p>WHATWG 有 <code>toAsciiString()</code> 和 <code>toUnicodeString()</code> 两种输出，分别用于机器处理和人类展示。</p>\n<h3 id=\"一个小但重要的行为输入中的分隔符\">一个小但重要的行为：输入中的分隔符</h3>\n<p>使用 WHATWG 时，如果你在设置 query/fragment 时不小心包含了 <code>?</code> 或 <code>#</code>，它会把它们当作分隔符并去掉：</p>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\n\n$url = new Url(\"https://example.com/\");\n$url = $url-&gt;withQuery(\"?foo\");\n$url = $url-&gt;withFragment(\"#bar\");\n\necho $url-&gt;getQuery();    // \"foo\"\necho $url-&gt;getFragment(); // \"bar\"\n</code></pre>\n<p>这个行为在文档中有明确说明。</p>\n<h2 id=\"规范化与编码避免重复编码和奇怪字符\">规范化与编码：避免重复编码和\"奇怪字符\"</h2>\n<p>URI 扩展的价值不止于 API 设计——它让你能控制 URL 的表示形式。</p>\n<h3 id=\"rfc-3986-给你两种表示原始-vs-规范化解码\">RFC 3986 给你两种表示：原始 vs 规范化解码</h3>\n<p>对于大多数组件，<code>Uri\\Rfc3986\\Uri</code> 暴露：</p>\n<ul>\n<li>raw：解析器给出的形式（最接近原始输入）</li>\n<li>规范化解码：规范化 + 百分号解码，意在成为规范形式且可往返转换</li>\n</ul>\n<p>RFC 解释了为什么需要两种形式以及何时使用——签名方和 API 客户端通常偏好 raw，而路由/缓存通常偏好规范化解码。</p>\n<p>具体效果：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\n\n$uri = new Uri(\"https://%61pple:p%61ss@ex%61mple.com:433/foob%61r?%61bc=%61bc#%61bc\");\n\necho $uri-&gt;getRawHost(); // \"ex%61mple.com\"\necho $uri-&gt;getHost();    // \"example.com\"\n\necho $uri-&gt;getRawPath(); // \"/foob%61r\"\necho $uri-&gt;getPath();    // \"/foobar\"\n\necho $uri-&gt;getRawQuery(); // \"%61bc=%61bc\"\necho $uri-&gt;getQuery();    // \"abc=abc\"\n</code></pre>\n<p>规范化示例：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\n\n$uri = new Uri(\"HTTPS://EXAMPLE.COM/foo/../bar/\");\n\necho $uri-&gt;getRawScheme(); // \"HTTPS\"\necho $uri-&gt;getScheme();    // \"https\"\n\necho $uri-&gt;getRawHost(); // \"EXAMPLE.COM\"\necho $uri-&gt;getHost();    // \"example.com\"\n\necho $uri-&gt;getRawPath(); // \"/foo/../bar/\"\necho $uri-&gt;getPath();    // \"/bar/\"\n</code></pre>\n<h3 id=\"whatwg-在修改时自动编码某些字符\">WHATWG 在修改时自动编码某些字符</h3>\n<p>如果你设置的 path 包含该组件必须百分号编码的字符，WHATWG 会自动编码：</p>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\n\n$url = new Url(\"https://example.com\");\n$url = $url-&gt;withPath(\"/?#:\");\n\necho $url-&gt;getPath(); // \"/%3F%23:\"\n</code></pre>\n<p>这能防止意外构建出损坏的 URL。</p>\n<h3 id=\"重复编码陷阱以及新-api-如何帮忙\">\"重复编码\"陷阱（以及新 API 如何帮忙）</h3>\n<p>重复编码通常这样发生：</p>\n<ol>\n<li>你用 <code>rawurlencode()</code> 编码一个值，因为\"它要放进 URL\"</li>\n<li>你手动把它加到 query string</li>\n<li>后来某处又编码了一次（框架、代理、客户端）</li>\n</ol>\n<p>使用 URI 对象，一个好做法是：</p>\n<ul>\n<li>数据保持为普通字符串（未编码）</li>\n<li>用 <code>http_build_query()</code> 构建 query（或你偏好的编码器）</li>\n<li>把结果传给 <code>withQuery()</code></li>\n</ul>\n<p>示例：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\n\n$base = new Uri(\"https://example.com/search\");\n\n$params = [\n    'q' =&gt; 'php 8.5 uri',\n    'tag' =&gt; 'url/encoding',\n];\n\n// RFC 3986 风格编码（空格用 %20 而非 +）\n$query = str_replace('+', '%20', http_build_query($params));\n\n$uri = $base-&gt;withQuery($query);\necho $uri-&gt;toString();\n</code></pre>\n<p><code>http_build_query()</code> 不一定适合所有场景，但好处是编码规则集中在一处，而非散落在各种字符串拼接里。</p>\n<p>如果确实需要对 path 段进行 RFC 3986 原始编码，PHP 的 <code>rawurlencode()</code> 遵循 RFC 3986 规则。</p>\n<h2 id=\"验证用户输入的-url实用的最低规则\">验证用户输入的 URL：实用的最低规则</h2>\n<p>如果输入来自用户（或不可信来源），验证应该有明确的立场。</p>\n<p>对于你打算 fetch 或跳转的 URL，一个务实的安全导向模式：</p>\n<ul>\n<li>必须解析成功（硬错误直接拒绝）</li>\n<li>只允许 http / https</li>\n<li>URL 中不能有用户名/密码</li>\n<li>域名白名单（用 ASCII 形式比较）</li>\n<li>可选：限制端口</li>\n<li>可选：要求绝对 URL（或相对 URL 基于已知 base 解析）</li>\n</ul>\n<h3 id=\"严格验证-helperwhatwg-版本\">严格验证 helper（WHATWG 版本）</h3>\n<p>WHATWG 适合处理\"浏览器风格\"URL 和 IDNA。</p>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\nuse Uri\\WhatWg\\InvalidUrlException;\n\nfunction validateExternalUrl(string $input, array $allowedHosts): Url\n{\n    try {\n        $softErrors = [];\n        $url = new Url($input, null, $softErrors);\n    } catch (InvalidUrlException $e) {\n        throw new InvalidArgumentException(\"Invalid URL.\");\n    }\n\n    // Scheme 白名单\n    $scheme = $url-&gt;getScheme();\n    if (!in_array($scheme, ['http', 'https'], true)) {\n        throw new InvalidArgumentException(\"Unsupported scheme.\");\n    }\n\n    // 禁止凭证\n    if ($url-&gt;getUsername() !== null || $url-&gt;getPassword() !== null) {\n        throw new InvalidArgumentException(\"Credentials in URL are not allowed.\");\n    }\n\n    // Host 白名单（ASCII 比较）\n    $host = $url-&gt;getAsciiHost();\n    if ($host === null || !in_array(strtolower($host), $allowedHosts, true)) {\n        throw new InvalidArgumentException(\"Host not allowed.\");\n    }\n\n    // 可选：端口限制\n    $port = $url-&gt;getPort();\n    if ($port !== null &amp;&amp; !in_array($port, [80, 443], true)) {\n        throw new InvalidArgumentException(\"Port not allowed.\");\n    }\n\n    return $url;\n}\n</code></pre>\n<p>几点说明：</p>\n<ul>\n<li><code>Url</code> 即使解析成功也可能返回软错误。RFC 解释了\"软 vs 硬\"模型，并给出了解析继续但报告验证错误的例子。</li>\n<li>WHATWG 同时暴露 <code>getAsciiHost()</code> 和 <code>getUnicodeHost()</code>；比较通常用 ASCII，展示用 Unicode。</li>\n</ul>\n<h3 id=\"rfc-3986-验证严格-uri-解析--规范化选项\">RFC 3986 验证：严格 URI 解析 + 规范化选项</h3>\n<p>如果你在验证通用 URI（不只是 HTTP URL），或者你关心 raw vs 规范化解码的表示，<code>Uri\\Rfc3986\\Uri</code> 是个好选择。</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\nuse Uri\\InvalidUriException;\n\nfunction validateHttpUri(string $input, array $allowedHosts): Uri\n{\n    try {\n        $uri = new Uri($input);\n    } catch (InvalidUriException $e) {\n        throw new InvalidArgumentException(\"Invalid URI.\");\n    }\n\n    $scheme = $uri-&gt;getScheme();\n    if (!in_array($scheme, ['http', 'https'], true)) {\n        throw new InvalidArgumentException(\"Unsupported scheme.\");\n    }\n\n    // Userinfo 在大多数 Web 应用中是个坑\n    if ($uri-&gt;getUserInfo() !== null) {\n        throw new InvalidArgumentException(\"Userinfo is not allowed.\");\n    }\n\n    $host = $uri-&gt;getHost();\n    if ($host === null || !in_array(strtolower($host), $allowedHosts, true)) {\n        throw new InvalidArgumentException(\"Host not allowed.\");\n    }\n\n    return $uri;\n}\n</code></pre>\n<h2 id=\"实际用例签名链接安全重定向规范-url\">实际用例：签名链接、安全重定向、规范 URL</h2>\n<h3 id=\"用例签名链接而不破坏编码\">用例：签名链接而不破坏编码</h3>\n<p>签名 URL（HMAC token、临时访问链接、CDN 认证）对表示形式极其敏感。一个小的\"规范化变更\"就能使签名失效。</p>\n<p>这也是 RFC 明确指出\"API 客户端或签名方\"通常偏好原始表示的原因。</p>\n<p>一个简单的签名 URL 方法：</p>\n<ol>\n<li>构建 URL</li>\n<li>对稳定的字符串表示计算签名</li>\n<li>把签名作为 query 参数加进去</li>\n</ol>\n<p>示例，使用 RFC 3986 和 <code>toRawString()</code> 签名：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\n\nfunction signUri(Uri $uri, string $secret): Uri\n{\n    // 当你想避免意外转换时，用 raw string\n    $baseString = $uri-&gt;toRawString();\n    $sig = hash_hmac('sha256', $baseString, $secret);\n\n    $query = $uri-&gt;getRawQuery();\n    $query = $query ? $query . '&amp;' : '';\n    $query .= 'sig=' . rawurlencode($sig);\n\n    return $uri-&gt;withQuery($query);\n}\n\n$uri = new Uri(\"https://download.example.com/file/%2Fsafe?expires=1736035200\");\n$signed = signUri($uri, \"super-secret-key\");\n\necho $signed-&gt;toRawString();\n</code></pre>\n<p>两个要点：</p>\n<ul>\n<li>你是故意选择 raw 的，因为签名关心的是\"线上实际发送的是什么\"。</li>\n<li>query 构建仍然要小心，避免编码错误。</li>\n</ul>\n<p>如果你的签名算法需要规范形式（有些确实需要），可以故意对 <code>toString()</code> 签名——但要知道自己在做什么，而不是意外这么做。</p>\n<h3 id=\"用例安全重定向避免开放重定向--解析混淆\">用例：安全重定向（避免开放重定向 + 解析混淆）</h3>\n<p>安全重定向问题通常长这样：</p>\n<ol>\n<li>你有 <code>/login?next=&lt;something&gt;</code></li>\n<li>登录后跳转到 next</li>\n<li>攻击者尝试 <code>next=https://evil.com</code> 或各种变体</li>\n</ol>\n<p>一个健壮的模式是：</p>\n<ol>\n<li>把用户输入解析为相对 URL 或绝对 URL</li>\n<li>相对 URL 基于你自己的已知 base 解析</li>\n<li>验证 host/scheme 是你的（或在允许列表中）</li>\n</ol>\n<p>使用 WHATWG：</p>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\n\nfunction safeRedirectTarget(string $next, string $appBase): string\n{\n    $base = new Url($appBase);\n\n    // 安全解析相对引用\n    $errors = [];\n    $resolved = Url::parse($next, $base, $errors);\n\n    if ($resolved === null) {\n        return $base-&gt;toAsciiString(); // fallback\n    }\n\n    // 只允许同 host\n    if (strtolower($resolved-&gt;getAsciiHost() ?? '') !== strtolower($base-&gt;getAsciiHost() ?? '')) {\n        return $base-&gt;toAsciiString();\n    }\n\n    // 只允许 http/https\n    if (!in_array($resolved-&gt;getScheme(), ['http', 'https'], true)) {\n        return $base-&gt;toAsciiString();\n    }\n\n    return $resolved-&gt;toAsciiString();\n}\n\necho safeRedirectTarget(\"/dashboard\", \"https://app.example.com\");\n</code></pre>\n<p>这里用到了 base URL 解析和引用解析功能。</p>\n<h3 id=\"用例规范-urlseo-和缓存的一致表示\">用例：规范 URL（SEO 和缓存的一致表示）</h3>\n<p>规范化通常意味着：</p>\n<ul>\n<li>scheme/host 小写</li>\n<li>移除 path 中的点号段</li>\n<li>移除默认端口</li>\n<li>query 保持一致（有时排序）</li>\n<li>决定如何处理末尾斜杠</li>\n</ul>\n<p>使用 <code>Uri\\Rfc3986\\Uri</code>，你已经可以通过 <code>get*()</code> vs <code>getRaw*()</code>（以及 <code>toString()</code> vs <code>toRawString()</code>）清晰地获得规范化行为。</p>\n<p>一个简单的规范化示例：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\n\nfunction canonicalizeForCache(Uri $uri): Uri\n{\n    // 从规范化解码的部分开始\n    $uri = $uri\n        -&gt;withScheme($uri-&gt;getScheme())\n        -&gt;withHost($uri-&gt;getHost())\n        -&gt;withPath($uri-&gt;getPath());\n\n    // 为缓存 key 排序 query 参数（这是一种策略选择）\n    $query = $uri-&gt;getQuery();\n    if ($query) {\n        parse_str($query, $params);\n        ksort($params);\n        $sorted = http_build_query($params, '', '&amp;', PHP_QUERY_RFC3986);\n        $uri = $uri-&gt;withQuery($sorted);\n    }\n\n    return $uri;\n}\n\n$u = new Uri(\"HTTPS://EXAMPLE.COM/foo/../bar/?b=2&amp;a=1\");\necho canonicalizeForCache($u)-&gt;toString();\n</code></pre>\n<p>这个函数不试图覆盖所有规范化策略（那是应用层面的事），但结构化 API 的好处是：规则可以显式构建、可以测试。</p>\n<h2 id=\"从-parse_url--字符串拼接迁移不痛苦\">从 parse_url() + 字符串拼接迁移（不痛苦）</h2>\n<p>你不需要一次性重写所有代码。最简单的迁移路径是：</p>\n<p><strong>1. 找出 URL 处理对安全敏感或容易出 bug 的地方：</strong></p>\n<ul>\n<li>重定向</li>\n<li>webhook 验证</li>\n<li>签名 URL</li>\n<li>域名白名单</li>\n<li>路由/缓存 key</li>\n</ul>\n<p><strong>2. 先替换这些。</strong></p>\n<p><strong>3. 纯展示用途的简单解析，等到有必要时再处理。</strong></p>\n<h3 id=\"迁移前数组解析--手动重建\">迁移前：数组解析 + 手动重建</h3>\n<pre><code class=\"language-php\">$parts = parse_url($input);\n$host  = $parts['host'] ?? null;\n\nif ($host !== 'example.com') {\n    throw new Exception(\"Bad host\");\n}\n\n$newUrl = $parts['scheme'] . '://' . $parts['host'] . '/new-path';\n</code></pre>\n<p>问题：</p>\n<ul>\n<li>没处理 port、userinfo、fragment、相对 URL</li>\n<li>没有规范化</li>\n<li>没有规范化选择</li>\n<li>容易构建出无效输出</li>\n</ul>\n<h3 id=\"迁移后使用-urluri改动局部化\">迁移后：使用 Url/Uri，改动局部化</h3>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\n\n$url = new Url($input);\n\nif (strtolower($url-&gt;getAsciiHost() ?? '') !== 'example.com') {\n    throw new InvalidArgumentException(\"Bad host\");\n}\n\n$new = $url-&gt;withPath('/new-path');\necho $new-&gt;toAsciiString();\n</code></pre>\n<p>代码更短，意图更明确，细节上更难出错。</p>\n<h3 id=\"rfc-3986-和-whatwg-怎么选经验法则\">RFC 3986 和 WHATWG 怎么选（经验法则）</h3>\n<p><strong>使用 <code>Uri\\WhatWg\\Url</code> 当：</strong></p>\n<ul>\n<li>你在处理来自浏览器/用户的 HTTP(S) URL</li>\n<li>你需要 IDNA/Unicode 域名处理（<code>getUnicodeHost()</code> 用于展示，<code>getAsciiHost()</code> 用于比较）</li>\n<li>你想要 WHATWG 解析行为和错误报告（软错误）</li>\n</ul>\n<p><strong>使用 <code>Uri\\Rfc3986\\Uri</code> 当：</strong></p>\n<ul>\n<li>你在处理\"Web URL\"之外的通用 URI</li>\n<li>你需要 raw vs 规范化解码的表示</li>\n<li>你在做签名或协议层面的工作，表示形式很重要</li>\n</ul>\n<p>RFC 明确区分了这两种方法，包括 Unicode/IDNA 支持的差异。</p>\n<h2 id=\"附加内容正确比较-urlequals-以及为什么没有-__tostring\">附加内容：正确比较 URL（equals() 以及为什么没有 __toString()）</h2>\n<p>一个值得留意的设计：内置 URI 类故意不实现 <code>__toString()</code>，因为松散比较（<code>==</code>）容易出错。</p>\n<p>取而代之，你用 <code>equals()</code> 比较：</p>\n<pre><code class=\"language-php\">use Uri\\Rfc3986\\Uri;\nuse Uri\\UriComparisonMode;\n\n$u1 = new Uri(\"https://example.COM#foo\");\n$u2 = new Uri(\"https://EXAMPLE.COM\");\n\nvar_dump($u1-&gt;equals($u2)); // true\nvar_dump($u1-&gt;equals($u2, UriComparisonMode::IncludeFragment)); // false\n</code></pre>\n<p>WHATWG 类似：</p>\n<pre><code class=\"language-php\">use Uri\\WhatWg\\Url;\nuse Uri\\UriComparisonMode;\n\n$a = new Url(\"https:////example.COM/\");\n$b = new Url(\"https://EXAMPLE.COM\");\n\nvar_dump($a-&gt;equals($b)); // true\n</code></pre>\n<h2 id=\"结论\">结论</h2>\n<p>PHP 8.5 的 URI 扩展看起来是个小功能，用一段时间后才会发现它挡掉了多少细微的 URL bug。</p>\n<p>核心价值在于<strong>控制</strong>：</p>\n<ul>\n<li>你可以选择 RFC 3986 vs WHATWG 语义。</li>\n<li>你可以决定要 raw 还是规范化解码的表示。</li>\n<li>你可以不可变且安全地修改组件。</li>\n<li>你可以用更难意外削弱的方式验证和比较 URL。</li>\n</ul>\n<p>如果你曾经发布过一个\"快速 URL 修复\"，后来在安全报告或生产事故中看到它，这个扩展值得尽早采用——从重定向和签名 URL 开始。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-16 07:48</span>&nbsp;\n<a href=\"https://www.cnblogs.com/catchadmin\">JaguarJack</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "MySQL性能优化：从底层原理到实战落地的全维度方案",
      "link": "https://www.cnblogs.com/liuziyi1/p/19489439",
      "published": "",
      "description": "<h2>\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/liuziyi1/p/19489439\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 22:26\">\n    <span>MySQL性能优化：从底层原理到实战落地的全维度方案</span>\n    \n\n</a>\n\n\t\t</h2>\n\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在数据驱动的业务场景中，MySQL作为主流开源关系型数据库，其性能直接决定系统响应速度、吞吐量与运维成本。尤其对于高并发、大数据量的平台（如DeepSeek这类AI服务场景），慢查询与不合理索引设计可能引发系统卡顿甚至雪崩。MySQL性能优化并非零散的“调参改SQL”，而是基于底层原理的系统性工程——既要掌握可落地的实战技巧，更要理解优化背后的核心逻辑，才能实现从“治标”到“治本”的突破。本文将融合底层理论与实战经验，构建“原理认知-问题定位-优化实施-工程保障”的完整体系，助力开发者实现MySQL性能的精准提升。</p>\n<h1 id=\"-一底层逻辑mysql性能的核心支撑与失衡本质\"># 一、底层逻辑：MySQL性能的核心支撑与失衡本质</h1>\n<p>MySQL性能的底层核心是“资源消耗与结构设计的平衡”，所有慢查询与性能瓶颈，本质都是存储结构、资源分配或执行逻辑出现了失衡。</p>\n<h2 id=\"-11-存储引擎核心b树与磁盘io的底层关联\">## 1.1 存储引擎核心：B+树与磁盘IO的底层关联</h2>\n<p>InnoDB作为MySQL默认存储引擎，其核心存储结构为B+树，性能优劣直接由“磁盘IO次数”决定。B+树的设计特性决定了查询效率的上限：</p>\n<ul>\n<li>\n<p>- 结构特性：B+树为平衡树，叶子节点存储全量数据，非叶子节点仅存储索引键与指针；单页大小默认16KB，高度通常为1-3层，高度3的B+树可存储约2000万行数据。</p>\n</li>\n<li>\n<p>- IO成本：每次查询的IO次数=B+树高度+回表次数（非覆盖索引场景）。全表扫描需遍历所有叶子节点，IO次数飙升至百万级，是慢查询的核心诱因。</p>\n</li>\n<li>\n<p>- 缓存价值：InnoDB缓冲池（innodb_buffer_pool）可缓存数据页与索引页，命中率理想值需超过99%，缓存命中可直接避免磁盘IO，大幅提升查询速度。</p>\n</li>\n</ul>\n<h2 id=\"-12-性能核心维度四大资源的消耗平衡\">## 1.2 性能核心维度：四大资源的消耗平衡</h2>\n<p>MySQL性能瓶颈最终可归结为CPU、磁盘IO、内存、锁四大资源的消耗失衡，其中磁盘IO占比最高，是优化的核心靶点：</p>\n<ul>\n<li>\n<p>- CPU：用于SQL解析、排序、分组、函数计算等操作，低效排序与复杂计算易导致CPU过载。</p>\n</li>\n<li>\n<p>- 磁盘IO：数据页/索引页的读取与写入，全表扫描、索引失效是IO消耗激增的主要原因。</p>\n</li>\n<li>\n<p>- 内存：缓冲池缓存数据页，内存不足会导致缓存命中率下降，被迫频繁读取磁盘。</p>\n</li>\n<li>\n<p>- 锁：行锁/表锁引发的查询等待，如更新操作阻塞查询、高并发下的锁竞争，会间接拉长查询耗时。</p>\n</li>\n</ul>\n<h2 id=\"-13-慢查询的本质执行逻辑与资源消耗的双重失衡\">## 1.3 慢查询的本质：执行逻辑与资源消耗的双重失衡</h2>\n<p>慢查询并非“执行时间长”的表面现象，而是底层执行逻辑与资源消耗的双重问题：一是执行计划不合理（如全表扫描、索引失效），导致IO次数过多；二是资源竞争（如锁等待、缓存失效），导致有效执行时间被拉长。优化慢查询，本质就是优化执行计划、减少资源消耗、化解资源竞争。</p>\n<h1 id=\"-二问题定位从慢查询捕捉到执行计划解析\"># 二、问题定位：从慢查询捕捉到执行计划解析</h1>\n<p>精准定位问题是优化的前提，核心依赖“慢查询日志捕捉+执行计划分析”，实现从“发现问题”到“定位根源”的闭环。</p>\n<h2 id=\"-21-慢查询日志性能瓶颈的第一重捕捉\">## 2.1 慢查询日志：性能瓶颈的第一重捕捉</h2>\n<p>慢查询日志是记录低效SQL的核心工具，需合理配置阈值与存储路径，确保精准捕捉关键问题SQL。</p>\n<h3 id=\"-211-日志配置临时生效永久固化\">### 2.1.1 日志配置（临时生效+永久固化）</h3>\n<p>临时配置（重启MySQL后失效，适用于快速排查）：</p>\n<pre><code class=\"language-sql\">-- 设置慢查询阈值（单位：秒，生产环境建议0.5-1秒，平衡灵敏度与日志量）\nSET GLOBAL long_query_time = 0.5; \n-- 开启慢查询日志\nSET GLOBAL slow_query_log = 'ON';\n-- 指定日志文件路径（需确保MySQL有写入权限）\nSET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';\n-- 记录未使用索引的查询（辅助定位索引失效场景）\nSET GLOBAL log_queries_not_using_indexes = 'ON';\n</code></pre>\n<p>永久配置（修改my.cnf文件，重启后生效，适用于生产环境常态化监控）：</p>\n<pre><code class=\"language-ini\">[mysqld]\nslow_query_log = 1\nslow_query_log_file = /var/log/mysql/slow.log\nlong_query_time = 0.5\nlog_queries_not_using_indexes = 1\n</code></pre>\n<h3 id=\"-212-日志分析工具提取核心问题sql\">### 2.1.2 日志分析工具：提取核心问题SQL</h3>\n<p>慢查询日志需通过工具解析，才能快速定位高频、高耗的核心SQL，常用工具分为两类：</p>\n<ul>\n<li>\n<p>- pt-query-digest（Percona Toolkit）：分析维度最全面，支持输出执行次数、平均耗时、扫描行数、锁等待时间等指标，适合复杂场景： <code>pt-query-digest /var/log/mysql/slow.log &gt; slow_report.txt</code></p>\n</li>\n<li>\n<p>- mysqldumpslow（MySQL自带工具）：轻量便捷，适合快速提取TopN慢查询： <code>-- 提取耗时最多的10条SELECT语句</code> <code>mysqldumpslow -s t -t 10 -g 'select' /var/log/mysql/slow.log</code></p>\n</li>\n</ul>\n<p>分析报告需重点关注“执行次数多+平均耗时长”“扫描行数多”“锁等待时间长”三类SQL，这类SQL对整体性能影响最大，优先纳入优化清单。</p>\n<h2 id=\"-22-explain执行计划读懂mysql的执行逻辑\">## 2.2 EXPLAIN执行计划：读懂MySQL的执行逻辑</h2>\n<p>捕捉到慢查询后，需通过EXPLAIN关键字分析执行计划，判断索引是否生效、查询是否存在低效操作，核心是读懂MySQL的“执行思路”。</p>\n<h3 id=\"-221-核心字段解读\">### 2.2.1 核心字段解读</h3>\n<p>执行<code>EXPLAIN SELECT * FROM orders WHERE user_id = 100 AND status = 'PAID';</code>后，重点关注以下字段：</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>字段</td>\n<td>核心意义</td>\n<td>优化判断标准</td>\n</tr>\n<tr>\n<td>type</td>\n<td>访问类型，反映查询效率</td>\n<td>从优到劣：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL；需避免ALL（全表扫描）</td>\n</tr>\n<tr>\n<td>key</td>\n<td>实际使用的索引</td>\n<td>NULL表示未使用索引，需排查索引失效原因</td>\n</tr>\n<tr>\n<td>rows</td>\n<td>预估扫描行数</td>\n<td>数值越大，IO消耗越高，需通过索引缩小范围</td>\n</tr>\n<tr>\n<td>Extra</td>\n<td>附加执行信息</td>\n<td>Using filesort/Using temporary需优化；Using index为理想状态（覆盖索引）</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"-222-关键判断逻辑\">### 2.2.2 关键判断逻辑</h3>\n<p>通过执行计划可快速定位核心问题：若type为ALL（全表扫描），优先排查索引是否缺失或失效；若Extra出现Using filesort，说明排序未使用索引，需优化排序字段；若rows远大于实际返回行数，说明索引选择性差，需调整索引设计。</p>\n<h1 id=\"-三核心优化索引设计与失效规避的实战指南\"># 三、核心优化：索引设计与失效规避的实战指南</h1>\n<p>索引是MySQL性能优化的核心手段，其本质是“基于B+树的有序数据结构”，目的是减少磁盘IO次数。优化索引需同时兼顾“设计合理性”与“避免失效”，遵循底层逻辑与实战原则。</p>\n<h2 id=\"-31-索引设计的三大核心原则\">## 3.1 索引设计的三大核心原则</h2>\n<p>索引设计并非“越多越好”，而是要在“查询效率”与“维护成本”之间找到平衡，核心遵循三大原则：</p>\n<h3 id=\"-311-选择性优先原则\">### 3.1.1 选择性优先原则</h3>\n<p>索引选择性=唯一值数量/总行数，选择性越高，索引定位精度越强，IO次数越少。设计时需将高选择性字段（如用户ID、订单号）放在联合索引前列，低选择性字段（如性别、状态，选择性&lt;0.1）尽量不单独建索引，避免优化器放弃使用。</p>\n<h3 id=\"-312-三星索引原则实战核心\">### 3.1.2 三星索引原则（实战核心）</h3>\n<p>三星索引是理想的索引设计标准，可最大化减少IO与计算消耗：</p>\n<ul>\n<li>\n<p>- 一星：WHERE条件列纳入索引，缩小扫描范围；</p>\n</li>\n<li>\n<p>- 二星：ORDER BY/GROUP BY列纳入索引，利用索引有序性避免排序（Using filesort）；</p>\n</li>\n<li>\n<p>- 三星：SELECT查询列被索引覆盖，避免回表操作（Extra显示Using index）。</p>\n</li>\n</ul>\n<p>示例：查询<code>SELECT user_id, username FROM users WHERE email = 'user@deepseek.com';</code>，设计覆盖索引<code>ALTER TABLE users ADD INDEX idx_email_cover (email, user_id, username);</code>，可实现无回表、无排序的高效查询。</p>\n<h3 id=\"-313-最小维护成本原则\">### 3.1.3 最小维护成本原则</h3>\n<p>索引会增加插入、更新、删除操作的维护成本（需调整B+树结构），设计时需：</p>\n<ul>\n<li>\n<p>- 控制单表索引数在5个以内，避免冗余索引（如已有(a,b)联合索引，单独a索引为冗余）；</p>\n</li>\n<li>\n<p>- 大文本、Blob字段不建索引，避免索引体积过大；</p>\n</li>\n<li>\n<p>- 联合索引需覆盖高频查询场景，减少重复索引。</p>\n</li>\n</ul>\n<h3 id=\"-314-联合索引的字段顺序技巧\">### 3.1.4 联合索引的字段顺序技巧</h3>\n<p>联合索引遵循“最左前缀原则”，本质是基于B+树的有序存储特性，设计时需遵循：</p>\n<ul>\n<li>\n<p>- 等值查询字段在前，范围查询字段在后（如(a,b)联合索引，a=1 AND b&gt;10可走索引，b&gt;10则不可）；</p>\n</li>\n<li>\n<p>- 高频查询字段在前，低频字段在后，确保更多查询能命中索引前缀。</p>\n</li>\n</ul>\n<p>示例：查询<code>SELECT * FROM sales WHERE region='Asia' AND category='Tech' AND sale_date BETWEEN '2023-01-01' AND '2023-12-31' ORDER BY revenue DESC;</code>，最优联合索引为<code>idx_region_category_date (region, category, sale_date)</code>。</p>\n<h2 id=\"-32-索引失效的十大典型场景与解决方案\">## 3.2 索引失效的十大典型场景与解决方案</h2>\n<p>索引失效是慢查询的主要诱因，本质是破坏了B+树的有序性或定位规则，以下是实战中最常见的场景及优化方案：</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>失效场景</td>\n<td>错误示例</td>\n<td>优化方案</td>\n</tr>\n<tr>\n<td>索引列参与计算/函数</td>\n<td>SELECT * FROM users WHERE YEAR(create_time) = 2023;</td>\n<td>SELECT * FROM users WHERE create_time BETWEEN '2023-01-01' AND '2023-12-31';</td>\n</tr>\n<tr>\n<td>隐式类型转换</td>\n<td>SELECT * FROM logs WHERE user_id = '123'（user_id为INT）;</td>\n<td>SELECT * FROM logs WHERE user_id = 123（匹配字段类型）;</td>\n</tr>\n<tr>\n<td>LIKE以%开头</td>\n<td>SELECT * FROM user WHERE userId LIKE '%123';</td>\n<td>改用覆盖索引或LIKE '123%';</td>\n</tr>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t<p class=\"postfoot\">\n\t\t\tposted on \n<span id=\"post-date\">2026-01-15 22:26</span>&nbsp;\n<a href=\"https://www.cnblogs.com/liuziyi1\">刘子毅</a>&nbsp;\n阅读(<span id=\"post_view_count\">56</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</p>"
    },
    {
      "title": "CLAUDE.md 全方位指南：构建高效 AI 开发上下文",
      "link": "https://www.cnblogs.com/didispace/p/19489098",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/didispace/p/19489098\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 20:09\">\n    <span>CLAUDE.md 全方位指南：构建高效 AI 开发上下文</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>如果你是 Claude 的日常用户，你一定熟悉这个场景：每次开启一个新的对话，都必须不厌其烦地重复设置项目背景、编码规范和特定的指令。这不仅耗时，也容易出错。当你忘记提醒某个关键细节时，就不得不花更多时间去修复那些不符合规范的代码。</p>\n<p>CLAUDE.md 文件正是解决这一痛点的关键。它就像 Claude 的项目专属记忆，让 AI 在每次对话开始前自动加载并记住你的所有偏好。这是一个简单而强大的功能，但大多数用户仅仅停留在基础层面。</p>\n<p>事实上，要真正释放 CLAUDE.md 的威力，需要掌握一些更深刻、甚至有些违反直觉的技巧。本文将为你揭示其中最关键的五个，帮助你将这个简单的配置文件，转变为一个能够持续进化的项目知识库。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h2 id=\"1-你的-claudemd-应该是一个活的文档而不是一次性配置\">1. 你的 CLAUDE.md 应该是一个“活的文档”，而不是“一次性配置”</h2>\n<p>许多人认为 CLAUDE.md 文件只需在项目开始时配置一次，然后就可以置之不理。这是一个巨大的误区。最有效的 CLAUDE.md 应该随着项目的演进而持续更新和优化。</p>\n<p>最佳的维护方式是在日常工作中“有机地”构建它。例如，当 Claude 做出了一个需要纠正的假设——比如它建议使用 console.log 进行调试，而你的团队规范是使用特定的日志库——不要只是临时修正。直接告诉 Claude：“将‘总是使用日志库而不是 console.log’这条规则添加到我的 CLAUDE.md 文件中。” 这样，你的修正就会沉淀下来，在未来的所有会话中生效。值得注意的是，早期版本的 Claude 有一个 # 快捷键来添加指令，但在 2.0.70 版本后已被移除。目前，直接请求 Claude 进行修改是官方推荐的最佳实践。</p>\n<p>这种做法的价值在于，它能实时捕捉并固化工作流程中的隐性知识。正如一个精妙的比喻所说：</p>\n<p>这就像在会议中做笔记，不同的是，这些笔记真的会被使用。</p>\n<p>更高级的维护方法是将其与团队协作流程结合。在代码审查（Code Review）中发现的未被文档化的规范，正是更新 CLAUDE.md 的绝佳时机。一个由 Boris Cherny 分享的高效工作流是：通过 GitHub Action，你甚至可以直接在 PR 评论中 @claude，让它将新规范添加到 CLAUDE.md 文件中。这创建了一个强大的反馈循环，将团队的集体智慧源源不断地沉淀到这个核心文件中。</p>\n<h2 id=\"2-少即是多上下文是宝贵资源精简至上\">2. 少即是多：上下文是宝贵资源，精简至上</h2>\n<p>人们普遍认为，提供给 AI 的上下文越多，结果就越好。然而在使用 CLAUDE.md 时，这个直觉可能是错误的。</p>\n<p>核心论点是：“上下文是宝贵的（Context is precious）”。CLAUDE.md 中的每一行内容，都在与你当前的工作指令竞争 AI 的注意力。一个臃肿、充满冗余信息的文件，反而可能稀释掉最关键的指令，导致 AI 抓不住重点。<br />\n因此，精简至上。一个很好的起点是使用 /init 命令，它会根据你的项目结构和技术栈生成一个初始文件。我的建议是，以此为基础，然后删除所有你不需要的内容。从现有内容中删除比从零开始创建要容易得多。</p>\n<p>一般建议将文件长度保持在 300 行以下。当然，这并非硬性规定。对于一些具有复杂约定或非寻常模式的 codebase，一个更长的 CLAUDE.md 反而能通过预先加载足够的上下文，有效防止 Claude 做出错误假设。关键在于，文件中的每一行都应该有其明确的价值。毫不留情地删除那些显而易见的废话（例如“请编写高质量代码”）或没有实际指导意义的“填充”信息。</p>\n<p>精简的 CLAUDE.md 迫使你仔细思考并只保留最重要的指令。这不仅能节省宝贵的上下文空间，更能确保 AI 在处理你的请求时，能够更准确地聚焦于核心要求，而不是在大量无关信息中迷失方向。</p>\n<h2 id=\"3-超越单个文件用模块化结构管理复杂性\">3. 超越单个文件：用模块化结构管理复杂性</h2>\n<p>许多用户只知道在项目根目录下创建一个 CLAUDE.md 文件。这对于小型项目来说足够了，但对于大型或结构复杂的项目，存在着更优雅、更强大的模块化管理方式。</p>\n<ul>\n<li>@imports 语法 你可以使用 @path/to/file 语法，从主 CLAUDE.md 文件中引用其他文件的内容。这能让主文件保持简洁，同时将详细的规范拆分到独立的文档中。例如，你可以将复杂的 API 设计模式放在 docs/api-patterns.md 中，然后在主文件里用 @docs/api-patterns.md 引用它。</li>\n<li>.claude/rules/ 目录 这是一个非常适合大型团队的结构。所有放在 .claude/rules/ 目录下的 .md 文件都会被 Claude 自动加载，无需手动 @import。这使得不同领域的团队可以独立维护各自的规则文件，例如，前端团队维护 code-style.md，安全团队维护 security.md。大家各司其职，有效避免了在单个大文件中频繁产生合并冲突。</li>\n<li>子目录中的 CLAUDE.md 对于 Monorepo（单一代码库）项目，这是一个绝佳的解决方案。你可以在项目的特定子目录（例如 api/ 或 packages/ui/）中放置 CLAUDE.md 文件。这些文件非常特殊：它们并不会在会话启动时加载，而只在 Claude 主动处理该特定子目录中的内容时才会被包含进来。这使得你可以为项目的不同模块定义截然不同的规范，实现真正精细化的上下文管理。<br />\n• 个人配置 CLAUDE.local.md 还有一个关键文件：CLAUDE.local.md。它用于存放那些不应提交到版本控制中的个人偏好，例如你习惯的编辑器 quirks 或偏好的代码冗余度。由于这是个人专属的，请务必将其添加到 .gitignore 文件中，以避免将个人配置泄露给整个团队。</li>\n</ul>\n<h2 id=\"4-魔鬼在细节中文件名是区分大小写的\">4. 魔鬼在细节中：文件名是区分大小写的</h2>\n<p>这是一个极其微小但至关重要的技术细节，也是最容易被忽略的陷阱之一：CLAUDE.md 这个文件名是区分大小写的。</p>\n<p>正确的文件名必须是“CLAUDE.md”——CLAUDE 部分为大写，.md 扩展名为小写。如果你将其命名为 claude.md、Claude.md 或其他任何变体，Claude 的系统将无法识别并加载它。</p>\n<p>这个细节之所以重要，是因为它是一个典型的“陷阱”（gotcha）。有趣的是，这一点在官方文档中并未明确说明。我是通过询问官方文档的 AI 助手才最终确认了这一规则。一旦出错，你可能会花费大量时间排查为什么自己精心编写的指令完全没有生效，最终才发现问题出在一个简单的大小写错误上。这个看似微不足道的细节，恰恰体现了与 AI 高效协作时，精确配置的重要性。</p>\n<h2 id=\"5-让-ai-优化-ai定期请-claude-审查自己的说明书\">5. 让 AI 优化 AI：定期请 Claude 审查自己的“说明书”</h2>\n<p>这是一个非常巧妙的“元认知”技巧：定期让 Claude 自己来审查和优化它的“说明书”——CLAUDE.md 文件。</p>\n<p>随着时间的推移，CLAUDE.md 中不可避免地会积累一些过时、冗余甚至相互冲突的指令。通过一个简单的提示，例如“请审查这个 CLAUDE.md 文件，并提出改进建议以使其更清晰、更高效”，你可以利用 Claude 自身的能力来发现这些问题。它可能会建议你合并重复的规则，或澄清模糊的表述。</p>\n<p>对于那些绝对不能违反的关键规则，你可以使用强调词来引起 Claude 的注意，比如 IMPORTANT: 或 YOU MUST。这能提高 Claude 遵循这些指令的概率，但请务必谨慎使用。正如一句古老的建议所说：如果所有东西都被标记为重要，那就没有什么是重要的了。</p>\n<p>诚然，这需要一些维护成本，但其回报是巨大的。正如源文所说：<br />\n这听起来像是维护开销。确实是。但它比在每个会话中重复自己的话，或修复那些忽略了你的规范的代码要省事得多。</p>\n<p>这不仅仅是一种文件维护策略，更是一种与 AI 协作的新范式。我们不再仅仅是 AI 的使用者，更是其成长过程中的引导者，让工具本身参与到自我完善的流程中，形成一个持续改进的良性循环。</p>\n<h2 id=\"结论\">结论</h2>\n<p>CLAUDE.md 远不止一个简单的配置文件。通过本文分享的五个高级技巧——将其视为活文档、保持精简、模块化管理、注意大小写，以及让 AI 自我优化——你可以将其从一个静态的指令列表，转变为一个强大的、与项目共同成长的动态知识库。</p>\n<p>这些策略代表了一种更深层次的思维方式：将你的 AI 上下文本身视为一个“代码库”。它也需要像代码一样被重构、被审查、被持续改进。你的 CLAUDE.md 值得你如此对待。现在，不妨思考一下：你的 CLAUDE.md 中沉淀了多少团队智慧？或许，现在就是开始构建它的最佳时机。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 20:09</span>&nbsp;\n<a href=\"https://www.cnblogs.com/didispace\">程序猿DD</a>&nbsp;\n阅读(<span id=\"post_view_count\">49</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "使用vue3. + vite 开发，打开控制台卡顿，关闭控制台流畅。打开控制台时，鼠标在页面上移动。此时浏览器进程的cpu利用率也有上升。",
      "link": "https://www.cnblogs.com/xiaohui666/p/19489060",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xiaohui666/p/19489060\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 19:56\">\n    <span>使用vue3. + vite 开发，打开控制台卡顿，关闭控制台流畅。打开控制台时，鼠标在页面上移动。此时浏览器进程的cpu利用率也有上升。</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>使用vue3. + vite 开发，打开控制台卡顿，关闭控制台流畅。打开控制台时，鼠标在页面上移动。此时浏览器进程的cpu利用率也有上升。</p>\n<p>&nbsp;</p>\n<p>打开任务管理器查看到 刚启动就 300M 左右，此时页面还正常流畅使用</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115192758334-1287855783.png\" /></p>\n<p>&nbsp;</p>\n<p>当打开浏览器F12控制台时 ，页面卡顿，内存占用直接到达惊人的1547M ,百思不得其解</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115193120944-1363009300.png\" /></p>\n<p>&nbsp;</p>\n<p>打开内存面板，查找原因，此时用了706M</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115193422895-368120997.png\" /></p>\n<p>&nbsp;</p>\n<p>筛选一下，发现是样式导入重复</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115193617511-1308370927.png\" /></p>\n<p>&nbsp;</p>\n<p>&nbsp;查看代码有三处地方引用</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115193908039-480362363.png\" /></p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115193912528-202535441.png\" /></p>\n<p>&nbsp;</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115193917151-2133524706.png\" /></p>\n<p>&nbsp;</p>\n<p>&nbsp;先注释&nbsp;vite.config.ts 里的</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115194950450-306480391.png\" /></p>\n<p>&nbsp;</p>\n<p>发现内存明显下降，问题解决</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115195104234-699812157.png\" /></p>\n<p>&nbsp;</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1692424/202601/1692424-20260115195120872-371187655.png\" /></p>\n<p>&nbsp;</p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 19:56</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xiaohui666\">小辉。</a>&nbsp;\n阅读(<span id=\"post_view_count\">61</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "我用 stock-sdk 构建了一个个人专属的 A 股行情仪表盘",
      "link": "https://www.cnblogs.com/chengzp/p/19488966/stock-dashboard",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/chengzp/p/19488966/stock-dashboard\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 19:06\">\n    <span>我用 stock-sdk 构建了一个个人专属的 A 股行情仪表盘</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"我用 stock-sdk 构建了一个个人专属的 A 股行情仪表盘\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/1265396/202601/1265396-20260115190434191-202601803.png\" />\n        这篇博客介绍了我用 stock-sdk 搭建的 A 股股票看板 stock-dashboard：基于 React + TypeScript + Vite 的纯前端项目，不依赖后端或定时脚本，直接在页面侧拉取行情并完成展示与筛选。文章从数据层封装（SDK 单例、重试、TTL 缓存、服务层统一出口）讲起，再按功能拆解搜索、Dashboard、热力图、板块/个股详情、自选、信号扫描与设置等模块。最后重点分享“一日持股法（尾盘选股）”的全市场扫描思路：先批量拉取 5000+ 行情做基础过滤，再分批拉分时计算强度指标并排序输出候选。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"这是个啥\">这是个啥</h2>\n<p>背景故事很简单：作为一个日常关注行情的“韭菜”，我有一个不太高效的习惯——同时打开无数个看盘软件和网页，在混乱的窗口切换中迷失自我，最终收获的往往只有焦虑，外加浏览器那令人窒息的标签页堆叠。为了彻底治愈这种低效，我决定动手打造一个专属工具：<strong>在一个页面内集成所有高频功能，涵盖实时行情、板块动态、分时走势、K 线分析、资金流向以及筛选器</strong>。</p>\n<p>这就诞生了 <code>stock-dashboard</code>：一个完全基于 React + TypeScript + Vite 技术栈的前端大屏。所有数据直接由 <a href=\"https://stock-sdk.linkdiary.cn/\" rel=\"noopener nofollow\" target=\"_blank\">stock-sdk</a> 驱动，这意味着项目完全摒弃了后端服务，不需要运行任何 Python 定时任务，也不依赖什么“神秘朋友的高端服务器”。纯前端直连数据源，所见即所得，一切都安排得井井有条。</p>\n<p>直接上在线演示链接：<a href=\"https://chengzuopeng.github.io/stock-dashboard/\" rel=\"noopener nofollow\" target=\"_blank\">stock-dashboard</a> （友情提示：摸鱼期间请谨慎使用，建议配合小窗口模式）。<br />\n<img alt=\"stock-overview\" class=\"lazyload\" /></p>\n<h2 id=\"核心解密数据层架构设计\">核心解密：数据层架构设计</h2>\n<p>为了保持代码整洁，我将所有针对 <code>stock-sdk</code> 的调用逻辑都封装在了 <code>src/services/sdk.ts</code> 中。</p>\n<p>这里主要实施了三个既实用又不矫情的工程化策略：</p>\n<ol>\n<li>\n<p><strong>全局单例与自动重试机制</strong><br />\n通过 <code>new StockSDK({ timeout, retry })</code> 初始化实例。面对网络波动或接口偶尔抽风的情况，SDK 内置的自动重试机制（支持最大 3 次重试及指数退避算法）能完美兜底。</p>\n</li>\n<li>\n<p><strong>智能内存缓存（TTL 策略）</strong><br />\n对于行业或概念列表这类变动频率极低的数据（毕竟它们不会在几秒内发生剧变），直接上缓存减少无效请求；而对于实时行情，则设置了 2~3 秒的生存期（TTL），既保证了数据的时效性，又避免了无意义的高频请求轰炸接口。</p>\n</li>\n<li>\n<p><strong>分层隔离：页面仅对接服务层</strong><br />\n翻阅 <code>src/pages/**</code> 下的代码，你几乎找不到 <code>new StockSDK()</code> 的身影。UI 层只负责调用诸如 <code>getFullQuotes / getTodayTimeline / getKlineWithIndicators</code> 等经过二次封装的业务方法，而类型定义则直接复用 <code>stock-sdk</code> 的导出。</p>\n</li>\n</ol>\n<p>顺便展示两段核心代码骨架，后续的所有功能模块皆构建于此基础之上：</p>\n<pre><code class=\"language-ts\">// src/services/sdk.ts\nexport const sdk = new StockSDK({ timeout: 30000, retry: { maxRetries: 3, baseDelay: 1000, maxDelay: 10000, backoffMultiplier: 2 } });\n\nexport async function getFullQuotes(codes: string[], useCache = true) {\n  const key = getCacheKey('getFullQuotes', codes);\n  if (useCache) {\n    return withCache(key, DEFAULT_TTL.quotes, () =&gt; sdk.getFullQuotes(codes));\n  }\n  return sdk.getFullQuotes(codes);\n}\n</code></pre>\n<pre><code class=\"language-ts\">// src/services/sdk.ts\nexport async function getAllAShareQuotes(options?: { batchSize?: number; concurrency?: number; onProgress?: (completed: number, total: number) =&gt; void }) {\n  return sdk.getAllAShareQuotes(options);\n}\n</code></pre>\n<hr />\n<h2 id=\"功能拆解各模块如何玩转-stock-sdk-数据\">功能拆解：各模块如何玩转 stock-sdk 数据？</h2>\n<p>路由配置位于 <code>src/router/index.tsx</code>，而各个功能页面则模块化地分布在 <code>src/pages/*</code> 目录下。接下也就是大家最关心的——按“用户交互路径”来逐一复盘。</p>\n<h3 id=\"1-全局搜索告别手动翻代码的痛苦\">1) 全局搜索：告别手动翻代码的痛苦</h3>\n<p>搜索栏组件位于 <code>src/components/layout/Header.tsx</code>，其背后的魔法仅需一行代码：</p>\n<ul>\n<li><code>search(keyword)</code> 映射到 <code>stock-sdk</code> 的 <code>sdk.search(keyword)</code></li>\n</ul>\n<p>为了优化体验，我添加了 300ms 的输入防抖处理。搜索结果完美支持个股与板块的混合查询，点击即达：</p>\n<ul>\n<li>行业板块跳转至：<code>/boards/industry/:code</code></li>\n<li>概念板块跳转至：<code>/boards/concept/:code</code></li>\n<li>个股详情跳转至：<code>/s/:code</code></li>\n</ul>\n<p>顺手还利用 localStorage 实现了一个简单的历史记录功能（<code>src/services/storage.ts</code>），毕竟很多时候，我们寻找的不是新标的，而是昨天没看完的那个它。</p>\n<hr />\n<h3 id=\"2-仪表盘-dashboard行情概览与自选速览\">2) 仪表盘 Dashboard：行情概览与自选速览</h3>\n<p>对应页面文件：<code>src/pages/Dashboard/Dashboard.tsx</code>。</p>\n<p>数据获取逻辑非常直白粗暴：</p>\n<ul>\n<li>指数行情：调用 <code>getFullQuotes(MAIN_INDICES)</code> 一次性获取上证、深成指、科创 50 等关键指数。</li>\n<li>板块概况：并行调用 <code>getIndustryList()</code> 和 <code>getConceptList()</code>。</li>\n<li>自选股预览：先从存储服务 <code>src/services/storage.ts</code> 读取自选列表，再通过 <code>getFullQuotes(watchlistCodes.slice(0, 50))</code> 批量获取前 50 只行情的快照。</li>\n</ul>\n<p>为了保证数据的鲜活度，配合 <code>usePolling</code> Hook（<code>src/hooks/usePolling.ts</code>）实现了每 5 秒自动轮询。贴心的是，当页面处于后台不可见状态时，轮询会自动挂起，绝不浪费你的浏览器资源。</p>\n<p>额外提一句：目前 Dashboard 上的“榜单”主要展示板块数据。如果想做全市场的个股排名，技术路径完全可以参考后面提到的“一日持股法”，也就是直接利用 <code>getAllAShareQuotes</code> 接口。</p>\n<hr />\n<h3 id=\"3-市场热力图-heatmap一图看懂资金流向\">3) 市场热力图 Heatmap：一图看懂资金流向</h3>\n<p><img alt=\"stock-heatmap\" class=\"lazyload\" /></p>\n<p>实现文件位于 <code>src/pages/Heatmap/Heatmap.tsx</code>，底层依赖 ECharts 的矩形树图（Treemap）。</p>\n<p>根据观察视角的不同，数据源也各异：</p>\n<ul>\n<li>行业视角：直接用 <code>getIndustryList()</code>，因为返回的数据中已经包含了涨跌幅、换手率及领涨股信息。</li>\n<li>概念视角：同理，调用 <code>getConceptList()</code>。</li>\n<li>自选视角：获取所有自选代码 <code>getAllWatchlistCodes()</code> 后，通过 <code>getAllQuotesByCodes(codes.slice(0, topK))</code> 批量拉取。</li>\n</ul>\n<p>至于“全市场个股”热力图（代码预留了接口，暂未开启），实现逻辑也不复杂：</p>\n<ol>\n<li>通过 <code>getIndustryConstituents(industryCode)</code> 获取特定板块成分股。</li>\n<li>用 <code>getAllQuotesByCodes(stockCodes)</code> 把行情数据补齐。</li>\n<li>最后组装数据喂给 Treemap 组件。</li>\n</ol>\n<p>热力图最大的魅力在于：<strong>告别枯燥的数字列表，红绿相间的色块让你瞬间洞察市场强弱结构。</strong></p>\n<hr />\n<h3 id=\"4-龙虎榜-rankings观察市场风向标\">4) 龙虎榜 Rankings：观察市场风向标</h3>\n<p><img alt=\"stock-leaderboard\" class=\"lazyload\" /></p>\n<p>页面路径：<code>src/pages/Rankings/Rankings.tsx</code>。</p>\n<p>实现方式属于“简单粗暴且有效”：</p>\n<ul>\n<li>并行获取 <code>getIndustryList()</code> 和 <code>getConceptList()</code>。</li>\n<li>前端直接根据 <code>changePercent</code>（涨跌幅）或 <code>turnoverRate</code>（换手率）进行排序，截取 Top 50。</li>\n</ul>\n<p>目前的榜单本质上是“板块排行榜”。如果未来要扩展到全市场个股排行，技术方案与后文的“选股器”一致。</p>\n<hr />\n<h3 id=\"5-板块透视追踪领涨先锋\">5) 板块透视：追踪领涨先锋</h3>\n<p>板块列表页位于 <code>src/pages/Boards/Boards.tsx</code>：</p>\n<ul>\n<li><code>getIndustryList()</code> 与 <code>getConceptList()</code> 一把梭。</li>\n<li>所谓的 Tab 切换，仅仅是前端对不同数据源数组的渲染切换。</li>\n<li>当然也支持按板块名称或领涨股进行检索。</li>\n</ul>\n<p>详情页见 <code>src/pages/Boards/BoardDetail.tsx</code>，这里展示了 API 的组合拳能力（按行业/概念分流）：</p>\n<ul>\n<li>基础信息：直接复用列表数据，减少一次网络请求。</li>\n<li>成分股列表：调用 <code>getIndustryConstituents(code)</code> 或 <code>getConceptConstituents(code)</code>。</li>\n<li>板块走势：拉取 <code>getIndustryKline</code> 或 <code>getConceptKline</code>。</li>\n<li>盘口快照：通过 <code>getIndustrySpot</code> 或 <code>getConceptSpot</code> 获取。</li>\n</ul>\n<p>为了保证流畅度，板块 K 线图目前只截取了最近 60 根数据，防止缩放图表时浏览器渲染压力过大。</p>\n<hr />\n<h3 id=\"6-自选监控-watchlist只看我在意的\">6) 自选监控 Watchlist：只看我在意的</h3>\n<p>核心页面：<code>src/pages/Watchlist/Watchlist.tsx</code>。所有的增删改查逻辑都封装在 <code>src/services/storage.ts</code> 中。</p>\n<p>行情刷新主要依赖：</p>\n<ul>\n<li><code>getAllQuotesByCodes(normalizedActiveCodes)</code></li>\n</ul>\n<p>特别提一下这里的细节处理：在请求前我会先通过 <code>normalizeStockCode</code>（位于 <code>src/utils/format.ts</code>）对代码进行标准化格式化，有效防止了 <code>SZ000001</code>、<code>sz000001</code> 和 <code>000001</code> 这种“一码多式”造成的去重失败或数据请求异常。</p>\n<hr />\n<h3 id=\"7-个股深度分析-stockdetail全维数据一览无余\">7) 个股深度分析 StockDetail：全维数据一览无余</h3>\n<p><img alt=\"stock-detail\" class=\"lazyload\" /></p>\n<p>页面位置：<code>src/pages/StockDetail/StockDetail.tsx</code>。这是整个项目中承载信息量最大的页面，因为它聚合了极高密度的信息。</p>\n<p>它聚合了多维度的 API 数据：</p>\n<ul>\n<li>实时报价：<code>getFullQuotes([code])</code></li>\n<li>当日分时图（1分钟级）：<code>getTodayTimeline(code)</code></li>\n<li>分钟级 K 线（5/15/30/60）：<code>getMinuteKline(code, { period })</code></li>\n<li>历史 K 线（日/周/月）及复权：<code>getKlineWithIndicators(code, { period, adjust: 'qfq', indicators })</code></li>\n<li>资金流向监测：<code>getFundFlow([code])</code></li>\n<li>盘口大单监控：<code>getPanelLargeOrder([code])</code></li>\n</ul>\n<p>我个人非常推崇 <code>getKlineWithIndicators</code> 这个接口：只需传入你想要的指标参数（如 MA, MACD, KDJ, RSI, BOLL等），SDK 就能把计算好的指标数据连同 K 线一起返回。前端只需负责绘图，彻底告别了在前端手写复杂技术指标计算逻辑的噩梦（少写代码 = 少出 Bug = 长命百岁）。</p>\n<p>在这里，轮询策略也做了精细化分层：</p>\n<ul>\n<li>基础行情：2 秒/次</li>\n<li>分时图：3 秒/次</li>\n<li>资金流向：10 秒/次</li>\n</ul>\n<hr />\n<h3 id=\"8-策略扫描器-scanner量化交易的初体验\">8) 策略扫描器 Scanner：量化交易的初体验</h3>\n<p>页面：<code>src/pages/Scanner/Scanner.tsx</code>。</p>\n<p>扫描逻辑简述如下：</p>\n<ol>\n<li><strong>确定股票池</strong>：\n<ul>\n<li>既可以是你的“自选股列表”。</li>\n<li>也可以是某个板块的成分股，例如调用 <code>getIndustryConstituents('BK0475')</code>。</li>\n</ul>\n</li>\n<li><strong>批量分析</strong>：\n<ul>\n<li>遍历每只股票，调用 <code>getKlineWithIndicators</code> 获取带指标的 K 线数据。</li>\n</ul>\n</li>\n<li><strong>信号匹配</strong>：\n<ul>\n<li>前端逻辑判断最近两根 K 线是否满足预设形态（如均线金叉、MACD 金叉、RSI 超买超卖等）。</li>\n</ul>\n</li>\n</ol>\n<p>虽然这个功能带有一定的“心里安慰”属性，但它确确实实把模糊的“看涨感觉”转化为了可执行的“触发条件”。</p>\n<hr />\n<h3 id=\"9-个性化设置-settings打造顺手的工具\">9) 个性化设置 Settings：打造顺手的工具</h3>\n<p><img alt=\"stock-settings\" class=\"lazyload\" /></p>\n<p>页面：<code>src/pages/Settings/Settings.tsx</code>。</p>\n<p>这个页面并没有调用任何 <code>stock-sdk</code> 接口，它的使命是将你的使用偏好（刷新频率、红涨绿跌配色、各类指标的默认参数等）持久化保存到 localStorage。这样，无论何时打开页面，它都还是那个你最熟悉的样子。</p>\n<hr />\n<h2 id=\"重头戏一日持股策略尾盘选股前端实现的全市场扫描\">重头戏：一日持股策略（尾盘选股）——前端实现的全市场扫描</h2>\n<p><img alt=\"stock-last\" class=\"lazyload\" /></p>\n<p>该功能位于 <code>src/pages/EndOfDayPicker/EndOfDayPicker.tsx</code>。我在这个页面实现了一套经典的“三步走”选股漏斗，其核心动力源自强大的 <strong><code>getAllAShareQuotes</code></strong> 接口。</p>\n<h3 id=\"第一阶段全量-a-股行情抓取\">第一阶段：全量 A 股行情抓取</h3>\n<pre><code class=\"language-ts\">// src/pages/EndOfDayPicker/EndOfDayPicker.tsx\nconst quotes = await getAllAShareQuotes({\n  batchSize: 500,\n  concurrency: 5,\n  onProgress: (completed, total) =&gt; setLoadingProgress({ completed, total, stage: '数据加载中...' }),\n});\n</code></pre>\n<p>这一步调用的是 SDK 的重磅接口：</p>\n<ul>\n<li><code>sdk.getAllAShareQuotes(options?: GetAllAShareQuotesOptions): Promise&lt;FullQuote[]&gt;</code></li>\n<li>参数 <code>batchSize</code> 控制单次批大小（默认 500），<code>concurrency</code> 控制并发数（默认 7）。</li>\n</ul>\n<p>我采取了相对稳健的策略（并发设为 5），兼顾了浏览器的性能负载和网络稳定性。配合 <code>onProgress</code> 回调，用户能看到实时的进度条反馈，体验流畅不卡顿，不会误以为网页卡死。</p>\n<h3 id=\"第二阶段基础指标粗筛\">第二阶段：基础指标粗筛</h3>\n<p>拿到全市场 5000+ 只股票的 <code>FullQuote</code> 数据后，我们先进行一轮粗筛（字段直接取自 <code>FullQuote</code>）：</p>\n<ul>\n<li>流通市值 (<code>circulatingMarketCap</code>)</li>\n<li>量比 (<code>volumeRatio</code>)</li>\n<li>涨跌幅 (<code>changePercent</code>)</li>\n<li>换手率 (<code>turnoverRate</code>)</li>\n<li>ST/风险股过滤</li>\n</ul>\n<p>这一步逻辑封装在 <code>filterStocksBasic()</code> 中，通常能把目标池从 5000+ 缩减到几百甚至几十只，如果不筛这一刀，后续拉取分时数据会直接把浏览器送走。</p>\n<h3 id=\"第三阶段分时图形态精选\">第三阶段：分时图形态精选</h3>\n<p>对于粗筛剩下的候选股，我们再进行更细致的分时图分析：</p>\n<ul>\n<li>调用 <code>getTodayTimeline(fullCode)</code> 拉取分时数据（注意拼接 sh/sz/bj 前缀）。</li>\n<li>计算核心强度指标：<code>timelineAboveAvgRatio</code>（即：现价高于均价的时间占比，由 <code>price</code> 和 <code>avgPrice</code> 对比得出）。</li>\n</ul>\n<p>为了防止浏览器崩溃，<code>filterWithTimeline()</code> 中手动控制了分时数据请求的并发量（batchSize = 5）。<br />\n最终结果按 <code>timelineAboveAvgRatio</code> 降序排列，并在列表中展示迷你的分时走势图。这样一来，尾盘选股的效率直接起飞。</p>\n<hr />\n<h2 id=\"写在最后谁需要这个工具\">写在最后：谁需要这个工具？</h2>\n<p>如果你渴望拥有一个“既能看盘、又能筛股、还能顺便管理自选”的轻量级看板，同时极其排斥维护后端服务或编写复杂的 Python 脚本，那么这个纯前端方案绝对是你的不二之选。<strong>核心思路就是利用 <code>stock-sdk</code> 将强大的数据能力引入前端，剩下的就是单纯的 UI 组装与逻辑编排</strong>。</p>\n<p>本地启动非常简单：</p>\n<pre><code class=\"language-bash\">yarn install\nyarn dev\n</code></pre>\n<p>最后不得不俗套地提醒一句：页面底部的 disclaimer “仅供学习参考，不构成投资建议”并非摆设。代码虽可自信敲，投资仍需谨慎行。</p>\n<hr />\n<h2 id=\"传送门\">传送门</h2>\n<ul>\n<li>在线看板： <a href=\"https://chengzuopeng.github.io/stock-dashboard/\" rel=\"noopener nofollow\" target=\"_blank\">https://chengzuopeng.github.io/stock-dashboard/</a></li>\n<li>SDK 文档： <a href=\"https://stock-sdk.linkdiary.cn/\" rel=\"noopener nofollow\" target=\"_blank\">https://stock-sdk.linkdiary.cn/</a></li>\n<li>SDK 演练场： <a href=\"https://stock-sdk.linkdiary.cn/playground/\" rel=\"noopener nofollow\" target=\"_blank\">https://stock-sdk.linkdiary.cn/playground/</a></li>\n</ul>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 19:06</span>&nbsp;\n<a href=\"https://www.cnblogs.com/chengzp\">程序猿的程</a>&nbsp;\n阅读(<span id=\"post_view_count\">65</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI → JSON → UI",
      "link": "https://www.cnblogs.com/guangzan/p/19487446",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/guangzan/p/19487446\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 15:23\">\n    <span>AI → JSON → UI</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"背景\">背景</h2>\n<p>过去两年，AI 生成 UI 的实践基本集中在两种路径上。第一种是直接让模型生成 JSX、HTML 或 CSS。这条路线的优势在于自由度极高，模型几乎不受约束，看起来“什么都能写”。但在真实工程环境中，这种方式几乎不可控：输出结构不稳定，无法保证组件边界，难以做权限与审计控制，生成的代码经常无法编译或违背工程约定，更重要的是，它与实际业务中的组件体系和设计系统严重脱节。</p>\n<p>另一条路线是低代码或 schema 驱动 UI，例如基于 JSON Schema 或表单 schema 的方案。这类方案在工程上是可控的，结构稳定、可校验、可复用，但它们本质上是为“人编写配置”设计的，而不是为“模型生成结构”设计的。schema 表达能力有限，扩展成本高，并且与自然语言之间的映射并不自然，Prompt 往往需要大量人工约束。</p>\n<p>Vercel 刚刚开源了 json-render，json-render 的出现，本质上是对这两条路线的重新切分与组合。它并没有试图让 AI 写前端代码，也没有把 AI 限制在传统低代码 schema 中，而是引入了一个中间层：<strong>JSON UI AST</strong>。AI 只能生成这种 AST，而 AST 的能力边界完全由开发者定义。渲染、状态、行为解释全部留在业务侧完成。开发者因此可以安全地让用户通过自然语言生成仪表盘、小部件或数据视图，而不需要把执行权交给模型。</p>\n<p><img alt=\"iShot_2026-01-15_15.25.26\" src=\"https://img2024.cnblogs.com/blog/1501373/202601/1501373-20260115152759556-1171653435.gif\" /></p>\n<h2 id=\"整体架构json-render-是一个-dsl-解释系统\">整体架构：json-render 是一个 DSL 解释系统</h2>\n<p>从架构视角看，json-render 并不是一个 UI 框架，而是一个 DSL 执行系统。系统由三层构成：最底层是 Catalog，用来声明“系统允许 AI 使用哪些 UI 能力”；中间层是 JSON UI Tree，这是 AI 的唯一输出形式；最上层是 Renderer，由业务侧实现，用于解释 JSON 并渲染真实 UI。</p>\n<p>它们之间的关系可以用下面这张结构图来理解：</p>\n<pre><code>┌────────────┐\n│   Prompt   │\n└─────┬──────┘\n      │\n      ▼\n┌──────────────────┐\n│  LLM / AI Model  │\n└─────┬────────────┘\n      │  JSON UI AST（受 Catalog 严格约束）\n      ▼\n┌──────────────────┐\n│     Catalog      │  ← 能力白名单 / Schema / Grammar\n└─────┬────────────┘\n      │ 校验 + 解析\n      ▼\n┌──────────────────┐\n│    Renderer      │  ← React / Vue / Native\n└─────┬────────────┘\n      │\n      ▼\n┌──────────────────┐\n│   Real UI View   │\n└──────────────────┘\n</code></pre>\n<p>在这个模型中，AI 只参与“结构生成”，不参与“执行”。这也是 json-render 在工程上成立的根本原因。</p>\n<h2 id=\"从-catalog-到-ui\">从 Catalog 到 UI</h2>\n<h3 id=\"1-catalog系统的能力边界定义\">1. Catalog：系统的能力边界定义</h3>\n<p>下面这段代码是整个系统中最重要的入口，它定义了 AI 能使用的全部 UI 能力。</p>\n<pre><code class=\"language-ts\">import { createCatalog } from '@json-render/core'\nimport { z } from 'zod'\n\nexport const catalog = createCatalog({\n  components: {\n    Card: {\n      props: z.object({\n        title: z.string()\n      }),\n      hasChildren: true\n    },\n    Metric: {\n      props: z.object({\n        label: z.string(),\n        valuePath: z.string(),\n        format: z.enum(['number', 'currency', 'percent']).optional()\n      })\n    }\n  },\n  actions: {\n    refresh: {\n      params: z.object({})\n    }\n  }\n})\n</code></pre>\n<p>这里没有任何 UI 代码，只有能力声明。props 使用 Zod 定义，这意味着它不仅是类型提示，还包含运行时校验规则。如果你对 Zod 没有了解，可以看看这篇博文，<a href=\"https://www.cnblogs.com/guangzan/p/19350726\" target=\"_blank\">Zod：TypeScript 类型守卫与数据验证</a>。action 并不是函数实现，而是一个“意图声明”，它只描述“可以发生什么”，不描述“怎么发生”。</p>\n<p>Catalog 在系统中的地位，相当于一门语言的语法定义文件。AI 后续生成的所有 JSON，本质上都必须符合这套 grammar。</p>\n<h3 id=\"2-ai-输出的-json-ui-ast\">2. AI 输出的 JSON UI AST</h3>\n<p>当用户输入类似“生成一个收入仪表盘”的提示时，模型生成的结果不是 JSX，而是下面这样的 JSON：</p>\n<pre><code class=\"language-json\">{\n  \"type\": \"Card\",\n  \"props\": { \"title\": \"Revenue Overview\" },\n  \"children\": [\n    {\n      \"type\": \"Metric\",\n      \"props\": {\n        \"label\": \"Total Revenue\",\n        \"valuePath\": \"/metrics/revenue\",\n        \"format\": \"currency\"\n      }\n    }\n  ]\n}\n</code></pre>\n<p>这个 JSON 有几个非常关键的特征。它不包含任何函数、不包含条件表达式、不包含样式或状态逻辑。它只是结构化地描述“使用哪个组件，用什么参数，组件之间如何嵌套”。所有能力完全来源于 Catalog，因此这个 JSON 是可校验、可存储、可 diff、可审计、可回放的。</p>\n<h3 id=\"3-rendererjson-的解释执行\">3. Renderer：JSON 的解释执行</h3>\n<p>在 React 侧，Renderer 扮演的是解释器的角色。</p>\n<pre><code class=\"language-tsx\">import { Renderer } from '@json-render/react'\nimport { catalog } from './catalog'\n\nfunction App() {\n  return (\n    &lt;Renderer\n      catalog={catalog}\n      components={{\n        Card: ({ title, children }) =&gt; (\n          &lt;div className=\"card\"&gt;\n            &lt;h2&gt;{title}&lt;/h2&gt;\n            {children}\n          &lt;/div&gt;\n        ),\n        Metric: ({ label, value }) =&gt; (\n          &lt;div&gt;\n            {label}: {value}\n          &lt;/div&gt;\n        )\n      }}\n      data={{\n        metrics: { revenue: 120000 }\n      }}\n    /&gt;\n  )\n}\n</code></pre>\n<p>Renderer 并不关心 UI 长什么样，它只做三件事：根据 type 找到对应组件定义，根据 Catalog 校验 props 和 children，根据 valuePath 等规则完成数据注入。</p>\n<h2 id=\"为什么-json-render-是可控的\">为什么 json-render 是“可控的”</h2>\n<p>下面的借助 AI 能力分析基于 <code>vercel-labs/json-render</code> 主仓库。如果你对此不感兴趣，跳过这部分内容。</p>\n<h3 id=\"1-createcatalog能力被冻结的起点\">1. createCatalog：能力被冻结的起点</h3>\n<p>文件路径位于 <code>packages/core/src/create-catalog.ts</code>。这个函数的核心作用不是“注册组件”，而是“冻结能力边界”。</p>\n<p>简化后的核心逻辑可以理解为：</p>\n<pre><code class=\"language-ts\">export function createCatalog(definition) {\n  return {\n    components: definition.components,\n    actions: definition.actions,\n    validateNode(node) {\n      // 校验 type 是否存在\n      // 校验 props 是否符合 Zod schema\n      // 校验 children 是否被允许\n    }\n  }\n}\n</code></pre>\n<p>每一行代码都在服务一个目标：让 Catalog 成为一个不可突破的白名单。Renderer 和 AI 都无法绕过它。这也是为什么 json-render 把 Catalog 放在 core 包中，而不是 React 包中。</p>\n<h3 id=\"2-schema-校验ai-输出必须先编译再执行\">2. Schema 校验：AI 输出必须“先编译再执行”</h3>\n<p>在 JSON Tree 进入 Renderer 之前，系统会逐节点校验。type 是否在 Catalog 中声明，props 是否通过 Zod 校验，children 是否符合 hasChildren 约束，action 是否存在于白名单。这一过程本质上就是一次 AST 校验。</p>\n<p>这意味着 AI 的输出不是“运行时报错”，而是“不通过即拒绝执行”。在 AI UI 系统中，这是一个极其关键但经常被忽视的工程点。</p>\n<h3 id=\"3-renderer真正的解释器模型\">3. Renderer：真正的解释器模型</h3>\n<p>React Renderer 的内部逻辑并不是简单的 switch-case，而是一个递归解释过程。它根据节点的 type 查 Catalog，构造 props，解析 valuePath 注入数据，绑定 action handler，然后递归渲染 children。</p>\n<p>从架构角度看，它更接近一个 JSON AST Interpreter，而不是模板引擎。这也是 json-render 可以跨 React、Vue、Native 复用核心思想的原因。</p>\n<h3 id=\"4-valuepath刻意避免-ai-参与状态逻辑\">4. valuePath：刻意避免 AI 参与状态逻辑</h3>\n<p>valuePath 使用字符串路径描述数据依赖，例如：</p>\n<pre><code class=\"language-json\">\"valuePath\": \"/metrics/revenue\"\n</code></pre>\n<p>这样设计的直接结果是，AI 不需要理解状态结构，也不需要写任何状态逻辑。Renderer 统一负责解析路径、读取数据、触发更新。这在架构上刻意切断了“AI 直接操作状态”的可能性。</p>\n<p>下面是仅包含新增内容的补充章节，重点放在可落到源码层面的机制，避免概念化描述。示例代码与解释均基于 <code>vercel-labs/json-render</code> 当前仓库结构与实现思路。</p>\n<h2 id=\"prompt-与-catalog-的自动对齐\">Prompt 与 Catalog 的自动对齐</h2>\n<p>Prompt 与 Catalog 的自动对齐：不是“调 Prompt”，而是“导出 Grammar”。json-render 中，Prompt 与 Catalog 的对齐并不是通过人肉 Prompt Engineering 完成的，而是通过从 Catalog 派生一份机器可理解的能力描述，并将其注入到模型上下文中。这一点在 <code>packages/core</code> 中的设计非常关键。</p>\n<p>在 core 层，Catalog 本身并不是一个简单的对象，它包含了完整的组件定义、props schema 以及 action 描述。这些信息会被转换为一种“描述性结构”，用于告诉模型当前系统支持的 UI grammar。</p>\n<p>类似这样的逻辑：</p>\n<pre><code class=\"language-ts\">export function catalogToPrompt(catalog) {\n  return `\nYou can generate a JSON UI tree.\nAvailable components:\n${Object.entries(catalog.components).map(([name, def]) =&gt; `\n- ${name}\n  props: ${describeSchema(def.props)}\n  hasChildren: ${def.hasChildren}\n`).join('\\n')}\n\nAvailable actions:\n${Object.keys(catalog.actions).join(', ')}\n\nRules:\n- Output must be valid JSON\n- Only use listed components\n- Follow prop schemas strictly\n`\n}\n</code></pre>\n<p>这里的关键点不在于字符串本身，而在于信息来源完全来自 Catalog。换句话说，Catalog 是 single source of truth，Prompt 只是它的一种序列化视图。当开发者新增或修改组件定义时，Prompt 中允许模型使用的能力会自动发生变化，不存在“代码和 Prompt 不一致”的问题。这也是 json-render 能够避免大量“Prompt 腐化”的根本原因。</p>\n<p>从模型视角看，它面对的不是一段模糊的自然语言说明，而是一套接近 BNF 的 UI grammar 描述。模型生成 JSON UI Tree 的过程，本质上类似于在给定语法约束下生成 AST。这也是为什么 json-render 要使用 Zod 而不是仅靠 TypeScript 类型。Zod schema 可以被同时用于运行时校验和 Prompt 语义描述，形成闭环。</p>\n<h2 id=\"streaming-ui-的实现细节\">Streaming UI 的实现细节</h2>\n<p>流式构建 AST，而不是流式拼字符串。json-render 的 Streaming UI 能力，核心并不在“模型支持流式输出”，而在于 UI 的中间表示是可增量合并的 JSON AST。这一点在 React 包中的实现非常清晰。</p>\n<p>在 <code>packages/react</code> 中，可以看到类似 <code>useUIStream</code> 的 hook，其核心职责是：<br />\n维护一棵当前 UI Tree，并在模型流式输出时不断向这棵树中合并新节点。</p>\n<p>简化后的内部结构大致如下：</p>\n<pre><code class=\"language-ts\">// packages/react/src/use-ui-stream.ts（概念结构）\nexport function useUIStream() {\n  const [tree, setTree] = useState&lt;UITree | null&gt;(null)\n\n  function onChunk(chunk: string) {\n    const partialNode = parseChunkToNode(chunk)\n    if (!partialNode) return\n\n    setTree(prevTree =&gt; {\n      return mergeTree(prevTree, partialNode)\n    })\n  }\n\n  return { tree, onChunk }\n}\n</code></pre>\n<p>这里有两个非常关键但容易被忽略的点。</p>\n<p><code>parseChunkToNode</code> 并不是简单的 <code>JSON.parse</code>。模型在 streaming 模式下输出的通常是不完整 JSON，因此 json-render 采用的是逐段解析、延迟成型的策略。只有当一个节点在结构上是完整且通过 Catalog 校验时，才会被提升为“可合并节点”。<code>mergeTree</code> 是一个纯函数。它不依赖外部状态，只根据已有 UI Tree 和新节点生成下一棵 Tree。这使得每一次更新都是确定性的，也天然适合 React 的状态模型。</p>\n<p>在 Renderer 层，这棵 Tree 会被直接用于递归渲染：</p>\n<pre><code class=\"language-tsx\">function RenderNode({ node }) {\n  const Component = components[node.type]\n\n  const resolvedProps = resolveProps(node.props)\n  const children = node.children?.map(child =&gt;\n    &lt;RenderNode key={child.id} node={child} /&gt;\n  )\n\n  return &lt;Component {...resolvedProps}&gt;{children}&lt;/Component&gt;\n}\n</code></pre>\n<p>由于 Tree 始终是“已校验的合法结构”，Renderer 不需要关心节点是否完整，只需要关心“当前有哪些节点已经存在”。这也是 Streaming UI 能在生成未完成时就安全渲染的根本原因。</p>\n<h2 id=\"streaming-与-catalog-校验如何协同工作\">Streaming 与 Catalog 校验如何协同工作</h2>\n<p>Streaming UI 并不是绕过校验机制的捷径，恰恰相反，它依赖校验机制才能成立。在实际流程中，每一个候选节点在被合并进 UI Tree 之前，都会经过 Catalog 的校验逻辑：</p>\n<pre><code class=\"language-ts\">// packages/core/src/validate-node.ts（概念结构）\nexport function validateNode(node, catalog) {\n  const def = catalog.components[node.type]\n  if (!def) throw new Error('Unknown component')\n\n  def.props.parse(node.props)\n\n  if (node.children &amp;&amp; !def.hasChildren) {\n    throw new Error('Children not allowed')\n  }\n}\n</code></pre>\n<p>Streaming 模式下，这个校验发生得更频繁，但粒度更小。系统宁可“暂时不渲染”，也不会把一个非法节点交给 Renderer。这保证了 UI 在任何时刻都是一个合法子集，而不是半成品垃圾状态。</p>\n<p>Prompt 与 Catalog 的自动对齐，确保模型“不会幻想不存在的能力”；Streaming UI 的 AST 级增量构建，确保 UI“可以在不完整时仍然正确运行”。两者结合，使 json-render 的执行模型更接近编译器与解释器，而不是模板生成器。从工程视角看，这意味着一个重要转变：<strong>UI 生成不再是一次性结果，而是一个可观察、可中断、可回滚的过程。</strong>这也是 json-render 能够真正进入生产系统，而不仅停留在 Demo 层面的根本原因。</p>\n<h2 id=\"json-render-真正解决了什么\">json-render 真正解决了什么</h2>\n<p>json-render 本身并不是一种全新的技术范式。<strong>“用受限结构描述 UI，再由运行时解释执行”这一思想，在前端工程中早已反复出现过。</strong>早期的 JSON Schema Form、react-jsonschema-form、Formily、本质上都是用结构化数据描述界面，再由渲染器生成真实 UI。低代码平台、搭建系统、配置化后台，几乎全部建立在同一逻辑之上。即便在 AI 出现之前，这种模式也已经非常成熟：工程师通过 schema 描述组件、属性和布局，运行时负责校验与渲染，业务侧只操作结构而不直接触碰代码。json-render 并没有发明这种模式，它继承的正是这一整条技术脉络。</p>\n<p>json-render 的不同之处在于，它首次把“模型生成”作为一等公民纳入设计前提。传统 schema UI 假设配置由人编写，因此更强调完整性、可读性和编辑体验；而 json-render 假设结构由模型生成，因此更强调语法边界清晰、失败可恢复、部分结果可执行，以及与 Prompt 的自动对齐能力。从这个角度看，json-render 更像是“为 AI 重新设计的一代 schema UI 执行模型”。它真正解决的问题并不是“怎么用 JSON 渲染 UI”，而是当结构来源变成不可靠的模型时，工程边界应该在哪里。它给出的答案非常明确：AI 只负责生成结构化意图，工程师负责能力定义、执行与渲染，JSON 作为唯一中介和约束层。这使得 AI UI 不再是一次性 Demo，而是可以进入生产系统的工程能力。在当前阶段，这是少数真正站在工程立场思考 AI UI 的方案之一。</p>\n<h2 id=\"参考资料\">参考资料</h2>\n<ul>\n<li><a href=\"https://json-render.dev/\" rel=\"noopener nofollow\" target=\"_blank\">json-render.dev</a></li>\n<li><a href=\"https://github.com/vercel-labs/json-render\" rel=\"noopener nofollow\" target=\"_blank\">github.com/vercel-labs/json-render</a></li>\n<li><a href=\"https://vercel.com/blog\" rel=\"noopener nofollow\" target=\"_blank\">vercel.com/blog</a></li>\n<li><a href=\"https://www.cnblogs.com/guangzan/p/19350726\" target=\"_blank\">Zod：TypeScript 类型守卫与数据验证</a></li>\n</ul>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 15:23</span>&nbsp;\n<a href=\"https://www.cnblogs.com/guangzan\">guangzan</a>&nbsp;\n阅读(<span id=\"post_view_count\">316</span>)&nbsp;\n评论(<span id=\"post_comment_count\">2</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "SeaTunnel(2.3.12)的高级用法（四）：多个source、多个sink",
      "link": "https://www.cnblogs.com/kakarotto-chen/p/19487270",
      "published": "",
      "description": "<h2>\n            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/kakarotto-chen/p/19487270\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 15:08\">\n    <span>SeaTunnel(2.3.12)的高级用法（四）：多个source、多个sink</span>\n    \n\n</a>\n\n        </h2>\n        <div class=\"postbody\">\n            <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"前置知识seatunnel配置中有数据流data-flow转的概念\">前置知识：seatunnel配置中有数据流（Data Flow）转的概念</h2>\n<p>见：<a href=\"https://www.cnblogs.com/kakarotto-chen/p/19487384\" target=\"_blank\">https://www.cnblogs.com/kakarotto-chen/p/19487384</a></p>\n<h2 id=\"demo1两个source汇聚到一个sink\">demo1：两个source汇聚到一个sink</h2>\n<ul>\n<li>\n<p>关键配置：sink的：plugin_input = [\"source_data1\", \"source_data2\"]</p>\n</li>\n<li>\n<p>对应模型</p>\n</li>\n</ul>\n<pre><code>┌──────────┐\n│ Source A │──┐\n└──────────┘  │\n              ├──▶  Sink\n┌──────────┐  │\n│ Source B │──┘\n└──────────┘\n</code></pre>\n<ul>\n<li>执行语句</li>\n</ul>\n<pre><code># ds-st-demo10-2-mysql2pgsql.conf\nsh /data/tools/seatunnel/seatunnel-2.3.12/bin/seatunnel.sh --config /data/tools/seatunnel/myconf/ds-st-demo10-2-mysql2pgsql.conf -i -DJvmOption=\"-Xms2G -Xmx2G\" -m local\n</code></pre>\n<ul>\n<li>建表</li>\n</ul>\n<pre><code>-- ds-st-demo10-2-mysql2pgsql.conf\nCREATE TABLE \"public\".\"t_8_100w_imp_st_ds_demo10\" (\n  id BIGINT PRIMARY KEY,\n  user_name VARCHAR(2000),\n  sex VARCHAR(20),\n  decimal_f NUMERIC(32, 6),\n  phone_number VARCHAR(20),\n  age INT,\n  create_time TIMESTAMP,\n  description TEXT,\n  address VARCHAR(2000) DEFAULT '未知',\n  my_status INT\n);\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"id\" IS '主键';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"user_name\" IS '名字';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"sex\" IS '性别：男；女';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"decimal_f\" IS '大数字';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"phone_number\" IS '电话';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"age\" IS '字符串年龄转数字';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"create_time\" IS '新增时间';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"description\" IS '大文本';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"address\" IS '空地址转默认值：未知';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"my_status\" IS '状态';\n\n</code></pre>\n<ul>\n<li>conf配置</li>\n</ul>\n<pre><code>env {\n  # 任务名字：业务中可以弄表id\n  job.name = \"ds-st-demo10.conf\"\n  # 最大批线程数：并行度（线程数）\n  parallelism = 5\n  # 任务模式：BATCH:批处理模式；STREAMING:流处理模式\n  job.mode = \"BATCH\"\n}\n\nsource {\n  # 第一个数据集\n  jdbc {\n    # 给这个数据集起个名字\n    plugin_output = \"source_data1\"\n  \n    url = \"jdbc:mysql://ip:port/cs1\"\n    driver = \"com.mysql.cj.jdbc.Driver\"\n    user = \"root\"\n    password = \"***\"\n    # sql\n    query = \"select id,name as user_name,sex,decimal_f,phone_number,CAST(age AS SIGNED) as age,create_time,description,address from t_8_100w where id &lt; 10\"\n    \n    # 并行读取配置\n    # 分片的字段：支持：String、Number(int, bigint, decimal, ...)、Date\n    partition_column = \"id\"\n    # 表的分割大小（行数）：每个分片的数据行（默认8096行）。最后分片数=表的总行数 / split.size\n    split.size = 50000\n    # 分片数，匹配并行度parallelism（2.3.12已不推荐配置了，用split.size来代替）\n    # partition_num = 5\n    # 最大批处理数:查询的行提取大小(指定当前任务每次执行时读取数据条数,该值(默认1000)受运行内存影响,若该值较大或单条数据量较大，需适当调整运行内存大小。)\n    fetch_size = 10000\n    \n    # 连接参数\n    # 连接超时时间300ms\n    connection_check_timeout_sec = 300\n    # 其他jdbc的参数\n    properties = {\n      useUnicode = true\n      characterEncoding = \"utf8\"\n      # 时区，不同数据库参数不一样\n      serverTimezone = \"Asia/Shanghai\"\n      # 使用游标提高大结果集性能\n      useCursorFetch = \"true\"\n      # 每次获取行数\n      defaultFetchSize = \"10000\"\n    }\n  }\n  \n  # 第二个数据集\n  jdbc {\n    # 给这个数据集起个名字\n    plugin_output = \"source_data2\"\n  \n    url = \"jdbc:mysql://ip:port/cs1\"\n    driver = \"com.mysql.cj.jdbc.Driver\"\n    user = \"root\"\n    password = \"***\"\n    # \n    query = \"select id,name as user_name,sex,decimal_f,phone_number,CAST(age AS SIGNED) as age,create_time,description,address from t_8_100w where id &gt; 10 and id &lt; 20\"\n    \n    # 并行读取配置\n    # 分片的字段：支持：String、Number(int, bigint, decimal, ...)、Date\n    partition_column = \"id\"\n    # 表的分割大小（行数）：每个分片的数据行（默认8096行）。最后分片数=表的总行数 / split.size\n    split.size = 50000\n    # 分片数，匹配并行度parallelism（2.3.12已不推荐配置了，用split.size来代替）\n    # partition_num = 5\n    # 最大批处理数:查询的行提取大小(指定当前任务每次执行时读取数据条数,该值(默认1000)受运行内存影响,若该值较大或单条数据量较大，需适当调整运行内存大小。)\n    fetch_size = 10000\n    \n    # 连接参数\n    # 连接超时时间300ms\n    connection_check_timeout_sec = 300\n    # 其他jdbc的参数\n    properties = {\n      useUnicode = true\n      characterEncoding = \"utf8\"\n      # 时区，不同数据库参数不一样\n      serverTimezone = \"Asia/Shanghai\"\n      # 使用游标提高大结果集性能\n      useCursorFetch = \"true\"\n      # 每次获取行数\n      defaultFetchSize = \"10000\"\n    }\n  }\n}\n\n# 清洗转换（简单的清洗转换，直接在source的query的sql中处理了就行）\ntransform {\n  # 1. 字段映射：sql中做了，实际生成中不在这里处理。直接在source的query的sql中处理了就行\n  # 还可以用：FieldMapper 插件，来映射字段\n  \n  # 转换age为数字类型（pgsql必须转）\n  \n  # 2. 手机号脱敏：13812341234 -&gt; 138****1234\n  \n  # 3. 年龄转换：字符串转整数（实际生产中，不用转换，也没有内置的转换插件，可以直接保存成功）\n\n  # 4. 性别转换：1-&gt;男，2-&gt;女\n  \n  # 5. 数据过滤：只保留 age &gt; 25 的记录。\n  \n  # 6. 地址默认值：空地址设为'未知'\n}\n\nsink {\n  jdbc {\n    # 接收的最终数据集（汇聚到一个结果中）\n    plugin_input = [\"source_data1\", \"source_data2\"]\n    \n    url = \"jdbc:postgresql://ip:5432/source_db\"\n    driver = \"org.postgresql.Driver\"\n    user = \"postgres\"\n    password = \"123456\"\n    # \n    # query = \"\"\n    \n    # 自动生成sql的配置，和query参数互斥\n    # 生成自动插入sql。如果目标库没有表，也会自动建表\n    generate_sink_sql = true\n    # database必须要，因为generate_sink_sql=true。\n    database = source_db\n    # 自动生成sql时，table必须要。\n    table = \"public.t_8_100w_imp_st_ds_demo10\"\n    # 生成类似：INSERT INTO …… ON CONFLICT (\"主键\") DO UPDATE SET …… 的sql\n    # enable_upsert = true\n    # 判断值唯一的健：此选项用于支持在自动生成 SQL 时进行 insert，delete 和 update 操作。\n    # primary_keys = [\"id\"]\n\n    # 表结构处理策略：表不存在时报错（任务失败），一般用：CREATE_SCHEMA_WHEN_NOT_EXIST（表不存在时创建表；表存在时跳过操作（保留数据））\n    schema_save_mode = \"ERROR_WHEN_SCHEMA_NOT_EXIST\"\n    # 插入数据的处理策略\n    # APPEND_DATA：保留表结构和数据，追加新数据（不删除现有数据）(一般用这个)\n    # DROP_DATA：保留表结构，删除表中所有数据（清空表）——实现清空重灌\n    # CUSTOM_PROCESSING :用户定义处理。需要配合：custom_sql使用\n    data_save_mode = \"DROP_DATA\"\n    # 当 data_save_mode 选择 CUSTOM_PROCESSING 时，您应该填写 CUSTOM_SQL 参数。此参数通常填入可执行的 SQL。SQL 将在同步任务之前执行。\n    #可以实现：同步删除（执行前置update、truncate的sql等）\n    #这个sql未执行，不知道为啥。\n    #这个sql已经执行。原因：因为generate_sink_sql=true的原因。才会执行custom_sql。（只有自动生成sql的时候，这个才会执行）\n    custom_sql = \"\"\"update \"source_db\".\"public\".\"t_8_100w_imp_st_ds_demo10\" set \"my_status\" = 23\"\"\"\n    \n    # 批量写入条数\n    batch_size = 10000\n    # 批次提交间隔\n    batch_interval_ms = 500\n    # 重试次数\n    max_retries = 3\n    \n    # 连接参数\n    # 连接超时时间300ms\n    connection_check_timeout_sec = 300\n    # 其他jdbc的参数\n    properties = {\n      # PostgreSQL专用参数\n      # PostgreSQL的批量优化（注意大小写）\n      reWriteBatchedInserts = \"true\"  \n      # 如果需要时区设置\n      options = \"-c timezone=Asia/Shanghai\"\n    }\n  }\n}\n</code></pre>\n<ul>\n<li>结果(汇聚了19条数据)</li>\n</ul>\n<pre><code>2026-01-15 14:28:15,952 INFO  [s.c.s.s.c.ClientExecuteCommand] [main] - \n***********************************************\n           Job Statistic Information\n***********************************************\nStart Time                : 2026-01-15 14:28:11\nEnd Time                  : 2026-01-15 14:28:15\nTotal Time(s)             :                   4\nTotal Read Count          :                  19\nTotal Write Count         :                  19\nTotal Failed Count        :                   0\n***********************************************\n</code></pre>\n<h2 id=\"demo2一个source分发到两个sink\">demo2：一个source分发到两个sink</h2>\n<p>……………………未完待续</p>\n\n</div>\n<div class=\"clear\"></div>\n\n        </div>\n        <p class=\"postfoot\">\n            posted on \n<span id=\"post-date\">2026-01-15 15:08</span>&nbsp;\n<a href=\"https://www.cnblogs.com/kakarotto-chen\">C_C_菜园</a>&nbsp;\n阅读(<span id=\"post_view_count\">57</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n        </p>"
    },
    {
      "title": "如何通过 C# 将 PPT 文档转换为 PDF 格式",
      "link": "https://www.cnblogs.com/jazz-z/p/19486170",
      "published": "",
      "description": "<div class=\"postcontent\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在日常开发和办公场景中，将 PowerPoint（PPT/PPTX） 转换为 PDF 格式是高频需求。PDF 格式具有跨平台兼容性强、格式固定不易篡改、便于分发归档等优势。本文将介绍如何使用一款 .NET PowerPoint 组件通过 C# 实现 PPT 转 PDF，并提供完整代码示例。</p>\n<h2 id=\"1-安装-net-库\">1. 安装 .NET 库</h2>\n<p>Spire.Presentation 是一款专门用于处理 PowerPoint 文档的 .NET 组件，无需依赖 Microsoft Office 或 PowerPoint 客户端即可完成 PPT 文档的读取、编辑和格式转换。推荐通过 NuGet 包管理器安装，步骤如下：</p>\n<ol>\n<li>打开 Visual Studio，创建任意 C# 项目（如Console App）；</li>\n<li>右键项目→“管理NuGet程序包”；</li>\n<li>搜索“Spire.Presentation”，选择对应版本安装；</li>\n<li>也可通过NuGet命令行安装：</li>\n</ol>\n<pre><code class=\"language-bash\">Install-Package Spire.Presentation\n</code></pre>\n<h2 id=\"2-基础示例单个-powerpoint-文件转-pdf\">2. 基础示例：单个 PowerPoint 文件转 PDF</h2>\n<p>这是最常用的场景，支持 PPT/PPTX 格式输入，直接通过 <code>SaveToFile</code> 方法输出为 PDF 文件：</p>\n<pre><code class=\"language-csharp\">using System;\nusing Spire.Presentation;\n\nnamespace PptToPdfDemo\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            try\n            {\n                // 1. 定义文件路径\n                string pptFilePath = @\"D:\\Demo\\source.pptx\"; // 输入PPT路径\n                string pdfFilePath = @\"D:\\Demo\\output.pdf\";   // 输出PDF路径\n\n                // 2. 加载PPT文档\n                Presentation presentation = new Presentation();\n                presentation.LoadFromFile(pptFilePath);\n\n                // 3. 转换为PDF并保存\n                // 可选参数：PDF导出选项（如压缩、权限等），此处使用默认配置\n                presentation.SaveToFile(pdfFilePath, FileFormat.PDF);\n\n                // 4. 释放资源（关键，避免内存泄漏）\n                presentation.Dispose();\n\n                Console.WriteLine(\"PPT转PDF成功！\");\n            }\n            catch (Exception ex)\n            {\n                // 异常处理：捕获文件不存在、格式不支持、权限不足等问题\n                Console.WriteLine($\"转换失败：{ex.Message}\");\n            }\n        }\n    }\n}\n</code></pre>\n<h3 id=\"3-批量转换转换多个-powerpoint-文件为-pdf\">3. 批量转换：转换多个 PowerPoint 文件为 PDF</h3>\n<p>通过遍历文件夹实现批量转换，适合处理大量 PPT 文件：</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.IO;\nusing Spire.Presentation;\n\nnamespace BatchPptToPdf\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            // 源PPT文件夹路径\n            string pptFolderPath = @\"D:\\Demo\\PptFiles\";\n            // 输出PDF文件夹路径\n            string pdfFolderPath = @\"D:\\Demo\\PdfFiles\";\n\n            // 确保输出文件夹存在\n            if (!Directory.Exists(pdfFolderPath))\n            {\n                Directory.CreateDirectory(pdfFolderPath);\n            }\n\n            // 遍历文件夹中的PPT/PPTX文件\n            string[] pptFiles = Directory.GetFiles(pptFolderPath, \"*\", SearchOption.TopDirectoryOnly)\n                .Where(file =&gt; file.EndsWith(\".ppt\", StringComparison.OrdinalIgnoreCase) \n                             || file.EndsWith(\".pptx\", StringComparison.OrdinalIgnoreCase))\n                .ToArray();\n\n            foreach (string pptFile in pptFiles)\n            {\n                try\n                {\n                    // 获取文件名（不含扩展名），用于生成PDF文件名\n                    string fileName = Path.GetFileNameWithoutExtension(pptFile);\n                    string pdfFile = Path.Combine(pdfFolderPath, $\"{fileName}.pdf\");\n\n                    // 加载并转换\n                    using (Presentation presentation = new Presentation())\n                    {\n                        presentation.LoadFromFile(pptFile);\n                        presentation.SaveToFile(pdfFile, FileFormat.PDF);\n                    }\n\n                    Console.WriteLine($\"已转换：{pptFile} → {pdfFile}\");\n                }\n                catch (Exception ex)\n                {\n                    Console.WriteLine($\"转换失败 {pptFile}：{ex.Message}\");\n                }\n            }\n\n            Console.WriteLine(\"批量转换完成！\");\n        }\n    }\n}\n</code></pre>\n<h3 id=\"4-进阶示例将-powerpoint-转换为加密的-pdf\">4. 进阶示例：将 PowerPoint 转换为加密的 PDF</h3>\n<p>还可以在转换时直接加密保护 PDF 文件，并为 PDF 设置权限：</p>\n<pre><code class=\"language-csharp\">using Spire.Presentation;\nusing Spire.Presentation.External.Pdf;\n\nnamespace ConvertToEncryptedPdf\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            // 定义明确的文件路径（建议替换为你的实际路径）\n            string inputPptPath = @\"C:\\Users\\Administrator\\Desktop\\Input.pptx\";\n            string outputPdfPath = @\"C:\\Users\\Administrator\\Desktop\\ToEncryptedPdf.pdf\";\n            \n            // 使用using语句自动释放Presentation资源（优于手动Dispose）\n            try\n            {\n                using (Presentation presentation = new Presentation())\n                {\n                    // 加载PPT文件\n                    presentation.LoadFromFile(inputPptPath);\n\n                    // 获取PDF保存选项\n                    SaveToPdfOption option = presentation.SaveToPdfOption;\n                    \n                    // 设置PDF密码和权限\n                    // 参数说明：\n                    // 1. 用户密码（打开PDF需要输入的密码）：abc-123\n                    // 2. 所有者密码（用于修改PDF权限的密码）：owner-456（可自定义）\n                    // 3. PDF权限：允许打印 + 允许填写表单\n                    // 4. 加密级别：默认128位（高安全性）\n                  option.PdfSecurity.Encrypt(\"abc-123\", \"owner-456\", \n                        PdfPermissionsFlags.Print | PdfPermissionsFlags.FillFields, \n                        PdfEncryptionKeySize.Key128Bit);\n\n                    // 保存为加密PDF\n                    presentation.SaveToFile(outputPdfPath, FileFormat.PDF, pdfOptions);\n\n                    Console.WriteLine(\"PPT已成功转换为加密PDF！\");\n                    Console.WriteLine($\"输出路径：{outputPdfPath}\");\n                }\n            }\n            catch (Exception ex)\n            {\n                // 捕获所有可能的异常并提示\n                Console.WriteLine($\"转换失败：{ex.Message}\");\n            }\n\n            // 暂停控制台，便于查看结果\n            Console.ReadLine();\n        }\n    }\n}\n</code></pre>\n<h2 id=\"5-关键注意事项\">5. 关键注意事项</h2>\n<ol>\n<li>\n<p><strong>格式兼容性</strong>：</p>\n<ul>\n<li>支持输入格式：PPT、PPTX、PPS、PPSX 等；</li>\n<li>复杂PPT元素（如3D图表、自定义动画、嵌入式视频）转换后可能丢失或显示异常（。</li>\n</ul>\n</li>\n<li>\n<p><strong>资源释放</strong>：</p>\n<ul>\n<li>必须通过 <code>Dispose()</code> 方法释放 <code>Presentation</code> 对象，或使用 <code>using</code> 语句（推荐），否则易导致内存泄漏，尤其批量转换时需注意。</li>\n</ul>\n</li>\n<li>\n<p><strong>权限问题</strong>：</p>\n<ul>\n<li>确保程序对输入/输出路径有读写权限，否则会抛出 <code>UnauthorizedAccessException</code>。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"6-替代方案参考\">6. 替代方案参考</h2>\n<ol>\n<li><strong>LibreOffice SDK</strong>：免费开源，需部署 LibreOffice 服务，API 较复杂；</li>\n<li><strong>OpenXML SDK + iTextSharp</strong>：仅支持 PPTX（OpenXML 格式），需自行处理布局转换，开发成本高；</li>\n<li><strong>GroupDocs.Conversion</strong>：有免费额度，云原生支持，但依赖网络。</li>\n</ol>\n<hr />\n<p>本文提供了可靠的 C# PowerPoint 转 PDF 解决方案，特别适合在服务器环境或无需安装 Microsoft Office 的场景中使用。其优点包括部署简单、API 设计清晰、支持多种输出选项等。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"itemdesc\">\n                发表于 \n<span id=\"post-date\">2026-01-15 10:37</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jazz-z\">LAYONTHEGROUND</a>&nbsp;\n阅读(<span id=\"post_view_count\">158</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n            </div>"
    },
    {
      "title": "【大数据 & AI】Flink Agents 源码解读 --- (7) ---  AgentsExecutionEnvironment",
      "link": "https://www.cnblogs.com/rossiXYZ/p/19489355",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/rossiXYZ/p/19489355\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 21:56\">\n    <span>【大数据 &amp; AI】Flink Agents 源码解读 --- (7) ---  AgentsExecutionEnvironment</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"大数据--aiflink-agents-源码解读-----7------agentsexecutionenvironment\">【大数据 &amp; AI】Flink Agents 源码解读 --- (7) ---  AgentsExecutionEnvironment</h1>\n<p></p><div class=\"toc\"><div class=\"toc-container-header\">目录</div><ul><li><a href=\"#大数据--aiflink-agents-源码解读-----7------agentsexecutionenvironment\" rel=\"noopener nofollow\">【大数据 &amp; AI】Flink Agents 源码解读 --- (7) ---  AgentsExecutionEnvironment</a><ul><li><a href=\"#0x00-概要\" rel=\"noopener nofollow\">0x00 概要</a></li><li><a href=\"#0x01-基础知识\" rel=\"noopener nofollow\">0x01 基础知识</a><ul><li><a href=\"#11-定义\" rel=\"noopener nofollow\">1.1 定义</a></li><li><a href=\"#12-功能\" rel=\"noopener nofollow\">1.2 功能</a></li><li><a href=\"#13-与原生-flink-environment-的区别\" rel=\"noopener nofollow\">1.3 与原生 Flink Environment 的区别</a></li></ul></li><li><a href=\"#0x02-localexecutionenvironment\" rel=\"noopener nofollow\">0x02 LocalExecutionEnvironment</a><ul><li><a href=\"#21-定义\" rel=\"noopener nofollow\">2.1 定义</a><ul><li><a href=\"#211-主要功能\" rel=\"noopener nofollow\">2.1.1 主要功能</a></li><li><a href=\"#212-代码\" rel=\"noopener nofollow\">2.1.2 代码</a></li><li><a href=\"#213-执行流程\" rel=\"noopener nofollow\">2.1.3 执行流程</a></li><li><a href=\"#214-组件关系\" rel=\"noopener nofollow\">2.1.4 组件关系</a></li></ul></li><li><a href=\"#22-localrunner\" rel=\"noopener nofollow\">2.2 LocalRunner</a><ul><li><a href=\"#221-localrunner-的主要功能\" rel=\"noopener nofollow\">2.2.1 LocalRunner 的主要功能</a></li><li><a href=\"#222-事件驱动执行模型\" rel=\"noopener nofollow\">2.2.2 事件驱动执行模型</a></li><li><a href=\"#223-agentplan-和-localrunner-的关系\" rel=\"noopener nofollow\">2.2.3 AgentPlan 和 LocalRunner 的关系</a></li><li><a href=\"#224-为什么要为每种-key-维护独立的上下文\" rel=\"noopener nofollow\">2.2.4 为什么要为每种 key 维护独立的上下文</a></li><li><a href=\"#225-实际应用场景示例\" rel=\"noopener nofollow\">2.2.5 实际应用场景示例</a></li></ul></li><li><a href=\"#23-localrunnercontext\" rel=\"noopener nofollow\">2.3 LocalRunnerContext</a><ul><li><a href=\"#231-设计目的\" rel=\"noopener nofollow\">2.3.1 设计目的</a></li><li><a href=\"#232-主要功能\" rel=\"noopener nofollow\">2.3.2 主要功能</a></li><li><a href=\"#233-定义\" rel=\"noopener nofollow\">2.3.3 定义</a></li></ul></li></ul></li><li><a href=\"#0x03-remoteexecutionenvironment\" rel=\"noopener nofollow\">0x03 RemoteExecutionEnvironment</a><ul><li><a href=\"#31-核心特色\" rel=\"noopener nofollow\">3.1 核心特色</a></li><li><a href=\"#32-与原生-flink-remoteenvironment-的对比\" rel=\"noopener nofollow\">3.2 与原生 Flink RemoteEnvironment 的对比</a></li><li><a href=\"#33-如何使用-actionexecutionoperator\" rel=\"noopener nofollow\">3.3 如何使用 ActionExecutionOperator</a></li><li><a href=\"#34-典型流程\" rel=\"noopener nofollow\">3.4 典型流程</a></li><li><a href=\"#35-交互逻辑\" rel=\"noopener nofollow\">3.5 交互逻辑</a></li><li><a href=\"#36-实现\" rel=\"noopener nofollow\">3.6 实现</a></li></ul></li><li><a href=\"#0xff-参考\" rel=\"noopener nofollow\">0xFF 参考</a></li></ul></li></ul></div><p></p>\n<h2 id=\"0x00-概要\">0x00 概要</h2>\n<p>AgentsExecutionEnvironment 是在Flink基础上构建的一个更高层次的执行环境，专门为Agent而设计，同时保留了与原生 Flink API 的兼容性。</p>\n<h2 id=\"0x01-基础知识\">0x01 基础知识</h2>\n<p>可以把 Flink 原生的 <code>StreamExecutionEnvironment</code> / <code>TableEnvironment</code> 理解成“Agent 话题里所说的<strong>执行环境</strong>（Execution Environment）”，但是其无法直接被 Agent 使用。因此需要在其上做一些封装和适配，这就是 AgentsExecutionEnvironment。</p>\n<ol>\n<li>Flink Environment = 纯粹的<strong>计算资源与运行时容器</strong>\n<ul>\n<li>负责申请 Slot、管理 Checkpoint、调度算子链、网络 Shuffle</li>\n<li>对“业务语义”一无所知，也不管用户写的是 ETL、CEP 还是 AI 推理</li>\n<li>对应 Agent 词汇表里的“Runtime / Cluster / Engine”这一层</li>\n</ul>\n</li>\n<li>Agent Environment = 在 Flink Runtime 之上包了一层<strong>语义抽象</strong>\n<ul>\n<li>替用户注册 Chat Model、Tools、Memory、Actions、Event Schema</li>\n<li>把“用户消息”或“传感器事件”封装成 Event，按 AgentPlan 去调大模型、执行业务动作</li>\n<li>内部仍然用 <code>StreamExecutionEnvironment</code> 去提交算子，只是看不到显式的 <code>keyBy()</code> 和 <code>process()</code>——框架帮用户生成了 <code>ActionExecutionOperator</code></li>\n</ul>\n</li>\n</ol>\n<p>类比关系如下：</p>\n<pre><code class=\"language-python\">传统 Flink 程序\n├─ StreamExecutionEnvironment   ← 纯运行时\n├─ your ProcessFunction         ← 用户的业务逻辑\n└─ DataStream\n\nFlink Agents 程序\n├─ AgentsExecutionEnvironment   ← Agent 语义环境，内部仍持有 StreamExecutionEnvironment\n├─ AgentPlan（your business）   ← 定义“遇到啥事件该干啥”\n└─ ActionExecutionOperator     ← 框架替用户生成的算子，负责调大模型、更新记忆\n</code></pre>\n<h3 id=\"11-定义\">1.1 定义</h3>\n<p>AgentsExecutionEnvironment 的代码如下。</p>\n<pre><code class=\"language-java\">/**\n * Base class for agent execution environment.\n *\n * &lt;p&gt;This class provides the main entry point for integrating Flink Agents with different types of\n * Flink data sources (DataStream, Table, or simple lists).\n */\npublic abstract class AgentsExecutionEnvironment {\n    protected final Map&lt;ResourceType, Map&lt;String, Object&gt;&gt; resources;\n\n    protected AgentsExecutionEnvironment() {\n        this.resources = new HashMap&lt;&gt;();\n        for (ResourceType type : ResourceType.values()) {\n            this.resources.put(type, new HashMap&lt;&gt;());\n        }\n    }\n    \n    /**\n     * Get agents execution environment.\n     *\n     * &lt;p&gt;Factory method that creates an appropriate execution environment based on the provided\n     * StreamExecutionEnvironment. If no environment is provided, a local execution environment is\n     * returned for testing and development.\n     *\n     * &lt;p&gt;When integrating with Flink DataStream/Table APIs, users should pass the Flink\n     * StreamExecutionEnvironment to enable remote execution capabilities.\n     *\n     * @param env Optional StreamExecutionEnvironment for remote execution. If null, a local\n     *     execution environment will be created.\n     * @param tEnv Optional StreamTableEnvironment for table-to-stream conversion.\n     * @return AgentsExecutionEnvironment appropriate for the execution context.\n     */\n    public static AgentsExecutionEnvironment getExecutionEnvironment(\n            StreamExecutionEnvironment env, @Nullable StreamTableEnvironment tEnv) {\n        if (env == null) {\n            // Return local execution environment for testing/development\n            try {\n                Class&lt;?&gt; localEnvClass =\n                        Class.forName(                                \"org.apache.flink.agents.runtime.env.LocalExecutionEnvironment\");\n                return (AgentsExecutionEnvironment)\n                        localEnvClass.getDeclaredConstructor().newInstance();\n            } catch (Exception e) {\n                throw new RuntimeException(\"Failed to create LocalExecutionEnvironment\", e);\n            }\n        } else {\n            // Return remote execution environment for Flink integration\n            try {\n                Class&lt;?&gt; remoteEnvClass =\n                        Class.forName(                                \"org.apache.flink.agents.runtime.env.RemoteExecutionEnvironment\");\n                return (AgentsExecutionEnvironment)\n                        remoteEnvClass\n                                .getDeclaredConstructor(\n                                        StreamExecutionEnvironment.class,\n                                        StreamTableEnvironment.class)\n                                .newInstance(env, tEnv);\n            } catch (Exception e) {\n                throw new RuntimeException(\"Failed to create RemoteExecutionEnvironment\", e);\n            }\n        }\n    }    \n</code></pre>\n<h3 id=\"12-功能\">1.2 功能</h3>\n<p>AgentsExecutionEnvironment 的功能如下：</p>\n<ul>\n<li>统一入口：\n<ul>\n<li>提供getExecutionEnvironment()系列静态工厂方法，可以依据是否传入Flink的 StreamExecutionEnvironment来创建本地或远程执行环境。</li>\n<li>支持从不同数据源构建构建Agent管道，包括List，DataStream和Table。</li>\n</ul>\n</li>\n<li>资源配置管理：\n<ul>\n<li>内置 resources 映射结构，支持按照资源类型管理各种资源。</li>\n<li>提供 addResource() 方法注册可序列化的资源或者资源描述符。</li>\n</ul>\n</li>\n<li>多输入源支持：\n<ul>\n<li>fromList()：支持从简单列表创建本地执行环境。</li>\n<li>fromDataStream()：集成Flink DataStream API</li>\n<li>fromTable()：集成Flink Table API</li>\n</ul>\n</li>\n<li>配置管理：\n<ul>\n<li>提供getConfig()抽象方法获取可写的配置对象。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"13-与原生-flink-environment-的区别\">1.3 与原生 Flink Environment 的区别</h3>\n<ul>\n<li>抽象层级更高：AgentExecutionEnvrionment是对Flink原生环境的封装，在其之上提供了面向Agent的编程模型。主要用于运行Agent，而非 直接处理数据流。</li>\n<li>执行模式：AgentExecutionEnvrionment 通过 LocalExecutionEnvironment 和 RemoteExecutionEnvironment 分别支持本地测试和远程集群执行。原生 Flink 主要通过配置参数控制执行模式。</li>\n<li>资源管理机制：AgentExecutionEnvrionment 内置了专门的资源注册和管理机制，支持按类型分类管理支援。原生 Flink 没有这种结构化的资源管理方式。</li>\n<li>API 设计目标：面向 Agent 编程范式，强调事件驱动的自主行为体模型。原生Flink 更关注数据流处理和批处理操作。</li>\n</ul>\n<h2 id=\"0x02-localexecutionenvironment\">0x02 LocalExecutionEnvironment</h2>\n<p>LocalExecutionEnvironment 是 AgentsExecutionEnvironment 的一种实现形式。</p>\n<pre><code class=\"language-java\">Class&lt;?&gt; localEnvClass = Class.forName(                                \"org.apache.flink.agents.runtime.env.LocalExecutionEnvironment\");\n                return (AgentsExecutionEnvironment)\n                        localEnvClass.getDeclaredConstructor().newInstance();\n</code></pre>\n<p>LocalExecutionEnvironment  主要用于</p>\n<ul>\n<li>开发阶段的快速测试和调试</li>\n<li>无需 Flink 集群即可验证代理逻辑</li>\n<li>简化Agent应用的本地开发流程</li>\n<li>它与远程执行环境形成对比，后者支持完整的 Flink DataStream 和 Table API 集成。</li>\n</ul>\n<h3 id=\"21-定义\">2.1 定义</h3>\n<h4 id=\"211-主要功能\">2.1.1 主要功能</h4>\n<p>LocalExecutionEnvironment  的主要功能如下：</p>\n<ul>\n<li>本地执行环境实现\n<ul>\n<li>集成自 AgentExecutionEnvrionment，为本地测试和开发提供执行环境</li>\n<li>不依赖 Flink 集群，可以在本地环境中运行和调试代理</li>\n</ul>\n</li>\n<li>数据源支持\n<ul>\n<li>通过from_list方法支持从列表数据源读取输入数据</li>\n<li>不支持 Flink 的DataStream 和 Table API（这些在远程执行环境中使用）</li>\n</ul>\n</li>\n<li>资源配置和管理\n<ul>\n<li>存储和管理通过 add_resource 方法注册的资源</li>\n<li>在构建Agent时，将环境中的资源注入到Agent实例中</li>\n</ul>\n</li>\n<li>代理执行管理\n<ul>\n<li>通过  set_agent 方法设置待执行的Agent、输入和输出</li>\n<li>使用 LocalRunner  在本地执行代理逻辑</li>\n<li>通过 execute 方法触发代理执行</li>\n</ul>\n</li>\n<li>结果收集\n<ul>\n<li>收集Agent执行的输出结果</li>\n<li>通过 to_list  方法返回执行结果</li>\n</ul>\n</li>\n</ul>\n<p>根据代码分析， execute 函数在以下情况下被调用：</p>\n<ul>\n<li>用户手动调用。当用户完成代理配置和输入数据设置后，需要显式调用execute()  方法来启动Agent 执行流程，这通常式配置完所有必要组件后的最后一步。</li>\n</ul>\n<h4 id=\"212-代码\">2.1.2 代码</h4>\n<p>LocalExecutionEnvironment 的定义如下，其中 LocalRunner 是核心。</p>\n<pre><code class=\"language-java\">class LocalExecutionEnvironment(AgentsExecutionEnvironment):\n    \"\"\"Implementation of AgentsExecutionEnvironment for local execution environment.\"\"\"\n\n    __input: List[Dict[str, Any]] = None\n    __output: List[Any] = None\n    __runner: LocalRunner = None\n    __executed: bool = False\n    __config: AgentConfiguration = AgentConfiguration()\n        \n    def set_agent(self, input: list, output: list, runner: LocalRunner) -&gt; None:\n        \"\"\"Set agent input, output and runner.\"\"\"\n        self.__input = input\n        self.__runner = runner\n        self.__output = output\n\n    def execute(self) -&gt; None:\n        \"\"\"Execute agent individually.\"\"\"\n        if self.__executed:\n            err_msg = (\n                \"LocalExecutionEnvironment doesn't support execute multiple times.\"\n            )\n            raise RuntimeError(err_msg)\n        self.__executed = True\n        for input in self.__input:\n            self.__runner.run(**input)\n        outputs = self.__runner.get_outputs()\n        for output in outputs:\n            self.__output.append(output)        \n</code></pre>\n<h4 id=\"213-执行流程\">2.1.3 执行流程</h4>\n<p>在 LocalExecutionEnvironment 中，execute 方法会：</p>\n<ul>\n<li>遍历所有输入数据项</li>\n<li>对每个输入调用 LocalRunner  的run方法进行处理</li>\n<li>收集所有输出结果</li>\n</ul>\n<p>典型的调用序列如下：</p>\n<pre><code class=\"language-python\"># 1. 获取执行环境\nenv = StreamExecutionEnvironment.get_execution_environment()\n# 2. 添加所需资源\nenv.add_resource(...)\n# 3. 设置输入数据\noutput_data = env.from_list(input_data) \n# 4. 应用代理\nbuilder.apply(agent) \n# 5. 执行代理\nenv.execute()\n# 6. 获取结果\nresults = builder.to_list()\n</code></pre>\n<p>关键特点：</p>\n<ul>\n<li>手动触发： execute() 不会自动调用，必须由用户显式调用</li>\n<li>一次性执行：只能调用一次，多次调用会抛出异常</li>\n<li>阻塞操作：在本地环境中，这是个同步阻塞操作，会等待所有输入处理完成</li>\n</ul>\n<h4 id=\"214-组件关系\">2.1.4 组件关系</h4>\n<p>组件关系图如下：</p>\n<pre><code class=\"language-python\">LocalExecutionEnvironmet\n    ↓\n    ↓ creates\nLocalAgentBuilder\n    ↓\n    ↓ creates\nLocalRunner\n    ↓\n    ↓ creates &amp; run(data)\nLocalRunnerContext\n</code></pre>\n<p>关系说明：</p>\n<ul>\n<li>LocalExecutionEnvironmet 是入口点，管理整个执行环境。通过 from_list() 创建LocalAgentBuilder</li>\n<li>LocalAgentBuilder 负责构建Agent 执行管道，通过 apply() 创建 LocalRunner</li>\n<li>LocalRunner 是实际的执行器</li>\n<li>LocalRunner 为每个处理的记录创建 LocalRunnerContext</li>\n</ul>\n<h3 id=\"22-localrunner\">2.2 LocalRunner</h3>\n<p>LocalRunner 是本地执行环境中的核心执行器，负责实际运行Agent逻辑。LocalRunner 提供了一个完整的本地执行环境，模拟了Flink 流处理的行为，使得Agent可以在本地进行开发和测试。</p>\n<h4 id=\"221-localrunner-的主要功能\">2.2.1 LocalRunner 的主要功能</h4>\n<p>LocalRunner 的主要功能：</p>\n<ul>\n<li>代理执行：将Agent转换为可执行的计划并执行</li>\n<li>上下文管理：为每个处理单元创建和管理 LocalRunnerContext</li>\n<li>事件处理：管理事件队列并驱动Agent的执行流程</li>\n<li>结果收集：收集和存储Agent执行的输出结果</li>\n</ul>\n<p>run函数是LocalRunner的核心方法，负责处理单个输入记录：</p>\n<ul>\n<li>从输入数据中提取键值，用于上下文管理</li>\n<li>为每个键创建或复用 LocalRunnerContext</li>\n<li>将输入数据包装成 InputEvent 并发送到事件队列</li>\n<li>事件循环处理，LocalRunner 的 run 函数使用 while 循环是为了处理事件驱动的代理执行模型。这种设计反映了代理执行的核心机制\n<ul>\n<li>持续处理事件队列直到为空</li>\n<li>如果是输出事件、收集结果</li>\n<li>根据事件类型查找对应的动作</li>\n<li>执行动作</li>\n<li>处理异步处理结果</li>\n</ul>\n</li>\n</ul>\n<pre><code class=\"language-python\">    @override\n    def run(self, **data: Dict[str, Any]) -&gt; Any:\n        \"\"\"Execute the agent to process the given data.\n\n        Parameters\n        ----------\n        **data : dict[str, Any]\n            input record from upstream.\n\n        Returns:\n        -------\n        key\n            The key of the input that was processed.\n        \"\"\"\n        if \"key\" in data:\n            key = data[\"key\"]\n        elif \"k\" in data:\n            key = data[\"k\"]\n        else:\n            key = uuid.uuid4()\n\n        if key not in self.__keyed_contexts:\n            self.__keyed_contexts[key] = LocalRunnerContext(self.__agent_plan, key, self.__config)\n        context = self.__keyed_contexts[key]\n\n        if \"value\" in data:\n            input_event = InputEvent(input=data[\"value\"])\n        elif \"v\" in data:\n            input_event = InputEvent(input=data[\"v\"])\n        else:\n            msg = \"Input data must be dict has 'v' or 'value' field\"\n            raise RuntimeError(msg)\n\n        context.send_event(input_event)\n\n        while len(context.events) &gt; 0:\n            event = context.events.popleft()\n            if isinstance(event, OutputEvent):\n                self.__outputs.append({key: event.output})\n                continue\n            event_type = f\"{event.__class__.__module__}.{event.__class__.__name__}\"\n            for action in self.__agent_plan.get_actions(event_type):\n                context.action_name = action.name\n                func_result = action.exec(event, context)\n                if isinstance(func_result, Generator):\n                    try:\n                        for _ in func_result:\n                            pass\n                    except Exception:\n                        logger.exception(\"Error in async execution\")\n                        raise\n        return key\n</code></pre>\n<h4 id=\"222-事件驱动执行模型\">2.2.2 事件驱动执行模型</h4>\n<p>在 Flink Agents 框架中，代理的执行是基于事件的。每个动作 (action)处理一个事件并可能产生新的事件，这些新事件又会触发其他动作的执行。因此需要一个循环来持续处理事件队列中的事件，直到没有更多事件需要处理。</p>\n<p>这种设计使得代理可以处理复杂的、多步骤的交互过程，而不需要预先确定执行步骤的数量。<code>while</code> 循环确保所有相关的事件都被处理完毕，形成了一个完整的事件处理链：</p>\n<ul>\n<li>持续从事件队列 context.events 中取出事件进行处理</li>\n<li>每个事件可能触发一个或多个动作的执行</li>\n<li>动作执行可能产生新的事件，这些事件被添加回队列</li>\n</ul>\n<p>递归事件处理：</p>\n<ul>\n<li>初始时只有一个 InputEvent</li>\n<li>动作处理 InputEvent 可能产生 ChatRequestEvent 等中间事件</li>\n<li>中间事件可能触发其他动作，产生更多事件</li>\n<li>最终产生 OutputEvent 完成处理</li>\n</ul>\n<p>完整处理链：</p>\n<pre><code class=\"language-python\">InputEvent  → →   start_action  → →   ChatRequestEvent  →  →  LLM处理  → →  ChatResponseEvent  → →   stop_action  → →   OutputEvent\n</code></pre>\n<p>具体执行流程</p>\n<ul>\n<li>\n<p>初始化：</p>\n<ul>\n<li>将输入数据包装成 InputEvent 并添加到事件队列</li>\n</ul>\n</li>\n<li>\n<p>循环处理：</p>\n<ul>\n<li>从队列取出事件</li>\n<li>根据事件类型找到对应的 actions</li>\n<li>执行所有监听该事件类型的动作</li>\n<li>动作可能通过 send_event 发送新事件到队列</li>\n</ul>\n</li>\n<li>\n<p>终止条件：</p>\n<ul>\n<li>当事件队列为空时，处理完成</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"223-agentplan-和-localrunner-的关系\">2.2.3 AgentPlan 和 LocalRunner 的关系</h4>\n<ul>\n<li>LocalRunner 在初始化时候使用 AgentPlan</li>\n<li>在执行过程中，LocalRunnerContext 通过 AgentPlan 获取资源和动作</li>\n<li>在事件处理中查找对应的动作</li>\n</ul>\n<p>具体为：LocalRunner 在初始化时候使用 AgentPlan</p>\n<pre><code class=\"language-python\">class LocalRunner(AgentRunner):\n    \"\"\"Agent runner implementation for local execution, which is\n    convenient for debugging.\n\n    Attributes:\n    ----------\n    __agent_plan : AgentPlan\n        Internal agent plan.\n    __keyed_contexts : dict[Any, LocalRunnerContext]\n        Dictionary of active contexts indexed by key.\n    __outputs:\n        Outputs generated by agent execution.\n    __config:\n        Internal configration.\n    \"\"\"\n\n    __agent_plan: AgentPlan\n    __keyed_contexts: Dict[Any, LocalRunnerContext]\n    __outputs: List[Dict[str, Any]]\n    __config: AgentConfiguration\n\n    def __init__(self, agent: Agent, config: AgentConfiguration) -&gt; None:\n        \"\"\"Initialize the runner with the provided agent.\n\n        Parameters\n        ----------\n        agent : Agent\n            The agent class to convert and run.\n        \"\"\"\n        self.__agent_plan = AgentPlan.from_agent(agent, config)\n        self.__keyed_contexts = {}\n        self.__outputs = []\n        self.__config = config\n\n\n</code></pre>\n<p>具体为：在执行过程中，LocalRunnerContext 通过 AgentPlan 获取资源和动作</p>\n<pre><code class=\"language-python\">class LocalRunnerContext(RunnerContext):\n    \"\"\"Implementation of RunnerContext for local agent execution.\n\n    Attributes:\n    ----------\n    __agent_plan : AgentPlan\n        Internal agent plan for this context.\n    __key : Any\n        Unique identifier for the context, correspond to the key in flink KeyedStream.\n    events : deque[Event]\n        Queue of events to be processed in this context.\n    action_name: str\n        Name of the action being executed.\n    \"\"\"\n\n    __agent_plan: AgentPlan\n    __key: Any\n    events: deque[Event]\n    action_name: str\n    _store: dict[str, Any]\n    _short_term_memory: MemoryObject\n    _config: AgentConfiguration\n\n    def __init__(self, agent_plan: AgentPlan, key: Any, config: AgentConfiguration) -&gt; None:\n        \"\"\"Initialize a new context with the given agent and key.\n\n        Parameters\n        ----------\n        agent_plan : AgentPlan\n            Agent plan used for this context.\n        key : Any\n            Unique context identifier, which is corresponding to the key in flink\n            KeyedStream.\n        \"\"\"\n        self.__agent_plan = agent_plan\n        self.__key = key\n        self.events = deque()\n        self._store = {}\n        self._short_term_memory = LocalMemoryObject(\n            self._store, LocalMemoryObject.ROOT_KEY\n        )\n        self._config = config\n        \n    @override\n    def get_resource(self, name: str, type: ResourceType) -&gt; Resource:\n        return self.__agent_plan.get_resource(name, type)        \n\n\n</code></pre>\n<p>具体为：在事件处理中查找对应的动作</p>\n<pre><code class=\"language-python\">            for action in self.__agent_plan.get_actions(event_type):\n                context.action_name = action.name\n                func_result = action.exec(event, context) # 执行\n                if isinstance(func_result, Generator):\n                    try:\n                        for _ in func_result:\n                            pass\n                    except Exception:\n                        logger.exception(\"Error in async execution\")\n                        raise\n\n</code></pre>\n<h4 id=\"224-为什么要为每种-key-维护独立的上下文\">2.2.4 为什么要为每种 key 维护独立的上下文</h4>\n<p>在 LocalRunner 中，每种 key 都有自己独立的上下文是为了模拟 Flink 流处理环境中 keyed state 的行为，并支持状态隔离和并发处理。</p>\n<ul>\n<li><strong>状态隔离</strong>：每个 key 对应的数据流需要维护自己的状态，避免不同 key 的数据相互干扰：</li>\n</ul>\n<pre><code class=\"language-python\"># 不同 key 的状态完全隔离\nkey1_context.short_term_memory.set(\"user_name\", \"Alice\")\nkey2_context.short_term_memory.set(\"user_name\", \"Bob\")\n# 两个 key 有不同的状态值，互不影响\n\n\n</code></pre>\n<ul>\n<li>\n<p><strong>模拟 Flink 的 KeyedStream 行为</strong>：在 Flink 中，当使用 <code>keyBy()</code> 操作时，每个 key 会有独立的状态管理。LocalRunner 通过这种方式模拟了相同的行为：</p>\n<ul>\n<li>相同 key 的事件会在同一个上下文中处理</li>\n<li>不同 key 的事件拥有各自独立的内存和事件队列</li>\n</ul>\n<p>这样保证了本地调试环境与实际 Flink 环境的一致性</p>\n</li>\n<li>\n<p><strong>支持并发处理</strong>：虽然 LocalRunner 是单线程运行的，但它模拟了多 key 并发处理的情况：</p>\n</li>\n<li>\n<p><strong>支持并发处理</strong>：虽然 LocalRunner 是单线程运行的，但它模拟了多 key 并发处理的情况：</p>\n</li>\n</ul>\n<pre><code class=\"language-python\"># 可以同时处理多个 key 的数据\nfor input_record in inputs:\n    runner.run(**input_record)  # 每个 key 有独立上下文\n\n</code></pre>\n<ul>\n<li>\n<p><strong>事件队列隔离</strong>：每个 key 都有自己的事件队列（events），这样可以保证：</p>\n<ul>\n<li>同一 key 的事件按顺序处理</li>\n<li>不同 key 的事件不会互相影响处理顺序</li>\n<li>每个 key 可以独立地维护待处理事件队列</li>\n</ul>\n</li>\n<li>\n<p><strong>内存状态管理</strong>：每个 LocalRunnerContext 都有自己的短期记忆对象（_short_term_memory），允许：</p>\n</li>\n</ul>\n<pre><code class=\"language-python\"># 每个 key 可以存储和检索自己的状态\ncontext.short_term_memory.set(\"step_count\", 1)\ncontext.short_term_memory.get(\"step_count\")  # 获取该 key 特定的状态\n\n</code></pre>\n<h4 id=\"225-实际应用场景示例\">2.2.5 实际应用场景示例</h4>\n<p>考虑一个客户服务聊天机器人应用：</p>\n<pre><code class=\"language-python\"># 用户 Alice 和 Bob 的对话分别用不同的 key 处理\nrunner.run(key=\"user_alice\", value={\"message\": \"Hello\"})\nrunner.run(key=\"user_bob\", value={\"message\": \"Hi there\"})\n# 每个用户的对话历史和状态都独立保存\n# Alice 的上下文不会影响 Bob 的上下文，反之亦然\n\n</code></pre>\n<p>这种设计使 LocalRunner 能够准确模拟真实分布式环境中的行为，方便开发者在本地测试和调试复杂的状态依赖型代理应用。</p>\n<h3 id=\"23-localrunnercontext\">2.3 LocalRunnerContext</h3>\n<p>LocalRunnerContext 是 Flink Agents 框架中用于本地执行环境的运行上下文实现。它的主要功能是为每个 key 提供独立的执行环境，模拟 Flink 分布式环境中 keyed state 的行为，确保了在本地环境中能够准确模拟基于 key 的状态管理和事件处理流程。</p>\n<h4 id=\"231-设计目的\">2.3.1 设计目的</h4>\n<p>LocalRunnerContext 的设计主要是为了：</p>\n<ul>\n<li>本地调试：在本地环境中模拟 Flink 分布式执行的行为</li>\n<li>状态隔离：确保不同 key 的处理状态完全隔离，避免相互干扰</li>\n<li>行为一致性：保证本地测试环境与实际 Flink 执行环境的行为一致</li>\n<li>简化开发：提供与生产环境相同的 API 接口，方便开发者测试和调试</li>\n</ul>\n<h4 id=\"232-主要功能\">2.3.2 主要功能</h4>\n<ul>\n<li>状态管理\n<ul>\n<li>为每个 key 维护独立的短期内存状态（_short_term_memory）</li>\n<li>提供内存对象的读写操作，确保不同 key 的状态隔离</li>\n</ul>\n</li>\n<li>事件处理\n<ul>\n<li>维护每个 key 的事件队列（events）</li>\n<li>提供 send_event 方法将事件添加到处理队列中</li>\n<li>记录事件处理日志，便于调试</li>\n</ul>\n</li>\n<li>资源访问\n<ul>\n<li>提供 get_resource 方法访问 agent 所需的资源（如模型、工具等）</li>\n<li>根据资源名称和类型获取相应的资源实例</li>\n</ul>\n</li>\n<li>配置管理\n<ul>\n<li>提供对 action 配置的访问（action_config, get_action_config_value）</li>\n<li>提供全局配置信息（config）</li>\n</ul>\n</li>\n<li>度量和监控\n<ul>\n<li>提供度量组访问接口（agent_metric_group, action_metric_group）</li>\n<li>目前在本地环境中尚未完全实现</li>\n</ul>\n</li>\n<li>异步执行支持\n<ul>\n<li>提供 execute_async 方法支持异步函数执行</li>\n<li>在本地环境中降级为同步执行并给出警告</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"233-定义\">2.3.3 定义</h4>\n<pre><code class=\"language-java\">class LocalRunnerContext(RunnerContext):\n    \"\"\"Implementation of RunnerContext for local agent execution.\n\n    Attributes:\n    ----------\n    __agent_plan : AgentPlan\n        Internal agent plan for this context.\n    __key : Any\n        Unique identifier for the context, correspond to the key in flink KeyedStream.\n    events : deque[Event]\n        Queue of events to be processed in this context.\n    action_name: str\n        Name of the action being executed.\n    \"\"\"\n\n    __agent_plan: AgentPlan\n    __key: Any\n    events: deque[Event]\n    action_name: str\n    _store: dict[str, Any]\n    _short_term_memory: MemoryObject\n    _config: AgentConfiguration\n\n    def __init__(self, agent_plan: AgentPlan, key: Any, config: AgentConfiguration) -&gt; None:\n        \"\"\"Initialize a new context with the given agent and key.\n\n        Parameters\n        ----------\n        agent_plan : AgentPlan\n            Agent plan used for this context.\n        key : Any\n            Unique context identifier, which is corresponding to the key in flink\n            KeyedStream.\n        \"\"\"\n        self.__agent_plan = agent_plan\n        self.__key = key\n        self.events = deque()\n        self._store = {}\n        self._short_term_memory = LocalMemoryObject(\n            self._store, LocalMemoryObject.ROOT_KEY\n        )\n        self._config = config\n\n    @property\n    def key(self) -&gt; Any:\n        \"\"\"Get the unique identifier for this context.\n\n        Returns:\n        -------\n        Any\n            The unique identifier for this context.\n        \"\"\"\n        return self.__key\n\n    @override\n    def send_event(self, event: Event) -&gt; None:\n        \"\"\"Send an event to the context's event queue and log it.\n\n        Parameters\n        ----------\n        event : Event\n            The event to be added to the queue.\n        \"\"\"\n        logger.info(\"key: %s, send_event: %s\", self.__key, event)\n        self.events.append(event)\n\n    @override\n    def get_resource(self, name: str, type: ResourceType) -&gt; Resource:\n        return self.__agent_plan.get_resource(name, type)\n\n    @property\n    @override\n    def action_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get config of the action.\"\"\"\n        return self.__agent_plan.get_action_config(action_name=self.action_name)\n\n    @override\n    def get_action_config_value(self, key: str) -&gt; Any:\n        \"\"\"Get config option value of the key.\"\"\"\n        return self.__agent_plan.get_action_config_value(\n            action_name=self.action_name, key=key\n        )\n\n    @property\n    @override\n    def short_term_memory(self) -&gt; MemoryObject:\n        \"\"\"Get the short-term memory object associated with this context.\n\n        Returns:\n        -------\n        MemoryObject\n            The root object of the short-term memory.\n        \"\"\"\n        return self._short_term_memory\n\n    @property\n    @override\n    def agent_metric_group(self) -&gt; MetricGroup:\n        # TODO: Support metric mechanism for local agent execution.\n        err_msg = \"Metric mechanism is not supported for local agent execution yet.\"\n        raise NotImplementedError(err_msg)\n\n    @property\n    @override\n    def action_metric_group(self) -&gt; MetricGroup:\n        # TODO: Support metric mechanism for local agent execution.\n        err_msg = \"Metric mechanism is not supported for local agent execution yet.\"\n        raise NotImplementedError(err_msg)\n\n    def execute_async(\n        self,\n        func: Callable[[Any], Any],\n        *args: Tuple[Any, ...],\n        **kwargs: Dict[str, Any],\n    ) -&gt; Any:\n        \"\"\"Asynchronously execute the provided function. Access to memory\n        is prohibited within the function.\n        \"\"\"\n        logger.warning(\n            \"Local runner does not support asynchronous execution; falling back to synchronous execution.\"\n        )\n        func_result = func(*args, **kwargs)\n        yield\n        return func_result\n\n    @property\n    @override\n    def config(self) -&gt; AgentConfiguration:\n        return self._config\n\n</code></pre>\n<h2 id=\"0x03-remoteexecutionenvironment\">0x03 RemoteExecutionEnvironment</h2>\n<p><code>RemoteExecutionEnvironment</code> 是 Flink Agents 对原生 Flink <code>RemoteEnvironment</code> 的封装（适配 Agent 语义），是 Agent（智能体）与 Flink 集群融合的核心载体，核心目标是让 Agent 能够在 Flink 集群中处理 DataStream 和 Table 类型的流式数据，具体承担以下关键职责：</p>\n<ul>\n<li>\n<p><code>RemoteExecutionEnvironment</code> 是 Flink Agents 实现 Agent 远程执行的核心环境组件，封装了远程集群连接、作业提交、资源管理等能力；</p>\n</li>\n<li>\n<p>核心价值是<strong>屏蔽 Flink 远程执行的底层细节</strong>，让用户聚焦 Agent 逻辑定义，而非集群操作；</p>\n</li>\n<li>\n<p>关键特性是兼容本地调试与远程执行，同时适配 Agent 特有的状态、事件、资源需求，是 Flink Agents 从 “本地单机” 走向 “分布式集群” 的核心支撑。</p>\n</li>\n<li>\n<p>桥接 Agent 框架与 Flink 运行时：将 Agent 的执行逻辑嵌入 Flink 的 StreamExecutionEnvironment/StreamTableEnvironment，使 Agent 能利用 Flink 的分布式计算能力处理流式数据；</p>\n</li>\n<li>\n<p>标准化 Agent 执行流程：提供统一的入口（fromDataStream/fromTable）、处理（apply Agent）、输出（toDataStream/toTable）接口，规范 Agent 在 Flink 中的数据处理链路；</p>\n</li>\n<li>\n<p>配置管理：加载 Flink 集群中 Agent 的专属配置文件（config.yaml），为 Agent 执行提供环境配置支撑；</p>\n</li>\n<li>\n<p>执行调度：最终触发 Flink 作业的执行（env.execute ()），完成 Agent 处理逻辑的分布式运行。</p>\n</li>\n</ul>\n<h3 id=\"31-核心特色\">3.1 核心特色</h3>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">特色维度</th>\n<th style=\"text-align: center;\">具体说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">环境适配性</td>\n<td style=\"text-align: center;\">专门面向 Flink 集群的<strong>远程执行</strong>场景设计，依赖 Flink 的流执行环境（StreamExecutionEnvironment）和表环境（StreamTableEnvironment），而非本地执行；</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">数据类型聚焦</td>\n<td style=\"text-align: center;\">仅支持 Flink 原生的 DataStream/Table 作为输入输出，明确禁用本地场景的 List 类型（fromList/toList），贴合流式计算场景；</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">分层设计</td>\n<td style=\"text-align: center;\">采用 “环境类（RemoteExecutionEnvironment）+ 构建器类（RemoteAgentBuilder）” 分层模式：环境类管控全局配置和 Flink 环境，构建器类聚焦单个 Agent 的执行链路；</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">灵活的 Key 选择</td>\n<td style=\"text-align: center;\">支持自定义 KeySelector 对输入数据分片，无自定义 Key 时默认使用数据自身作为 Key，适配不同的分布式处理需求；</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">配置解耦</td>\n<td style=\"text-align: center;\">从 Flink 配置目录加载 Agent 专属配置，配置文件与代码解耦，便于集群环境下的配置管理；</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">容错与校验</td>\n<td style=\"text-align: center;\">包含关键流程校验（如调用 toDataStream 前必须先 apply Agent）、空值处理（TableEnvironment 懒加载）、异常封装（配置加载 / Agent 执行异常统一抛出 RuntimeException）；</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">表与流互通</td>\n<td style=\"text-align: center;\">支持 Table 与 DataStream 的双向转换（Table 转 DataStream 作为 Agent 输入、DataStream 转 Table 作为输出），适配 Flink SQL / 流处理双场景；</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">资源关联</td>\n<td style=\"text-align: center;\">为 Agent 绑定运行时资源（resources），保障 Agent 执行所需的资源依赖；</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"32-与原生-flink-remoteenvironment-的对比\">3.2 与原生 Flink RemoteEnvironment 的对比</h3>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>Flink 原生 RemoteEnvironment</th>\n<th>Flink Agents RemoteExecutionEnvironment</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>核心定位</td>\n<td>通用 Flink Job 的远程执行环境</td>\n<td>Agent 语义专属的远程执行环境</td>\n</tr>\n<tr>\n<td>封装层级</td>\n<td>底层 Flink Job 提交</td>\n<td>上层 Agent/AgentPlan 提交</td>\n</tr>\n<tr>\n<td>核心适配</td>\n<td>无 Agent 语义，仅处理通用 JobGraph</td>\n<td>适配 AgentPlan → JobGraph 编译、Agent 状态管理</td>\n</tr>\n<tr>\n<td>资源管理</td>\n<td>通用 Slot / 资源分配</td>\n<td>按 Agent 实例隔离资源，适配工具 / 动作资源需求</td>\n</tr>\n<tr>\n<td>事件 / 状态处理</td>\n<td>无内置事件语义</td>\n<td>封装 Agent 事件（Event）的跨集群传输、状态序列化</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"33-如何使用-actionexecutionoperator\">3.3 如何使用 ActionExecutionOperator</h3>\n<p>RemoteExecutionEnvironment 通过 Python 层的 RemoteAgentBuilder.to_datastream() 方法间接使用ActionExecutionOperator，过程为：</p>\n<ul>\n<li>用户定义了一个Agent并应用到执行环境中</li>\n<li>调用 to_datastream() 方法触发实际的操作符创建</li>\n<li>通过 JNI 调用 Java 层的 CompileUtils.connectToAgent() 方法</li>\n<li>最终在 Flink 作业图中创建并配置  ActionExecutionOperator</li>\n</ul>\n<p>connectToAgent()中会：</p>\n<ul>\n<li>接收输入的 Java DATa Stream 对象</li>\n<li>接收序列化的 AgentPlan（包含所有动作和资源配置）</li>\n<li>创建并连接 ActionExecutionOperator 到数据流处理图中</li>\n</ul>\n<pre><code class=\"language-java\">    // ============================ basic ====================================\n    /**\n     * Connects the given KeyedStream to the Flink Agents agent.\n     *\n     * &lt;p&gt;This method accepts a keyed DataStream and applies the specified agent plan to it. The\n     * source of the input stream determines the data format: Java streams provide Objects, while\n     * Python streams use serialized byte arrays.\n     *\n     * @param keyedInputStream The input keyed DataStream.\n     * @param agentPlan The agent plan to be executed.\n     * @param inputIsJava A flag indicating whether the input stream originates from Java. - If\n     *     true, input and output types are Java Objects. - If false, input and output types are\n     *     byte[].\n     * @param &lt;K&gt; The type of the key used in the keyed DataStream.\n     * @param &lt;IN&gt; The type of the input data (Object or byte[]).\n     * @param &lt;OUT&gt; The type of the output data (Object or byte[]).\n     * @return The processed DataStream as the result of the agent.\n     */\n    private static &lt;K, IN, OUT&gt; DataStream&lt;OUT&gt; connectToAgent(\n            KeyedStream&lt;IN, K&gt; keyedInputStream,\n            AgentPlan agentPlan,\n            TypeInformation&lt;OUT&gt; outTypeInformation,\n            boolean inputIsJava) {\n        return (DataStream&lt;OUT&gt;)\n                keyedInputStream\n                        .transform(\n                                \"action-execute-operator\",\n                                outTypeInformation,\n                                new ActionExecutionOperatorFactory(agentPlan, inputIsJava))\n                        .setParallelism(keyedInputStream.getParallelism());\n    }\n\n</code></pre>\n<h3 id=\"34-典型流程\">3.4 典型流程</h3>\n<p>当用户编写基于 RemoteExecutionEnvironment  的应用程序时，典型流程如下：</p>\n<pre><code class=\"language-python\"># 获取Flink执行环境\nenv = StreamExecutionEnvironment.get_execution_environment()\n# 创建 Agent 执行环境\nagent_env = AgentsExecutionEnvironment.get_execution_environment(env)\n# 添加所需资源\nagent_env.add_resource(...)\n# 设置输入数据流到Agent\ninput_stream = agent_env.from_Collection(input_data) \nbuilder = agent_env.from_datastream(input_stream)\n# 应用代理逻辑\nagent = MyCustomAgent()\nbuilder.apply(agent) \n# 执行代理\noutput_stream = builder.to_datastream()\nenv.execute()\n\n</code></pre>\n<h3 id=\"35-交互逻辑\">3.5 交互逻辑</h3>\n<p>RemoteExecutionEnvironment、ActionExecutionOperator 和 ActionTask 之间的交互逻辑如下。</p>\n<p>这三个组件在 Flink Agents 框架中扮演不同的角色，协同工作来执行 Agent 逻辑：</p>\n<ul>\n<li><strong>RemoteExecutionEnvironment</strong>：提供远程执行环境，负责构建和连接 Agent 到 Flink 数据流</li>\n<li><strong>ActionExecutionOperator</strong>：Flink 流处理操作符，实际执行 Agent 的动作逻辑</li>\n<li><strong>ActionTask</strong>：表示单个动作执行任务的抽象概念</li>\n</ul>\n<p><strong>交互流程图解</strong></p>\n<p><img alt=\"Flink-7-1\" class=\"lazyload\" /></p>\n<p>详细交互流程如下：</p>\n<ol>\n<li>初始化阶段</li>\n</ol>\n<p><img alt=\"Flink-7-2\" class=\"lazyload\" /></p>\n<ol start=\"2\">\n<li>运行时事件处理流程</li>\n</ol>\n<p><img alt=\"Flink-7-3\" class=\"lazyload\" /></p>\n<h3 id=\"36-实现\">3.6 实现</h3>\n<p>RemoteAgentBuilder 的代码如下</p>\n<pre><code class=\"language-python\">class RemoteAgentBuilder(AgentBuilder):\n    \"\"\"RemoteAgentBuilder for integrating datastream/table and agent.\"\"\"\n\n    __input: DataStream\n    __agent_plan: AgentPlan = None\n    __output: DataStream = None\n    __t_env: StreamTableEnvironment\n    __config: AgentConfiguration\n    __resources: Dict[ResourceType, Dict[str, Any]] = None\n\n    def __init__(\n        self,\n        input: DataStream,\n        config: AgentConfiguration,\n        t_env: StreamTableEnvironment | None = None,\n        resources: Dict[ResourceType, Dict[str, Any]] | None = None,\n    ) -&gt; None:\n        \"\"\"Init method of RemoteAgentBuilder.\"\"\"\n        self.__input = input\n        self.__t_env = t_env\n        self.__config = config\n        self.__resources = resources\n\n    @property\n    def t_env(self) -&gt; StreamTableEnvironment:\n        \"\"\"Get or crate table environment.\"\"\"\n        if self.__t_env is None:\n            self.__t_env = StreamTableEnvironment.create(\n                stream_execution_environment=self.__env\n            )\n        return self.__t_env\n\n    def apply(self, agent: Agent) -&gt; \"AgentBuilder\":\n        \"\"\"Set agent of execution environment.\n\n        Parameters\n        ----------\n        agent : Agent\n            The agent user defined to run in execution environment.\n        \"\"\"\n        if self.__agent_plan is not None:\n            err_msg = \"RemoteAgentBuilder doesn't support apply multiple agents yet.\"\n            raise RuntimeError(err_msg)\n\n        # inspect refer actions and resources from env to agent.\n        for type, name_to_resource in self.__resources.items():\n            agent.resources[type] = name_to_resource | agent.resources[type]\n\n        self.__agent_plan = AgentPlan.from_agent(agent, self.__config)\n\n        return self\n\n    def to_datastream(self, output_type: TypeInformation | None = None) -&gt; DataStream:\n        \"\"\"Get output datastream of agent execution.\n\n        Returns:\n        -------\n        DataStream\n            Output datastream of agent execution.\n        \"\"\"\n        if self.__agent_plan is None:\n            err_msg = \"Must apply agent before call to_datastream/to_table.\"\n            raise RuntimeError(err_msg)\n\n        # return the same output datastream when call to_datastream multiple.\n        if self.__output is None:\n            j_data_stream_output = invoke_method(\n                None,\n                \"org.apache.flink.agents.runtime.CompileUtils\",\n                \"connectToAgent\",\n                [\n                    self.__input._j_data_stream,\n                    self.__agent_plan.model_dump_json(serialize_as_any=True),\n                ],\n                [\n                    \"org.apache.flink.streaming.api.datastream.KeyedStream\",\n                    \"java.lang.String\",\n                ],\n            )\n            output_stream = DataStream(j_data_stream_output)\n            self.__output = output_stream.map(\n                lambda x: cloudpickle.loads(x), output_type=output_type\n            )\n        return self.__output\n\n    def to_table(self, schema: Schema, output_type: TypeInformation) -&gt; Table:\n        \"\"\"Get output Table of agent execution.\n\n        Parameters\n        ----------\n        schema : Schema\n            Indicate schema of the output table.\n        output_type : TypeInformation\n            Indicate schema corresponding type information.\n\n        Returns:\n        -------\n        Table\n            Output Table of agent execution.\n        \"\"\"\n        return self.t_env.from_data_stream(self.to_datastream(output_type), schema)\n\n    def to_list(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get output list of agent execution.\n\n        This method is not supported for remote execution environments.\n        \"\"\"\n        msg = \"RemoteAgentBuilder does not support to_list.\"\n        raise NotImplementedError(msg)\n\n\n</code></pre>\n<p>RemoteExecutionEnvironment 的代码如下。</p>\n<pre><code class=\"language-python\">class RemoteExecutionEnvironment(AgentsExecutionEnvironment):\n    \"\"\"Implementation of AgentsExecutionEnvironment for execution with DataStream.\"\"\"\n\n    __env: StreamExecutionEnvironment\n    __t_env: StreamTableEnvironment\n    __config: AgentConfiguration\n\n    def __init__(\n        self,\n        env: StreamExecutionEnvironment,\n        t_env: StreamTableEnvironment | None = None,\n    ) -&gt; None:\n        \"\"\"Init method of RemoteExecutionEnvironment.\"\"\"\n        super().__init__()\n        self.__env = env\n        self.__t_env = t_env\n        self.__config = AgentConfiguration()\n        self.__load_config_from_flink_conf_dir()\n\n    @property\n    def t_env(self) -&gt; StreamTableEnvironment:\n        \"\"\"Get or crate table environment.\"\"\"\n        if self.__t_env is None:\n            self.__t_env = StreamTableEnvironment.create(\n                stream_execution_environment=self.__env\n            )\n        return self.__t_env\n\n    def get_config(self, path: str | None = None) -&gt; AgentConfiguration:\n        \"\"\"Get the writable configuration for flink agents.\n\n        Returns:\n        -------\n        LocalConfiguration\n            The configuration for flink agents.\n        \"\"\"\n        return self.__config\n\n    @staticmethod\n    def __process_input_datastream(\n        input: DataStream, key_selector: KeySelector | Callable | None = None\n    ) -&gt; KeyedStream:\n        if isinstance(input, KeyedStream):\n            return input\n        else:\n            if key_selector is None:\n                msg = \"KeySelector must be provided.\"\n                raise RuntimeError(msg)\n            input = input.key_by(key_selector)\n            return input\n\n    def from_datastream(\n        self, input: DataStream, key_selector: KeySelector | Callable | None = None\n    ) -&gt; RemoteAgentBuilder:\n        \"\"\"Set input datastream of agent.\n\n        Parameters\n        ----------\n        input : DataStream\n            Receive a DataStream as input.\n        key_selector : KeySelector\n            Extract key from each input record, must not be None when input is\n            not KeyedStream.\n        \"\"\"\n        input = self.__process_input_datastream(input, key_selector)\n\n        return RemoteAgentBuilder(\n            input=input,\n            config=self.__config,\n            t_env=self.__t_env,\n            resources=self.resources,\n        )\n\n    def from_table(\n        self,\n        input: Table,\n        key_selector: KeySelector | Callable | None = None,\n    ) -&gt; AgentBuilder:\n        \"\"\"Set input Table of agent.\n\n        Parameters\n        ----------\n        input : Table\n            Receive a Table as input.\n        key_selector : KeySelector\n            Extract key from each input record.\n        \"\"\"\n        input = self.t_env.to_data_stream(table=input)\n\n        input = input.map(lambda x: x, output_type=PickledBytesTypeInfo())\n\n        input = self.__process_input_datastream(input, key_selector)\n        return RemoteAgentBuilder(\n            input=input,\n            config=self.__config,\n            t_env=self.t_env,\n            resources=self.resources,\n        )\n\n    def from_list(self, input: List[Dict[str, Any]]) -&gt; \"AgentsExecutionEnvironment\":\n        \"\"\"Set input list of agent execution.\n\n        This method is not supported for remote execution environments.\n        \"\"\"\n        msg = \"RemoteExecutionEnvironment does not support from_list.\"\n        raise NotImplementedError(msg)\n\n    def execute(self) -&gt; None:\n        \"\"\"Execute agent.\"\"\"\n        self.__env.execute()\n\n\n    def __load_config_from_flink_conf_dir(self) -&gt; None:\n        \"\"\"Load agent configuration from FLINK_CONF_DIR if available.\"\"\"\n        flink_conf_dir = os.environ.get(\"FLINK_CONF_DIR\")\n        if flink_conf_dir is None:\n            return\n\n        # Try to find config file, with fallback to legacy name\n        config_path = self.__find_config_file(flink_conf_dir)\n\n        if config_path is None:\n            logging.error(f\"Config file not found in {flink_conf_dir}\")\n        else:\n            self.__config.load_from_file(str(config_path))\n\n    def __find_config_file(self, flink_conf_dir: str) -&gt; Path | None:\n        \"\"\"Find config file in the given directory, checking both new and legacy names.\n\n        Parameters\n        ----------\n        flink_conf_dir : str\n            Directory to search for config files.\n\n        Returns:\n        -------\n        Path | None\n            Path to the config file if found, None otherwise.\n        \"\"\"\n        # Try legacy config file name first\n        legacy_config_path = Path(flink_conf_dir).joinpath(_LEGACY_CONFIG_FILE_NAME)\n        if legacy_config_path.exists():\n            logging.warning(\n                f\"Using legacy config file {_LEGACY_CONFIG_FILE_NAME}\"\n            )\n            return legacy_config_path\n\n        # Try new config file name as fallback\n        primary_config_path = Path(flink_conf_dir).joinpath(_CONFIG_FILE_NAME)\n        if primary_config_path.exists():\n            return primary_config_path\n\n        return None\n\n</code></pre>\n<h2 id=\"0xff-参考\">0xFF 参考</h2>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 21:56</span>&nbsp;\n<a href=\"https://www.cnblogs.com/rossiXYZ\">罗西的思考</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}