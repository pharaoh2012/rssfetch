{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "收藏！RAG核心工具大全: 7大解析工具+向量模型+数据库+检索排序",
      "link": "https://www.cnblogs.com/aifrontiers/p/19613559",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/aifrontiers/p/19613559\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 21:04\">\n    <span>收藏！RAG核心工具大全: 7大解析工具+向量模型+数据库+检索排序</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>原文: <a href=\"https://mp.weixin.qq.com/s/5XAWHqjZspU9xtC_CckV3w\" rel=\"noopener nofollow\" target=\"_blank\">https://mp.weixin.qq.com/s/5XAWHqjZspU9xtC_CckV3w</a></p>\n<p><strong>关注gzh: AI-Frontiers</strong></p>\n<p><strong>RAG往期文章推荐</strong></p>\n<p><a href=\"https://mp.weixin.qq.com/s/VV29xpdOMEkbz4iXmD_szg\" rel=\"noopener nofollow\" target=\"_blank\">RAG效果差？7个指标让你的准确率大幅提升</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/am89yasxAvuYUToEAWNyTA\" rel=\"noopener nofollow\" target=\"_blank\">RAG评测完整指南：指标、测试和最佳实践</a></p>\n<p>检索增强生成（Retrieval-Augmented Generation, RAG）架构已成为LLM落地企业级应用的核心范式。但，在实际部署中，普遍面临「垃圾进，垃圾出」（Garbage In, Garbage Out）的困境。RAG系统的上限往往不由模型（如GPT-4或DeepSeek-V3）决定，而是由上游的数据处理流水线（ETL Pipeline）所限制。</p>\n<p>本文旨在对构建高性能开源RAG系统的关键模块进行详尽的技术拆解，并附带所有核心工具的GitHub、官方文档及模型下载地址。我们将深入剖析七大主流开源解析工具（Unstructured, Marker, PyMuPDF, Docling, MinerU, PaddleOCR, DeepSeek-OCR）的架构原理与性能特征。随后，将沿着数据流向，系统性梳理切块、向量化、检索及排序等下游模块的最新开源技术进展，意在为构建企业级、高精度的RAG系统提供理论依据与实战参考。</p>\n<h1 id=\"核心模块深度解析文档解析与版面分析\">核心模块深度解析：文档解析与版面分析</h1>\n<p>文档解析模块的任务是将非结构化的文档，如PDF/Images/PPT/word/excel，还原为机器可理解的结构化文本，即Markdown/JSON。该过程涉及OCR（Optical Character Recognition，光学字符识别）、OLA（ Layout Analysis，版面分析）、TSR（Table Structure Recognition，表格结构识别）及阅读顺序重构等多个复杂子任务。</p>\n<h2 id=\"unstructured全能型etl中间件架构\">Unstructured：全能型ETL中间件架构</h2>\n<ul>\n<li>\n<p>官方文档: <a href=\"https://docs.unstructured.io/\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.unstructured.io</a></p>\n</li>\n<li>\n<p>Github: <a href=\"https://github.com/Unstructured-IO/unstructured\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/Unstructured-IO/unstructured</a></p>\n</li>\n<li>\n<p><strong>适用场景:</strong> 企业级通用ETL流程，处理多源异构数据（邮件、办公文档、PDF混合）</p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>目前RAG生态中覆盖面最广的通用ETL框架，其设计哲学是提供一个标准化的归一化层，将包括PDF、HTML、Email、PPTX在内的25种以上异构格式转换为统一的JSON Schema</p>\n<p><strong>架构原理与分区策略</strong></p>\n<p>Unstructured的核心是分区机制，该机制并非依赖单一模型，而是根据文档类型动态匹配不同处理管线：</p>\n<ul>\n<li>\n<p><strong>基于规则的快速解析（Fast Strategy）</strong>：针对原生数字PDF，通过pdfminer.six等底层库直接提取文本流，速度快、CPU开销低，但无法处理扫描件，且易丢失复杂双栏阅读顺序；</p>\n</li>\n<li>\n<p><strong>高精度视觉解析（Hi-Res Strategy）</strong>：作为处理复杂文档的核心，借助YOLOX/Detectron2架构的目标检测模型，将页面分割为标题、正文、列表项、表格、图片等语义区块，能精准识别并剔除页眉页脚，避免干扰RAG 上下文；</p>\n</li>\n<li>\n<p><strong>表格处理子系统</strong>：检测到表格区块时触发专属识别模块，开源版本依赖Tesseract OCR或简单HTML转换，商业 API则集成更高级视觉模型恢复复杂行列结构。</p>\n</li>\n</ul>\n<p><strong>局限性与生态位分析</strong></p>\n<p>Unstructured因格式支持广泛成为RAG初学者首选，但开源版与商业版性能差距显著：开源版缺少针对金融报表、学术论文等特定领域微调的OCR模型，无法使用最新VLM（视觉语言模型）功能；hi_res策略处理长文档计算成本高，且依赖Tesseract作为OCR引擎，处理非英语文档时精度受限。不过其标准化的元数据输出（含父子节点关系、页码坐标），为下游混合切块提供了优质数据基础。</p>\n<h2 id=\"marker专注于科学文献的高精度转换管线\">Marker：专注于科学文献的高精度转换管线</h2>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/VikParuchuri/marker\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/VikParuchuri/marker</a></p>\n</li>\n<li>\n<p>适用场景: 学术论文、教科书、技术手册（公式/代码密集型文档）</p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>由Vik Paruchuri开发，专为将PDF转换为高质量Markdown而设计，特别针对数学公式、代码块和学术排版进行了深度优化。</p>\n<p><strong>深度学习流水线机制</strong></p>\n<ul>\n<li>\n<p><strong>Surya版面分析</strong>: 作为高精度OCR与版面分析模型，能精准检测文本行、阅读顺序、列边界，还可通过视觉特征判断文本逻辑流向，解决多栏排版（如双栏论文）的乱序问题。</p>\n</li>\n<li>\n<p><strong>Texify公式引擎</strong>: 针对科学文献的数学公式痛点，可将位图/PDF绘制指令形式的公式转换为标准LaTeX代码，让Marker处理arXiv论文、技术手册时语义完整性远超传统OCR。</p>\n</li>\n<li>\n<p><strong>混合****OCR</strong>：优先提取PDF内嵌文本层保证速度，对公式区域/扫描图片自动切换视觉模型识别，兼顾精度与吞吐量。</p>\n</li>\n</ul>\n<p><strong>增强的后处理与LLM融合</strong></p>\n<p>Marker新增可选--use_llm参数，可在后处理阶段调用轻量级 LLM（如 Gemini Flash、本地模型），解决文档解析 「最后一公里」问题：合并跨页表格、修正 OCR 乱码、从复杂表单提取结构化键值对。测试表明，开启LLM辅助后，Marker表格还原度优于单一模型。</p>\n<h2 id=\"pymupdf-fitz极致速度的底层流式解析\">PyMuPDF (Fitz)：极致速度的底层流式解析</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/pymupdf/PyMuPDF\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/pymupdf/PyMuPDF</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://pymupdf.readthedocs.io/\" rel=\"noopener nofollow\" target=\"_blank\">https://pymupdf.readthedocs.io</a></p>\n</li>\n<li>\n<p>适用场景: 海量原生数字PDF清洗，纯CPU环境，对速度要求极高（毫秒级）的场景。</p>\n</li>\n</ul>\n<p>PyMuPDF是MuPDF引擎的Python绑定，代表了文档解析的另一个极端，极致的工程化效率。与基于AI的视觉模型不同，PyMuPDF直接操作PDF文件的内部对象结构和渲染流。</p>\n<p><strong>文本块与字典模式的技术原理</strong></p>\n<p>PyMuPDF的核心能力在于其对PDF底层指令的解析：</p>\n<ul>\n<li>\n<p><code>get_text(\"blocks\")</code>：通过分析文本在页面上的物理位置坐标，利用启发式算法将相邻的文本行聚类为段落，该方法速度极快（毫秒级），但对排版的理解是浅层的，容易将页眉页脚误判为正文。</p>\n</li>\n<li>\n<p><code>get_text(\"dict\")</code>：更为精细的提取模式，返回一个层级化的JSON对象：<code>Page -&gt; Block -&gt; Line -&gt; Span -&gt; Char</code>。其中，Span（文本跨度）是包含相同字体、大小和颜色的最小文本单元。这一层级信息对于RAG至关重要，开发者可以通过分析Span的字体大小来区分标题和正文，或通过颜色过滤掉水印。</p>\n</li>\n</ul>\n<p><strong>局限性与适用场景</strong></p>\n<p>PyMuPDF不支持扫描件（需外接Tesseract），也无原生语义理解能力（仅能识别文本位置，无法区分摘要、参考文献等语义）。但在处理海量原生数字化合同、财报、电子书时，其纯CPU运行、超高吞吐量的优势，使其成为清洗大规模预训练语料的首选；对于简单RAG应用，通过定制Python脚本过滤页眉页脚后，PyMuPDF的性价比也最高。</p>\n<h2 id=\"doclingibm的企业级多模态文档理解框架\">Docling：IBM的企业级多模态文档理解框架</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/docling-project/docling\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/docling-project/docling</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://docling-project.github.io/docling/\" rel=\"noopener nofollow\" target=\"_blank\">https://docling-project.github.io/docling/</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/ibm-granite/granite-docling-258M\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/ibm-granite/granite-docling-258M</a></p>\n</li>\n<li>\n<p>适用场景: Agentic RAG（需要理解文档结构供Agent调用），高精度表格还原</p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>Docling是IBM Research推出的新一代文档处理库，旨在打通传统文档与生成式AI的壁垒，特别强调对表格结构和文档层级的还原。Docling不仅仅是一个解析器，定义了一种统一的文档对象模型，旨在为Agentic RAG（代理式RAG）提供结构化支撑。</p>\n<p><strong>统一文档表示与VLM集成</strong></p>\n<ul>\n<li>\n<p><strong>DoclingDocument 对象模型</strong>：将PDF、DOCX、HTML等输入转为统一内部表示，保留Section、Group、Body、Furniture层级结构，支持RAG应用「基于结构的切块」，如仅检索指定章节表格，而非盲目文本切片。</p>\n</li>\n<li>\n<p><strong>Granite VLM流水线</strong>：Docling集成IBM自研Granite视觉语言模型，以端到端方式理解页面，除文字转录外，还能解析图表（如将柱状图/折线图转为数据描述），适配金融研报分析需求。</p>\n</li>\n<li>\n<p>表格结构恢复：采用TableFormer等变体算法，高保真重建含合并单元格、无框线的复杂表格，支持导出为 HTML/Markdown 格式。</p>\n</li>\n</ul>\n<p><strong>与Agent生态的深度融合</strong></p>\n<p>Docling设计高度适配Agentic Workflow，提供MCP（Model Context Protocol）服务，可让Claude Desktop等 AI代理直接调用其文档解析能力。在构建复杂RAG Agent时，Docling可作为工具被动态调用，按用户意图提取指定信息。</p>\n<h2 id=\"mineru-pdf-extract-kit面向llm训练的高质量语料清洗\">MinerU (PDF-Extract-Kit)：面向LLM训练的高质量语料清洗</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/opendatalab/MinerU\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/opendatalab/MinerU</a></p>\n</li>\n<li>\n<p>HuggingFace: <a href=\"https://huggingface.co/opendatalab\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/opendatalab</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://opendatalab.github.io/MinerU/\" rel=\"noopener nofollow\" target=\"_blank\">https://opendatalab.github.io/MinerU/</a></p>\n</li>\n<li>\n<p>适用场景: 构建高质量知识库，处理包含复杂数学符号和双栏排版的学术文献</p>\n</li>\n</ul>\n<p>MinerU是OpenDataLab（上海人工智能实验室）为支持InternLM（书生·浦语）大模型预训练而开发的专用工具。其核心目标是从最复杂的科学文献中提取出零噪声、语义连贯的Markdown数据。</p>\n<p><strong>PDF-Extract-Kit与高精度管线</strong></p>\n<p>MinerU的后端引擎被称为PDF-Extract-Kit，这是一个集成了多种SOTA模型的综合工具包：</p>\n<ul>\n<li>\n<p><strong>布局分析</strong>：利用基于YOLO架构改进的模型，精确区分正文、标题、图片、表格、脚注和边注。MinerU特别强调对边注和页眉页脚的剔除，确保生成的Markdown文本流在语义上是连续的，不会被页面元数据打断。</p>\n</li>\n<li>\n<p><strong>公式与符号转换</strong>：针对学术论文中的数学符号，MinerU内置了强大的转换逻辑，能够将复杂的数学表达式还原为LaTeX。这解决了传统OCR将\"$$x^2$$\"识别为\"x2\"的常见错误，对于构建理工科知识库具有决定性意义。</p>\n</li>\n<li>\n<p><strong>乱码自动检测</strong>：在处理PDF时，经常遇到编码崩坏（Garbled Text）的情况。MinerU内置了自动检测机制，一旦发现直接提取的文本乱码率过高，会立即无缝切换至OCR模式，利用视觉信息重构文本，保证了极高的召回率 。</p>\n</li>\n</ul>\n<p><strong>产学研结合的工程优化</strong></p>\n<p>MinerU支持CUDA加速，还适配华为昇腾NPU、Apple Silicon MPS，适配性广泛；其 多模态Markdown输出格式保留图片占位符并关联高分辨率图像，适合构建多模态RAG（MM-RAG）系统。</p>\n<h2 id=\"paddleocr-pp-structure工业级表格与版面还原\">PaddleOCR (PP-Structure)：工业级表格与版面还原</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/PaddlePaddle/PaddleOCR\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/PaddlePaddle/PaddleOCR</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://paddlepaddle.github.io/PaddleOCR/\" rel=\"noopener nofollow\" target=\"_blank\">https://paddlepaddle.github.io/PaddleOCR/</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/PaddlePaddle/PaddleOCR-VL\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/PaddlePaddle/PaddleOCR-VL</a></p>\n</li>\n<li>\n<p>适用场景: 金融报表识别、票据处理、需要私有化部署到边缘设备的场景。</p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>PaddleOCR 是百度飞桨体系下的明星项目，其PP-Structure模块代表了工业界在文档结构化领域的最高水平。与学术界的实验性模型不同，PaddleOCR极其强调模型在服务器、移动端及嵌入式设备上的部署能力 。</p>\n<p><strong>PP-StructureV2/V3核心技术</strong></p>\n<ul>\n<li>\n<p><strong>版面分析（Layout Analysis）</strong>：PP-Structure视版面分析为典型的计算机视觉任务，利用轻量级的主干网络（如MobileNet）配合FPN结构，快速检测页面元素。其优势在于拥有庞大的中文及多语言预训练数据，对中文文档的版面理解能力尤为突出。</p>\n</li>\n<li>\n<p><strong>表格识别（Table Recognition）</strong>：这是PaddleOCR的杀手锏。它采用了SLANet（Structure-Location Alignment Network）等先进算法，将表格识别解耦为结构预测和单元格坐标回归。即使是扭曲、倾斜或光照不均的拍照文档表格，PP-Structure也能还原出精确的Excel或HTML结构。</p>\n</li>\n<li>\n<p><strong>键值对提取（KIE）</strong>：针对发票、表单等半结构化文档，PP-Structure集成了SER（语义实体识别）和RE（关系抽取）模型，能够直接提取「姓名-张三」、「金额-100元」等键值对关系，这对于财务RAG系统极具价值。</p>\n</li>\n</ul>\n<h2 id=\"deepseek-ocr端到端的生成式解析革命\">DeepSeek-OCR：端到端的生成式解析革命</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/deepseek-ai/DeepSeek-OCR</a></p>\n</li>\n<li>\n<p>Github: <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR-2\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/deepseek-ai/DeepSeek-OCR-2</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/deepseek-ai/DeepSeek-OCR</a></p>\n</li>\n<li>\n<p>适用场景: 传统OCR无法处理的极复杂排版（如报纸、杂志、手稿），需要视觉理解的任务</p>\n</li>\n</ul>\n<p>DeepSeek-OCR代表了文档解析技术的最新范式转移，从流水线式向端到端生成式演进。基于DeepSeek-VL2多模态大模型，该系统不再将解析拆解为检测、识别、拼接等步骤，而是直接看图说话。</p>\n<p><strong>视觉因果流（Visual Causal Flow）</strong></p>\n<ul>\n<li>\n<p><strong>视觉Token化</strong>：DeepSeek-VL2引入了动态分辨率策略，将高分辨率的文档图像切片并编码为视觉Token序列。这些Token与文本Token共享同一个Transformer嵌入空间。</p>\n</li>\n<li>\n<p><strong>生成式输出</strong>：模型接收文档图像作为输入，直接自回归地生成Markdown代码。这种方法的革命性在于它具备了推理能力。例如，在解析一个复杂的流程图时，传统OCR只能输出零散的文字，而DeepSeek-OCR可以根据箭头和布局生成描述性的文本：步骤A导致了步骤B。它能理解复选框旁边的文字是标签，从而输出\"Checked: Yes\"。</p>\n</li>\n<li>\n<p><strong>上下文光学压缩</strong>：为了处理长文档，DeepSeek设计了视觉压缩机制，在保留高频细节（如文字笔画）的同时压缩低频背景信息，使得在有限的Context Window内处理多页文档成为可能。</p>\n</li>\n</ul>\n<h2 id=\"文档解析模块对比总结表\">文档解析模块对比总结表</h2>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>特性/工具</td>\n<td>Unstructured</td>\n<td>Marker</td>\n<td>PyMuPDF</td>\n<td>Docling</td>\n<td>MinerU</td>\n<td>PaddleOCR</td>\n<td>DeepSeek-OCR</td>\n</tr>\n<tr>\n<td>核心架构</td>\n<td>混合策略（规则+视觉模型）</td>\n<td>深度学习流水线 (Surya + Texify)</td>\n<td>底层PDF流解析 (C++绑定)</td>\n<td>混合架构 (统一DOM + VLM)</td>\n<td>深度学习流水线 (PDF-Extract-Kit)</td>\n<td>深度学习 (PP-Structure / OCR)</td>\n<td>端到端生成式VLM (MoE架构)</td>\n</tr>\n<tr>\n<td>解析策略</td>\n<td>分区</td>\n<td>版面检测 -&gt; Markdown生成</td>\n<td>文本块/跨度提取</td>\n<td>对象模型重构 -&gt; 导出</td>\n<td>布局分析 -&gt; 多模态MD</td>\n<td>检测+识别+结构化回归</td>\n<td>视觉Token -&gt; 文本生成</td>\n</tr>\n<tr>\n<td>最佳适用场景</td>\n<td>通用ETL，多格式混合处理</td>\n<td>科学论文、数学公式、书籍</td>\n<td>海量原生数字PDF清洗</td>\n<td>企业级文档、Agentic RAG</td>\n<td>学术文献、LLM预训练数据</td>\n<td>表单、票据、复杂表格</td>\n<td>极复杂排版、视觉推理任务</td>\n</tr>\n<tr>\n<td>表格处理能力</td>\n<td>HTML/CSV (开源版能力一般)</td>\n<td>高精度 (支持跨页合并)</td>\n<td>基础 (仅依赖文本位置)</td>\n<td>高精度 (结构恢复强)</td>\n<td>高精度 (转HTML)</td>\n<td>SOTA (擅长扭曲/无框线表)</td>\n<td>生成式 (语义描述/结构化)</td>\n</tr>\n<tr>\n<td>公式/数学支持</td>\n<td>基础 (依赖OCR)</td>\n<td>卓越 (Texify转LaTeX)</td>\n<td>无 (仅原始字符)</td>\n<td>良好 (支持LaTeX)</td>\n<td>卓越 (LaTeX转换)</td>\n<td>良好 (字符识别)</td>\n<td>卓越 (生成LaTeX)</td>\n</tr>\n<tr>\n<td>处理速度</td>\n<td>中等 (取决于策略)</td>\n<td>中等 (建议GPU)</td>\n<td>极快 (纯CPU)</td>\n<td>中等偏慢 (VLM模式)</td>\n<td>中等 (建议GPU)</td>\n<td>快 (提供轻量级模型)</td>\n<td>慢 (大模型推理开销)</td>\n</tr>\n<tr>\n<td>输入模态</td>\n<td>多格式 (PDF, PPT, HTML, Email)</td>\n<td>PDF, EPUB, Images</td>\n<td>PDF, XPS, Ebook</td>\n<td>多格式 (PDF, DOCX, Images)</td>\n<td>PDF, Images</td>\n<td>Images, PDF</td>\n<td>Images, PDF</td>\n</tr>\n<tr>\n<td>输出格式</td>\n<td>JSON (标准化Schema)</td>\n<td>Markdown, JSON, HTML</td>\n<td>Dict, String</td>\n<td>Markdown, JSON, Doctags</td>\n<td>Markdown (多模态), JSON</td>\n<td>JSON, Excel, HTML</td>\n<td>Markdown, JSON</td>\n</tr>\n<tr>\n<td>主要依赖</td>\n<td>Tesseract, Poppler, NLTK</td>\n<td>PyTorch, Surya, Texify</td>\n<td>MuPDF (无外部依赖)</td>\n<td>PyTorch, Granite Model</td>\n<td>PyTorch, YOLO, Ray</td>\n<td>PaddlePaddle</td>\n<td>PyTorch, FlashAttention</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"切块模块从文本流到语义单元\">切块模块：从文本流到语义单元</h1>\n<p>解析后的数据必须经过切块才能进入向量空间。切块策略直接决定了检索的粒度与上下文完整性。</p>\n<h2 id=\"策略演进\">策略演进</h2>\n<ul>\n<li>\n<p><strong>固定窗口切块</strong>：这是最基础的方法，利用LangChain的<code>RecursiveCharacterTextSplitter</code>按字符数（如512 tokens）切分，并设置重叠（Overlap）。优点是稳定，缺点是容易切断语义，例如将一句话截断在两个块中。</p>\n</li>\n<li>\n<p><strong>语义切块</strong>：利用嵌入模型计算句与句之间的余弦相似度。当相似度骤降时，意味着话题发生了转换，系统在此处进行切分。这种方法能保证每个块包含一个完整的语义主题。开源库Chonkie和SemChunk提供了轻量级的实现，支持在不依赖重型框架的情况下快速进行语义切分。</p>\n</li>\n<li>\n<p><strong>层级切块</strong>：利用Docling或MinerU输出的结构化信息（Header, Section），先按章节切大块，再在大块内切小块。检索时匹配小块，但返回大块（Parent Document Retrieval），兼顾了检索的精准度与生成的上下文丰富度。</p>\n</li>\n</ul>\n<h2 id=\"智能agent切块\">智能Agent切块</h2>\n<p>最新的趋势是Agentic Chunking，即利用LLM将文本重写为独立的「命题」。例如，将「张三，百度的工程师，去了北京」拆解为「张三是百度的工程师」和「张三去了北京」两个独立事实。虽然成本高，但能显著提升复杂问题的检索召回率。</p>\n<h2 id=\"资源链接\">资源链接</h2>\n<p><strong>Chonkie</strong></p>\n<ul>\n<li>\n<p>一个轻量级、极速的RAG切块库，专注于语义切分。</p>\n<ul>\n<li>\n<p><strong>GitHub</strong>: <a href=\"https://github.com/chonkie-inc/chonkie\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/chonkie-inc/chonkie</a></p>\n</li>\n<li>\n<p><strong>Docs</strong>: <a href=\"https://docs.chonkie.ai/\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.chonkie.ai/</a></p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Semantic</strong> <strong>Router</strong></p>\n<ul>\n<li>\n<p>虽然主要用于路由，但也包含强大的语义分析层，可用于高级切块。</p>\n<ul>\n<li>GitHub: <a href=\"https://github.com/aurelio-labs/semantic-router\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/aurelio-labs/semantic-router</a></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"向量化模块语义空间的构建\">向量化模块：语义空间的构建</h1>\n<p>向量化是将文本投影到高维语义空间的过程。在2025-2026年，MTEB (Massive Text Embedding Benchmark) 排行榜成为了衡量模型性能的黄金标准。开源模型在这一领域已经全面追平甚至超越了闭源商业模型（如OpenAI text-embedding-3）</p>\n<h2 id=\"bge-baai-general-embedding-系列\">BGE (BAAI General Embedding) 系列</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/FlagOpen/FlagEmbedding\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/FlagOpen/FlagEmbedding</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/BAAI/bge-m3\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/BAAI/bge-m3</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://bge-model.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://bge-model.com/</a></p>\n</li>\n<li>\n<p>参考资料：<a href=\"https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models\" rel=\"noopener nofollow\" target=\"_blank\">https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models</a></p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>由北京智源人工智能研究院开发。BGE-M3是目前的SOTA模型，支持多语言（100+）、多功能（稠密、稀疏、多向量检索）及长文本（8192 tokens）。其独特的混合检索能力使其成为构建多路召回系统的首选。该模型集成了三种检索范式：</p>\n<ul>\n<li>\n<p><strong>密集检索（Dense Retrieval）</strong>：基于语义向量。</p>\n</li>\n<li>\n<p><strong>稀疏检索（****Sparse</strong> <strong>Retrieval）</strong>：类似于BM25的词汇匹配，用于弥补密集检索在专有名词匹配上的不足。</p>\n</li>\n<li>\n<p><strong>多向量检索（Multi-Vector/ColBERT）</strong>：保留每个Token的向量进行细粒度交互。 这种“全能型”设计使得 BGE-M3 能够适应多语言（100+）、长文本（8192 Token）的复杂场景。</p>\n</li>\n</ul>\n<h2 id=\"qwen-embedding\">Qwen-Embedding</h2>\n<ul>\n<li>Huggingface: <a href=\"https://huggingface.co/collections/Qwen/qwen3-embedding\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/collections/Qwen/qwen3-embedding</a></li>\n</ul>\n<p>阿里巴巴推出的Qwen3-Embedding系列模型在MTEB榜单上表现优异，特别是在多语言任务和长上下文任务中。</p>\n<ul>\n<li><strong>弹性维度（Matryoshka Representation Learning, MRL）</strong>：Qwen模型支持弹性输出维度。用户可以根据存储预算，灵活选择使用前1024维甚至512维向量，而性能损失微乎其微。这使得RAG系统可以在速度和精度之间进行动态权衡。</li>\n</ul>\n<h2 id=\"e5系列\">E5系列</h2>\n<ul>\n<li>Hugging Face: <a href=\"https://huggingface.co/intfloat/multilingual-e5-large\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/intfloat/multilingual-e5-large</a></li>\n</ul>\n<p>E5系列 (EmbEddings from bidirEcTional Encoder rEpresentations)模型，如multilingual-e5-large，通过指令微调优化了非对称搜索任务（短Query找长Passage）。使用时需添加前缀<code>query:</code>和<code>passage:</code>，在MTEB榜单上长期霸榜。</p>\n<h2 id=\"jina-embeddings-v3\"><strong>Jina Embeddings v3</strong></h2>\n<ul>\n<li>Hugging Face: <a href=\"https://huggingface.co/jinaai/jina-embeddings-v3\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/jinaai/jina-embeddings-v3</a></li>\n</ul>\n<p>专为长文档和代码设计，利用ALiBi位置编码外推上下文长度，非常适合技术文档库的RAG。</p>\n<h1 id=\"检索模块retrieval向量数据库选型\">检索模块（Retrieval）：向量数据库选型</h1>\n<p>向量数据库是RAG系统的长时记忆。当前市场已经形成了以Milvus、Qdrant和Weaviate为代表的开源三巨头，以及pgvector这种依托于PostgreSQL的轻量化方案。</p>\n<h2 id=\"milvus云原生时代的巨舰\">Milvus：云原生时代的巨舰</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/milvus-io/milvus\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/milvus-io/milvus</a></p>\n</li>\n<li>\n<p>文档: <a href=\"https://milvus.io/docs\" rel=\"noopener nofollow\" target=\"_blank\">https://milvus.io/docs</a></p>\n</li>\n</ul>\n<p>由Zilliz开发的Milvus采用了存储与计算分离的云原生架构，专为处理十亿级（Billion-scale）向量数据而设计。</p>\n<ul>\n<li><strong>核心特性</strong>：支持分布式部署、Kubernetes 编排、多租户隔离以及多种索引类型（HNSW, DiskANN）。它适合需要极高吞吐量和大规模数据分片的企业级应用。</li>\n</ul>\n<h2 id=\"qdrant高性能与灵活性的平衡\">Qdrant：高性能与灵活性的平衡</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/qdrant/qdrant\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/qdrant/qdrant</a></p>\n</li>\n<li>\n<p>文档: <a href=\"https://qdrant.tech/documentation/\" rel=\"noopener nofollow\" target=\"_blank\">https://qdrant.tech/documentation/</a></p>\n</li>\n</ul>\n<p>Qdrant基于Rust语言开发，以高性能和低延迟著称。</p>\n<ul>\n<li><strong>核心特性</strong>：将向量索引与Payload（元数据）索引紧密结合，支持强大的过滤查询（Filtering）。与传统的先检索后过滤不同，Qdrant能够在索引遍历过程中应用过滤条件，极大提升了混合查询的效率。其推荐系统和去重功能也非常完善。</li>\n</ul>\n<h2 id=\"weaviateai原生的模块化数据库\">Weaviate：AI原生的模块化数据库</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/weaviate/weaviate\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/weaviate/weaviate</a></p>\n</li>\n<li>\n<p>文档: <a href=\"https://weaviate.io/developers/weaviate\" rel=\"noopener nofollow\" target=\"_blank\">https://weaviate.io/developers/weaviate</a></p>\n</li>\n</ul>\n<p>Weaviate不仅仅是一个数据库，更像是一个AI中间件平台。</p>\n<ul>\n<li><strong>核心特性</strong>：内置了大量的模块，可以直接在数据库层面集成OpenAI、HuggingFace等模型的向量化能力。用户只需存入文本，Weaviate会自动调用模型生成向量。其GraphQL接口也为复杂的数据关联查询提供了便利。</li>\n</ul>\n<h2 id=\"pgvectorpostgresql用户的务实之选\">pgvector：PostgreSQL用户的务实之选</h2>\n<ul>\n<li>GitHub: <a href=\"https://github.com/pgvector/pgvector\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/pgvector/pgvector</a></li>\n</ul>\n<p>pgvector是PostgreSQL的一个开源扩展，为现有的关系型数据库添加了向量存储和相似度搜索功能。</p>\n<ul>\n<li><strong>核心特性</strong>：允许向量数据与业务数据（如用户信息、订单记录）存储在同一张表中，天然支持ACID事务和复杂的SQL Join操作。对于数据量在千万级别以下，且已有Postgres基础设施的团队，这是成本最低的方案。</li>\n</ul>\n<h1 id=\"排序模块精度的最后一道防线\">排序模块：精度的最后一道防线</h1>\n<p>检索模块通常返回 Top-K（如50-100）个候选片段，但这些片段是基于粗略的向量相似度获取的。为了让LLM获得最精准的上下文，需要引入重排序模块。重排序模型通常采用交叉编码器架构，它将查询和文档同时输入模型，计算它们之间的深层交互，从而给出极高精度的相关性评分。</p>\n<h2 id=\"bge-reranker开源重排序的标杆\">BGE-Reranker：开源重排序的标杆</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/FlagOpen/FlagEmbedding\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/FlagOpen/FlagEmbedding</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/BAAI/bge-reranker-v2-m3\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/BAAI/bge-reranker-v2-m3</a></p>\n</li>\n</ul>\n<p>BGE-Reranker系列（v2, v2.5）是目前开源社区中最强大的重排序模型之一。</p>\n<ul>\n<li>\n<p><strong>多语言能力</strong>：支持中英等多语言混合排序。</p>\n</li>\n<li>\n<p><strong>轻量化蒸馏</strong>：最新的BGE-Reranker-v2.5-Gemma2-Lightweight通过层级蒸馏技术，在保持LLM级排序能力的同时，大幅降低了推理延迟，使其能够部署在实时性要求较高的生产环境中。</p>\n</li>\n</ul>\n<h2 id=\"rankgpt利用llm进行列表排序\">RankGPT：利用LLM进行列表排序</h2>\n<ul>\n<li><strong>GitHub</strong>: <a href=\"https://github.com/sunnweiwei/RankGPT\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/sunnweiwei/RankGPT</a></li>\n</ul>\n<p>RankGPT探索了一种不同的路径：利用生成式 LLM（如 GPT-4, Qwen）的推理能力进行排序。</p>\n<ul>\n<li><strong>列表式（Listwise）排序</strong>：不同于Cross-Encoder 对每对 (Query, Doc) 单独打分，RankGPT将Query和一组文档（如10个）同时输入LLM，并提示模型「请按相关性对这些文档进行排序」。这种方法考虑了文档之间的相对关系，往往能获得比Pairwise方法更高的MAP指标，但延迟和成本也显著增加。</li>\n</ul>\n<h2 id=\"flashrank极致轻量化的cpu方案\">FlashRank：极致轻量化的CPU方案</h2>\n<ul>\n<li>GitHub: <a href=\"https://github.com/PrithivirajDamodaran/FlashRank\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/PrithivirajDamodaran/FlashRank</a></li>\n</ul>\n<p><strong>FlashRank</strong> 是一个专为无 GPU 环境设计的 Python 库。</p>\n<ul>\n<li><strong>核心特性</strong>：基于量化后的TinyBERT等微型模型，能够在普通CPU上实现毫秒级的重排序。这对于部署在AWS Lambda等Serverless环境或边缘设备上的RAG应用至关重要 。</li>\n</ul>\n<h1 id=\"选型建议\">选型建议</h1>\n<p>构建开源RAG系统已不再是简单的模型堆砌，而是对数据处理全链路的精细化工程。</p>\n<ul>\n<li>\n<p><strong>解析层</strong>：对于通用场景，推荐使用Unstructured进行快速原型开发；对于科学文献和复杂报表，Marker和MinerU是目前的最佳实践；若需处理海量原生PDF，PyMuPDF不可或缺；而对于追求极致结构化和Agent交互的企业级应用，Docling展现了巨大的潜力。</p>\n</li>\n<li>\n<p><strong>数据流层</strong>：应摒弃固定的字符切块，转向语义切块或层级切块。</p>\n</li>\n<li>\n<p><strong>模型层</strong>：BGE-M3（Embedding）配合BGE-Reranker-v2-m3（Reranking）构成了目前最强的开源语义理解组合，配合Milvus或Qdrant可构建出性能媲美商业闭源方案的RAG系统。</p>\n</li>\n</ul>\n<p>随着DeepSeek-OCR等生成式解析技术的成熟，未来的文档解析将逐渐具备推理能力，进一步模糊感知与认知的边界，为RAG系统注入更深层的智能。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 21:04</span>&nbsp;\n<a href=\"https://www.cnblogs.com/aifrontiers\">AI-Frontiers</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "【Azure App Service】32位 Windows App Service 最大能使用多少内存？",
      "link": "https://www.cnblogs.com/lulight/p/19613358",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/lulight/p/19613358\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 19:13\">\n    <span>【Azure App Service】32位 Windows App Service 最大能使用多少内存？</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"问题描述\">问题描述</h2>\n<p>在使用 Windows-based Azure Web App（32位）时，经常遇到以下疑问：</p>\n<ul>\n<li>进程内存上限是多少？</li>\n<li>不同托管模式下可用内存如何计算？</li>\n</ul>\n<p>本文将针对这些问题进行详细解答。<br />\n<img alt=\"image\" class=\"lazyload\" /></p>\n<hr />\n<h2 id=\"问题解答\">问题解答</h2>\n<h3 id=\"一32-位程序最大能使用多少内存\">一、32 位程序最大能使用多少内存？</h3>\n<p><strong>理论上限约为 4GB</strong></p>\n<p>32 位程序的内存地址由 32 个二进制位组成，因此理论上可以有 2³² = 4,294,967,296 种不同的内存地址。每个内存地址指向 1 Byte 的空间，所以：</p>\n<blockquote>\n<p>32 位地址空间 = 2³² Byte (4<em>1024</em>1024*1024 B) ≈ 4GB</p>\n</blockquote>\n<p><strong>为什么文档中提到 2GB？</strong></p>\n<p>Windows 默认将 4GB 虚拟地址空间划分为：</p>\n<ul>\n<li><strong>2GB 用户态</strong>：供应用程序使用</li>\n<li><strong>2GB 内核态</strong>：供操作系统使用</li>\n</ul>\n<p>因此，默认情况下单进程可用用户态内存为 <strong>2GB</strong>。这只是默认行为，并非 32 位程序的绝对上限。在某些情况下（例如启用 Large Address Aware + 特定系统配置），可以超过 2GB。</p>\n<hr />\n<h3 id=\"二in-process-与-out-of-process-模型对内存的影响\">二、In-Process 与 Out-Of-Process 模型对内存的影响</h3>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<h4 id=\"两种托管模式对比\">两种托管模式对比</h4>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>In-Process</th>\n<th>Out-of-Process</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>宿主进程</td>\n<td><code>w3wp.exe</code>（IIS 工作进程）</td>\n<td><code>dotnet.exe</code>（独立进程）</td>\n</tr>\n<tr>\n<td>进程数量</td>\n<td>应用与 IIS 共享同一进程</td>\n<td>应用运行在独立进程中</td>\n</tr>\n<tr>\n<td>内存隔离</td>\n<td>与 IIS 共享内存空间</td>\n<td>独立内存空间</td>\n</tr>\n<tr>\n<td>性能</td>\n<td>更高（无进程间通信开销）</td>\n<td>略低（需通过 HTTP 代理）</td>\n</tr>\n</tbody>\n</table>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<h4 id=\"in-process-模式内存行为\">In-Process 模式内存行为</h4>\n<ul>\n<li>应用代码直接运行在 <code>w3wp.exe</code> 进程中</li>\n<li>内存上限受 <code>w3wp.exe</code> 进程限制：\n<ul>\n<li>32 位：约 <strong>2GB</strong>（Windows 用户态默认限制）</li>\n<li>64 位：受 Sandbox 限制</li>\n</ul>\n</li>\n<li><strong>注意</strong>：应用内存 + IIS 模块内存 共同占用进程空间</li>\n</ul>\n<h4 id=\"out-of-process-模式内存行为\">Out-of-Process 模式内存行为</h4>\n<ul>\n<li>应用运行在独立的 <code>dotnet.exe</code> 进程中</li>\n<li>Kestrel 作为边缘服务器，IIS 仅作为反向代理</li>\n<li>内存上限独立计算：\n<ul>\n<li>32 位：约 <strong>4GB</strong>（可寻址上限）</li>\n<li>64 位：受 Sandbox 限制</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"azure-app-service-sandbox-限制\">Azure App Service Sandbox 限制</h4>\n<p>在 Azure App Service 中，存在一个核心限制：</p>\n<blockquote>\n<p><strong>Sandbox 限制</strong>：进程实际能获得的最大物理内存 = 机器物理内存 × 75%</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>App Service Plan</th>\n<th>物理内存</th>\n<th>64位进程可用内存（约）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>B1/S1</td>\n<td>1.75 GB</td>\n<td>~1.3 GB</td>\n</tr>\n<tr>\n<td>B2/S2</td>\n<td>3.5 GB</td>\n<td>~2.6 GB</td>\n</tr>\n<tr>\n<td>B3/S3</td>\n<td>7 GB</td>\n<td>~5.25 GB</td>\n</tr>\n<tr>\n<td>P1v2</td>\n<td>3.5 GB</td>\n<td>~2.6 GB</td>\n</tr>\n<tr>\n<td>P2v2</td>\n<td>7 GB</td>\n<td>~5.25 GB</td>\n</tr>\n<tr>\n<td>P3v2</td>\n<td>14 GB</td>\n<td>~10.5 GB</td>\n</tr>\n</tbody>\n</table>\n<p><strong>总结：</strong></p>\n<ul>\n<li><strong>32 位进程</strong>：永远无法突破约 4GB 的可寻址限制（受托管模式影响不大）</li>\n<li><strong>64 位进程</strong>：会触及 Sandbox 限制（最大约为物理内存的 75%）</li>\n</ul>\n<hr />\n<h3 id=\"三多个虚拟目录时的内存计算方式\">三、多个虚拟目录时的内存计算方式</h3>\n<p>当同一 App Service 下存在多个虚拟目录（vdir）时：</p>\n<h4 id=\"in-process-模式\">In-Process 模式</h4>\n<ul>\n<li>所有虚拟目录共享同一个 <code>w3wp.exe</code> 进程</li>\n<li>内存上限为该进程的总上限（32位约2GB，64位受Sandbox限制）</li>\n<li>各应用间无内存隔离，一个应用内存泄漏可能影响其他应用</li>\n</ul>\n<h4 id=\"out-of-process-模式\">Out-of-Process 模式</h4>\n<ul>\n<li>每个虚拟目录会生成独立的 <code>dotnet.exe</code> 进程</li>\n<li>每个进程单独计算可用内存上限：\n<ul>\n<li>32 位 → 约 4GB（实际可能更低，取决于系统配置）</li>\n<li>64 位 → 受 Sandbox 限制（机器内存 × 75%）</li>\n</ul>\n</li>\n<li>多个站点共享同一台 VM 的物理内存，进程间会相互竞争资源</li>\n<li><strong>优势</strong>：进程隔离，一个应用崩溃不影响其他应用</li>\n</ul>\n<hr />\n<h3 id=\"四scmkudu进程是否计入总内存\">四、SCM（Kudu）进程是否计入总内存？</h3>\n<p><strong>是的</strong>。SCM 进程也运行在同一台 VM 上，其内存占用会一并计入 App Service Plan 的物理内存使用量。</p>\n<p><strong>Kudu 典型内存占用</strong>：约 200MB - 500MB（视操作而定）</p>\n<hr />\n<h3 id=\"五如何监控-app-service-内存使用\">五、如何监控 App Service 内存使用？</h3>\n<h4 id=\"方法一azure-portal-指标\">方法一：Azure Portal 指标</h4>\n<p>在 Azure Portal 中查看 App Service 的 <strong>Metrics</strong>：</p>\n<ul>\n<li><code>Memory working set</code>：当前工作集内存</li>\n<li><code>Private Bytes</code>：进程私有内存</li>\n</ul>\n<h4 id=\"方法二kudu-进程管理器\">方法二：Kudu 进程管理器</h4>\n<p>访问 <code>https://&lt;your-app&gt;.scm.chinacloudsites.cn/ProcessExplorer/</code> 查看：</p>\n<ul>\n<li>各进程的内存占用详情</li>\n<li><code>w3wp.exe</code> 和 <code>dotnet.exe</code> 的实时内存状态</li>\n</ul>\n<h4 id=\"方法三application-insights\">方法三：Application Insights</h4>\n<p>启用 Application Insights 后，可监控：</p>\n<ul>\n<li>内存使用趋势</li>\n<li>内存异常告警</li>\n<li>GC 行为分析</li>\n</ul>\n<hr />\n<h2 id=\"总结\">总结</h2>\n<table>\n<thead>\n<tr>\n<th>场景</th>\n<th>内存上限</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32 位进程（理论）</td>\n<td>~4GB</td>\n</tr>\n<tr>\n<td>32 位进程（Windows 默认）</td>\n<td>~2GB</td>\n</tr>\n<tr>\n<td>64 位进程</td>\n<td>物理内存 × 75%</td>\n</tr>\n<tr>\n<td>In-Process（32位）</td>\n<td>~2GB（与IIS共享）</td>\n</tr>\n<tr>\n<td>Out-of-Process（32位）</td>\n<td>~4GB（独立进程）</td>\n</tr>\n</tbody>\n</table>\n<p><strong>建议</strong>：</p>\n<ol>\n<li>如果应用对内存需求较高，推荐使用 <strong>64 位</strong> 配置</li>\n<li>对于需要进程隔离的场景，选择 <strong>Out-of-Process</strong> 模式</li>\n<li>定期监控内存使用，避免触及上限导致应用异常</li>\n</ol>\n<hr />\n<h2 id=\"参考资料\">参考资料</h2>\n<ul>\n<li>\n<p>Virtual Address Space (Memory Management) : <a href=\"https://learn.microsoft.com/en-us/windows/win32/memory/virtual-address-space\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/windows/win32/memory/virtual-address-space</a></p>\n</li>\n<li>\n<p>.NET GC - Heap hard limit percent : <a href=\"https://learn.microsoft.com/en-us/dotnet/core/runtime-config/garbage-collector#heap-hard-limit-percent\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/dotnet/core/runtime-config/garbage-collector#heap-hard-limit-percent</a></p>\n</li>\n<li>\n<p>ASP.NET Core Module (In-Process vs Out-of-Process) : <a href=\"https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/aspnet-core-module\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/aspnet/core/host-and-deploy/aspnet-core-module</a></p>\n</li>\n<li>\n<p>Azure App Service Sandbox : <a href=\"https://github.com/projectkudu/kudu/wiki/Azure-Web-App-sandbox\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/projectkudu/kudu/wiki/Azure-Web-App-sandbox</a></p>\n</li>\n</ul>\n\n\n</div>\n<div id=\"MySignature\">\n    <div style=\"background: #1c5f55; height: 36px; width: 618px; padding: 14px 5px 0px 3px;\">\n  <p style=\"font-weight: bold; color: white;\">当在复杂的环境中面临问题，格物之道需：浊而静之徐清，安以动之徐生。 云中，恰是如此!</p>\n</div>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 19:13</span>&nbsp;\n<a href=\"https://www.cnblogs.com/lulight\">编码者卢布</a>&nbsp;\n阅读(<span id=\"post_view_count\">19</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "小容量32单片机也上bootloader？拆机烧录的苦谁懂，能上抓紧上",
      "link": "https://www.cnblogs.com/pie-o/p/19613316",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pie-o/p/19613316\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 18:54\">\n    <span>小容量32单片机也上bootloader？拆机烧录的苦谁懂，能上抓紧上</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        基于表驱动构造的插件机制，实现单片机上的bootloader，主要是对通信，协议以及boot过程进行了插件化改造\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>首发于21ic论坛<br />\n<a href=\"https://bbs.21ic.com/icview-3453440-1-1.html\" rel=\"noopener nofollow\" target=\"_blank\">小容量32单片机也上bootloader？拆机烧录的苦谁懂，能上抓紧上</a></p>\n</blockquote>\n<h2 id=\"前言\">前言</h2>\n<p>在研发阶段需要更新程序时，直接使用调试器进行烧录即可，但是如果想要对一个封装好的产品进行程序升级时，一般都是没有引出烧录接口的，此时只有拆机一途。</p>\n<p>如果只有一两个需要更新，那么拆也就拆了，但有上百个呢，此时非bootloader不可。本文主旨是在小容量的32单片机上进行开发，比如stm32f103芯片，这样的主控芯片资源比较紧张，但是如果有条件上bootloader，建议一定要做的，功能不需要复杂，只需实现最基本的全量更新就行。</p>\n<p>在这基础上，我更想实现的是一插件式的工具，包括boot流程、交互协议、数据流以及flash驱动这4个相关的插件，当面对多种应用场景时，这样的插件式设计将极大提高代码的复用能力，方便替换、新增以及删除，从而改变程序的行为，并且更为重要的是这种改变没有影响到程序的主体框架。</p>\n<p>比如一个项目中通过CAN总线进行更新，而另一个项目使用485，这时候只需要更换相应的插件就可实现功能的调整，这就是所谓的<strong>策略同机制分离</strong></p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<hr />\n<h2 id=\"技术要点\">技术要点</h2>\n<h3 id=\"表驱动设计\">表驱动设计</h3>\n<p>网上关于表驱动的介绍有很多，并且实现起来也是各式各样，但是总体而言使用这种方法的核心还是抽象与辨识，这要求开发者必须对要实现的应用有一个全面的了解和认知，这样才能从无序中识别出有序的主体部分，而这部分必然成为程序的框架部分，是静态的，固定的，而其余的依托框架存在的零散部分，属于易变的，可替换的。</p>\n<p>要做到以上辨识和分离的操作，还真的需要不少功底。其实只要涉及到某种范式的使用，都或多或少的有一些瓶颈，我现在也正在学习这种方式，它的难点在于怎么构造这些表，表的结构该怎么设计，可以认为这个表就是一个结构化的解空间，所谓解空间就是对表进行输入的所有响应。</p>\n<p>所以一个好的表，必然有一个可以覆盖可能输入情况的所有响应，当然除了解空间的确定外，求解的方法，或者说把输入变换到解空间的方法也是不可缺少的，这些都极大考验开发者的能力和水平。</p>\n<p>相比于我之前写代码的思路，那都是直截了当的，平心而论，从任一个局部来看这种代码，完全是心中所思所想的一一映射，也就是完全把我们的思考过程转化为代码，这样的代码应该是很符合逻辑的不是吗。</p>\n<p>但是从局部上升的系统的整体，就会发现局部的片面和松散，这样的代码的每个部分都会符合原本的预期，但也仅此而已，一把钥匙只能配一把锁，但唯有铁丝才不挑锁。</p>\n<h3 id=\"表驱动与状态机结合\">表驱动与状态机结合</h3>\n<p>上面只介绍了表驱动的概念，现在介绍一种应用场景，那就是实现状态机。我们都知道在程序中的状态机就是通过定义一组有限状态和其转移的规则来控制系统行为的模型，一般有4个组成部分：状态、事件、转换和动作，那么要想基于表驱动实现状态机，就需要把这4个部分添加到表结构中，如下所示：</p>\n<pre><code class=\"language-c\">typedef struct{\n\tState CurState;\n\tEvent CurEvent;\n\tvoid (*Action)(void);\n\tState NextState;\n\n}TransitionItem_t;\n</code></pre>\n<p>这样基本的结构就是表中每一项的内容，而一个表中有多少项，就看开发者自己怎么理解问题和解析问题的了。举个例子，现在有一个按键和一个LED灯，要实现单击时，LED灯常亮，在常亮过程中长按时，LED闪烁，在闪烁过程中单击，LED常灭。</p>\n<p>在遇到这种应用时，不要直接开始写代码，无论多简单，先分析一下，画个状态图：</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>然后根据状态图，定义状态表：</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>上面这个状态表就可以完全设计到我们的代码中，如下所示：</p>\n<pre><code class=\"language-c\">TransitionItem_t FsmTable[] = {\n\t{LED_OFF，KEY_CLICK,LightUp,LED_ON},\n\t{LED_ON，KEY_LONG,Toggle,LED_TOGGLE},\n\t{LED_TOGGLE，KEY_CLICK,LightOff,LED_OFF}\n}\n</code></pre>\n<p>那么现在表结构有了，或者说解空间有了，怎么求解呢，最简单的就是查表，对比两个元素，当前状态以及发生的事件，在这个例子中就是(LED_OFF,KEY_CLICK)、(LED_ON,KEY_LONG)以及(LED_TOGGLE，KEY_CLICK)，（有没有点像多元离散函数，可以结合着加深对比理解）。</p>\n<p>只要符合这三个定义好的组合之一，那么必然发生动作，如果不属于三个之一，那么什么反应都没有，也不影响现有状态，驱动代码如下所示：</p>\n<pre><code class=\"language-c\">void RunFsm(TransitionItem_t *fsm,int fsm_size){\n\t/* 判断是否为空指针 */\n\tif(fsm){\n\t\tfor(int i = 0;i&lt;fsm_size;i++){\n\t\t/* 条件对比 */\n\t\t\tif(fsm[i].CurState == GlobalState &amp;&amp; fsm[i].CurEvent == GlobalEvent){\n\t\t\t/* 执行动作 */\n\t\t\t\tfsm[i].Action();\n\t\t\t\t/* 执行状态转移 */\n\t\t\t\tGlobalState = fsm[i].NextState;\n\t\t\t}\n\t\t}\n\t}\n}\n</code></pre>\n<h3 id=\"bootloader简介\">Bootloader简介</h3>\n<p>Bootloader是嵌入式系统中一段特殊的引导程序，用于固件加载或更新等功能，在芯片发生复位时首先会进入Bootloader进行引导，决定是否更新或跳转应用程序。</p>\n<p>在这里使用的是小容量的单片机，所以Bootloader程序尽量精简，这样可以给应用程序让出更多的空间，一般简单的应用下我们会进行分区，分为Bootloader区和App区，如下图所示：</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>可以看到大致上最简单的分区方案就是这样，其中app有效标志放在了Bootloader区末尾，这个标志就是Bootloader进行引导的条件。</p>\n<p>对于固件升级来说，实际上是通过Bootloader对App区域进行擦写动作，数据来源于外部，内容就是App工程编译出来的bin文件，如果在没有其他额外Flash的支持下，进行升级的风险还是蛮大的，所以需要有一定的安全校验措施。</p>\n<p>总的来说，要实现一个基本的Bootloader，流程还是比较简单的，大致如下图所示：</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>可以看到，上图的流程中显示了两种复位情况，即上电复位和软件复位，其中软件复位是从应用程序中进行的复位，是专门为固件升级设定的，表示存在更新请求，当然可以附加更多的信息，同时软件复位有一个特点，就是不会改变RAM的数据。</p>\n<p>不管哪种复位，只要存在更新请求，就会第一时间把App有效标志位清除，这样如果在更新过程中发生任何问题，标志位都是无效的，避免发生错误跳转的问题，只有完成全部的更新流程，包括校验等操作后才会标记有效。</p>\n<p>同时，如果是正常的上电复位，或者更新失败后，都会停留在Bootloader中，等待超时，判断有效标志，决定是否跳转。</p>\n<h2 id=\"综合实践\">综合实践</h2>\n<p>现在经过上面的介绍，已经大致对涉及的技术要求有了一定的了解，那么接下来就是对插件式的Bootloader进行设计了。</p>\n<p>有一个很核心的概念，就是<strong>策略与机制分离</strong>，对应到我们的设计中，机制就是Bootloader功能，策略就是实现功能的方式。</p>\n<p>乍一看，策略还比较好理解，机制要怎么理解呢，可以认为是一种容器、框架或模型，甚至就是一套固化规则，策略千千万，但都必须映射到机制的规则域中才能在机制的世界中存在。</p>\n<p>当然每个人有每个人的理解，不同的理解也产生不同的代码，现在介绍一下我对Bootloader机制的理解，先上图大致描述一下：</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>从上图中可以看到有4个主要的元素，也可以看作节点，包括通信流程、协议交互流程、Boot流程以及Flash驱动，可以认为要实现一个最基础的Bootloader必须要有这4个元素，而这些也是机制所固有的成分，所有策略都是作用在这4个元素之上的。</p>\n<p>其中为什么Flash与其他3个不一样，是因为在实现中，Flash读写都是一次性的，并且由Boot流程直接控制；而流程的控制就可以使用状态机来实现，本质上来说，状态机是一个独立的机制，而现在通过表驱动方法，增强了这一机制的通用性和灵活性，只需要更换状态表，就可以实现策略的改变。</p>\n<p>那么现在局部元素都介绍完了，该怎么在各元素之间建立关联呢，可以有消息、订阅发布等方式，但是，这些都大材小用了，通过梳理，发现完全可以使用链式通知的方式来建立关系。</p>\n<p>在上图中可以看到流向中标识的1.1、1.2等字样，用前后级来描述的话就是，由前级<strong>主动通知</strong>后级该做什么，否则后级什么都不做，当然这个前后级可以不是固定的前后级，是相对的前后级，可以想象一下环形链表，以上就是我所设计的机制内容，它的约束，或者说应用条件可以归纳以下几点：</p>\n<ul>\n<li>单向流控，链式触发</li>\n<li>机制结构固定，需识别或转化策略以满足结构要求</li>\n</ul>\n<p>下面给出实现机制的主要代码：<br />\n状态机的结构：</p>\n<pre><code class=\"language-c\">/* 状态表结构定义 */\ntypedef struct\n{\n/* 状态表事件项 */\n&nbsp; &nbsp; uint32_t Event;\n/* 状态表状态项 */\n&nbsp; &nbsp; uint32_t State;\n/* 状态表动作项 */\n&nbsp; &nbsp; int (*Action)(uint32_t *event, void *arg);\n/* 状态表转移项 */\n&nbsp; &nbsp; uint32_t NextState;\n\n} TransitionItem_t;\n\ntypedef struct\n\n/* 基类状态机定义 */\n{\n/* 状态表 */\n&nbsp; &nbsp; TransitionItem_t *FsmTable;\n/* 当前状态 */\n&nbsp; &nbsp; uint32_t CurState;\n/* 当前发生事件 */\n&nbsp; &nbsp; uint32_t CurEvent;\n/* 状态表尺寸 */\n&nbsp; &nbsp; uint32_t FsmSize;\n\n} BaseFsm_t;\n\n</code></pre>\n<p>定义了状态表的结构，以及基类状态机的结构，基类状态机提供给4个元素使用，其次是运行状态机的驱动代码：</p>\n<pre><code class=\"language-c\">int RunFsm(BaseFsm_t *fsm, void *arg)\n{\n/* 防止空指针错误 */\n&nbsp; &nbsp; if (fsm-&gt;FsmTable != NULL)\n&nbsp; &nbsp; {\n\n&nbsp; &nbsp; &nbsp; &nbsp; for (uint32_t i = 0; i &lt; fsm-&gt;FsmSize; i++)\n&nbsp; &nbsp; &nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; /* (状态,事件)元组对比 */\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (fsm-&gt;CurState == fsm-&gt;FsmTable[i].State &amp;&amp; fsm-&gt;CurEvent == fsm-&gt;FsmTable[i].Event)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {\n\t\t\t\t/* 防止空指针错误 */\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if (fsm-&gt;FsmTable[i].Action != NULL)\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; /* 执行动作 */\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fsm-&gt;FsmTable[i].Action(&amp;fsm-&gt;CurEvent, arg);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n\t\t\t\t/* 状态转移 */\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fsm-&gt;CurState = fsm-&gt;FsmTable[i].NextState;\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; &nbsp; &nbsp; }\n\n&nbsp; &nbsp; }\n&nbsp; &nbsp; else\n\n&nbsp; &nbsp; {\n&nbsp; &nbsp; &nbsp; &nbsp; return -1;\n&nbsp; &nbsp; }\n\n&nbsp; &nbsp; return 0;\n\n}\n</code></pre>\n<p>接下来是通信流程的结构设计：</p>\n<pre><code class=\"language-c\">/* 通信流程的结构设计 */\ntypedef struct\n{\n&nbsp; &nbsp; struct\n&nbsp; &nbsp; {\n\t&nbsp; &nbsp; /* 流程控制状态表 */\n&nbsp; &nbsp; &nbsp; &nbsp; BaseFsm_t *Fsm;\n&nbsp; &nbsp; &nbsp; &nbsp; /* 驱动器由外部提供 主要是每种通信驱动方式差异比较大，做不到通用 */\n&nbsp; &nbsp; &nbsp; &nbsp; void *Driver;\n&nbsp; &nbsp; &nbsp; &nbsp; /* 用于通知流程路径中下一节点，适用于单一路径 */\n&nbsp; &nbsp; &nbsp; &nbsp; void *Linkto;\n&nbsp; &nbsp; } PrivateArea;\n\n&nbsp; &nbsp; /* 初始化 传入状态机和驱动器 都由用户根据框架自定义 */\n&nbsp; &nbsp; void (*Init)(BaseFsm_t *fsm, void *driver, void *link);\n\t/* 获取状态机 */\n&nbsp; &nbsp; BaseFsm_t *(*GetFsm)(void);\n\t/* 获取通信驱动器 */\n&nbsp; &nbsp; void *(*GetDriver)(void);\n\t/* 获取下一节点 */\n&nbsp; &nbsp; void *(*GetLink)(void);\n\n} DataStreamHandle_t;\n</code></pre>\n<p>通信节点可以认为是整个机制的触发节点，因为所有的事件流都可以由通信来引导，包括无通信造成的超时事件流；继承了上面的基类状态表，扩展了通信必要的驱动器，还有一些方法定义。</p>\n<p>其次是交互协议流程的结构定义：</p>\n<pre><code class=\"language-c\">typedef struct\n{\n&nbsp; &nbsp; struct\n&nbsp; &nbsp; {\n\t&nbsp; &nbsp; /* 流程控制状态表 */\n&nbsp; &nbsp; &nbsp; &nbsp; BaseFsm_t *Fsm;\n&nbsp; &nbsp; &nbsp; &nbsp; /* 用于通知流程路径中下一节点，适用于单一路径 */\n&nbsp; &nbsp; &nbsp; &nbsp; void *Linkto;\n&nbsp; &nbsp; &nbsp; &nbsp; /* 协议体 - 封装与解析协议中的数据或协议特征 */\n&nbsp; &nbsp; &nbsp; &nbsp; void *ProtoBody;\n&nbsp; &nbsp; } PrivateArea;\n\t/* 初始化 传入状态机和协议体 都由用户根据框架自定义 */\n&nbsp; &nbsp; void (*Init)(BaseFsm_t *fsm, void *link, void *data_body);\n\t/* 获取状态机 */\n&nbsp; &nbsp; BaseFsm_t *(*GetFsm)(void);\n\t/* 获取协议体 */\n&nbsp; &nbsp; void *(*GetProtoBody)(void);\n\t/* 获取下一节点 */\n&nbsp; &nbsp; void *(*GetLink)(void);\n\n} ProtocalHandle_t;\n</code></pre>\n<p>同样的，继承了基类的状态表，同时扩展了一个协议体属性，这个属性的结构是自定义的，体现协议的通式，用于封装和解析数据使用。</p>\n<p>最后就是Boot流程的结构定义：</p>\n<pre><code class=\"language-c\">typedef struct\n{\n&nbsp; &nbsp; struct\n&nbsp; &nbsp; {\n\t&nbsp; &nbsp; /* 流程控制状态表 */\n&nbsp; &nbsp; &nbsp; &nbsp; BaseFsm_t *Fsm;\n\t\t/* flash驱动器 */\n&nbsp; &nbsp; &nbsp; &nbsp; void *FlashDriver;\n\t\t/* 用于通知流程路径中下一节点，适用于单一路径 */\n&nbsp; &nbsp; &nbsp; &nbsp; void *Linkto;\n\n&nbsp; &nbsp; } PrivateArea;\n\n&nbsp; &nbsp; void (*Init)(BaseFsm_t *fsm, void *link, void *driver);\n&nbsp; &nbsp; BaseFsm_t *(*GetFsm)(void);\n&nbsp; &nbsp; void *(*GetFlashDriver)(void);\n&nbsp; &nbsp; void *(*GetLink)(void);\n\n} BootloaderHandle_t;\n</code></pre>\n<p>基本和上面的结构大同小异，至此整个机制的结构就定义完成了，接下来就是框架的搭建了。</p>\n<p>首先，一般的通信接收功能都会在中断中进行，因为这样实时性最高，能及时的处理数据信息，所以我们的通信流程的触发可以结合接收中断来进行，以CAN接收中断举例：</p>\n<pre><code class=\"language-c\">void HAL_CAN_RxFifo0MsgPendingCallback(CAN_HandleTypeDef *hcan)\n\n{\n\tuint8_t i = 0;\n\t\n&nbsp; if (hcan-&gt;Instance == CAN1)\n&nbsp; {\n\n&nbsp; &nbsp; HAL_CAN_GetRxMessage(hcan, CAN_RX_FIFO0, &amp;CANxRxHeader, CANRecvBuf);\n&nbsp; &nbsp; if (CANxRxHeader.DLC &gt; 0)\n&nbsp; &nbsp; {\n&nbsp; &nbsp;  /* 作为触发条件 */\n&nbsp; &nbsp; &nbsp; CanMsgRecved = 1;\n&nbsp; &nbsp; }\n&nbsp; }\n}\n</code></pre>\n<p>这样，通过状态机就可以控制通信的流程，同时传入通信用的驱动器，因为我们设计的时候都是采用<code>void *</code>这种抽象指针，所以灵活程度很高。然后就是协议交互流程和Boot流程的使用，这些都可以放到以某一时基为周期运行的代码块中：</p>\n<pre><code class=\"language-c\">&nbsp;while (1)\n&nbsp;{\n\t&nbsp;\n\t/* 1ms时基 */\n&nbsp; &nbsp; if (TIMEBASE_HOOK(TimeBaseScope, OPT_TIMEBASE_1MS))\n&nbsp; &nbsp; {\n\n&nbsp; &nbsp; &nbsp; TIMEBASE_DROP(TimeBaseScope, OPT_TIMEBASE_1MS);\n&nbsp; &nbsp; &nbsp; /* 单路径流程，总是通过前级主动通知后级该做什么(自定义事件)，同时后级继承(传入)前级的遗产(输出内容) */\n\t\t/* 放入状态机在直接接收数据的地方 */\n&nbsp; &nbsp; &nbsp; RunFsm(DataStreamHandle.GetFsm(), DataStreamHandle.GetDriver());\n&nbsp; &nbsp; &nbsp; \n&nbsp; &nbsp; &nbsp; /* 协议状态机，传入数据收发接口的原生数据形式 比如can报文形式，串口形式 */\n&nbsp; &nbsp; &nbsp; RunFsm(ProtocolHandle.GetFsm(), DataStreamHandle.GetDriver());\n\n&nbsp; &nbsp; &nbsp; /* boot流程状态机 */\n&nbsp; &nbsp; &nbsp; RunFsm(BootloaderHandle.GetFsm(), ProtocolHandle.GetDataBody());\n&nbsp; &nbsp; }\n}\n</code></pre>\n<p>因为通过链式传递消息通知，在单路径流程中，总是可以通过前级主动通知后级该做什么(自定义事件)，同时后级继承(传入)前级的遗产(输出内容)。</p>\n<p>以上整个机制的框架的完成了，接下来主要就是策略的定义了，其实就是识别出自定义策略中的事件、状态以及转移，并把这些以状态表的方式呈现出来。</p>\n<p>首先给出通信状态图：</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>通信流程中定义的事件和状态宏定义，以及状态表定义：</p>\n<pre><code class=\"language-c\">/* 通信事件流自定义宏 &gt;&gt;&gt;&gt;&gt;&gt; */\n\n/* 无事件 */\n#define E_DATA_NONE 0\n/* 有数据进入 */\n#define E_DATA_IN (CAST_U32(0X01) &lt;&lt; 0)\n/* 数据超时 */\n#define E_DATA_TIMEOUT (CAST_U32(0X01) &lt;&lt; 1)\n/* 数据接收结束 */\n#define E_DATA_END (CAST_U32(0X01) &lt;&lt; 2)\n/* 数据请求协议处理 因为只有自己知道自己什么情况，当然得主动通知 */\n#define E_DATA_LINK_PTO (CAST_U32(0X01) &lt;&lt; 3)\n/* 等待协议给过来消息 */\n#define E_DATA_RET_PTO (CAST_U32(0X01) &lt;&lt; 4)\n  \n/* 自定义状态宏 */\n\n/* 空状态 */\n#define S_DATA_NONE 0\n/* 接收状态 */\n#define S_DATA_READING (CAST_U32(0X01) &lt;&lt; 0)\n/* 数据接收完成状态 */\n#define S_DATA_READ_END (CAST_U32(0X01) &lt;&lt; 1)\n/* 等待协议反馈状态 */\n#define S_DATA_WAIT_PTO (CAST_U32(0X01) &lt;&lt; 2)\n/* 通信事件流自定义宏 &lt;&lt;&lt;&lt;&lt;&lt; */\n\nTransitionItem_t DataStreamTable[] = {\n\n&nbsp; &nbsp; {E_DATA_IN, S_DATA_NONE, BufData, S_DATA_READING},\n&nbsp; &nbsp; {E_DATA_TIMEOUT, S_DATA_READING, ResetData, S_DATA_NONE},\n&nbsp; &nbsp; {E_DATA_END, S_DATA_READING, LinkProto, S_DATA_READ_END},\n&nbsp; &nbsp; {E_DATA_LINK_PTO, S_DATA_READ_END, NULL, S_DATA_WAIT_PTO},\n&nbsp; &nbsp; {E_DATA_RET_PTO, S_DATA_WAIT_PTO, SendResp, S_DATA_NONE}\n\n};\n\n</code></pre>\n<p>其次是交互协议状态图：</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>对应的事件、状态以及状态表定义如下：</p>\n<pre><code class=\"language-c\">/* 协议收到通信的通知 */\n\n#define E_PROTO_RECV_DATA (CAST_U32(0X01) &lt;&lt; 0)\n\n/* 协议解析完成 */\n\n#define E_PROTO_PARSE_OK (CAST_U32(0X01) &lt;&lt; 1)\n\n/* Boot反馈 */\n\n#define E_PROTO_RET_BOOT (CAST_U32(0X01) &lt;&lt; 2)\n\n/* 空状态 */\n#define S_PROTO_NONE 0\n/* 解析状态 */\n#define S_PROTO_PARSE (CAST_U32(0X01) &lt;&lt; 1)\n/* 等待Boot反馈状态 */\n#define S_PROTO_WAIT_BOOT (CAST_U32(0X01) &lt;&lt; 2)\n/* 状态表定义 */\nTransitionItem_t ProtoTable[] = {\n\n&nbsp; &nbsp; {E_PROTO_RECV_DATA, S_PROTO_NONE, ParseData, S_PROTO_PARSE},\n&nbsp; &nbsp; {E_PROTO_PARSE_OK, S_PROTO_PARSE, LinkBoot, S_PROTO_WAIT_BOOT},\n&nbsp; &nbsp; {E_PROTO_RET_BOOT, S_PROTO_WAIT_BOOT, RetData, S_PROTO_NONE}\n\n};\n</code></pre>\n<p>最后是Boot流程的状态图：</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>对应的事件、状态以及状态表定义如下：</p>\n<pre><code class=\"language-c\">/* Boot收到协议的通知 */\n#define E_BOOT_RECV_PROTO (CAST_U32(0X01) &lt;&lt; 0)\n/* Boot起始命令 */\n#define E_BOOT_CMD_START (CAST_U32(0X01) &lt;&lt; 1)\n/* Boot更新命令 */\n#define E_BOOT_CMD_UPDATE (CAST_U32(0X01) &lt;&lt; 2)\n/* Boot结束命令 */\n#define E_BOOT_CMD_END (CAST_U32(0X01) &lt;&lt; 3)\n/* Boot跳转命令 */\n#define E_BOOT_CMD_JUMP (CAST_U32(0X01) &lt;&lt; 4)\n\n#define S_BOOT_NONE 0\n/* Boot准备状态 */\n#define S_BOOT_READY (CAST_U32(0X01) &lt;&lt; 1)\n/* Boot更新状态 */\n#define S_BOOT_UPDATE (CAST_U32(0X01) &lt;&lt; 2)\n/* Boot结束状态 */\n#define S_BOOT_END (CAST_U32(0X01) &lt;&lt; 3)\n\n/* 状态表定义 */\nTransitionItem_t BootTable[] = {\n&nbsp; &nbsp; {E_BOOT_RECV_PROTO | E_BOOT_CMD_START, S_BOOT_NONE ResetBoot, S_BOOT_READY},\n&nbsp; &nbsp; {E_BOOT_RECV_PROTO | E_BOOT_CMD_UPDATE, S_BOOT_READY, Update, S_BOOT_UPDATE},\n&nbsp; &nbsp; {E_BOOT_RECV_PROTO | E_BOOT_CMD_UPDATE, S_BOOT_UPDATE, Update, S_BOOT_UPDATE},\n&nbsp; &nbsp; {E_BOOT_RECV_PROTO | E_BOOT_CMD_END, S_BOOT_UPDATE, Check, S_BOOT_END},\n&nbsp; &nbsp; {E_BOOT_RECV_PROTO | E_BOOT_CMD_JUMP, S_BOOT_END, JumpApp, S_BOOT_NONE}\n\n};\n</code></pre>\n<p>可以看到状态表中的事件项都是多种事件的组合，因为我们的事件宏定义是通过移位操作进行的，所以提供了组合事件的能力。</p>\n<p>至此，Bootloader的机制和策略都制定完毕了，这里只演示了比较简单的策略功能，但因为机制提供了策略更换的能力，所以实现更复杂的功能只需要制定相应的策略状态表即可。</p>\n<p>实现这个机制也算是一个尝试，学到了很多，同时呢也接触到更高级的内容，还有待进一步深入了解。</p>\n<p>微信公众号：软趴趴的工程师</p>\n<p><img alt=\"dd83fca0b5541e763cf671018f89542e\" class=\"lazyload\" /></p>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/pie-o/\" target=\"_blank\">pie_thn</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/pie-o/p/19613316\" target=\"_blank\">https://www.cnblogs.com/pie-o/p/19613316</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 18:54</span>&nbsp;\n<a href=\"https://www.cnblogs.com/pie-o\">pie_thn</a>&nbsp;\n阅读(<span id=\"post_view_count\">14</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "【5分钟学一个新技能】Git：改变世界的协作革命",
      "link": "https://www.cnblogs.com/sean537/p/19612709",
      "published": "",
      "description": "<div class=\"posthead\">\n\t\t\t<h2>\n\t\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/sean537/p/19612709\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 16:41\">\n    <span>【5分钟学一个新技能】Git：改变世界的协作革命</span>\n    \n\n</a>\n\n\t\t\t</h2>\n \t\t\tPosted on \n<span id=\"post-date\">2026-02-13 16:41</span>&nbsp;\n<a href=\"https://www.cnblogs.com/sean537\">山地奥斯卡537</a>&nbsp;\n阅读(<span id=\"post_view_count\">110</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t\t\n\t\t\t\n\t\t</div>\n\t\t<div class=\"postbody\">    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"【5分钟学一个新技能】Git：改变世界的协作革命\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3664555/202602/3664555-20260213203143351-2015099335.png\" />\n        十年前，每个程序员都经历过代码丢失的焦虑：熬夜修改的程序无法回滚，团队协作时版本混乱如乱麻。正是这种困境催生了版本管理工具的革命。Git的诞生，标志着编程世界从\"文件备份\"迈入\"时光机管理\"时代。\n2005年，Linux之父Linus用十天时间创造了Git。这个分布式系统允许开发者离线工作，每个本地仓库都完整保存项目历史，彻底打破中央服务器的桎梏。GitHub随后将Git可视化，通过Pull Request机制让开源协作变得像\"数字接力赛\"，任何人都能通过Fork-修改-提交的三步操作参与全球项目。\nGit的分支管理堪称神来之笔：开发者可自由创建实验性分支，失败代码随时丢弃，成功功能无缝合并。这种\"数字分身术\"让代码迭代既安全又高效。如今，从微软.NET到阿里巴巴开源项目，全球超过2亿开发者都在使用这套体系。\n更深远的影响在于思维革命：开发者不再畏惧重构，分支隔离让并行开发成为常态，每次提交都是可追溯的时光印记。当我们在IDE点击\"git init\"时，实际上是在参与一场持续二十年的技术民主化运动——让每个普通人都能掌控代码演进的脉络。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>刚学编程那阵子，我最怕两件事：一是代码写不出来，二是代码写出来却弄丢了。</p>\n<p>辛辛苦苦改了一晚上代码，原本能跑，更新后如果出了问题就难以回滚亦或是修复，只能盯着空白发呆；</p>\n<p>和别人一起开发项目，QQ互传压缩包，有时也不知道对方改了哪里，遇到报错更是头晕，只能对七八个版本傻眼——到底哪个能用？</p>\n<p>调侃之余，有群友说：“你们该用版本管理。”</p>\n<p>那是我第一次听说这个词。</p>\n<h2 id=\"什么是版本管理\"><strong>什么是版本管理？</strong></h2>\n<p>简单说，版本管理就是给你的代码“拍照片”。</p>\n<p>想象一下你写文档。写完一稿，存成“报告v1.docx”。改了点，存成“报告v2.docx”。又改了点，存成“报告最终版.docx”。再改，变成“报告真的最终版.docx”。</p>\n<p>这就是最原始的版本管理——<strong>手动命名，手动备份</strong>。问题很明显：版本多了就乱，想找回某个中间版本得一个个打开看。</p>\n<p>专业的版本管理系统，就是帮你自动化这个过程的工具。它会：</p>\n<ol>\n<li>记录每次改动的内容</li>\n<li>记录谁在什么时候改的</li>\n<li>记录为什么要改</li>\n<li>让你随时回到任意历史版本</li>\n</ol>\n<p>在Git出现前，已经有CVS、Subversion这样的版本控制工具。但它们都有个共同问题：必须连接中央服务器。这就像在图书馆借书——书只有一本，你得登记谁借走了，别人得等着。</p>\n<h2 id=\"linus的十日革命\"><strong>Linus的“十日革命”</strong></h2>\n<p>2005年，Linux内核开发团队就面临这样的困境。他们用的BitKeeper突然要收费，而现有的开源工具又满足不了需求。</p>\n<p>Linux之父Linus Torvalds后来在采访中说：“当时的情况是，要么我们继续用不合适的工具，要么我自己造一个合适的。”他选择了后者。</p>\n<p>4月3日，Linus开始写Git。他在设计文档里写了三个目标：</p>\n<ol>\n<li>速度要比其他工具快十倍</li>\n<li>完全分布式，不需要中央服务器</li>\n<li>能处理像Linux内核这样的大项目</li>\n</ol>\n<p>十天后，Git第一个版本发布。4月16日，Linus用Git管理Linux内核源码。4月20日，他宣布Git已经能自我托管——用Git来管理Git自己的代码。</p>\n<p>早期的Git很难用。命令复杂，概念抽象。有开发者抱怨：“这玩意儿比Linux内核还难懂。”Linus的回复很直接：“不爽不要用。”</p>\n<p>但Git有两个设计太超前了：</p>\n<p>第一，分布式架构。每个人的电脑上都有完整的仓库历史，不需要联网也能提交代码。这就像每个人都有一本完整的图书馆目录，不需要去总馆查资料。</p>\n<p>第二，分支管理。分支是同时控制不同版本代码的绝佳工具。例如，你希望在稳定版代码的基础上加点新功能，就可以基于主分支创建新分支，在新分支上随意更改，这让“尝试性开发”成为可能——想到什么点子，开个分支试试，不行就删掉。</p>\n<h2 id=\"github让git飞入寻常百姓家\"><strong>GitHub：让Git飞入寻常百姓家</strong></h2>\n<p>Git很强，但直到2008年，它主要还是极客在用。直到三个年轻人创立了GitHub。</p>\n<p>GitHub做了件简单但革命性的事：给Git套了个网页界面。</p>\n<p>突然之间，事情变得直观了。代码仓库变成网页，提交历史变成时间轴，分支合并变成可视化操作。更重要的是，他们创造了<strong>Pull Request（PR，拉取请求）</strong>。这是GitHub最伟大的创新。你不再需要邮件发送补丁，而是：</p>\n<ol>\n<li>Fork（复制）别人的仓库</li>\n<li>在自己的副本上修改</li>\n<li>发起Pull Request，请求原作者合并你的修改</li>\n<li>在PR中进行代码审查、讨论</li>\n</ol>\n<p>这种模式极大地降低了开源贡献的门槛。2015年，微软将.NET框架开源并放到GitHub上时，社区震惊了。这个曾经视开源为敌的公司，现在主动拥抱开源。</p>\n<p>我清楚地记得第一次在GitHub上提交PR的场景。那是个朋友的开源小项目，我发现文档内容有误。点了“Fork”按钮，把项目复制到我的账户。修改，提交，然后发起PR。项目维护者看到了，点了个“Merge”。两分钟后，我的修改成了项目的一部分。</p>\n<p>那种感觉很难形容——好像突然之间，我也能为那些遥不可及的大项目做贡献了。</p>\n<p>GitHub火了。到2012年，它已经有超过200万个仓库。Ruby on Rails、jQuery、Node.js这些知名项目都搬了上去。微软也把.NET框架开源放到了GitHub，这在以前是不可想象的。</p>\n<p>GitHub的成功引来了竞争者。2011年，GitLab诞生。它的卖点是“可以自己部署”。企业可以把GitLab装在自己的服务器上，代码不出内网。除了代码托管，它还集成了：</p>\n<ul>\n<li><strong>CI/CD流水线</strong>：自动测试、构建、部署</li>\n<li><strong>问题追踪</strong>：bug报告、功能请求</li>\n<li><strong>Wiki</strong>：项目文档</li>\n<li><strong>代码质量分析</strong>：自动化代码检查</li>\n</ul>\n<p>中国这边，2013年出现了Gitee（码云）。起初很多人觉得“是山寨GitHub”，但后来发现了它的价值：国内访问快，符合中国法规，还有中文界面。</p>\n<h2 id=\"不只是代码\"><strong>不只是代码</strong></h2>\n<p>Git的影响远远超出了编程世界。</p>\n<ul>\n<li><strong>学术研究</strong>：学者用Git管理论文草稿，每次修改都有记录</li>\n<li><strong>法律文件</strong>：律师事务所用类似Git的系统管理合同版本</li>\n<li><strong>图书出版</strong>：出版社用Git协调作者、编辑、校对的工作</li>\n<li><strong>政府文档</strong>：有些政府部门用Git追踪政策文件的修订</li>\n</ul>\n<h2 id=\"实际怎么用\"><strong>实际怎么用？</strong></h2>\n<h3 id=\"第一步初始化与配置\"><strong>第一步：初始化与配置</strong></h3>\n<p>先安装Git，安装步骤略。</p>\n<hr />\n<p>开始前，先告诉Git你是谁：</p>\n<pre><code class=\"language-bash\">git config --global user.name ”你的名字“\ngit config --global user.email ”你的邮箱“\n</code></pre>\n<p>这很重要，因为Git的所有提交都会记录作者信息。</p>\n<hr />\n<p>创建一个新仓库：</p>\n<pre><code class=\"language-bash\">mkdir my-project # 新建一个名为“my-project”的文件夹\ncd my-project # 进入该文件夹\ngit init # Git仓库初始化\n</code></pre>\n<p>执行<code>git init</code>后，当前目录会出现一个隐藏的.git文件夹。这就是Git的“大脑”，里面存储着所有的版本信息、配置和索引。</p>\n<p>或者，你也可以直接下载云端仓库：</p>\n<pre><code class=\"language-bash\">git clone https://github.com/username/repo.git # 此处链接为你仓库的实际在线链接，clone就是“克隆”的意思，会把仓库下载到当前目录下\ncd repo # 进入仓库文件夹，文件夹名称为仓库名\n</code></pre>\n<hr />\n<p>查看仓库状态：</p>\n<pre><code class=\"language-bash\">git status\n</code></pre>\n<p>这是你最常用的命令之一。它会告诉你：</p>\n<ul>\n<li>哪些文件被修改了但还没暂存（红色）</li>\n<li>哪些文件已经暂存等待提交（绿色）</li>\n<li>是否有未跟踪的新文件</li>\n</ul>\n<hr />\n<h3 id=\"第二步基础工作流\"><strong>第二步：基础工作流</strong></h3>\n<p>假设你写了一个简单的Python脚本：</p>\n<pre><code class=\"language-python\"># hello.py\nprint(”Hello, World!“)\n</code></pre>\n<p>把文件添加到暂存区：</p>\n<pre><code class=\"language-bash\">git add hello.py # add后为你的实际文件名\n</code></pre>\n<p>或者添加所有修改：</p>\n<pre><code class=\"language-bash\">git add . # .表示所有文件\n</code></pre>\n<p>暂存区（Staging Area）是Git的精妙设计。它就像购物车——你可以把多个改动放进去，最后一次性结账。这让你可以精心组织每次提交的内容。</p>\n<hr />\n<p>正式提交：</p>\n<pre><code class=\"language-bash\">git commit -m ”添加hello.py脚本“\n</code></pre>\n<p>每次提交都应该有一个清晰的描述。好的提交信息应该像新闻标题：简明扼要地说明做了什么。比如：</p>\n<ul>\n<li>❌ 错误：”修复bug“、”更新代码“</li>\n<li>✅ 正确：”修复用户登录时的空指针异常“、”添加用户头像上传功能“</li>\n</ul>\n<blockquote>\n<p>目前业界公认度最高的提交信息标准是 <strong>Conventional Commits（约定式提交）</strong>。它最初源于 Angular 团队的规范，现在已成为许多开源项目（如 Vue、React、Babel 等）和大型团队的通用标准。</p>\n<h3 id=\"1-核心格式\"><strong>1. 核心格式</strong></h3>\n<p>遵循<code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;</code>的结构，例如：</p>\n<ul>\n<li><code>feat(auth): add OAuth2 login flow</code></li>\n<li><code>fix(ui): correct button alignment on dashboard</code></li>\n</ul>\n<h3 id=\"2-关键规则\"><strong>2. 关键规则</strong></h3>\n<ul>\n<li><strong>类型 (Type)</strong>：必须使用特定关键词，如 feat（新功能）、fix（修复）、docs（文档）、style（格式）、refactor（重构）、test（测试）、chore（构建/工具）等。</li>\n<li><strong>时态</strong>：使用<strong>现在时</strong>（如 ”add“ 而非 ”added“），且首字母<strong>不大写</strong>。</li>\n<li><strong>长度</strong>：标题行建议不超过 50 字符，正文行建议不超过 72 字符。</li>\n<li><strong>空行</strong>：标题与正文之间必须有一个空行。</li>\n</ul>\n<h3 id=\"3-为什么用这个标准\"><strong>3. 为什么用这个标准？</strong></h3>\n<ul>\n<li><strong>自动化工具</strong>：能自动生成 CHANGELOG（更新日志）。</li>\n<li><strong>语义化版本</strong>：能根据 feat 和 fix 自动决定版本号升级（如 1.0.0 → 1.1.0）。</li>\n<li><strong>可读性</strong>：让代码历史像一本清晰的说明书，便于团队协作。</li>\n</ul>\n</blockquote>\n<hr />\n<p>查看提交历史：</p>\n<pre><code class=\"language-bash\">git log\n</code></pre>\n<p>默认显示完整的提交信息。如果想要简洁视图：</p>\n<pre><code class=\"language-bash\">git log --oneline --graph --all\n</code></pre>\n<p>这会显示一个可视化的分支图，特别适合查看复杂的分支结构。</p>\n<hr />\n<h3 id=\"第三步撤销与回滚\"><strong>第三步：撤销与回滚</strong></h3>\n<p>写代码难免犯错，Git提供了多种“后悔药”。</p>\n<p><strong>场景1：还没暂存就发现写错了</strong></p>\n<pre><code class=\"language-bash\"># 放弃某个文件的修改\ngit checkout -- hello.py\n\n# 放弃所有修改（危险！请确认你真的不需要这些修改）\ngit checkout -- .\n</code></pre>\n<p><strong>场景2：已经暂存但想撤销</strong></p>\n<pre><code class=\"language-bash\"># 把文件从暂存区移出，但保留修改\ngit reset HEAD hello.py\n\n# 然后可以用checkout放弃修改\ngit checkout -- hello.py\n</code></pre>\n<p><strong>场景3：已经提交但想撤销</strong></p>\n<pre><code class=\"language-bash\"># 查看提交历史，找到要回退的版本\ngit log --oneline\n\n# 假设我们看到：\n# a1b2c3d 添加新功能\n# e4f5g6h 修复bug\n# i7j8k9l 初始提交\n\n# 回到修复bug的那个版本\ngit reset --hard e4f5g6h\n</code></pre>\n<p>--hard参数会彻底丢弃之后的提交，慎用！如果你只是想撤销某个提交但保留更改：</p>\n<pre><code class=\"language-bash\">git revert e4f5g6h\n</code></pre>\n<p>这会创建一个新的提交来撤销指定提交的修改，更安全。</p>\n<hr />\n<h3 id=\"第四步分支管理\"><strong>第四步：分支管理</strong></h3>\n<p>分支是Git最强大的功能之一。它让你可以：</p>\n<ul>\n<li>同时开发多个功能</li>\n<li>安全地实验新想法</li>\n<li>隔离bug修复和功能开发</li>\n</ul>\n<hr />\n<p>创建并切换到新分支：</p>\n<pre><code class=\"language-bash\">git checkout -b new-feature\n</code></pre>\n<p>这等价于：</p>\n<pre><code class=\"language-bash\">git branch new-feature    # 创建分支\ngit checkout new-feature  # 切换分支\n</code></pre>\n<hr />\n<p>在新分支上开发完成后，切换回主分支并合并：</p>\n<pre><code class=\"language-bash\">git checkout main\ngit merge new-feature\n</code></pre>\n<p>如果合并顺利，Git会执行“快进合并”（Fast-forward）。如果有冲突，Git会提示你解决。</p>\n<hr />\n<p>删除已合并的分支：</p>\n<pre><code class=\"language-bash\">git branch -d new-feature\n</code></pre>\n<hr />\n<p>强制删除未合并的分支：</p>\n<pre><code class=\"language-bash\">git branch -D experimental\n</code></pre>\n<hr />\n<p>查看所有分支：</p>\n<pre><code class=\"language-bash\">git branch     # 本地分支\ngit branch -r  # 远程分支\ngit branch -a  # 所有分支\n</code></pre>\n<hr />\n<h3 id=\"第五步远程协作\"><strong>第五步：远程协作</strong></h3>\n<p>本地Git再强大，也需要与团队协作。这就是远程仓库的用武之地。</p>\n<hr />\n<p>添加远程仓库：</p>\n<pre><code class=\"language-bash\">git remote add origin https://github.com/username/repo.git\n</code></pre>\n<p>origin是远程仓库的默认别名，你可以用其他名字。</p>\n<hr />\n<p>查看远程仓库：</p>\n<pre><code class=\"language-bash\">git remote -v\n</code></pre>\n<hr />\n<p>推送本地提交到远程：</p>\n<pre><code class=\"language-bash\">git push origin main\n</code></pre>\n<p>第一次推送时可能需要指定上游分支：</p>\n<pre><code class=\"language-bash\">git push -u origin main\n</code></pre>\n<p>-u参数设置上游关联，之后可以直接用git push</p>\n<hr />\n<p>拉取远程更新：</p>\n<pre><code class=\"language-bash\">git pull origin main\n</code></pre>\n<p>这等价于：</p>\n<pre><code class=\"language-bash\">git fetch origin   # 下载远程更新\ngit merge origin/main  # 合并到当前分支\n</code></pre>\n<p>有时候你只想查看远程有什么更新，而不想立即合并：</p>\n<pre><code class=\"language-bash\">git fetch origin\ngit log origin/main --oneline  # 查看远程分支的提交\n</code></pre>\n<hr />\n<h3 id=\"第六步处理冲突\"><strong>第六步：处理冲突</strong></h3>\n<p>冲突是团队协作的必经之路。当两个人修改了同一文件的同一区域时，Git无法自动合并，需要人工解决。</p>\n<p>发生冲突时，Git会在文件中标记冲突：</p>\n<pre><code class=\"language-python\">&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nprint(”Hello from Alice!“)\n=======\nprint(”Hello from Bob!“)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-branch\n</code></pre>\n<p>你需要：</p>\n<ol>\n<li>编辑文件，选择保留哪部分（或都保留）</li>\n<li>删除冲突标记（<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>、<code>=======</code>、<code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>）</li>\n<li>添加解决后的文件：git add filename</li>\n<li>完成合并：git commit</li>\n</ol>\n<p>查看合并状态：</p>\n<pre><code class=\"language-bash\">git merge --abort  # 取消合并，回到合并前状态\ngit status         # 查看当前冲突文件\n</code></pre>\n<hr />\n<h3 id=\"第七步高级技巧\"><strong>第七步：高级技巧</strong></h3>\n<h4 id=\"储藏更改stashing\"><strong>储藏更改</strong>（Stashing）：</h4>\n<p>当你需要切换分支但当前工作还没完成时：</p>\n<pre><code class=\"language-bash\">git stash          # 储藏当前修改\ngit stash list     # 查看储藏列表\ngit stash pop      # 应用最新的储藏并删除\ngit stash apply    # 应用储藏但不删除\n</code></pre>\n<h4 id=\"查看差异\"><strong>查看差异</strong>：</h4>\n<pre><code class=\"language-bash\">git diff                    # 工作区与暂存区的差异\ngit diff --staged          # 暂存区与最新提交的差异\ngit diff commit1 commit2   # 两个提交之间的差异\n</code></pre>\n<h4 id=\"重写历史谨慎使用\"><strong>重写历史</strong>（谨慎使用！）：</h4>\n<h5 id=\"修改最后一次提交\">修改最后一次提交：</h5>\n<pre><code class=\"language-bash\">git commit --amend\n</code></pre>\n<h5 id=\"交互式变基修改多个提交\">交互式变基（修改多个提交）：</h5>\n<pre><code class=\"language-bash\">git rebase -i HEAD~3  # 修改最近3个提交\n</code></pre>\n<h4 id=\"子模块submodules\"><strong>子模块</strong>（Submodules）：</h4>\n<p>用于在项目中包含其他Git仓库：</p>\n<pre><code class=\"language-bash\">git submodule add https://github.com/other/repo.git\ngit submodule update --init --recursive\n</code></pre>\n<h3 id=\"实际工作流示例\"><strong>实际工作流示例</strong></h3>\n<p>让我分享一个真实的工作流。假设我们要开发一个新功能：</p>\n<ol>\n<li><strong>从最新代码开始</strong></li>\n</ol>\n<pre><code class=\"language-bash\">git checkout main\ngit pull origin main\n</code></pre>\n<ol start=\"2\">\n<li><strong>创建功能分支</strong></li>\n</ol>\n<pre><code class=\"language-bash\">git checkout -b feature/user-profile   \n</code></pre>\n<ol start=\"3\">\n<li><strong>开发、测试、提交</strong></li>\n</ol>\n<pre><code class=\"language-bash\"># 多次小提交，而不是一次大提交\ngit add user/profile.py\ngit commit -m ”添加用户基本信息模型“\n    \ngit add user/views.py\ngit commit -m ”实现个人资料页面“\n    \ngit add tests/test_profile.py\ngit commit -m ”添加个人资料功能测试“   \n</code></pre>\n<ol start=\"4\">\n<li><strong>推送到远程</strong></li>\n</ol>\n<pre><code class=\"language-bash\">git push -u origin feature/user-profile    \n</code></pre>\n<ol start=\"5\">\n<li>\n<p><strong>创建Pull Request</strong></p>\n<ul>\n<li>在GitHub/GitLab上发起PR</li>\n<li>同事审查代码，提出建议</li>\n<li>根据反馈修改，再次推送</li>\n<li>通过CI/CD流水线的自动化测试</li>\n<li>合并到主分支</li>\n</ul>\n</li>\n<li>\n<p><strong>清理分支</strong></p>\n</li>\n</ol>\n<pre><code class=\"language-bash\">git checkout main\ngit pull origin main\ngit branch -d feature/user-profile \n</code></pre>\n<h2 id=\"改变了什么\"><strong>改变了什么？</strong></h2>\n<p>Git和它的衍生平台改变了三件事：</p>\n<p>第一，降低了协作门槛。以前贡献开源项目要发邮件、下源码包、打补丁。现在点个Fork，改完提交PR就行。这让开源从“精英游戏”变成了“全民运动”。</p>\n<p>第二，让工作流标准化。无论你在哪个公司，用的都是相似的Git工作流。新人入职，半天就能上手代码管理流程。</p>\n<p>第三，创造了新的职业。DevOps工程师、平台开发，这些岗位很大程度上是因为Git生态出现的。</p>\n<p>但最深层的改变，是思维方式的转变。</p>\n<p>我现在写代码时的心态完全不同了。知道有Git托底，就敢尝试激进的重构。知道分支是安全的，就敢同时开展多个实验性功能。知道每次提交都有记录，写commit message时会更认真。</p>\n<h2 id=\"尾声\"><strong>尾声</strong></h2>\n<p>有时候我会想，如果2005年Linus没被逼到必须自己写工具，今天的世界会怎样？</p>\n<p>可能我们还在用QQ传代码压缩包。可能开源不会这么繁荣。可能远程协作还是噩梦。</p>\n<p>但历史没有如果。一个脾气暴躁的程序员，花了十天时间解决自己的问题，顺便解决了全世界程序员的问题。</p>\n<p>这就是技术的魅力——最好的工具往往诞生于具体的困境，却解决了普遍的需求。</p>\n<p>现在每次我开始新项目，都会习惯性地<code>git init</code>。这个简单的动作，连接着从个人备份到全球协作的整个历史。</p>\n<p>而每次<code>git commit</code>时，我知道自己不仅是在保存代码。我是在参与一场持续了将近二十年的革命——一场让创造变得更有序、更协作、更持久的革命。</p>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/sean537/\" target=\"_blank\">山地奥斯卡537</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/sean537/p/19612709\" target=\"_blank\">https://www.cnblogs.com/sean537/p/19612709</a></p>\n</div>\n<div class=\"clear\"></div>\n</div>"
    },
    {
      "title": "Volcano v1.14 重磅发布！迈向 AI 统一调度新纪元",
      "link": "https://www.cnblogs.com/huaweiyun/p/19612682",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/huaweiyun/p/19612682\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 16:35\">\n    <span>Volcano v1.14 重磅发布！迈向 AI 统一调度新纪元</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>本文分享自华为云社区《<a href=\"https://bbs.huaweicloud.com/blogs/474065?utm_source=zhihu&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content\" rel=\"noopener nofollow\" target=\"_blank\">Volcano v1.14 重磅发布！迈向 AI 统一调度新纪元</a>》</p>\n<p>北京时间2026年1月，<strong>Volcano</strong>[1] v1.14 正式发布。随着 AI 业务形态从单一的离线训练向在线推理、Agent 智能体等多元化场景延伸，调度系统面临着前所未有的挑战。<strong>v1.14</strong>[2] 通过架构级的创新，在保持大规模批量计算优势的同时，补齐了对延迟敏感型业务的调度短板，向着 <strong>“AI训推、RL、Agent全场景统一调度平台”</strong> 的目标迈出了坚实一步。</p>\n<p><img alt=\"download\" class=\"lazyload\" /></p>\n<h1><span class=\"prefix\"><span class=\"content\"><strong>版本亮点</strong></span></span></h1>\n<p>v1.14.0 版本带来以下重磅更新：</p>\n<p><strong>统一调度平台架构</strong></p>\n<ul>\n<li>多调度器架构升级：动态节点分片机制 (Alpha)</li>\n<li>AI Agent 工作负载极速调度能力 (Alpha)</li>\n</ul>\n<p><strong>网络拓扑感知调度增强</strong></p>\n<ul>\n<li>HyperNode 级 Binpack 策略</li>\n<li>SubGroup 级精细化拓扑感知</li>\n<li>PodGroup 与 SubGroup 多层级 Gang Scheduling</li>\n<li>Volcano Job 分区支持</li>\n</ul>\n<p><strong>混部能力增强</strong></p>\n<ul>\n<li>全面支持通用操作系统（Ubuntu、CentOS 等）</li>\n<li>Cgroup V2 全面适配</li>\n<li>CPU 动态压制</li>\n<li>基于 Cgroup V2 的内存 QoS</li>\n<li>支持 CPU Burst</li>\n<li>支持 systemd driver 自动检测</li>\n</ul>\n<p><strong>异构硬件支持</strong></p>\n<ul>\n<li>昇腾 vNPU 调度（支持 MindCluster 和 HAMi 模式）</li>\n</ul>\n<p><strong>Volcano Global 增强</strong></p>\n<ul>\n<li>HyperJob 多集群作业自动拆分</li>\n<li>数据感知多集群调度</li>\n</ul>\n<p><strong>Volcano Dashboard 增强</strong></p>\n<ul>\n<li>PodGroup 全景可视化</li>\n<li>Job / Queue 全生命周期管理</li>\n</ul>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>多调度器架构升级：动态节点分片机制 (Alpha)</strong></span></span></h2>\n<p>随着 Volcano 承载的工作负载类型日益丰富、规模持续扩大，单一调度器架构逐渐显露瓶颈。批量训练、AI Agent、微服务等不同类型的工作负载，对调度时延、资源利用模式的诉求各不相同。单调度器难以兼顾，而静态资源划分又会造成利用率低下。<br />\n新引入的 Sharding Controller 构建了一套可扩展的多调度器架构，能够根据集群实时状态，为每个调度器动态计算候选节点池。与传统的静态分区不同，Sharding Controller 采用动态计算的方式划分资源，而非强制硬隔离。这种灵活机制让 Volcano 真正成为\"一个平台调度所有负载\"的统一调度平台，同时保持高吞吐、低时延。<br />\n</p>\n<p><strong>核心能力</strong>：</p>\n<ul>\n<li><strong>动态分片策略</strong>：支持多种策略来计算动态候选节点池。当前版本率先支持基于 CPU 利用率的分片策略，并采用了可扩展的架构设计，以便未来轻松集成更多分片算法。</li>\n<li><strong>节点池化管理</strong>：引入 NodeShard CRD，为特定调度器管理专属的动态候选节点池。</li>\n<li><strong>支持大规模集群</strong>：通过在多个调度器之间灵活分配负载，天然适配大规模集群场景。</li>\n<li><strong>多调度器协同</strong>：支持多种调度器组合的无缝协作。无论是部署多个 Batch Scheduler 进行负载分担，还是混合部署 Agent Scheduler 与 Batch Scheduler 以应对不同业务需求，都能灵活适配。</li>\n\n</ul>\n<p>配置示例：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\"># Sharding Controller 启动参数\n--scheduler-configs=\"volcano:volcano:0.0:0.6:false:2:100,agent-scheduler:agent:0.7:1.0:true:2:100\"\n--shard-sync-period=60s\n--enable-node-event-trigger=true\n\n# 参数格式: name:type:min_util:max_util:prefer_warmup:min_nodes:max_nodes</pre>\n</div>\n<p>相关 PR：https://github.com/volcano-sh/volcano/pull/4777<br />\n设计文档：<strong>Sharding Controller Design</strong>[3]<br />\n感谢社区开发者：@ssfffss, @Haoran, @qi-min</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>AI Agent 工作负载极速调度 (Alpha)</strong></span></span></h2>\n<p>AI Agent 类业务对时延极度敏感，任务创建频繁且生命周期短，对调度器的响应速度和吞吐量提出了严苛要求。原生 Volcano Batch Scheduler 专为批量计算设计，按固定周期处理 Pod，难以满足 Agent 场景的毫秒级响应需求。<br />\n为打造兼容批量计算与延迟敏感型业务的统一调度平台，v1.14 引入了专用的 <strong>Agent Scheduler</strong>。它通过 Sharding Controller 与批量调度器协同工作，各司其职又无缝配合，真正实现\"一个平台、多种负载\"。<br />\n</p>\n<p><strong>核心能力</strong>：</p>\n<ul>\n<li><strong>极速调度通道</strong>：专为延迟敏感型负载（如 AI Agent）打造的独立调度器，提供极致响应速度。</li>\n<li><strong>多 Worker 并行处理</strong>：采用多 Worker 并发消费调度队列的架构，大幅提升调度吞吐量。</li>\n<li><strong>乐观并发控制</strong>：引入 Conflict-Aware Binder 机制，在实际绑定前预先解决冲突，减少无效操作。</li>\n<li><strong>增强型调度队列</strong>：优化队列机制，支持紧急任务重试，确保关键任务不阻塞。</li>\n<li><strong>统一平台融合</strong>：通过 Sharding Controller 与批量调度器无缝协作，共享集群资源。</li>\n\n</ul>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4804,<br />\nhttps://github.com/volcano-sh/volcano/pull/4801,<br />\nhttps://github.com/volcano-sh/volcano/pull/4805<br />\n设计文档：<strong>Agent Scheduler Design</strong>[4]<br />\n感谢社区开发者：@qi-min, @JesseStutler, @handan-yxh</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>网络拓扑感知调度增强</strong></span></span></h2>\n<p>Volcano v1.14.0 对网络拓扑感知调度进行了进一步增强，满足分布式工作负载（包括 LLM 训练、推理、HPC 和其他网络密集型应用）日益增长的需求。<br />\n</p>\n<p><strong>核心增强</strong>：</p>\n<ul>\n<li><strong>SubGroup 级精细化拓扑感知</strong>：支持在 SubGroup / Partition 粒度设置网络拓扑约束，调度颗粒度更精细。</li>\n<li><strong>灵活的网络层级约束</strong>：新增 highestTierName，支持按名称指定允许跨越的最高网络层级。</li>\n<li><strong>多层级 Gang Scheduling</strong>：同时支持 PodGroup 级别和 SubGroup 级别的 Gang Scheduling，确保分布式任务的整体性。</li>\n<li><strong>Volcano Job 分区</strong>：支持将 Job 拆分为多个分区（Partition），便于管理 TP/PP/DP 等并行策略，并优化网络亲和性。</li>\n<li><strong>HyperNode 级 Binpacking</strong>：在 HyperNode（如交换机、机架）层级进行资源装箱，减少网络碎片，提升通信效率。</li>\n\n</ul>\n<p>配置示例 - Volcano Job：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">apiVersion: batch.volcano.sh/v1alpha1\nkind: Job\nmetadata:\nname: llm-training-job\nspec:\nnetworkTopology:\nmode: hard\nhighestTierAllowed: 2 # 整个 Job 最多跨越 Tier 2 HyperNode\ntasks:\n- name: trainer\nreplicas: 8\npartitionPolicy:\ntotalPartitions: 2 # 拆分为 2 个分区\npartitionSize: 4 # 每个分区 4 个 Pod\nminPartitions: 2 # 至少需要 2 个分区\nnetworkTopology:\nmode: hard\nhighestTierAllowed: 1 # 单个分区必须在 Tier 1 内\ntemplate:\nspec:\ncontainers:\n- name: trainer\nimage: training-image:v1\nresources:\nrequests:\nnvidia.com/gpu: 8</pre>\n</div>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4721,<br />\nhttps://github.com/volcano-sh/volcano/pull/4810,<br />\nhttps://github.com/volcano-sh/volcano/pull/4795,<br />\nhttps://github.com/volcano-sh/volcano/pull/4785,<br />\nhttps://github.com/volcano-sh/volcano/pull/4889</p>\n<p>设计文档：<strong>Network Topology Aware Scheduling</strong>[5]<br />\n感谢社区开发者：@ouyangshengjia, @3sunny, @zhaoqi, @wangyang0616, @MondayCha, @Tau721</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>混部能力全面升级：支持通用操作系统</strong></span></span></h2>\n<p>本次发布对 Volcano 的混部能力进行了全面改进，其中一个重要里程碑是：Volcano 混部能力<strong>正式支持通用操作系统</strong>（Ubuntu、CentOS 等），不再局限于 OpenEuler。这意味着更多用户可以使用 Volcano Agent 实现在离线混部，提升集群整体资源利用率。</p>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>CPU 动态压制 (CPU Suppression)</strong></span></span></h3>\n<p>在线业务流量通常具有潮汐特性。为了在保障在线业务 SLA 的同时最大化资源利用，离线 Pod 的 CPU 配额需要随在线用量动态调整：在线用量高时压制离线配额，用量回落时逐步恢复，实现自适应的资源分配。</p>\n<p>核心设计：</p>\n<ul>\n<li>根据节点可分配 CPU 和实时用量，动态调整 BestEffort root cgroup 的 CPU 配额。</li>\n<li>采用\"监控-事件-处理\"架构，并实施保守更新策略，有效抑制抖动。</li>\n\n</ul>\n<p>配置示例：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">cpuThrottlingConfig:\n\nenable: true\n\ncpuThrottlingThreshold: 80 # BE 配额上限为可分配 CPU 的 80%\n\ncpuJitterLimitPercent: 1 # 配额变化超过 1% 才触发更新\ncpuRecoverLimitPercent: 10 # 单次恢复上限 10%</pre>\n</div>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>内存 QoS (Cgroup V2)</strong></span></span></h3>\n<p>基于 Cgroup V2 实现混部场景的内存隔离。新增 ColocationConfiguration CRD，支持为指定工作负载配置内存 QoS 策略。</p>\n<p>核心能力：</p>\n<ul>\n<li><strong>New API</strong>：通过标签选择器定义内存隔离策略的 ColocationConfiguration CRD</li>\n<li><strong>动态计算</strong>：\n<ul>\n<li>memory.high = pod.limits.memory * highRatio %</li>\n<li>memory.low = pod.requests.memory * lowRatio %</li>\n<li>memory.min = pod.requests.memory * minRatio %</li>\n</ul>\n</li>\n<li><strong>统一接口</strong>：可靠检测和支持 Cgroup V2 环境</li>\n</ul>\n<p>使用示例：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">apiVersion: config.volcano.sh/v1alpha1\n\nkind: ColocationConfiguration\n\nmetadata:\n\nname: colo-config1\n\nspec:\n\nselector:\n\n*matchLabels:*\n\n  *app: offline-test*\nmemoryQos:\n\n*highRatio: 100  \\# memory.high \\= memory.limits \\* 100%*\n\n*lowRatio: 50    \\# memory.low \\= memory.requests \\* 50%*  \n*minRatio: 0     \\# memory.min \\= memory.requests \\* 0%*</pre>\n</div>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>CPU Burst 与 Cgroup V2 全面支持</strong></span></span></h3>\n<p>CPU Burst 能力已扩展至通用操作系统。同时，Volcano Agent 现已全面适配 Cgroup V2 环境，支持自动检测 Cgroup 版本及驱动类型（如 systemd driver），无需人工干预即可在现代 Linux 发行版上无缝运行。</p>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4632,</p>\n<p>https://github.com/volcano-sh/volcano/pull/4945,</p>\n<p>https://github.com/volcano-sh/volcano/pull/4913,</p>\n<p>https://github.com/volcano-sh/volcano/pull/4984</p>\n<p>设计文档：<strong>CPU Throttle Design</strong>[6], <strong>Agent Cgroup V2 Adaptation</strong>[7]</p>\n<p>感谢社区开发者：@Haibara-Ai97, @JesseStutler, @ouyangshengjia</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>昇腾 vNPU 调度</strong></span></span></h2>\n<p>v1.14 原生集成了昇腾 vNPU（虚拟 NPU）调度能力，实现昇腾 AI 处理器在多个工作负载之间的高效算力复用。提供两种模式，灵活适配不同部署场景。</p>\n<p><strong>支持模式</strong>：</p>\n<p><strong>1. MindCluster 模式</strong></p>\n<ul>\n<li>集成自 Ascend MindCluster 调度插件：https://gitcode.com/Ascend/mind-cluster\n<ul>\n<li>支持昇腾 310P 系列的动态虚拟化</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. HAMi 模式</strong></p>\n<ul>\n<li>由 HAMi 社区开发\n<ul>\n<li>同时支持昇腾 310 和 910 系列</li>\n<li>支持异构昇腾集群（910A、910B2、910B3、310P）</li>\n</ul>\n</li>\n</ul>\n<p>调度器配置：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\"># MindCluster 模式\n\n- name: deviceshare\n\narguments:\n\n*deviceshare.AscendMindClusterVNPUEnable: true*\n# HAMi 模式\n\n- name: deviceshare\n\narguments:\n\n*deviceshare.AscendHAMiVNPUEnable: true*  \n*deviceshare.SchedulePolicy: binpack  \\# 或 spread*</pre>\n</div>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4656,<br />\nhttps://github.com/volcano-sh/volcano/pull/4717<br />\n使用文档：<strong>How to Use vNPU</strong>[8]<br />\n感谢社区开发者：@JackyTYang, @DSFans2014</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>Volcano Global 增强</strong></span></span></h2>\n<p>Volcano Global v0.3.0 引入了两个重要功能，通过基于计算资源和数据局部性的智能调度，显著扩展了 Volcano Global 对 AI/ML 和大数据工作负载的能力。</p>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>HyperJob：多集群作业自动拆分</strong></span></span></h3>\n<p>随着 AI 训练工作负载规模和复杂性的增长，企业越来越面临跨多个异构集群管理大规模训练作业的挑战。HyperJob 是构建在 Volcano Job 之上的更高级抽象。它组合多个 Volcano Job 模板，将训练能力扩展到单集群边界之外，同时保留每个集群内现有 Volcano Job 的全部能力。</p>\n<p><strong>核心能力</strong>：</p>\n<ul>\n<li><strong>Karmada 深度集成</strong>：自动生成 PropagationPolicy，精准配置集群亲和性与副本调度。</li>\n<li><strong>状态统一聚合</strong>：将各集群子任务状态汇总为统一的 HyperJob 状态，全局可观测。</li>\n<li><strong>自动资源生成</strong>：根据 ReplicatedJob 定义自动创建 VCJob 和 PropagationPolicy。</li>\n\n</ul>\n<p>HyperJob 资源示例（跨 2 个集群拆分大规模训练作业，共 256 个 GPU）：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">apiVersion: training.volcano.sh/v1alpha1\n\nkind: HyperJob\n\nmetadata:\n\nname: llm-training\n\nspec:\n\nreplicatedJobs:\n\n- name: trainer\n\nreplicas: 2\n\ntemplateSpec:\n\n  tasks:\n\n  \\- name: worker\n\n    replicas: 128\n\n    template:\n\n      spec:\n\n        containers:\n\n        \\- name: trainer\n\n          image: training-image:v1\n\n          resources:\n\n            requests:  \n              nvidia.com/gpu: 1</pre>\n</div>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>数据感知调度</strong></span></span></h3>\n<p>在 AI 训练和大数据分析等高性能计算场景中，任务执行不仅依赖计算资源，还严重依赖数据资源。在多集群环境中，调度器可能会将任务分派到与数据源物理距离较远的集群，导致跨地域带宽成本过高和 I/O 延迟过高。</p>\n<p>数据感知调度框架引入 <strong>DataDependencyController</strong>，打通了逻辑数据需求与物理集群分布的壁垒。通过外部插件（如 Amoro）实时获取数据分布信息，自动将调度约束注入 Karmada，实现\"计算随数据而动\"的全自动工作流。</p>\n<p><strong>核心能力</strong>：</p>\n<ul>\n<li><strong>插件化架构</strong>：可扩展支持 Amoro、Hive、S3 等多种数据系统。</li>\n<li><strong>声明式 API</strong>：DataSourceClaim / DataSource CRD，采用\"声明-缓存\"模式。</li>\n<li><strong>自动亲和注入</strong>：将数据局部性转化为 ClusterAffinity 约束，注入 ResourceBinding。</li>\n</ul>\n<p>详见： <strong>Volcano Global v0.3.0 Release Notes</strong>[9]<br />\n感谢社区开发者：@JesseStutler, @fx147, @Monokaix, @zhoujinyu, @anryko, @tanberBro</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>Volcano Dashboard v0.2.0</strong></span></span></h2>\n<p>Volcano Dashboard v0.2.0 对资源管理能力进行了重大增强，使得通过 Web 界面管理 Volcano 资源更加便捷。<br />\n<strong>核心增强</strong>：</p>\n<ul>\n<li><strong>PodGroup 全景可视化</strong>：跨命名空间查看、搜索、过滤 PodGroup，支持 YAML 语法高亮。</li>\n<li><strong>Job 生命周期管理</strong>：直接在界面创建、删除 Volcano Job，操作更便捷。</li>\n<li><strong>Queue 管理增强</strong>：在线编辑 Queue 配额、权重，支持 YAML 直接修改。</li>\n<li><strong>安全加固</strong>：默认配置 SELinux、Seccomp、非 root 运行及禁止提权，保障生产安全。</li>\n\n</ul>\n<p>详见： <strong>Volcano Dashboard v0.2.0 Release Notes</strong>[10]<br />\n感谢社区开发者：@vzhou-p, @Shrutim1505, @JesseStutler, @karanBRAVO, @Sayan4444, @jayesh9747, @Alivestars24, @kuldeep, @Monokaix</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>调度器稳定性与性能提升</strong></span></span></h2>\n<p><strong>Reclaim 重构与增强</strong><br />\n对 Reclaim Action 进行了全面重构，并修复了 Capacity Plugin 中的关键逻辑问题，大幅提升多租户集群资源回收的准确性、稳定性和性能。<br />\n主要改进：</p>\n<ul>\n<li><strong>Reclaim Action 重构</strong>：重构了 reclaim 工作流，提高代码可读性、可维护性和测试覆盖率。</li>\n<li><strong>增强的 Capacity Plugin 逻辑</strong>：修复了 reclaimableFn 和 preemptiveFn，正确处理标量资源并防止错误的抢占决策。</li>\n<li><strong>稳定性提升</strong>：解决了资源计算中的边缘情况，防止调度死循环和误驱逐。</li>\n\n</ul>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4794,<br />\nhttps://github.com/volcano-sh/volcano/pull/4659,<br />\nhttps://github.com/volcano-sh/volcano/pull/4919<br />\n感谢社区开发者：@guoqinwill, @hajnalmt</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>支持 Kubernetes 1.34</strong></span></span></h2>\n<p>Volcano 版本紧跟 Kubernetes 社区。v1.14 已全面支持最新的 Kubernetes v1.34，并通过完整的单元测试和 E2E 测试保障功能与稳定性。<br />\n相关 PR：https://github.com/volcano-sh/volcano/pull/4704<br />\n感谢社区开发者：@suyiiyii, @tunedev</p>\n<h1><span class=\"prefix\"><span class=\"content\"><strong>总结：Volcano v1.14.0 — AI 时代的统一调度平台</strong></span></span></h1>\n<p>Volcano v1.14 是一个里程碑式的版本。通过引入多调度器架构和 Agent Scheduler，Volcano 正式迈入统一调度平台新阶段，既能高效处理批量 AI 训练，又能满足 AI Agent 的极致时延要求。网络拓扑感知增强、通用操作系统混部支持、昇腾 vNPU 集成，进一步夯实了 Volcano 在 AI 基础设施领域的领先地位。<br />\n同时，Volcano Global v0.3.0 通过 HyperJob 实现大规模分布式训练和数据感知调度，扩展了多集群能力。Volcano Dashboard v0.2.0 通过全面的资源管理功能显著改善了用户体验。</p>\n<p><strong>立即体验 Volcano v1.14，共启 AI 时代统一调度新篇章！</strong></p>\n<p><strong>v1.14.0 发布地址：</strong>https://github.com/volcano-sh/volcano/releases/tag/v1.14.0<br />\n<strong>Volcano Global v0.3.0 发布地址：</strong>https://github.com/volcano-sh/volcano-global/releases/tag/v0.3.0<br />\n<strong>Volcano Dashboard v0.2.0 发布地址：</strong>https://github.com/volcano-sh/dashboard/releases/tag/v0.2.0</p>\n<h1><span class=\"prefix\"><span class=\"content\"><strong>致   谢</strong></span></span></h1>\n<p>Volcano v1.14 生态版本（含 Volcano Global v0.3.0、Dashboard v0.2.0）共有 55 位社区贡献者参与。衷心感谢每一位贡献者：</p>\n<table>\n<tbody>\n<tr><th>@3sunny</th><th>@3th4novo</th><th>@acsoto</th>\n</tr>\n<tr>\n<td>@Alivestars24</td>\n<td>@Aman-Cool</td>\n<td>@anryko</td>\n\n\n</tr>\n<tr>\n<td>@archlitchi</td>\n<td>@dafu-wu</td>\n<td>@DSFans2014</td>\n\n\n</tr>\n<tr>\n<td>@FAUST-BENCHOU</td>\n<td>@fengruotj</td>\n<td>@Freshwlnd</td>\n\n\n</tr>\n<tr>\n<td>@fx147</td>\n<td>@goyalpalak18</td>\n<td>@guoqinwill</td>\n\n\n</tr>\n<tr>\n<td>@Haibara-Ai97</td>\n<td>@hajnalmt</td>\n<td>@halcyon-r</td>\n\n\n</tr>\n<tr>\n<td>@handan-yxh</td>\n<td>@JackyTYang</td>\n<td>@jayesh9747</td>\n\n\n</tr>\n<tr>\n<td>@JesseStutler</td>\n<td>@jiahuat</td>\n<td>@karanBRAVO</td>\n\n\n</tr>\n<tr>\n<td>@kingeasternsun</td>\n<td>@kiritoxkiriko</td>\n<td>@kube-gopher</td>\n\n\n</tr>\n<tr>\n<td>@kuldeep</td>\n<td>@LiZhenCheng9527</td>\n<td>@medyagh</td>\n\n\n</tr>\n<tr>\n<td>@MondayCha</td>\n<td>@Monokaix</td>\n<td>@mvinchoo</td>\n\n\n</tr>\n<tr>\n<td>@neeraj542</td>\n<td>@nitindhiman314e</td>\n<td>@ouyangshengjia</td>\n\n\n</tr>\n<tr>\n<td>@PersistentJZH</td>\n<td>@qi-min</td>\n<td>@rhh777</td>\n\n\n</tr>\n<tr>\n<td>@ruanwenjun</td>\n<td>@RushabhMehta2005</td>\n<td>@sailorvii</td>\n\n\n</tr>\n<tr>\n<td>@Sayan4444</td>\n<td>@Shrutim1505</td>\n<td>@ssfffss</td>\n\n\n</tr>\n<tr>\n<td>@suyiiyii</td>\n<td>@tanberBro</td>\n<td>@Tau721</td>\n\n\n</tr>\n<tr>\n<td>@vzhou-p</td>\n<td>@wangyang0616</td>\n<td>@weapons97</td>\n\n\n</tr>\n<tr>\n<td>@Wonki4</td>\n<td>@zhaoqi612</td>\n<td>@zhengchenyu</td>\n\n\n</tr>\n<tr>\n<td>@zhoujinyu</td>\n<td>@zjj2wry</td>\n<td>&nbsp;</td>\n\n\n</tr>\n\n\n</tbody>\n\n\n</table>\n<p><strong>相关链接</strong></p>\n<p>[1] Volcano: <em>https://volcano.sh/en/</em><br />\n[2] Volcano v1.14.0: <em>https://github.com/volcano-sh/volcano/releases/tag/v1.14.0</em><br />\n[3] Sharding Controller Design: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/sharding_controller.md</em><br />\n[4] Agent Scheduler Design: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/agent-scheduler.md</em><br />\n[5] Network Topology Aware Scheduling: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/Network%20Topology%20Aware%20Scheduling.md</em><br />\n[6] CPU Throttle Design: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/cpu-throttle-design.md</em><br />\n[7] Agent Cgroup V2 Adaptation: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/agent-cgroup-v2-adaptation.md</em><br />\n[8] How to Use vNPU: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/user-guide/how_to_use_vnpu.md</em><br />\n[9] Volcano Global v0.3.0 Release Notes: <em>https://github.com/volcano-sh/volcano-global/releases/tag/v0.3.0</em><br />\n[10] Volcano Dashboard v0.2.0 Release Notes: <em>https://github.com/volcano-sh/dashboard/releases/tag/v0.2.0</em></p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 16:35</span>&nbsp;\n<a href=\"https://www.cnblogs.com/huaweiyun\">华为云开发者联盟</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "LLVM的混淆之旅(六)-字符串加密",
      "link": "https://www.cnblogs.com/ClownLMe/p/19612575",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ClownLMe/p/19612575\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 16:11\">\n    <span>LLVM的混淆之旅(六)-字符串加密</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"简介\">简介</h1>\n<p>在上一个教学中，学习了如何写一个LLVM Pass示例来进行控制流平坦化，这篇文章，来学习如何二利用LLVM实现字符串的加密。</p>\n<h1 id=\"案例\">案例</h1>\n<h3 id=\"需要加密的对象\">需要加密的对象</h3>\n<p>这是一个经典的c语言样例，目的是输出字符串：hello world</p>\n<pre><code class=\"language-c\">#include &lt;stdio.h&gt;\n\nint main(){\n&nbsp; &nbsp; printf(\"hello world\\n\");\n&nbsp; &nbsp; return 0;\n}\n</code></pre>\n<h3 id=\"加密pass编写\">加密pass编写</h3>\n<p>下面是完整的pass代码：<br />\n<strong>代码的目的是：让字符串在编译的过程中进行异或加密，并且在运行的过程中动态解密。</strong></p>\n<pre><code class=\"language-cpp\">#include \"llvm/Passes/PassBuilder.h\"\n#include \"llvm/Passes/PassPlugin.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/Instructions.h\"\n#include \"llvm/IR/IRBuilder.h\"\n#include \"llvm/IR/PassManager.h\"\n#include \"llvm/IR/GlobalVariable.h\"\n#include \"llvm/IR/Constants.h\"\n\nusing namespace llvm;\n\nnamespace{\n    struct mypass : public PassInfoMixin&lt;mypass&gt;{\n        const uint8_t KEY = 0x42;\n\n        Function *createDecryptFunc(Module *M){\n            LLVMContext &amp;Ctx = M-&gt;getContext();\n            std::vector&lt;Type*&gt; paramTypes = { PointerType::getUnqual(Ctx), Type::getInt32Ty(Ctx) };\n            FunctionType *funcType = FunctionType::get(Type::getVoidTy(Ctx),paramTypes,false);\n\n            Function *Func = Function::Create(funcType, GlobalValue::InternalLinkage, \"my_decrypt_func\", M);\n\n            Argument *ArgData = Func-&gt;getArg(0);\n            Argument *ArgLen  = Func-&gt;getArg(1);\n\n            BasicBlock *EntryBB = BasicBlock::Create(Ctx, \"entry\", Func);\n            BasicBlock *LoopCheckBB = BasicBlock::Create(Ctx, \"loop_check\", Func);\n            BasicBlock *LoopBodyBB  = BasicBlock::Create(Ctx, \"loop_body\", Func);\n            BasicBlock *ExitBB      = BasicBlock::Create(Ctx, \"exit\", Func);\n\n            IRBuilder&lt;&gt; builder(EntryBB);\n            builder.CreateBr(LoopCheckBB);\n            builder.SetInsertPoint(LoopCheckBB);\n\n            PHINode *LoopVar = builder.CreatePHI(Type::getInt32Ty(Ctx), 2, \"i\");\n            LoopVar-&gt;addIncoming(builder.getInt32(0), EntryBB); \n\n            Value *Cond = builder.CreateICmpSLT(LoopVar, ArgLen, \"cond\");\n            builder.CreateCondBr(Cond, LoopBodyBB, ExitBB);\n\n            builder.SetInsertPoint(LoopBodyBB);\n\n            Value *Ptr = builder.CreateInBoundsGEP(builder.getInt8Ty(), ArgData, LoopVar);\n            Value *ByteVal = builder.CreateLoad(builder.getInt8Ty(), Ptr);\n            Value *XorVal = builder.CreateXor(ByteVal, builder.getInt8(KEY));\n            builder.CreateStore(XorVal, Ptr);\n\n            Value *NextVar = builder.CreateAdd(LoopVar, builder.getInt32(1));\n            LoopVar-&gt;addIncoming(NextVar, LoopBodyBB);\n\n            builder.CreateBr(LoopCheckBB);\n            builder.SetInsertPoint(ExitBB);\n            builder.CreateRetVoid();\n\n            return Func;\n        }\n        PreservedAnalyses run(Module &amp;M, ModuleAnalysisManager &amp;AM){\n            errs() &lt;&lt; \"Running On-Demand String Encryption Pass...\\n\";\n            std::vector&lt;GlobalVariable*&gt; EncryptedGlobals;\n            for(GlobalVariable &amp;GV : M.globals()){\n                if (!GV.hasInitializer() || !GV.isConstant() || GV.getName().contains(\"llvm.\")) continue;\n\n                Constant *Init = GV.getInitializer();\n                ConstantDataArray* DataArray = dyn_cast&lt;ConstantDataArray&gt;(Init);\n\n                if(DataArray &amp;&amp; DataArray-&gt;isString()){\n                    StringRef OriginalString = DataArray-&gt;getAsString();\n\n                    if(OriginalString.size() &lt; 2) continue;\n\n                    errs() &lt;&lt; \"Encrypting string: \" &lt;&lt; OriginalString &lt;&lt; \"\\n\";\n                    std::string EncryptedStr = OriginalString.str();\n                    for(int i = 0; i &lt; EncryptedStr.size(); i++){\n                        EncryptedStr[i] ^= KEY;\n                    }\n\n                    Constant *NewInit = ConstantDataArray::getString(M.getContext(), EncryptedStr, false);\n                    GV.setInitializer(NewInit);\n                    GV.setConstant(false);\n                    \n                    EncryptedGlobals.push_back(&amp;GV);\n                }\n            }\n\n            if(EncryptedGlobals.empty()) return PreservedAnalyses::all();\n\n            Function *DecryptFunc = createDecryptFunc(&amp;M);\n\n            for(GlobalVariable *GV : EncryptedGlobals){\n                std::vector&lt;Instruction*&gt; user;\n                for(User *U: GV-&gt;users()){\n                    Instruction* ins = dyn_cast&lt;Instruction&gt;(U);\n                    if(ins-&gt;getFunction() == DecryptFunc) continue;\n                    user.push_back(ins);\n                }\n                \n                for(Instruction *ins : user){\n                    IRBuilder&lt;&gt; builder(ins);\n                    ConstantDataArray* DataArray = dyn_cast&lt;ConstantDataArray&gt;(GV-&gt;getInitializer());\n                    int len = DataArray-&gt;getNumElements();\n                    builder.CreateCall(DecryptFunc, {GV, builder.getInt32(len)});\n                }\n            }\n            return PreservedAnalyses::none();\n        }\n    };\n}\n\nextern \"C\" LLVM_ATTRIBUTE_WEAK ::llvm::PassPluginLibraryInfo\nllvmGetPassPluginInfo(){\n    return{\n        LLVM_PLUGIN_API_VERSION,\n        \"mypass\",\n        \"v0.1\",\n        [](PassBuilder &amp;PB){\n            PB.registerPipelineParsingCallback(\n                [](StringRef Name, ModulePassManager &amp;MPM, ArrayRef&lt;PassBuilder::PipelineElement&gt;){\n                    if(Name == \"mypass\"){\n                        MPM.addPass(mypass());\n                        return true;\n                    }\n                    return false;\n                }\n            );\n        }\n    };\n}\n</code></pre>\n<h3 id=\"代码解释\">代码解释</h3>\n<p>看到上面的pass代码，我们会感到头晕目眩，没关系，下面我们一步步拆解。<br />\n上面的代码分主要分为两部分：</p>\n<ol>\n<li>构建异或的解密函数（<code>createDecryptFunc</code>）</li>\n<li>加密字符串，并且将上面构建的解密函数插入代码。</li>\n</ol>\n<h3 id=\"cmakeliststxt\">CMakeLists.txt</h3>\n<p>编译可以直接用下面的cmake配置文件，编译过程在之前文章详细讲解</p>\n<pre><code class=\"language-python\">cmake_minimum_required(VERSION 4.1.1)\nproject(mypass) \n\nset(LLVM_DIR \"D:/LLVM/llvm-project/build/lib/cmake/llvm\")\nfind_package(LLVM REQUIRED CONFIG)\nlist(APPEND CMAKE_MODULE_PATH \"${LLVM_CMAKE_DIR}\")\ninclude(AddLLVM)\ninclude_directories(${LLVM_INCLUDE_DIRS})\nadd_definitions(${LLVM_DEFINITIONS})\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nadd_library(mypass MODULE mypass.cpp)\nset_target_properties(mypass PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS ON) \ntarget_link_libraries(mypass LLVMCore LLVMSupport)     \ntarget_compile_options(mypass PRIVATE /utf-8)#用中文\n</code></pre>\n<h1 id=\"使用效果展示\">使用效果展示</h1>\n<h3 id=\"编译并执行\">编译并执行</h3>\n<p>图片中可以看出，可以正常运行<br />\n<img alt=\"命令运行\" class=\"lazyload\" /></p>\n<h3 id=\"反编译查看区别\">反编译查看区别</h3>\n<h4 id=\"原始testexe\">原始test.exe</h4>\n<p><img alt=\"字符串加密\" class=\"lazyload\" /></p>\n<h4 id=\"加密test_optexe\">加密test_opt.exe</h4>\n<p><img alt=\"原始\" class=\"lazyload\" /></p>\n<p>根据上面的图片，可以看到，上面使用的pass，成功将HelloWorld加密了。</p>\n<p><strong>如果❤喜欢❤本系列教程，就点个关注吧，后续不定期更新~</strong></p>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/ClownLMe/\" target=\"_blank\">ClownLMe</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/ClownLMe/p/19612575\" target=\"_blank\">https://www.cnblogs.com/ClownLMe/p/19612575</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 16:11</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ClownLMe\">ClownLMe</a>&nbsp;\n阅读(<span id=\"post_view_count\">3</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Admin.NET开源版微服务改造记录",
      "link": "https://www.cnblogs.com/shiningrise/p/19612443",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/shiningrise/p/19612443\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 15:44\">\n    <span>Admin.NET开源版微服务改造记录</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"adminnet开源版微服务改造记录\">Admin.NET开源版微服务改造记录</h1>\n<h2 id=\"将adminnetcore项目拆分成两个项目adminnetcommonadminnetcore\">将Admin.NET.Core项目拆分成两个项目：Admin.NET.Common，Admin.NET.Core</h2>\n<p>Admin.NET.Common放基础工具类</p>\n<p>Admin.NET.Core放框架核心类库</p>\n<h2 id=\"aspireappapphost中的apphostcs配置\">AspireApp.AppHost中的AppHost.cs配置：</h2>\n<pre><code>using Aspire.Hosting;\nusing Aspire.Hosting.Dapr;\nusing AspireApp.AppHost;\n\nvar builder = DistributedApplication.CreateBuilder(args);\n\nvar postgresQL = builder.AddPostgres(\"postgresQL\")\n                        .WithImage(\"ankane/pgvector\")\n                        .WithImageTag(\"latest\")\n                        .WithLifetime(ContainerLifetime.Persistent)\n                        .WithHealthCheck()\n                        .WithPgWeb();\nvar postgres = postgresQL.AddDatabase(\"postgres\");\nvar postgres2 = postgresQL.AddDatabase(\"postgres2\");\n\n//var redis = builder.AddRedis(\"redis\").WithLifetime(ContainerLifetime.Persistent)\n//                    .WithHealthCheck()\n//                    .WithRedisCommander();\n\n// 使用 RabbitMQ 作为 Pub/Sub 组件\n//var rabbitmq = builder.AddRabbitMQ(\"rabbitmq\")\n//                        .WithLifetime(ContainerLifetime.Persistent)\n//                        .WithHealthCheck()\n//                        .WithManagementPlugin();\n\n// 1. 定义共享目录的绝对路径（建议指向 admin-net-core 的实际目录或独立的 shared 目录）\nvar uploadPath = Path.GetFullPath(\"../Admin.NET/Admin.NET.Core/wwwroot/upload\");\n\n// 确保目录存在\nif (!Directory.Exists(uploadPath))\n{\n    Directory.CreateDirectory(uploadPath);\n}\n\n// Admin.NET.Core 使用 Furion 的 Knife4j UI，不使用 Aspire 的 Swagger UI\nvar core = builder.AddProject&lt;Projects.Admin_NET_Core&gt;(\"admin-net-core\")\n    .WithReference(postgres)\n    .WaitFor(postgres)\n    .WithSwaggerUI();\n\nvar baseApi = builder.AddProject&lt;Projects.Base&gt;(\"base\")\n    .WithReference(postgres2)\n    .WithSwaggerUI()\n    .WithReference(core)\n    .WaitFor(postgres2)\n    .WaitFor(core);\n\nvar yarp = builder.AddProject&lt;Projects.AspireApp_Yarp&gt;(\"aspireapp-yarp\")\n    .WithEndpoint( port: 5008, scheme: \"http\", name: \"yarp-http\")    // 指定唯一名称\n    .WithEndpoint( port: 7008, scheme: \"https\", name: \"yarp-https\") // 指定唯一名称\n    .WithReference(core)\n    .WithReference(baseApi)\n    .WaitFor(core)\n    .WaitFor(baseApi);\n\n//var frontend = builder.AddNodeApp(\n//       \"frontend\",\n//       scriptPath: \"scripts/run-pnpm.cjs\",\n//       workingDirectory: \"../Web\",\n//       args: new[] { \"dev\" }\n//   )\n//   .WithReference(yarp)\n//   .WaitFor(yarp);\n\nbuilder.Build().Run();\n\n\n</code></pre>\n<h2 id=\"aip网关aspireappyarp\">AIP网关：AspireApp.Yarp</h2>\n<p>Program.cs</p>\n<pre><code>var builder = WebApplication.CreateBuilder(args);\n\nbuilder.AddServiceDefaults();\nbuilder.Services.AddReverseProxy()\n                .LoadFromConfig(builder.Configuration.GetSection(\"ReverseProxy\"))\n                .AddServiceDiscoveryDestinationResolver();\n\nvar app = builder.Build();\n\napp.MapDefaultEndpoints();\napp.MapReverseProxy();\n\napp.MapGet(\"/\", () =&gt; \"Hello World!\");\n\napp.Run();\n\n</code></pre>\n<p>appsettings.json</p>\n<pre><code>{\n  \"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Information\",\n      \"Microsoft.AspNetCore\": \"Warning\"\n    }\n  },\n  \"AllowedHosts\": \"*\",\n  \"ReverseProxy\": {\n    \"Routes\": {\n      \"core\": {\n        \"ClusterId\": \"core\",\n        \"Match\": {\n          \"Path\": \"/core/{**remainder}\"\n        },\n        \"Transforms\": [\n          { \"PathRemovePrefix\": \"/core\" },\n          { \"PathPrefix\": \"/\" },\n          { \"RequestHeaderOriginalHost\": \"true\" }\n        ]\n      },\n      \"base\": {\n        \"ClusterId\": \"base\",\n        \"Match\": {\n          \"Path\": \"/base/{**remainder}\"\n        },\n        \"Transforms\": [\n          { \"PathRemovePrefix\": \"/base\" },\n          { \"PathPrefix\": \"/\" },\n          { \"RequestHeaderOriginalHost\": \"true\" }\n        ]\n      }\n    },\n    \"Clusters\": {\n      \"core\": {\n        \"Destinations\": {\n          \"base_destination\": {\n            \"Address\": \"http+https://admin-net-core\"\n          }\n        }\n      },\n      \"base\": {\n        \"Destinations\": {\n          \"base_destination\": {\n            \"Address\": \"http+https://base\"\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<h2 id=\"业务项目base项目调用核心项目core项目的方法\">业务项目(base项目)调用核心项目(core项目）的方法</h2>\n<p>我们使用Refit来服务调用</p>\n<p>在base项目中的Startup.cs中</p>\n<pre><code>        // 注册 JwtTokenHandler - 用于自动添加 JWT Token 到 Refit 请求\n        services.AddTransient&lt;JwtTokenHandler&gt;();\n        \n        // 配置 Refit 客户端并添加 JWT Token 处理器\n        //系统配置\n        services.AddRefitClient&lt;IConfigService&gt;(refitSettings)\n               .ConfigureHttpClient(c =&gt; c.BaseAddress = new(\"https+http://admin-net-core\"))\n               .AddHttpMessageHandler&lt;JwtTokenHandler&gt;();\n        //系统菜单     \n        services.AddRefitClient&lt;IMenuService&gt;(refitSettings)\n               .ConfigureHttpClient(c =&gt; c.BaseAddress = new(\"https+http://admin-net-core\"))\n               .AddHttpMessageHandler&lt;JwtTokenHandler&gt;();\n        //组织机构\n        services.AddRefitClient&lt;IOrgService&gt;(refitSettings)\n               .ConfigureHttpClient(c =&gt; c.BaseAddress = new(\"https+http://admin-net-core\"))\n               .AddHttpMessageHandler&lt;JwtTokenHandler&gt;();\n        //文件\n        services.AddRefitClient&lt;IFileService&gt;(refitSettings)\n               .ConfigureHttpClient(c =&gt; c.BaseAddress = new(\"https+http://admin-net-core\"))\n               .AddHttpMessageHandler&lt;JwtTokenHandler&gt;();\n\n        services.AddRefitClient&lt;ITest&gt;(refitSettings)\n            .ConfigureHttpClient(c =&gt; c.BaseAddress = new(\"https+http://apiservice\"))\n            .AddHttpMessageHandler&lt;JwtTokenHandler&gt;();\n</code></pre>\n<p>其中最重要的权限接口调用</p>\n<pre><code>using Refit;\nusing GetAttribute = Refit.GetAttribute;\nnamespace Base.Rest;\n\npublic interface IMenuService\n{\n    [Get(\"/api/sysMenu/getOwnBtnPermList\")]\n    Task&lt;List&lt;string&gt;&gt; GetOwnBtnPermList();\n\n    [Get(\"/api/sysMenu/getAllBtnPermList\")]\n    Task&lt;List&lt;string&gt;&gt; GetAllBtnPermList();\n}\n</code></pre>\n<p>修改base项目的权限验证方法JwtHandler.cs</p>\n<p>IConfigService  IMenuService 就是我们定义的Refit接口，这样，每次权限验证就会调用core项目的API接口</p>\n<pre><code>using Admin.NET.Core;\nusing Admin.NET.Core.Service;\nusing Base.Rest;\nusing Furion;\nusing Furion.Authorization;\nusing Furion.DataEncryption;\nusing Microsoft.AspNetCore.Authorization;\nusing Microsoft.AspNetCore.Http;\nusing System;\nusing System.Threading.Tasks;\n\nnamespace Admin.NET.Application;\n\npublic class JwtHandler : AppAuthorizeHandler\n{\n    private readonly SysCacheService _sysCacheService = App.GetRequiredService&lt;SysCacheService&gt;();\n    private readonly IConfigService _sysConfigService = App.GetRequiredService&lt;IConfigService&gt;();\n    private static readonly IMenuService SysMenuService = App.GetRequiredService&lt;IMenuService&gt;();\n\n    /// &lt;summary&gt;\n    /// 自动刷新Token\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"context\"&gt;&lt;/param&gt;\n    /// &lt;param name=\"httpContext\"&gt;&lt;/param&gt;\n    /// &lt;returns&gt;&lt;/returns&gt;\n    public override async Task HandleAsync(AuthorizationHandlerContext context, DefaultHttpContext httpContext)\n    {\n        // 若当前账号存在黑名单中则授权失败\n        if (_sysCacheService.ExistKey($\"{CacheConst.KeyBlacklist}{context.User.FindFirst(ClaimConst.UserId)?.Value}\"))\n        {\n            context.Fail();\n            context.GetCurrentHttpContext().SignoutToSwagger();\n            return;\n        }\n\n        var tokenExpire = await _sysConfigService.GetTokenExpire();\n        var refreshTokenExpire = await _sysConfigService.GetRefreshTokenExpire();\n        if (JWTEncryption.AutoRefreshToken(context, context.GetCurrentHttpContext(), tokenExpire.Result, refreshTokenExpire.Result))\n        {\n            await AuthorizeHandleAsync(context);\n        }\n        else\n        {\n            context.Fail(); // 授权失败\n            var currentHttpContext = context.GetCurrentHttpContext();\n            if (currentHttpContext == null) return;\n\n            // 跳过由于 SignatureAuthentication 引发的失败\n            if (currentHttpContext.Items.ContainsKey(SignatureAuthenticationDefaults.AuthenticateFailMsgKey)) return;\n            currentHttpContext.SignoutToSwagger();\n        }\n    }\n\n    public override async Task&lt;bool&gt; PipelineAsync(AuthorizationHandlerContext context, DefaultHttpContext httpContext)\n    {\n        // 已自动验证 Jwt Token 有效性\n        return await CheckAuthorizeAsync(httpContext);\n    }\n\n    /// &lt;summary&gt;\n    /// 权限校验核心逻辑\n    /// &lt;/summary&gt;\n    /// &lt;param name=\"httpContext\"&gt;&lt;/param&gt;\n    /// &lt;returns&gt;&lt;/returns&gt;\n    private static async Task&lt;bool&gt; CheckAuthorizeAsync(DefaultHttpContext httpContext)\n    {\n        // 登录模式判断PC、APP\n        if (App.User.FindFirst(ClaimConst.LoginMode)?.Value == ((int)LoginModeEnum.APP).ToString())\n            return true;\n\n        // 排除超管\n        if (App.User.FindFirst(ClaimConst.AccountType)?.Value == ((int)AccountTypeEnum.SuperAdmin).ToString())\n            return true;\n\n        // 路由名称\n        var routeName = httpContext.Request.Path.StartsWithSegments(\"/api\")\n            ? httpContext.Request.Path.Value![5..].Replace(\"/\", \":\")\n            : httpContext.Request.Path.Value![1..].Replace(\"/\", \":\");\n\n        // 获取用户拥有按钮权限集合\n        var ownBtnPermList = await SysMenuService.GetOwnBtnPermList();\n        if (ownBtnPermList.Exists(u =&gt; routeName.Equals(u, StringComparison.CurrentCultureIgnoreCase)))\n            return true;\n\n        // 获取系统所有按钮权限集合\n        var allBtnPermList = await SysMenuService.GetAllBtnPermList();\n        return allBtnPermList.TrueForAll(u =&gt; !routeName.Equals(u, StringComparison.CurrentCultureIgnoreCase));\n    }\n}\n</code></pre>\n<h2 id=\"项目地址\">项目地址</h2>\n<p><a href=\"https://gitee.com/shiningrise/aspire-app\" rel=\"noopener nofollow\" target=\"_blank\">AspireApp: Admin.NET微服务版</a></p>\n<p><a href=\"https://gitee.com/zuohuaijun/Admin.NET\" rel=\"noopener nofollow\" target=\"_blank\">Admin.NET: 🔥🔥🔥 Admin.NET 基于 .NET8/10 (Furion/SqlSugar) 实现的通用权限开发框架，前端采用 Vue3/Element-plus，代码简洁、易扩展。整合最新技术，模块插件式开发，前后端分离，开箱即用。集成多租户、缓存、数据校验、鉴权、事件总线、动态API、通讯、远程请求、任务调度、打印等众多黑科技。让开发更简单、更通用、更流行！</a></p>\n\n\n</div>\n<div id=\"MySignature\">\n    欢迎光临:<font size=\"3\"><font size=\"3\"><a href=\"http://shiningrise.cnblogs.com/\" target=\"_blank\"><font size=\"3\"><font size=\"3\">http://shiningrise.cnblogs.com</font></font></a><br /><br /></font></font>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 15:44</span>&nbsp;\n<a href=\"https://www.cnblogs.com/shiningrise\">shiningrise</a>&nbsp;\n阅读(<span id=\"post_view_count\">78</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "我的17岁",
      "link": "https://www.cnblogs.com/jayzorn-ljz/p/19611417",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jayzorn-ljz/p/19611417\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 13:46\">\n    <span>我的17岁</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"我的十七岁\">我的十七岁</h1>\n<p><em>你为什么要总结自己？你要写些什么？你要思考什么？</em></p>\n<hr />\n<h2 id=\"和自己对话\">和自己对话</h2>\n<p>这是第几次总结自己了？每次回头看，总觉得有些东西没写透，有些话说得不对。不过，it doesn't matter. 只要能留下点有用的，就够了。</p>\n<p>十七岁了——虽然还有四个月就十八了。你有没有活成自己想要的样子？你现在开心吗？能考上985、211吗？哈哈。说来有趣，长大之后，居然真的能回答小时候问自己的那些问题。</p>\n<p>是，我活成了自己想要的样子。我很开心，但不确定能不能考上985、211。我考虑复读，想试试。现在的我，很喜欢学习，喜欢阅读，尤其爱读那些感情激烈、冲破牢笼、情意浓烈、意蕴深沉的书，比如《平凡的世界》《罪与罚》《围城》《约翰·克利斯朵夫》《边城》。我喜欢沉浸进去，体验他们的生活，那些与我不同的生活。有时也读读哲学。我还喜欢弹吉他、摄影、旅行、写日记、运动健身。对了，编程我也没放下。</p>\n<p>你知道吗，我喜欢上了一个同校的女孩。她很好，很美，带点天真，喜欢吃好吃的，喜欢拍好看的照片。我喜欢她生活的样子，也喜欢她的陪伴。人生的路上，又遇到一个可爱的同路人。总之，我很喜欢她。</p>\n<p>不知道以前的你是怎么想的。那么功利，那么看重成绩，却不敢改方法，不敢直面失败。你真是个懦夫。</p>\n<p>你还记得吗，那个喜欢你的女孩，对你真的很好。可你犯了很严重的错。你支配她，你自私，你想上进却干涉她的生活。你想占有她，你不爱她——你只是满足自己的虚荣心，满足那种暧昧的成就感。你他妈就是个混蛋。</p>\n<p>幸好她走了，离你越远越好，不至于再受你的折磨。</p>\n<p>还有，以前你就是个“嘉豪”，为了装逼学电脑技术。不过，你确实——确实是把它当成了热爱。我很高兴你一直没丢。可你的思想太狭隘了，是个典型的“Junior geek”。做点小工具就沾沾自喜，面对大型项目、需要系统学习计算机知识的时候，又不敢动手。不过，幸亏你没硬来，不然这高中怕是真要废了。</p>\n<p>我从C++之父那儿总结了几条经验（<a href=\"https://www.bilibili.com/video/BV1jiZcYnEXe/?share_source=copy_web&amp;vd_source=2cc479c274bd7a9c44e39239f22b7ba6\" rel=\"noopener nofollow\" target=\"_blank\">C++创始人访谈、编程经验教训、行业发展</a>），希望你放在心上：</p>\n<ol>\n<li>不要现在就专业化——你还不确定以后做什么。车开太快，转不了弯。</li>\n<li>别把自己幻想成孤僻的天才。没必要，也不具性价比。学会交流，人类是群居动物，合作是我们爬上食物链顶端的最大优势，别丢掉它。</li>\n<li>学历史。夫以铜为镜，可以正衣冠；以古为镜，可以知兴替；以人为镜，可以明得失。学会沟通。把技术当挡箭牌、轻视交流的人，必死无疑。</li>\n<li>花时间平衡生活，为将来做好准备。</li>\n<li>广泛的学习和经历，是你在机会来临时能抓住它的前提。只学技术，必死无疑。</li>\n</ol>\n<p>最后，我想说，对不起。</p>\n<p>我没有好好学那些应试的功课，把太多时间花在编程上，把你带进了险境。我们可能只能读个大专了……但别怕。我会挣扎的，我会带你闯出去。</p>\n<hr />\n<h2 id=\"我想怎么做\">我想怎么做</h2>\n<p>好啦，聊天到此为止。说说现在的生活和计划吧。</p>\n<p>我的目标很简单：先过本科线，然后回来复读，最后冲一冲985。我相信我可以。我已做好赴死的准备。</p>\n<p>生活倒是很丰富。我不再只沉迷编程，也开始听歌、看电影、阅读、写作、学专业课、玩《死亡搁浅》、摄影、散步、健身、和喜欢的女孩聊天。</p>\n<p>这时候肯定有人质问：离高考只剩一百天，你怎么还敢做这么多课外的事？</p>\n<p>我想说，这是策略问题。但这就是我的生活——我想做，也会做。工作日里，我的学习时间能占到每天的45.83%，虽然常常因为情绪问题骤降到33.3%不到——这大概也是成绩不好的原因之一。法定休息日可能只剩22.92%。但没关系，我有能力支配自己的生活，不至于跌入深渊。</p>\n<hr />\n<h2 id=\"改变你最大的是哪件事\">改变你最大的，是哪件事？</h2>\n<p>我很早就开始想：怎么达成目标？怎么做？怎么改变自己？——我想长话短说。</p>\n<p>高一，我加入了“灵动计算机社”。高二，核心社员和社长被选拔出来，组建了一个叫“科创部”的学生机构，隶属于校团委会。学校的学生机构以“会—部”架构运行，有团委会、学生会、社联理事会等，各会下设不同部门，比如团委会下有组织部，社联理事会有媒体制作部。科创部是学校五大部门之一，接触管理层的机会挺多。</p>\n<p>我是2026届的，科创部第二任部长，也是灵动计算机社不知第几任社长。前任部长是2025届的学长们。他们技术强、知识广、思维也前卫。我从他们身上学到太多——因为他们，我才开始读书、重视历史，尝试那些我从未做过、甚至害怕做的事。他们是我最好的老师。我感谢他们。</p>\n<p>其中有一位部长，我称他为“子冯”。子冯个子不高，看着有点呆。我对他了解不多，彼此的关系很像师生——那种中式师生。</p>\n<p>2026年1月24日那晚，子冯为我开导人生。我问了很多：为什么我总是失败？为什么有些事不能做？为什么要活在当下？他都一一作答。那一晚过后，我觉得人生好像亮起了一点光。后来，我照他的推荐读了《讨厌的勇气》，心里豁然开朗。</p>\n<p>原来，我害怕失败，害怕失去；我追求功利，自我认知矛盾；我缺爱，渴望陪伴；我过度在意他人评价，又不敢改变……</p>\n<p>好在，我能接受，也终于承认了。</p>\n<p>从那时起，我才真正开始生活——把前面说的那些，一件一件，做了下去。</p>\n<h2 id=\"给自己留句话吧\">给自己留句话吧</h2>\n<p>希望你将来能考上985，珍惜身边喜欢的人，勇敢的去爱你爱的人，保持自己理想的生活，活在当下。</p>\n<p>我才十七岁。我接受世界的考验，我——想给自己来个痛快的洗礼！</p>\n<hr />\n<p><em>真正的光明绝不是永没有黑暗的时间，只是永不被黑暗所掩蔽罢了。真正的英雄绝不是永没有卑下的情操，只是永不被卑下的情操所屈服罢了。</em><br />\n<em>所以在你要战胜外来的敌人之前，先得战胜你内在的敌人；你不必害怕沉沦堕落，只消你能不断地自拔与更新......</em><br />\n<em>战士啊，当你知道世界上受苦的不止你一个时，你定会减少痛楚，而你的希望也将永远在绝望中再生了罢！</em></p>\n<p>​                                                                                                   ——傅雷一九三七年译《约翰克利斯朵夫》时所献(节选)</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 13:46</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jayzorn-ljz\">JayZorn</a>&nbsp;\n阅读(<span id=\"post_view_count\">659</span>)&nbsp;\n评论(<span id=\"post_comment_count\">10</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "我的GIS实践与思考：新书《GIS基础原理与技术实践》分享",
      "link": "https://www.cnblogs.com/charlee44/p/19611747",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/charlee44/p/19611747\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 13:27\">\n    <span>我的GIS实践与思考：新书《GIS基础原理与技术实践》分享</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        作者结合自身GIS开发经验，分享职业路径思考、新书《GIS基础原理与技术实践》的写作初衷，并附上多篇技术节选与开源资源，倡导“原理+实践”融合的学习方式。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"1-引言\">1. 引言</h1>\n<p>经过几年的积累与打磨，笔者的新书《GIS基础原理与技术实践》终于正式出版了。其实谈不上什么宏大意义，只是把这些年在 GIS 开发中踩过的坑、理清的思路、写过的代码，系统地整理出来。趁着新书面世，也想借此机会再分享一些体会与思考。</p>\n<p><img alt=\"GIS基础原理与技术实践\" class=\"lazyload\" /></p>\n<h1 id=\"2-gis之路\">2. GIS之路</h1>\n<p>GIS 是一本综合性很强的学科，所以学习 GIS 以后从业不一定是走 GIS 开发这条路。比如比较稳妥的就是去体制内，各级自然资源局、测绘院、规划院、水利/交通/环保等政府部门及下属事业单位等等。他们可能不写代码，但需要理解空间数据、会用专业软件、能参与项目设计或成果审查，工作相对稳定，也深度参与国家基础地理信息体系建设。当然，体制内的岗位类型多样，职责与节奏各有不同，并非一概轻松——适合与否，终究要看个人志趣与所长。而在这样的环境中，专业能力固然是基础，但沟通协作、理解组织运行逻辑，往往也影响着长远的发展空间。</p>\n<p>但是，如果选择体制外的发展路径，做 GIS 开发会是一个更有发展空间的方向。如果只是会用 GIS 软件处理数据、出图，在现在这个自动化越来越普及的环境下，这类技能其实很容易被替代，也很难形成自己的优势。说白了，这更多是一种执行性的工作。而 GIS 本身的应用面其实不算特别宽，但如果加上开发能力，情况就不一样了——你可以自己写工具、改进流程，甚至参与到智慧城市、实景三维、时空平台这些实际项目中去。现在会编程的人确实不少，但既懂 GIS 的业务逻辑，又能把空间数据、坐标系统、服务发布这些事情用代码真正跑起来的人才并不多。</p>\n<p>更深入地说，GIS 真正的价值，不在于它本身，而在于它的“连接性”——它天然具备与各种前沿技术融合的能力。单纯做传统的 GIS 工作，无论是数据处理还是制图分析，职业发展的天花板往往比较明显。但几乎每一次技术浪潮——比如人工智能、遥感解译、时空大数据、物联网、低空经济、数字孪生——都离不开对“位置”和“空间关系”的理解，而这正是 GIS 的核心。事实上，不少从事这些新兴领域的工程师或研究人员，都有 GIS 或地理信息相关的背景。他们之所以能脱颖而出，往往不是因为只懂 GIS，而是把 GIS 的空间思维，和编程、算法、系统架构等底层能力结合起来。</p>\n<p>所以，与其把自己局限在“GIS 从业者”的标签里，不如沉下去，去掌握那些支撑 GIS 应用的基础技术：坐标系统、数据结构、服务架构、图形渲染、甚至数学和算法。只有这样，才能在技术变革中真正站稳脚跟，而不是被工具替代。</p>\n<h1 id=\"3-书籍序言\">3. 书籍序言</h1>\n<p>地理信息系统（Geographic Information System，简称 GIS）自诞生以来，已从一门边缘技术逐步成长为支撑现代社会空间认知与决策的重要工具。无论是城市规划、环境监测、应急管理，还是商业选址、智慧交通、数字孪生，GIS 的身影无处不在。它不仅连接了现实世界的地理空间与数字世界的抽象表达，更在人工智能、大数据、物联网等新兴技术浪潮中不断焕发新的生命力。</p>\n<p>当前，GIS 行业正处于深刻变革的关键阶段：在国家“信创”战略推动下，国产化替代加速，自主可控技术栈日益成熟；实景三维中国建设带动三维 GIS 快速发展，空间表达从二维迈向全空间立体化；人工智能与 GIS 深度融合，显著提升了遥感解译、空间预测等能力；同时，云原生架构的普及正推动 GIS 服务向微服务化、弹性伸缩和高并发共享演进，WebGIS 与空间数据中台逐步成为行业基础设施。这些趋势不断拓展 GIS 的边界，也带来全新的机遇与挑战。</p>\n<p>然而，对于初学者而言，GIS 往往显得庞杂而抽象。它横跨地理学、测绘学、遥感、计算机科学、数据科学等多个学科，既有严谨的数学基础，又有复杂的工程实现；既要理解坐标系统的理论逻辑，又要掌握数据处理与可视化的实操技能。许多人在学习过程中容易陷入“知其然，不知其所以然”的困境——记住了操作步骤，却难以理解背后的原理；学会了软件使用，却无法应对真实场景中的复杂问题。</p>\n<p>本书正是为弥合这一鸿沟而写。作为作者，我深知理论脱离实践的无力，也体会过面对海量 GIS 概念时的迷茫。因此，本书不以堆砌术语为目标，也不满足于对软件功能的简单复述，而是坚持“原理为基、实践为桥”的理念，力求在清晰阐释核心概念的同时，通过大量可运行、可调试、可扩展的技术案例，引导读者亲手构建对 GIS 的系统性理解。</p>\n<p>全书围绕一条清晰的知识主线展开：从空间参考系统这一“地理世界的语言”出发，依次剖析矢量、栅格、地形与三维模型四种核心数据模型的本质与应用；继而探讨现代 GIS 如何通过服务化方式共享数据，如何通过可视化呈现价值，最终落脚于空间查询与分析这一 GIS 的灵魂所在。每一章都配有详尽的代码实现，所有案例均托管于开源平台https://github.com/fafa1899/GISBasic ），鼓励读者边读边做，在实践中深化认知。</p>\n<p>需要坦诚说明的是，由于成书时间较为紧促，加之本人学识有限，书中难免存在疏漏与不足。尤其空间分析与智能应用这一块，虽有初步涉猎，但未能深入展开，实为一大遗憾。未来版本中，我将尽力补充相关内容，也恳请广大读者不吝指正，共同完善这本面向实践的 GIS 入门之作。</p>\n<p>愿你在阅读本书的过程中，不仅能学会“怎么做”，更能领悟“为什么这么做”；不仅掌握技术，更能建立起属于自己的 GIS 知识体系。当你能够自如地将地理空间思维应用于实际问题时，便是本书最大的成功。</p>\n<p>谨以此书，献给所有对地理世界充满好奇、愿意动手探索的你。</p>\n<h1 id=\"4-内容节选\">4. 内容节选</h1>\n<ol>\n<li><a href=\"https://charlee44.com/post.html?id=fd717a0e06a1402a90ea1a1c53320a92\" rel=\"noopener nofollow\" target=\"_blank\">GIS中的“高度”到底指什么？一文厘清正高、正常高与大地高的区别</a>：深入解析 GIS 中高程参考系统的核心概念——大地水准面、似大地水准面与参考椭球面的关系，厘清正高、正常高与大地高的区别及转换方法，并介绍我国高程基准与全球重力模型。</li>\n<li><a href=\"https://charlee44.com/post.html?id=51b4a939e17b4426acd704b11f0031df\" rel=\"noopener nofollow\" target=\"_blank\">GIS开发必知：WKT 与 EPSG 如何表达空间参考坐标系？附 GDAL 实现</a>：深入解析 GIS 中空间参考坐标系的标准化表达方式——WKT 与 EPSG 编码，并通过 GDAL 代码实战演示如何创建与输出地理坐标系和高斯-克吕格投影坐标系。</li>\n<li><a href=\"https://charlee44.com/post.html?id=0494eaa683ad43ddbd4d87900b093050\" rel=\"noopener nofollow\" target=\"_blank\">从Shapefile到GeoJSON：用GDAL实现GIS矢量数据读写与空间分析</a>：通过 GDAL/OGR 实现了 GIS 矢量数据的完整处理流程——从 Shapefile 读取、WGS84 到 Web 墨卡托坐标转换，到 GeoJSON 写入，并演示了点与多边形拓扑关系（如 Contains）的判断方法。</li>\n<li><a href=\"https://charlee44.com/post.html?id=cad0147cbff74ad69e2c68c5203eb5f5\" rel=\"noopener nofollow\" target=\"_blank\">不只是图片：深入理解 GIS 栅格数据本质与 GDAL 读写实战</a>：深入剖析 GIS 栅格数据的本质——它不只是普通图片，还能表示高程、降雨、土地类型等空间信息，并通过 GDAL 演示了 GeoTIFF 的读取与创建实战。</li>\n<li><a href=\"https://charlee44.com/post.html?id=120dc48f44be4395934acf73046ea1b2\" rel=\"noopener nofollow\" target=\"_blank\">从DEM到等高线：手撕矢量与栅格两种地形表达</a>：深入解析等高线地形图的矢量与栅格两种生成原理，并通过 C++ 代码从零实现 DEM 到等高线的完整流程，揭示 GIS 地形表达的核心逻辑。</li>\n<li><a href=\"https://charlee44.com/post.html?id=8c60d37fd0f74da2a7190894971f8673\" rel=\"noopener nofollow\" target=\"_blank\">从DEM到三维地形：用PLY、OBJ、glTF构建GIS可视化模型</a>：通过从DEM生成PLY白模、OBJ纹理模型到glTF标准资产的完整代码示例，系统揭示了三维GIS模型构建的核心原理与技术演进。</li>\n<li><a href=\"https://charlee44.com/post.html?id=411a16083323420d855655c1ec41d9a5\" rel=\"noopener nofollow\" target=\"_blank\">三维模型瓦片服务三剑客：3D Tiles、I3S与S3M全解析</a>:系统解析了三维GIS中三大主流瓦片标准——Cesium 3D Tiles、Esri I3S 和超图 S3M 的核心机制、数据结构与适用场景，涵盖瓦片树、包围体、几何误差、要素化设计及样式表达等关键技术。</li>\n<li><a href=\"https://charlee44.com/post.html?id=9a3aec3a04b54708a52f321784eb5152\" rel=\"noopener nofollow\" target=\"_blank\">GIS前沿技术</a>：探讨了GIS技术的最新前沿发展方向及其应用价值，涵盖三维GIS、数字孪生、人工智能、时空大数据云平台以及低空经济等多个领域；展示了GIS在智慧城市、自然资源管理、无人机应用等方面的重要作用和广阔前景。</li>\n<li><a href=\"https://charlee44.com/post.html?id=db441070211c496e9d2553af5b29e09e\" rel=\"noopener nofollow\" target=\"_blank\">地图服务器GeoServer的安装与配置</a>：详细介绍了地图服务器GeoServer的安装与配置过程。</li>\n<li><a href=\"https://charlee44.com/post.html?id=003c0b5046da427dbca50ad300d8b62d\" rel=\"noopener nofollow\" target=\"_blank\">GeoServer发布地图服务（WMS、WFS）</a>：详细论述了使用GeoServer发布地图服务WMS和WFS的过程。</li>\n<li><a href=\"https://charlee44.com/post.html?id=067f76316c76430c97c1440044c13f19\" rel=\"noopener nofollow\" target=\"_blank\">网络地图服务（WMS）详解</a>：根据具体实例详细论述了GIS中网络地图服务（WMS）的内容。</li>\n<li><a href=\"https://charlee44.com/post.html?id=41fb5cab870c4f7c8760716671338826\" rel=\"noopener nofollow\" target=\"_blank\">网络要素服务（WFS）详解</a>：通过实例详细介绍了WebGIS中网络要素服务（WFS）的具体内容。</li>\n<li><a href=\"https://charlee44.com/post.html?id=bee53ec2f8bd4ce4a04c8f0abd277a14\" rel=\"noopener nofollow\" target=\"_blank\">倾斜单体化模型技术实现</a>：详细介绍了倾斜单体化模型的四种实现思路。</li>\n</ol>\n<hr />\n<blockquote>\n<p>本书《<strong>GIS基础原理与技术实践</strong>》已于 2025 年 11 月由北京航空航天大学出版社正式出版。全书共十章，系统覆盖空间参考、四大核心数据模型（矢量/栅格/地形/三维）、地理信息服务、可视化及空间分析，并配套 <strong>C++/Python/C#/Java/JavaScript 多语言示例代码</strong>，所有资源开源托管于 GitHub。</p>\n<p>📘 <strong>欢迎感兴趣的读者通过以下渠道了解或支持正版</strong>：<br />\n🔗 <a href=\"https://item.jd.com/14603137.html\" rel=\"noopener nofollow\" target=\"_blank\">京东购买</a>｜<a href=\"https://product.dangdang.com/29988568.html\" rel=\"noopener nofollow\" target=\"_blank\">当当购买</a><br />\n💻 <strong>配套开源项目</strong>：<a href=\"https://github.com/fafa1899/GISBasic\" rel=\"noopener nofollow\" target=\"_blank\">GitHub</a> | <a href=\"https://gitcode.com/charlee44/GISBasic\" rel=\"noopener nofollow\" target=\"_blank\">GitCode</a></p>\n</blockquote>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 13:27</span>&nbsp;\n<a href=\"https://www.cnblogs.com/charlee44\">charlee44</a>&nbsp;\n阅读(<span id=\"post_view_count\">7</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Nano-vLLM-Ascend(持续更新中)",
      "link": "https://www.cnblogs.com/linzm14/p/19611415",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/linzm14/p/19611415\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 11:38\">\n    <span>Nano-vLLM-Ascend(持续更新中)</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"nano-vllm-ascend\"><a href=\"https://github.com/linzm1007/nano-vllm-ascend\" rel=\"noopener nofollow\" target=\"_blank\">Nano-vLLM-Ascend</a></h2>\n<p>项目链接：<a href=\"https://github.com/linzm1007/nano-vllm-ascend\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/linzm1007/nano-vllm-ascend</a><br />\nnano-vllm是github开源的一个gpu推理项目，基于开源版本弄的一个ascend npu版本推理小demo，旨在帮助初学者了解推理的整体流程，区别于vllm，nano-vllm体量更小，麻雀虽小五脏俱全，更有助于初学者学习，非常适合用于相关概念的理解。</p>\n<h2 id=\"框架层流程图\">框架层流程图</h2>\n<p><img alt=\"nona-vllm框架\" class=\"lazyload\" /></p>\n<h2 id=\"模型层流程图\">模型层流程图</h2>\n<p><img alt=\"Qwen3-0.6B\" class=\"lazyload\" /></p>\n<h2 id=\"特性\">特性</h2>\n<ul>\n<li>📖 <strong>可读代码库</strong> - 核心约2428行Python代码的清晰实现</li>\n<li>⚡ <strong>优化套件</strong> - 张量并行、torchair Ascend IR图编译和图缓存、融合算子、前缀缓存等</li>\n</ul>\n<ul>\n<li>[✅] 待完成：目前只支持单算子, npu图模式实现</li>\n<li>[✅] 支持CPU环境运行：<a href=\"https://github.com/linzm1007/nano-vllm-cpu\" rel=\"noopener nofollow\" target=\"_blank\">nano-vllm-cpu 代码仓库</a></li>\n<li>[✅] 性能优化</li>\n<li>[⏳] 支持模型: Qwen3-0.6B、Qwen3-32B、Qwen2-0.5B、Qwen2.5-0.5B、Qwen2.5-0.5B-Instruct、Llama-3.2-1B-Instruct、Qwen3-30B-A3B、Qwen3-VL-2B-Instruct、MiniCPM4-0.5B</li>\n<li>[✅] 支持一个moe模型:Qwen3-30B-A3B(暂时不支持入图)</li>\n<li>[📅] 支持一个omni模型</li>\n<li>[✅] 支持一个vl模型:Qwen3-VL-2B-Instruct(暂时不支持入图)</li>\n<li>[✅] 实现page attention</li>\n<li>[📅] 实现一个自定义算子</li>\n<li>[📅] 支持在线推理</li>\n</ul>\n<p>torchair接口参考 <a href=\"https://www.hiascend.com/document/detail/zh/Pytorch/710/modthirdparty/torchairuseguide/torchair_00008.html\" rel=\"noopener nofollow\" target=\"_blank\">https://www.hiascend.com/document/detail/zh/Pytorch/710/modthirdparty/torchairuseguide/torchair_00008.html</a><br />\n融合算子接口参考 <a href=\"https://www.hiascend.com/document/detail/zh/Pytorch/720/apiref/torchnpuCustomsapi/context/torch_npu-npu_fused_infer_attention_score_v2.md\" rel=\"noopener nofollow\" target=\"_blank\">https://www.hiascend.com/document/detail/zh/Pytorch/720/apiref/torchnpuCustomsapi/context/torch_npu-npu_fused_infer_attention_score_v2.md</a><br />\nattention实现参考 <a href=\"https://gitee.com/omniai/omniinfer/blob/master/omni/layers/attention/backend/attention.py\" rel=\"noopener nofollow\" target=\"_blank\">https://gitee.com/omniai/omniinfer/blob/master/omni/layers/attention/backend/attention.py</a>  forward_vanilla函数</p>\n<h2 id=\"支持的模型\">支持的模型</h2>\n<table>\n<thead>\n<tr>\n<th>架构</th>\n<th>模型</th>\n<th>示例 HF 模型</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Qwen3ForCausalLM</td>\n<td>Qwen3-0.6B,Qwen3-32B</td>\n<td></td>\n</tr>\n<tr>\n<td>Qwen2ForCausalLM</td>\n<td>Qwen2-0.5B</td>\n<td></td>\n</tr>\n<tr>\n<td>LlamaForCausalLM</td>\n<td>Llama-3.2-1B-Instruct</td>\n<td></td>\n</tr>\n<tr>\n<td>Qwen3MoeForCausalLM</td>\n<td>Qwen3-30B-A3B</td>\n<td></td>\n</tr>\n<tr>\n<td>Qwen3VLForConditionalGeneration</td>\n<td>Qwen2.5-VL-3B-Instruct</td>\n<td></td>\n</tr>\n<tr>\n<td>MiniCPMForCausalLM</td>\n<td>MiniCPM4-0.5B</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"代码行数\">代码行数</h2>\n<p>📊 总体数据</p>\n<table>\n<thead>\n<tr>\n<th>范围</th>\n<th>文件数</th>\n<th>总行数</th>\n<th>占比</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>nanovllm 全部</td>\n<td>20 个</td>\n<td>4,652 行</td>\n<td>100%</td>\n</tr>\n<tr>\n<td>models 目录</td>\n<td>5 个</td>\n<td>2,224 行</td>\n<td>47.8%</td>\n</tr>\n<tr>\n<td>除 models 外</td>\n<td>15 个</td>\n<td>2,428 行</td>\n<td>52.2%</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"推理优化技术大纲\">推理优化技术大纲</h2>\n<p>📚 <strong>完整技术文档</strong>：<a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/inference_optimization_guide.md\" rel=\"noopener nofollow\" target=\"_blank\">LLM 推理优化技术大纲</a></p>\n<p>本文档整理了 LLM 推理领域的 <strong>20 大类关键技术</strong>，分为 7 个层次：</p>\n<h3 id=\"-核心技术基础必备\">🔥 核心技术（基础必备）</h3>\n<ul>\n<li><strong>KV Cache 管理</strong>：PageAttention、Prefix Caching、KV Cache 压缩</li>\n<li><strong>Attention 优化</strong>：FlashAttention、GQA/MQA、稀疏注意力</li>\n<li><strong>批处理策略</strong>：Continuous Batching、Dynamic Batching</li>\n</ul>\n<h3 id=\"-性能优化进阶\">🚀 性能优化（进阶）</h3>\n<ul>\n<li><strong>量化技术</strong>：INT8/INT4/FP8、AWQ、GPTQ、GGUF</li>\n<li><strong>投机采样</strong>：Speculative Decoding、Medusa、Lookahead</li>\n<li><strong>解码优化</strong>：Parallel Decoding、Token Tree Verification</li>\n</ul>\n<h3 id=\"️-系统架构\">🏗️ 系统架构</h3>\n<ul>\n<li><strong>调度策略</strong>：FCFS、SJF、Priority-based、Preemption</li>\n<li><strong>内存优化</strong>：Memory Pool、Swapping、Offloading</li>\n<li><strong>并行策略</strong>：Tensor/Pipeline/Expert/Sequence Parallelism</li>\n</ul>\n<h3 id=\"-特殊场景\">🧠 特殊场景</h3>\n<ul>\n<li><strong>长上下文</strong>：RoPE Scaling、StreamingLLM、Ring Attention</li>\n<li><strong>多模态</strong>：Vision-Language、Audio-Language、Unified Architecture</li>\n<li><strong>MoE 优化</strong>：Expert Routing、Load Balancing、All-to-All 通信</li>\n</ul>\n<h3 id=\"-底层优化\">⚡ 底层优化</h3>\n<ul>\n<li><strong>图编译</strong>：TorchAir、TensorRT-LLM、Torch.compile</li>\n<li><strong>算子融合</strong>：QKV Fusion、Custom CUDA/Triton Kernels</li>\n<li><strong>通信优化</strong>：NCCL/HCCL、RDMA、GPUDirect</li>\n</ul>\n<h3 id=\"-评估观测\">📊 评估观测</h3>\n<ul>\n<li><strong>性能分析</strong>：Memory/Compute Profiling、Roofline Analysis</li>\n<li><strong>关键指标</strong>：TTFT、TPOT、Throughput、GPU Utilization</li>\n</ul>\n<h3 id=\"-前沿趋势\">🔮 前沿趋势</h3>\n<ul>\n<li><strong>模型架构</strong>：Mamba/RWKV、Mixture of Depths、RetNet</li>\n<li><strong>服务化</strong>：Disaggregated Serving、Elastic Scaling</li>\n<li><strong>新兴方向</strong>：推理蒸馏、Early Exit、Hardware-Aware NAS</li>\n</ul>\n<hr />\n<h2 id=\"attention\">Attention</h2>\n<h3 id=\"pageattention\">PageAttention</h3>\n<p><strong>PageAttention</strong> 是 vLLM 的核心创新技术，灵感来自操作系统的<strong>虚拟内存分页机制</strong>，用于高效管理 LLM 推理中的 KV Cache。</p>\n<h4 id=\"核心概念\">核心概念</h4>\n<p>传统 KV Cache 分配方式会为每个序列预分配最大可能长度的连续内存，导致严重的内存浪费和碎片。PageAttention 借鉴操作系统分页思想，将 KV Cache 划分为固定大小的 block，按需动态分配。</p>\n<h4 id=\"关键技术点\">关键技术点</h4>\n<table>\n<thead>\n<tr>\n<th>技术</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Block 管理</strong></td>\n<td>将 KV Cache 划分为固定大小的 block（如 16/32 tokens），每个 block 独立分配</td>\n</tr>\n<tr>\n<td><strong>Block Table</strong></td>\n<td>类似页表的数据结构，记录逻辑 token 位置到物理 block 的映射关系</td>\n</tr>\n<tr>\n<td><strong>非连续存储</strong></td>\n<td>同一序列的 KV Cache 可以分散在多个不连续的 block 中</td>\n</tr>\n<tr>\n<td><strong>内存共享</strong></td>\n<td>并行解码（如 beam search）时可共享 prompt 的 KV cache</td>\n</tr>\n<tr>\n<td><strong>Copy-on-Write</strong></td>\n<td>写时复制机制，仅在需要修改时才复制 block</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"内存使用对比\">内存使用对比</h4>\n<pre><code>传统方式：\n- 序列长度 1000，最大支持 4096\n- 内存占用：4096 * hidden_size\n- 浪费率：约 75%\n\nPageAttention：\n- 序列长度 1000，block_size=16\n- 需要 block：1000/16 = 63 个\n- 实际分配：63 * 16 = 1008 tokens\n- 浪费率：仅 0.8%\n</code></pre>\n<h4 id=\"代码实现\">代码实现</h4>\n<pre><code class=\"language-python\"># nanovllm/layers/attention.py\n# Block Table 映射\nblock_table = context.block_tables  # 映射表\n\n# Slot Mapping - 将 token 映射到 block 中的具体位置\n# slot_mapping 格式：[block_idx, offset_in_block]\ncontext.slot_mapping\n\n# 分页存储 KV Cache\ntorch_npu._npu_reshape_and_cache(\n    k, v,\n    k_cache.view(num_blocks, block_size, num_kv_heads, head_dim),\n    v_cache.view(num_blocks, block_size, num_kv_heads, head_dim),\n    slot_mapping.int()\n)\n</code></pre>\n<h4 id=\"优势\">优势</h4>\n<ol>\n<li><strong>内存效率</strong>：按需分配，无内部碎片</li>\n<li><strong>动态扩展</strong>：序列增长时只需分配新 block</li>\n<li><strong>内存共享</strong>：多个序列可共享相同的 prompt KV Cache</li>\n<li><strong>高吞吐量</strong>：支持更大的 batch size</li>\n</ol>\n<h4 id=\"详细对比\">详细对比</h4>\n<p>📄 <strong>详细技术文档</strong>：<a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/pageattention_comparison.md\" rel=\"noopener nofollow\" target=\"_blank\">HuggingFace Transformers 早期实现与 PageAttention 对比</a></p>\n<p>包含：</p>\n<ul>\n<li>早期 Transformers 代码实现分析</li>\n<li>内存浪费的量化对比（75% vs 6.25%）</li>\n<li>不同 batch size 和序列长度的详细对比表</li>\n<li>vLLM 论文数据来源说明</li>\n<li>实际代码示例和场景分析</li>\n</ul>\n<hr />\n<h3 id=\"flashattention\">FlashAttention</h3>\n<p><strong>FlashAttention</strong> 是斯坦福大学提出的 <strong>IO 感知</strong> 注意力优化算法，通过分块计算和减少 HBM（高带宽内存）访问来提升性能。</p>\n<h4 id=\"核心问题\">核心问题</h4>\n<p>标准 Attention 实现需要存储中间结果（注意力矩阵）到 HBM，导致：</p>\n<ul>\n<li><strong>内存瓶颈</strong>：HBM 带宽远低于计算速度</li>\n<li><strong>O(N²) 内存</strong>：序列长度的平方级内存增长</li>\n<li><strong>多次数据搬运</strong>：Q、K、V 需要多次读写 HBM</li>\n</ul>\n<h4 id=\"核心创新\">核心创新</h4>\n<table>\n<thead>\n<tr>\n<th>技术</th>\n<th>原理</th>\n<th>效果</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>Tiling（分块）</strong></td>\n<td>将 Q、K、V 分块加载到高速 SRAM</td>\n<td>减少 HBM 访问次数</td>\n</tr>\n<tr>\n<td><strong>Online Softmax</strong></td>\n<td>流式计算 softmax，无需完整注意力矩阵</td>\n<td>内存降至 O(N)</td>\n</tr>\n<tr>\n<td><strong>Recomputation</strong></td>\n<td>反向传播时重新计算中间值</td>\n<td>牺牲计算换内存</td>\n</tr>\n<tr>\n<td><strong>Kernel Fusion</strong></td>\n<td>多个操作融合为单个 CUDA kernel</td>\n<td>减少 kernel 启动开销</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"内存层次结构对比\">内存层次结构对比</h4>\n<pre><code>GPU 内存层次：\n┌─────────────────────────────────────┐\n│  HBM (High Bandwidth Memory)        │  ← 1.5 TB/s，容量大但速度慢\n│  - 容量：40-80 GB                   │  ← 标准 Attention 在此频繁读写\n│  - 延迟：高                         │\n├─────────────────────────────────────┤\n│  SRAM (Static RAM / Shared Memory)  │  ← 19 TB/s，容量小但速度快\n│  - 容量：~100 KB per SM             │  ← FlashAttention 主要在此计算\n│  - 延迟：极低                       │\n└─────────────────────────────────────┘\n</code></pre>\n<h4 id=\"计算流程对比\">计算流程对比</h4>\n<p><strong>标准 Attention：</strong></p>\n<pre><code>1. 从 HBM 加载 Q, K, V\n2. 计算 S = QK^T → 写入 HBM\n3. 计算 P = softmax(S) → 写入 HBM  \n4. 计算 O = PV → 写入 HBM\n❌ 多次 HBM 读写，内存占用 O(N²)\n</code></pre>\n<p><strong>FlashAttention：</strong></p>\n<pre><code>1. 分块加载 Qᵢ, Kⱼ, Vⱼ 到 SRAM\n2. 在 SRAM 中计算 softmax\n3. 累加结果到输出\n4. 丢弃中间结果，重复直到完成\n✅ 仅需 O(N) 内存，大幅减少 HBM 访问\n</code></pre>\n<h4 id=\"代码实现-1\">代码实现</h4>\n<pre><code class=\"language-python\"># nanovllm/layers/attention_ori.py\nfrom flash_attn import (\n    flash_attn_varlen_func,      # Prefill 阶段\n    flash_attn_with_kvcache      # Decode 阶段\n)\n\n# Prefill - 处理变长序列，支持 PageAttention\nflash_attn_varlen_func(\n    q, k, v,\n    max_seqlen_q=max_seqlen_q,\n    cu_seqlens_q=cu_seqlens_q,    # 累计长度支持变长\n    causal=True,                  # 因果掩码\n    block_table=block_table       # PageAttention block table\n)\n\n# Decode - 单 token 推理，复用分页 KV Cache\nflash_attn_with_kvcache(\n    q.unsqueeze(1),               # [batch, 1, num_heads, head_dim]\n    k_cache, v_cache,             # 分页 KV 缓存\n    cache_seqlens=context_lens,   # 实际序列长度\n    block_table=block_table       # block table 映射\n)\n</code></pre>\n<h4 id=\"性能收益\">性能收益</h4>\n<ul>\n<li><strong>内存效率</strong>：从 O(N²) 降至 O(N)</li>\n<li><strong>计算速度</strong>：A100 上可达 <strong>2-4 倍</strong>加速</li>\n<li><strong>序列长度</strong>：支持更长的上下文（如 100K+ tokens）</li>\n</ul>\n<hr />\n<h3 id=\"三种-attention-实现对比\">三种 Attention 实现对比</h3>\n<p>本项目包含三种 Attention 实现，适用于不同场景：</p>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>attention_ori.py</th>\n<th>attention.py</th>\n<th>attention_torch_native.py</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>底层实现</strong></td>\n<td>Flash Attention 库</td>\n<td>NPU 原生算子</td>\n<td>PyTorch 原生</td>\n</tr>\n<tr>\n<td><strong>适用平台</strong></td>\n<td>CUDA GPU</td>\n<td>华为昇腾 NPU</td>\n<td>通用（CPU/GPU）</td>\n</tr>\n<tr>\n<td><strong>性能</strong></td>\n<td>⭐⭐⭐⭐⭐</td>\n<td>⭐⭐⭐⭐⭐</td>\n<td>⭐⭐</td>\n</tr>\n<tr>\n<td><strong>可读性</strong></td>\n<td>⭐⭐</td>\n<td>⭐⭐</td>\n<td>⭐⭐⭐⭐⭐</td>\n</tr>\n<tr>\n<td><strong>用途</strong></td>\n<td>生产环境（GPU）</td>\n<td>生产环境（NPU）</td>\n<td>学习调试</td>\n</tr>\n</tbody>\n</table>\n<p><strong>选择建议：</strong></p>\n<ul>\n<li><strong>生产环境（GPU）</strong>：使用 <code>attention_ori.py</code>（Flash Attention）</li>\n<li><strong>生产环境（昇腾 NPU）</strong>：使用 <code>attention.py</code>（NPU 算子）</li>\n<li><strong>学习/调试</strong>：使用 <code>attention_torch_native.py</code>（易于理解）</li>\n</ul>\n<h2 id=\"文档索引\">文档索引</h2>\n<p>本目录包含项目各模块的详细流程图文档，使用 Mermaid 语法绘制。</p>\n<h3 id=\"engine-模块流程图\">Engine 模块流程图</h3>\n<table>\n<thead>\n<tr>\n<th>文档</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/sequence_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/engine/sequence_flowchart.md</a></td>\n<td>Sequence 序列状态管理流程</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/block_manager_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/engine/block_manager_flowchart.md</a></td>\n<td>BlockManager KV缓存块管理流程</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/scheduler_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/engine/scheduler_flowchart.md</a></td>\n<td>Scheduler 调度器流程</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/model_runner_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/engine/model_runner_flowchart.md</a></td>\n<td>ModelRunner 模型运行器流程</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/engine/llm_engine_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/engine/llm_engine_flowchart.md</a></td>\n<td>LLMEngine 主引擎流程</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"layers-模块流程图\">Layers 模块流程图</h3>\n<table>\n<thead>\n<tr>\n<th>文档</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/linear_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/linear_flowchart.md</a></td>\n<td>线性层与张量并行策略</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/attention_torch_native_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/attention_torch_native_flowchart.md</a></td>\n<td>PyTorch原生Attention实现</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/attention_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/attention_flowchart.md</a></td>\n<td>NPU专用Attention实现</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/attention_ori_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/attention_ori_flowchart.md</a></td>\n<td>Flash Attention优化实现</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/sampler_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/sampler_flowchart.md</a></td>\n<td>采样器（温度采样）</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/rotary_embedding_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/rotary_embedding_flowchart.md</a></td>\n<td>RoPE位置编码</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/layernorm_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/layernorm_flowchart.md</a></td>\n<td>RMSNorm归一化层</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/embed_head_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/embed_head_flowchart.md</a></td>\n<td>词嵌入与LM Head</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/layers/activation_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/layers/activation_flowchart.md</a></td>\n<td>SwiGLU激活函数</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"models-模块流程图\">Models 模块流程图</h3>\n<table>\n<thead>\n<tr>\n<th>文档</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/llama_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/models/llama_flowchart.md</a></td>\n<td>Llama模型架构</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/qwen3_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/models/qwen3_flowchart.md</a></td>\n<td>Qwen3模型架构</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/qwen3_vl_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/models/qwen3_vl_flowchart.md</a></td>\n<td>Qwen3-VL多模态模型架构</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/qwen3_moe_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/models/qwen3_moe_flowchart.md</a></td>\n<td>Qwen3-MoE稀疏专家模型架构</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/mini_cpm4_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/models/mini_cpm4_flowchart.md</a></td>\n<td>MiniCPM4模型架构</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/linzm1007/nano-vllm-ascend/blob/main/docs/models/models_map_flowchart.md\" rel=\"noopener nofollow\" target=\"_blank\">docs/models/models_map_flowchart.md</a></td>\n<td>模型注册映射关系</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"bench数据\">bench数据</h2>\n<p>仅供参考，硬软条件不同，跑出的数据也会有差异</p>\n<h4 id=\"不同模型对比\">不同模型对比</h4>\n<table>\n<thead>\n<tr>\n<th>model</th>\n<th>Output Tokens</th>\n<th>Time (s)</th>\n<th>Throughput (tokens/s)</th>\n<th>TP</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Qwen3-0.6B</td>\n<td>143,770</td>\n<td>36.82</td>\n<td>3904.20</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Qwen2-0.5B</td>\n<td>143,770</td>\n<td>20.71</td>\n<td>6940.84</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Qwen2.5-0.5B-Instruct</td>\n<td>143,770</td>\n<td>19.82</td>\n<td>7252.67</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Llama-3.2-1B-Instruct</td>\n<td>143,770</td>\n<td>25.45</td>\n<td>5648.50</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Qwen3-32B</td>\n<td>143,770</td>\n<td>206.69</td>\n<td>695.59</td>\n<td>2</td>\n</tr>\n<tr>\n<td>Qwen3-32B</td>\n<td>143,770</td>\n<td>119.86</td>\n<td>1199.50</td>\n<td>4</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"其他框架对比2025-12-30\">其他框架对比(2025-12-30)</h4>\n<p>vLLM Nano-vLLM 数据来源 <a href=\"https://github.com/GeeeekExplorer/nano-vllm\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/GeeeekExplorer/nano-vllm</a></p>\n<table>\n<thead>\n<tr>\n<th>Inference Engine</th>\n<th>Output Tokens</th>\n<th>Time (s)</th>\n<th>Throughput (tokens/s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>vLLM</td>\n<td>133,966</td>\n<td>98.37</td>\n<td>1361.84</td>\n</tr>\n<tr>\n<td>Nano-vLLM</td>\n<td>133,966</td>\n<td>93.41</td>\n<td>1434.13</td>\n</tr>\n<tr>\n<td>Nano-vLLM-Ascend python torch原生实现</td>\n<td>4805</td>\n<td>257.49</td>\n<td>18.66</td>\n</tr>\n<tr>\n<td>Nano-vLLM-Ascend 融合算子+图编译bs=256</td>\n<td>133,966</td>\n<td>33.88</td>\n<td>3954.20</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"图模式不同bs对比2025-12-30\">图模式不同bs对比(2025-12-30)</h4>\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th>Output Tokens</th>\n<th>Time (s)</th>\n<th>Throughput (tokens/s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>bs=16</td>\n<td>133,966</td>\n<td>107.23</td>\n<td>1249.37</td>\n</tr>\n<tr>\n<td>bs=32</td>\n<td>133,966</td>\n<td>75.89</td>\n<td>1765.35</td>\n</tr>\n<tr>\n<td>bs=48</td>\n<td>133,966</td>\n<td>64.84</td>\n<td>2066.22</td>\n</tr>\n<tr>\n<td>bs=64</td>\n<td>133,966</td>\n<td>54.06</td>\n<td>2478.31</td>\n</tr>\n<tr>\n<td>bs=128</td>\n<td>133,966</td>\n<td>43.08</td>\n<td>3109.56</td>\n</tr>\n<tr>\n<td>bs=256</td>\n<td>133,966</td>\n<td>33.88</td>\n<td>3954.20</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"单算子padding和non-padding对比2025-12-30\">单算子Padding和Non-padding对比(2025-12-30)</h4>\n<p>bs=256</p>\n<table>\n<thead>\n<tr>\n<th>Prepare Strategy</th>\n<th>Output Tokens</th>\n<th>Time (s)</th>\n<th>Throughput (tokens/s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Padding</td>\n<td>133,966</td>\n<td>158.46</td>\n<td>845.41</td>\n</tr>\n<tr>\n<td>Non-padding</td>\n<td>133,966</td>\n<td>152.14</td>\n<td>880.55</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"环境搭建参考vllm-ascend\">环境搭建（参考vllm-ascend）</h2>\n<p><a href=\"https://docs.vllm.ai/projects/vllm-ascend-cn/zh-cn/latest/quick_start.html\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.vllm.ai/projects/vllm-ascend-cn/zh-cn/latest/quick_start.html</a></p>\n<p>ubuntu</p>\n<pre><code># Update DEVICE according to your device (/dev/davinci[0-7])\nexport DEVICE=/dev/davinci0\n# Update the vllm-ascend image\n# Atlas A2:\n# export IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1\n# Atlas A3:\n# export IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1-a3\nexport IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1\ndocker run --rm \\\n--name vllm-ascend \\\n--shm-size=1g \\\n--device $DEVICE \\\n--device /dev/davinci_manager \\\n--device /dev/devmm_svm \\\n--device /dev/hisi_hdc \\\n-v /usr/local/dcmi:/usr/local/dcmi \\\n-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n-v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n-v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n-v /etc/ascend_install.info:/etc/ascend_install.info \\\n-v /root/.cache:/root/.cache \\\n-p 8000:8000 \\\n-it $IMAGE bash\n# Install curl\napt-get update -y &amp;&amp; apt-get install -y curl\n</code></pre>\n<h2 id=\"安装依赖\">安装依赖</h2>\n<pre><code class=\"language-bash\">pip install .\n</code></pre>\n<h2 id=\"模型下载\">模型下载</h2>\n<pre><code class=\"language-bash\">huggingface-cli download --resume-download Qwen/Qwen3-0.6B \\\n  --local-dir ~/huggingface/Qwen3-0.6B/ \\\n  --local-dir-use-symlinks False\n</code></pre>\n<h2 id=\"快速开始\">快速开始</h2>\n<p>请参见 example.py 了解用法。该 API 与 vLLM 的接口基本一致，仅在 LLM.generate 方法上存在一些细微差异：</p>\n<pre><code class=\"language-python\">from nanovllm import LLM, SamplingParams\nllm = LLM(\"/YOUR/MODEL/PATH\", enforce_eager=True, tensor_parallel_size=1)\nsampling_params = SamplingParams(temperature=0.6, max_tokens=256)\nprompts = [\"Hello, Nano-vLLM.\"]\noutputs = llm.generate(prompts, sampling_params)\noutputs[0][\"text\"]\n</code></pre>\n<h2 id=\"example运行结果\">example运行结果</h2>\n<p><img alt=\"result-image\" class=\"lazyload\" /></p>\n<h2 id=\"bench环境\">bench环境</h2>\n<p>仅供参考<br />\nascend-dmi -c #查看</p>\n<ul>\n<li>硬件环境​：\n<ul>\n<li>1.显卡:A3 910C</li>\n<li>2.驱动版本:24.1.rc3.10</li>\n<li>3.固件版本:7.5.0.109.220</li>\n</ul>\n</li>\n<li>​软件环境​：\n<ul>\n<li>1.CANN包 8.3.RC1</li>\n<li>2.PTA版本：torch-npu 2.5.1.post2+gitd7a85f8，torch 2.5.1</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"benchmark\">Benchmark</h2>\n<p>See <code>bench.py</code> for benchmark.</p>\n<p><strong>Test Configuration:</strong></p>\n<ul>\n<li>Model: Qwen3-0.6B</li>\n<li>Total Requests: 256 sequences</li>\n<li>Input Length: Randomly sampled between 100–1024 tokens</li>\n<li>Output Length: Randomly sampled between 100–1024 tokens</li>\n</ul>\n<p><strong>Performance Results:</strong><br />\nNano-vLLM-Ascend 实在太慢了只跑了10条seq</p>\n<table>\n<thead>\n<tr>\n<th>Inference Engine</th>\n<th>Output Tokens</th>\n<th>Time (s)</th>\n<th>Throughput (tokens/s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>vLLM</td>\n<td>133,966</td>\n<td>98.37</td>\n<td>1361.84</td>\n</tr>\n<tr>\n<td>Nano-vLLM</td>\n<td>133,966</td>\n<td>93.41</td>\n<td>1434.13</td>\n</tr>\n<tr>\n<td>Nano-vLLM-Ascend</td>\n<td>4805</td>\n<td>257.49</td>\n<td>18.66</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"qwen3-06b-layers\">qwen3-0.6B layers</h2>\n<pre><code class=\"language-angular2html\">ModuleList(\n  (0-27): 28 x Qwen3DecoderLayer(\n    (self_attn): Qwen3Attention(\n      (qkv_proj): QKVParallelLinear()\n      (o_proj): RowParallelLinear()\n      (rotary_emb): RotaryEmbedding()\n      (attn): Attention()\n      (q_norm): RMSNorm()\n      (k_norm): RMSNorm()\n    )\n    (mlp): Qwen3MLP(\n      (gate_up_proj): MergedColumnParallelLinear()\n      (down_proj): RowParallelLinear()\n      (act_fn): SiluAndMul()\n    )\n    (input_layernorm): RMSNorm()\n    (post_attention_layernorm): RMSNorm()\n  )\n)\n\n</code></pre>\n\n\n</div>\n<div id=\"MySignature\">\n    蓝天和白云是标配。\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 11:38</span>&nbsp;\n<a href=\"https://www.cnblogs.com/linzm14\">linzm14</a>&nbsp;\n阅读(<span id=\"post_view_count\">15</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}