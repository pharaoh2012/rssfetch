{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "FastAPI实战：用懒加载与Lifespan优雅管理重型依赖",
      "link": "https://www.cnblogs.com/ymtianyu/p/19582659",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ymtianyu/p/19582659\" id=\"cb_post_title_url\" title=\"发布于 2026-02-06 09:28\">\n    <span>FastAPI实战：用懒加载与Lifespan优雅管理重型依赖</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        本文针对FastAPI应用在部署文生图等大模型时遇到的启动慢、资源占用高问题，深入剖析了应用启动（冷启动）与请求处理（热路径）的区别。核心介绍了利用懒加载模式与Lifespan事件管理上下文，将耗时的初始化操作从启动迁移到首次请求时或进行异步预热，从而实现服务的快速启动与高效资源利用，并提供了详细的代码示例与避坑指南。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<h2>你的FastAPI服务，是不是也在启动时\"负重跑步\"？</h2>\n<p>有没有遇到过这种场景：你兴冲冲地写完了一个文生图AI服务的接口，本地测试美滋滋。结果一上服务器，<code style=\"color: rgba(186, 55, 42, 1);\">docker build</code> 完，<code style=\"color: rgba(186, 55, 42, 1);\">docker run</code> 的那一瞬间，你感觉仿佛过了一个世纪——服务怎么还没起来？</p>\n<p>然后看日志，好家伙，卡在<code style=\"color: rgba(186, 55, 42, 1);\">Loading model...</code> 这一步了。模型好几个G，加载慢如牛。更糟的是，你的K8s健康检查可能因为启动超时，反复杀掉了还在\"热身\"的Pod，导致服务永远无法就绪🎯。</p>\n<p>今天，咱们就聊聊怎么给FastAPI服务\"减负\"，让启动飞快，同时又能优雅地管理那些\"重型武器\"（比如大模型、大数据连接）。核心就俩概念：<strong style=\"color: rgba(186, 55, 42, 1);\">懒加载</strong>和<strong style=\"color: rgba(186, 55, 42, 1);\">Lifespan事件</strong>。</p>\n<hr />\n<h2>🎯 先搞清问题：启动 vs 运行时</h2>\n<p>咱们得先分清两个阶段，这就像餐厅开业：</p>\n<div>\n<p>🔥 <strong>冷启动（应用启动）</strong>：相当于餐厅第一天开业。你不能让客人在门口等厨师把所有菜都做一遍尝过才开门。我们的目标是<strong style=\"color: rgba(186, 55, 42, 1);\">越快开门越好</strong>。</p>\n<p>🍳 <strong>热路径（请求处理）</strong>：客人点单后，后厨开始炒菜。这时候追求的是单道菜的出菜速度和质量。</p>\n</div>\n<p>很多兄弟（包括当初的我）会把加载模型这种\"备菜\"工作，直接扔在全局变量里，在应用启动时执行。结果就是\"开业\"仪式巨长无比。</p>\n<p>你可能会问：\"那我不用的时候不加载，用的时候再加载，不就行了？\"</p>\n<p>Bingo！这就是<strong style=\"color: rgba(186, 55, 42, 1);\">懒加载（Lazy Loading）</strong>的核心思想：把耗时初始化推迟到第一次真正需要它的时候。但在Web服务里，怎么优雅地实现它，并且管理它的生命周期呢？这就轮到<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>出场了。</p>\n<h2>🤖 核心武器：Lifespan 事件管理器</h2>\n<p>在FastAPI（实际上是背后的Starlette）中，<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code> 是一个上下文管理器，它让你能精确控制应用<strong style=\"color: rgba(186, 55, 42, 1);\">启动前</strong>和<strong style=\"color: rgba(186, 55, 42, 1);\">关闭后</strong>该做什么。</p>\n<p>官方文档可能讲得有点抽象，我打个比方：它就像是你服务的\"私人管家\"。服务上线前（<code style=\"color: rgba(186, 55, 42, 1);\">startup</code>），管家帮你预热游泳池、打开花园灯；服务下线时（<code style=\"color: rgba(186, 55, 42, 1);\">shutdown</code>），管家帮你关灯、放掉泳池水，收拾得干干净净。</p>\n<p>重点来了：这个\"管家\"出现的时间点，比你所有接口的<code style=\"color: rgba(186, 55, 42, 1);\">dependencies</code>都要早！这意味着你可以在<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>里准备好一些\"工厂\"或者\"连接池\"，但<strong style=\"color: rgba(186, 55, 42, 1);\">不一定非要立刻加载所有重型资源</strong>。</p>\n<pre class=\"language-python highlighter-hljs\"><code>from contextlib import asynccontextmanager\nfrom fastapi import FastAPI\nimport asyncio\n\n# 这是一个假的\"重型模型\"\nclass HeavyModel:\n    def __init__(self):\n        self.loaded = False\n    \n    async def load(self):\n        print(\"开始加载模型...这可能需要很久\")\n        await asyncio.sleep(5) # 模拟加载耗时\n        self.loaded = True\n        print(\"模型加载完毕！\")\n    \n    async def predict(self, text: str):\n        if not self.loaded:\n            await self.load() # 懒加载发生在这里！\n        return f\"预测结果 for: {text}\"\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup: 这里我们只初始化\"模型容器\"，但不加载模型本身\n    print(\"应用启动中...\")\n    model_container = {\"model\": HeavyModel()}\n    app.state.model = model_container[\"model\"]\n    \n    yield model_container # 这里的model_container会注入到请求的`app.state`中\n    \n    # Shutdown: 清理工作，比如关闭模型、释放GPU内存等\n    print(\"应用关闭中，执行清理...\")\n    app.state.model = None\n\napp = FastAPI(lifespan=lifespan)\n\n@app.get(\"/generate\")\nasync def generate(prompt: str):\n    # 首次请求时，才会真正触发模型加载\n    result = await app.state.model.predict(prompt)\n    return {\"result\": result}</code></pre>\n<p>看上面代码，<code style=\"color: rgba(186, 55, 42, 1);\">HeavyModel</code>在<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>的启动阶段只是被<strong style=\"color: rgba(186, 55, 42, 1);\">实例化</strong>了，并没有调用耗时的<code style=\"color: rgba(186, 55, 42, 1);\">load()</code>方法。真正的加载发生在第一个请求调用<code style=\"color: rgba(186, 55, 42, 1);\">predict</code>时。</p>\n<p><strong style=\"color: rgba(186, 55, 42, 1);\">这样做的好处是什么？</strong></p>\n<div>\n<p>1️⃣ <strong>启动速度飞起</strong>：你的服务几乎可以秒级就绪，通过健康检查。</p>\n<p>2️⃣ <strong>资源按需使用</strong>：如果某个Pod一直没收到相关请求，模型就永远不会加载，节省了宝贵的GPU内存。</p>\n<p>3️⃣ <strong>生命周期可控</strong>：你依然在<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>的掌控之中，可以在关闭时优雅地释放资源。</p>\n</div>\n<h2>⚠️ 但是！小心这个\"天坑\"</h2>\n<p>懒加载虽好，但直接用在生产环境，可能会让第一个用户成为\"大冤种\"。想象一下，用户第一次请求，要白屏等待模型加载的几十秒，体验极差，而且这个请求很可能超时。</p>\n<p>所以，更优的生产级实践是：<strong style=\"color: rgba(186, 55, 42, 1);\">懒加载 + 异步预热</strong>。</p>\n<p>我们可以在<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>启动完成后，<strong style=\"color: rgba(186, 55, 42, 1);\">悄悄地、异步地</strong>开始加载模型，而不是阻塞启动过程。</p>\n<pre class=\"language-python highlighter-hljs\"><code>@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    print(\"应用启动中...\")\n    model = HeavyModel()\n    app.state.model = model\n\n    # **关键技巧：创建一个后台任务异步预热**\n    async def _warm_up():\n        try:\n            await model.load()\n            print(\"模型预热完成！\")\n        except Exception as e:\n            print(f\"模型预热失败: {e}\")\n\n    # 不await，让它后台运行\n    asyncio.create_task(_warm_up())\n\n    yield\n    # Shutdown\n    print(\"应用关闭中...\")</code></pre>\n<p>这样，服务能立刻启动并响应健康检查。模型在后台默默加载，加载完成后才真正提供预测服务。对于加载期间的请求，你可以根据业务决定是返回一个\"服务预热中\"的友好提示，还是用队列让其等待。</p>\n<h2>🔧 更工程化的封装与注意事项</h2>\n<p>在实际项目中，我们不会把逻辑全写<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>主函数里。我的习惯是封装一个<code style=\"color: rgba(186, 55, 42, 1);\">ModelManager</code>单例类，来统一管理加载状态、重试和并发安全。</p>\n<p>再说几个容易翻车的点：</p>\n<div>\n<p>🎯 <strong>并发请求时的重复加载</strong>：如果第一个请求A触发加载没完，请求B又来了，要确保不会初始化两个模型实例把内存撑爆。记得用锁（<code style=\"color: rgba(186, 55, 42, 1);\">asyncio.Lock</code>）或者检查状态变量。</p>\n<p>🎯 <strong>健康检查的设计</strong>：你的<code style=\"color: rgba(186, 55, 42, 1);\">/health</code>端点应该反映服务的真实状态。可以设计成：<code style=\"color: rgba(186, 55, 42, 1);\">{\"status\": \"warming_up\"}</code>，<code style=\"color: rgba(186, 55, 42, 1);\">{\"status\": \"ready\"}</code>。这样K8s的<code style=\"color: rgba(186, 55, 42, 1);\">readinessProbe</code>可以在模型就绪后才导入流量。</p>\n<p>🎯 <strong>关闭时的优雅终止</strong>：如果模型正在推理，直接关闭可能会导致GPU内存泄漏或数据错误。在<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>的<span style=\"color: rgba(186, 55, 42, 1);\"><code>shutdown</code></span>阶段，最好设置一个标志位，让正在处理的请求完成，并拒绝新请求。</p>\n</div>\n<hr />\n<h3>最后啰嗦一句</h3>\n<p>技术选型没有银弹，懒加载和预热策略也要根据你的具体场景权衡。如果你的服务要求百分百确定性（比如金融风控），可能就需要在启动时忍受加载耗时，确保服务完全就绪。但对于大多数AI模型服务、推荐系统，<strong style=\"color: rgba(186, 55, 42, 1);\">\"快速启动，异步预热\"</strong>绝对是提升部署体验和资源效率的神器。</p>\n<p>希望这篇分享，能让你下次部署\"大家伙\"时，不再手忙脚乱。毕竟，谁不想让自己的服务既跑得快，又省资源呢？</p>\n<p>如果觉得有用，别忘了收藏一下，说不定下次部署前就得翻出来看看。你在部署重型服务时还踩过哪些坑？欢迎在评论区聊聊，咱们一起避坑！</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 09:28</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ymtianyu\">一名程序媛呀</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "概念解析：机器视觉如何赋予机器“三维双眼”——3D重建技术全景指南",
      "link": "https://www.cnblogs.com/ChenAI-TGF/p/19582574",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ChenAI-TGF/p/19582574\" id=\"cb_post_title_url\" title=\"å‘å¸ƒäºŽ 2026-02-06 09:04\">\n    <span>æ¦‚å¿µè§£æžï¼šæœºå™¨è§†è§‰å¦‚ä½•èµ‹äºˆæœºå™¨â€œä¸‰ç»´åŒçœ¼â€â€”â€”3Dé‡å»ºæŠ€æœ¯å…¨æ™¯æŒ‡å—</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        åœ¨äººå·¥æ™ºèƒ½çš„æµªæ½®ä¸­ï¼Œå¦‚æžœè¯´ä¼ ç»Ÿçš„2Då›¾åƒè¯†åˆ«æ˜¯è®©æœºå™¨â€œè®¤å‡ºâ€ç‰©ä½“ï¼Œé‚£ä¹ˆ**3Dé‡å»ºï¼ˆ3D Reconstructionï¼‰**åˆ™æ˜¯è®©æœºå™¨çœŸæ­£â€œç†è§£â€ç‰©ç†ä¸–ç•Œã€‚é€šè¿‡æœºå™¨è§†è§‰å®žçŽ°3Dé‡å»ºï¼Œæ˜¯èµ‹äºˆæœºå™¨äººã€æ— äººæœºå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›çš„æ ¸å¿ƒæŠ€æœ¯ã€‚\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<hr />\n<p>@</p><div class=\"toc\"><div class=\"toc-container-header\">ç›®å½•</div><ul><li><a href=\"#å‰è¨€\" rel=\"noopener nofollow\">å‰è¨€</a></li><li><a href=\"#1-ä»€ä¹ˆæ˜¯é€šè¿‡æœºå™¨è§†è§‰å®žçŽ°3dé‡å»º\" rel=\"noopener nofollow\">1. ä»€ä¹ˆæ˜¯é€šè¿‡æœºå™¨è§†è§‰å®žçŽ°3Dé‡å»ºï¼Ÿ</a><ul><li><a href=\"#å®šä¹‰\" rel=\"noopener nofollow\">å®šä¹‰</a></li><li><a href=\"#æ ¸å¿ƒæœ¬è´¨ä»Ž2dåˆ°3dçš„é€†å‘æŠ•å½±\" rel=\"noopener nofollow\">æ ¸å¿ƒæœ¬è´¨ï¼šä»Ž2Dåˆ°3Dçš„é€†å‘æŠ•å½±</a></li><li><a href=\"#å…³é”®æ¦‚å¿µ\" rel=\"noopener nofollow\">å…³é”®æ¦‚å¿µ</a></li></ul></li><li><a href=\"#2-æ•ˆæžœæ¼”ç¤º3dé‡å»ºçš„è¿‡ç¨‹\" rel=\"noopener nofollow\">2. æ•ˆæžœæ¼”ç¤ºï¼š3Dé‡å»ºçš„è¿‡ç¨‹</a></li><li><a href=\"#3-ç›®å‰çš„ä¸»æµæ–¹æ³•\" rel=\"noopener nofollow\">3. ç›®å‰çš„ä¸»æµæ–¹æ³•</a><ul><li><a href=\"#ä¸€è¢«åŠ¨è§†è§‰passive-vision-ä»…ä¾èµ–çŽ¯å¢ƒå…‰\" rel=\"noopener nofollow\">ä¸€ã€è¢«åŠ¨è§†è§‰ï¼ˆPassive Visionï¼‰â€”â€” ä»…ä¾èµ–çŽ¯å¢ƒå…‰</a><ul><li><a href=\"#a--å•ç›®é‡å»ºmonocular-reconstruction\" rel=\"noopener nofollow\">A.  å•ç›®é‡å»ºï¼ˆMonocular Reconstructionï¼‰ï¼š</a><ul><li><a href=\"#ä»€ä¹ˆæ˜¯-sfmstructure-from-motion\" rel=\"noopener nofollow\">ä»€ä¹ˆæ˜¯ SfMï¼ˆStructure from Motionï¼‰ï¼Ÿ</a></li><li><a href=\"#sfm-çš„å…¨æµç¨‹æžç®€æ­¥éª¤ç‰ˆ\" rel=\"noopener nofollow\">SfM çš„å…¨æµç¨‹ï¼ˆæžç®€æ­¥éª¤ç‰ˆï¼‰</a></li><li><a href=\"#1-ç‰¹å¾æå–ä¸ŽåŒ¹é…-feature-extraction--matching\" rel=\"noopener nofollow\">1. ç‰¹å¾æå–ä¸ŽåŒ¹é… (Feature Extraction &amp; Matching)</a></li><li><a href=\"#2-è®¡ç®—å‡ ä½•å…³ç³»-estimating-epipolar-geometry\" rel=\"noopener nofollow\">2. è®¡ç®—å‡ ä½•å…³ç³» (Estimating Epipolar Geometry)</a></li><li><a href=\"#3-ä¸‰è§’æµ‹é‡-triangulation\" rel=\"noopener nofollow\">3. ä¸‰è§’æµ‹é‡ (Triangulation)</a></li><li><a href=\"#4-å¢žé‡å¼é‡å»º-incremental-reconstruction\" rel=\"noopener nofollow\">4. å¢žé‡å¼é‡å»º (Incremental Reconstruction)</a></li><li><a href=\"#5-å…¨å±€ä¼˜åŒ–å…‰æŸæ³•å¹³å·®-bundle-adjustment-ba\" rel=\"noopener nofollow\">5. å…¨å±€ä¼˜åŒ–ï¼šå…‰æŸæ³•å¹³å·® (Bundle Adjustment, BA)</a></li><li><a href=\"#ä¸ºä»€ä¹ˆå•ç›®-sfm-æ— æ³•æ„ŸçŸ¥ç»å¯¹å°ºåº¦\" rel=\"noopener nofollow\">ä¸ºä»€ä¹ˆå•ç›® SfM æ— æ³•æ„ŸçŸ¥â€œç»å¯¹å°ºåº¦â€ï¼Ÿ</a></li><li><a href=\"#sfm-çš„ä¸¤ç§ä¸»æµæµæ´¾\" rel=\"noopener nofollow\">SfM çš„ä¸¤ç§ä¸»æµæµæ´¾</a></li><li><a href=\"#å‚è€ƒæ–‡ä»¶\" rel=\"noopener nofollow\">å‚è€ƒæ–‡ä»¶</a></li></ul></li><li><a href=\"#b-åŒç›®å¤šç›®ç«‹ä½“è§†è§‰stereo-vision\" rel=\"noopener nofollow\">B. åŒç›®/å¤šç›®ç«‹ä½“è§†è§‰ï¼ˆStereo Visionï¼‰ï¼š</a><ul><li><a href=\"#1-æ ¸å¿ƒå®šä¹‰\" rel=\"noopener nofollow\">1. æ ¸å¿ƒå®šä¹‰</a></li><li><a href=\"#2-æ ¸å¿ƒæ•°å­¦åŽŸç†ä¸‰è§’æµ‹é‡triangulation\" rel=\"noopener nofollow\">2. æ ¸å¿ƒæ•°å­¦åŽŸç†ï¼šä¸‰è§’æµ‹é‡ï¼ˆTriangulationï¼‰</a></li><li><a href=\"#3-æ ‡å‡†ç®—æ³•æµç¨‹stereo-pipeline\" rel=\"noopener nofollow\">3. æ ‡å‡†ç®—æ³•æµç¨‹ï¼ˆStereo Pipelineï¼‰</a></li><li><a href=\"#4-åŒç›®è§†è§‰-vs-å•ç›®-sfm-çš„åŒºåˆ«\" rel=\"noopener nofollow\">4. åŒç›®è§†è§‰ vs. å•ç›® SfM çš„åŒºåˆ«</a></li><li><a href=\"#5-é€‚ç”¨åœºæ™¯ä¸Žå±€é™æ€§\" rel=\"noopener nofollow\">5. é€‚ç”¨åœºæ™¯ä¸Žå±€é™æ€§</a></li><li><a href=\"#6-å‚è€ƒèµ„æ–™ä¸Žç»å…¸æ–‡çŒ®\" rel=\"noopener nofollow\">6. å‚è€ƒèµ„æ–™ä¸Žç»å…¸æ–‡çŒ®</a></li></ul></li><li><a href=\"#c-ç¥žç»è¾å°„åœºnerf-neural-radiance-fields\" rel=\"noopener nofollow\">C. ç¥žç»è¾å°„åœºï¼ˆNeRF, Neural Radiance Fieldsï¼‰</a><ul><li><a href=\"#1-ä»€ä¹ˆæ˜¯-nerf\" rel=\"noopener nofollow\">1. ä»€ä¹ˆæ˜¯ NeRFï¼Ÿ</a></li><li><a href=\"#2-nerf-çš„æ ¸å¿ƒåŽŸç†5d-å‡½æ•°\" rel=\"noopener nofollow\">2. NeRF çš„æ ¸å¿ƒåŽŸç†ï¼š5D å‡½æ•°</a></li><li><a href=\"#3-nerf-çš„å·¥ä½œæµç¨‹\" rel=\"noopener nofollow\">3. NeRF çš„å·¥ä½œæµç¨‹</a></li><li><a href=\"#4-ä¸ºä»€ä¹ˆ-nerf-æ•ˆæžœè¿™ä¹ˆéœ‡æ’¼\" rel=\"noopener nofollow\">4. ä¸ºä»€ä¹ˆ NeRF æ•ˆæžœè¿™ä¹ˆéœ‡æ’¼ï¼Ÿ</a></li><li><a href=\"#5-å±€é™æ€§ä¸Žç›®å‰çš„ç“¶é¢ˆ\" rel=\"noopener nofollow\">5. å±€é™æ€§ä¸Žç›®å‰çš„ç“¶é¢ˆ</a></li><li><a href=\"#6-è¿›åŒ–ä¹‹è·¯nerf-çš„å­å­™ä»¬\" rel=\"noopener nofollow\">6. è¿›åŒ–ä¹‹è·¯ï¼ˆNeRF çš„å­å­™ä»¬ï¼‰</a></li><li><a href=\"#7-å‚è€ƒæ–‡çŒ®\" rel=\"noopener nofollow\">7. å‚è€ƒæ–‡çŒ®</a></li></ul></li><li><a href=\"#d-3d-gaussian-splatting-3dgs\" rel=\"noopener nofollow\">D. 3D Gaussian Splatting (3DGS)ï¼š</a><ul><li><a href=\"#1-ä»€ä¹ˆæ˜¯-3d-gaussian-splatting-3dgs\" rel=\"noopener nofollow\">1. ä»€ä¹ˆæ˜¯ 3D Gaussian Splatting (3DGS)ï¼Ÿ</a></li><li><a href=\"#2-æ ¸å¿ƒåŽŸç†3d-é«˜æ–¯æ¤­çƒä½“çš„åŸºå›\" rel=\"noopener nofollow\">2. æ ¸å¿ƒåŽŸç†ï¼š3D é«˜æ–¯æ¤­çƒä½“çš„â€œåŸºå› â€</a></li><li><a href=\"#3-å·¥ä½œæµç¨‹ä»Žç‚¹äº‘åˆ°ç²¾ç¾Žå»ºæ¨¡\" rel=\"noopener nofollow\">3. å·¥ä½œæµç¨‹ï¼šä»Žç‚¹äº‘åˆ°ç²¾ç¾Žå»ºæ¨¡</a></li><li><a href=\"#5-ç›®å‰çš„ç“¶é¢ˆä¸ŽæŒ‘æˆ˜\" rel=\"noopener nofollow\">5. ç›®å‰çš„ç“¶é¢ˆä¸ŽæŒ‘æˆ˜</a></li><li><a href=\"#6-å‚è€ƒæ–‡çŒ®\" rel=\"noopener nofollow\">6. å‚è€ƒæ–‡çŒ®</a></li></ul></li></ul></li><li><a href=\"#äºŒ-ä¸»åŠ¨è§†è§‰active-vision-è‡ªå¸¦å…‰æº\" rel=\"noopener nofollow\">äºŒ. ä¸»åŠ¨è§†è§‰ï¼ˆActive Visionï¼‰â€”â€” è‡ªå¸¦å…‰æº</a><ul><li><a href=\"#a-ç»“æž„å…‰æŠ€æœ¯\" rel=\"noopener nofollow\">A. ç»“æž„å…‰æŠ€æœ¯</a><ul><li><a href=\"#å®šä¹‰-1\" rel=\"noopener nofollow\"><strong>å®šä¹‰ï¼š</strong></a></li><li><a href=\"#2-æ ¸å¿ƒæ•°å­¦åŽŸç†å˜å½¢çš„ä¸‰è§’æµ‹é‡\" rel=\"noopener nofollow\">2. æ ¸å¿ƒæ•°å­¦åŽŸç†ï¼šå˜å½¢çš„ä¸‰è§’æµ‹é‡</a></li><li><a href=\"#3-ç¼–ç æ–¹å¼ç»“æž„å…‰çš„è¯­è¨€\" rel=\"noopener nofollow\">3. ç¼–ç æ–¹å¼ï¼šç»“æž„å…‰çš„â€œè¯­è¨€â€</a></li><li><a href=\"#4-æ·±åº¦è§£æžiphone-faceid-æ˜¯å¦‚ä½•å·¥ä½œçš„\" rel=\"noopener nofollow\">4. æ·±åº¦è§£æžï¼šiPhone FaceID æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ</a></li><li><a href=\"#5-ç»“æž„å…‰çš„ä¼˜ç¼ºç‚¹\" rel=\"noopener nofollow\">5. ç»“æž„å…‰çš„ä¼˜ç¼ºç‚¹</a></li></ul></li><li><a href=\"#6-å‚è€ƒæ–‡çŒ®-1\" rel=\"noopener nofollow\">6. å‚è€ƒæ–‡çŒ®</a></li><li><a href=\"#é£žè¡Œæ—¶é—´æ³•tof-time-of-flight\" rel=\"noopener nofollow\">é£žè¡Œæ—¶é—´æ³•ï¼ˆToF, Time of Flightï¼‰</a><ul><li><a href=\"#1-ä»€ä¹ˆæ˜¯-tof-é£žè¡Œæ—¶é—´æ³•\" rel=\"noopener nofollow\">1. ä»€ä¹ˆæ˜¯ ToF (é£žè¡Œæ—¶é—´æ³•)ï¼Ÿ</a></li><li><a href=\"#2-æ ¸å¿ƒæ•°å­¦åŽŸç†æžé€Ÿå…¬å¼\" rel=\"noopener nofollow\">2. æ ¸å¿ƒæ•°å­¦åŽŸç†ï¼šæžé€Ÿå…¬å¼</a></li><li><a href=\"#3-tof-çš„å·¥ä½œæµç¨‹\" rel=\"noopener nofollow\">3. ToF çš„å·¥ä½œæµç¨‹</a></li><li><a href=\"#4-tof-vs-ç»“æž„å…‰æœ‰ä»€ä¹ˆåŒºåˆ«\" rel=\"noopener nofollow\">4. ToF vs. ç»“æž„å…‰ï¼šæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</a></li><li><a href=\"#5-ç›®å‰çš„ç“¶é¢ˆä¸ŽæŒ‘æˆ˜-1\" rel=\"noopener nofollow\">5. ç›®å‰çš„ç“¶é¢ˆä¸ŽæŒ‘æˆ˜</a></li><li><a href=\"#6-å‚è€ƒæ–‡çŒ®-2\" rel=\"noopener nofollow\">6. å‚è€ƒæ–‡çŒ®</a></li></ul></li></ul></li></ul></li><li><a href=\"#4-é€‚ç”¨åœºæ™¯\" rel=\"noopener nofollow\">4. é€‚ç”¨åœºæ™¯</a></li><li><a href=\"#5-ç›®å‰çš„ç“¶é¢ˆä¸Žé—®é¢˜\" rel=\"noopener nofollow\">5. ç›®å‰çš„ç“¶é¢ˆä¸Žé—®é¢˜</a></li><li><a href=\"#æ€»ç»“\" rel=\"noopener nofollow\">æ€»ç»“</a></li><li><a href=\"#å‚è€ƒæ–‡çŒ®\" rel=\"noopener nofollow\">å‚è€ƒæ–‡çŒ®</a></li></ul></div><p></p>\n<h1 id=\"å‰è¨€\">å‰è¨€</h1>\n<p>åœ¨äººå·¥æ™ºèƒ½çš„æµªæ½®ä¸­ï¼Œå¦‚æžœè¯´ä¼ ç»Ÿçš„2Då›¾åƒè¯†åˆ«æ˜¯è®©æœºå™¨â€œè®¤å‡ºâ€ç‰©ä½“ï¼Œé‚£ä¹ˆ<strong>3Dé‡å»ºï¼ˆ3D Reconstructionï¼‰</strong>åˆ™æ˜¯è®©æœºå™¨çœŸæ­£â€œç†è§£â€ç‰©ç†ä¸–ç•Œã€‚é€šè¿‡æœºå™¨è§†è§‰å®žçŽ°3Dé‡å»ºï¼Œæ˜¯èµ‹äºˆæœºå™¨äººã€æ— äººæœºå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›çš„æ ¸å¿ƒæŠ€æœ¯ã€‚</p>\n<h1 id=\"1-ä»€ä¹ˆæ˜¯é€šè¿‡æœºå™¨è§†è§‰å®žçŽ°3dé‡å»º\">1. ä»€ä¹ˆæ˜¯é€šè¿‡æœºå™¨è§†è§‰å®žçŽ°3Dé‡å»ºï¼Ÿ</h1>\n<h2 id=\"å®šä¹‰\">å®šä¹‰</h2>\n<p>é€šè¿‡æœºå™¨è§†è§‰å®žçŽ°çš„3Dé‡å»ºï¼Œæ˜¯æŒ‡åˆ©ç”¨å…‰å­¦ä¼ æ„Ÿå™¨ï¼ˆå¦‚ç›¸æœºï¼‰èŽ·å–çš„2Då›¾åƒåºåˆ—ï¼Œç»“åˆè®¡ç®—æœºè§†è§‰ç®—æ³•ï¼Œæ¢å¤ç‰©ä½“çš„ä¸‰ç»´å‡ ä½•å½¢çŠ¶ã€ç©ºé—´ä½ç½®ä»¥åŠè¡¨é¢çº¹ç†çš„è¿‡ç¨‹ã€‚</p>\n<h2 id=\"æ ¸å¿ƒæœ¬è´¨ä»Ž2dåˆ°3dçš„é€†å‘æŠ•å½±\">æ ¸å¿ƒæœ¬è´¨ï¼šä»Ž2Dåˆ°3Dçš„é€†å‘æŠ•å½±</h2>\n<p>åœ¨ç‰©ç†ä¸–ç•Œä¸­ï¼Œ3Dç‰©ä½“é€šè¿‡ç›¸æœºçš„é€é•œæˆåƒåœ¨2Dæ„Ÿå…‰å…ƒä»¶ä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ª<strong>é™ç»´</strong>çš„è¿‡ç¨‹ï¼ˆä¸¢å¤±äº†æ·±åº¦ä¿¡æ¯$Z$ï¼‰ã€‚3Dé‡å»ºçš„ç›®æ ‡å°±æ˜¯é€šè¿‡æ•°å­¦æ¨¡åž‹å’Œç®—æ³•ï¼Œå°†è¿™äº›ä¸¢å¤±çš„æ·±åº¦ä¿¡æ¯æ‰¾å›žæ¥ï¼ŒæŠŠåƒç´ ç‚¹è¿˜åŽŸåˆ°ä¸‰ç»´åæ ‡ç³»ï¼ˆ$X, Y, Z$ï¼‰ä¸­ã€‚</p>\n<h2 id=\"å…³é”®æ¦‚å¿µ\">å…³é”®æ¦‚å¿µ</h2>\n<ul>\n<li><strong>ç‚¹äº‘ï¼ˆPoint Cloudï¼‰ï¼š</strong> é‡å»ºçš„ç¬¬ä¸€æ­¥é€šå¸¸æ˜¯ç”Ÿæˆå¤§é‡å¸¦æœ‰ç©ºé—´åæ ‡çš„é‡‡æ ·ç‚¹ã€‚</li>\n<li><strong>ä¸‰è§’å‰–åˆ†ï¼ˆTriangulationï¼‰ï¼š</strong> åˆ©ç”¨å‡ ä½•å…³ç³»ç¡®å®šç‚¹åœ¨ç©ºé—´ä¸­çš„ä½ç½®ã€‚</li>\n<li><strong>æ·±åº¦å›¾ï¼ˆDepth Mapï¼‰ï¼š</strong> æ¯ä¸ªåƒç´ ç‚¹ä»£è¡¨è·ç¦»ç›¸æœºè·ç¦»çš„å›¾åƒã€‚</li>\n</ul>\n<hr />\n<h1 id=\"2-æ•ˆæžœæ¼”ç¤º3dé‡å»ºçš„è¿‡ç¨‹\">2. æ•ˆæžœæ¼”ç¤ºï¼š3Dé‡å»ºçš„è¿‡ç¨‹</h1>\n<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸‰ä¸ªé˜¶æ®µæ¥æƒ³è±¡é‡å»ºçš„è§†è§‰æ•ˆæžœï¼š</p>\n<ol>\n<li><strong>ç¨€ç–é‡å»ºé˜¶æ®µï¼ˆSparse Reconstructionï¼‰ï¼š</strong><br />\nå±å¹•ä¸Šå‡ºçŽ°é›¶æ•£çš„ç‰¹å¾ç‚¹ï¼Œçœ‹èµ·æ¥åƒæ˜¯ä¸€ç¾¤å‘å…‰çš„è¤ç«è™«æž„æˆäº†ç‰©ä½“çš„è½®å»“ã€‚æ­¤æ—¶å¯ä»¥çœ‹æ¸…ç›¸æœºçš„è¿åŠ¨è½¨è¿¹ã€‚</li>\n<li><strong>ç¨ å¯†é‡å»ºé˜¶æ®µï¼ˆDense Reconstructionï¼‰ï¼š</strong><br />\nç‚¹äº‘å˜å¾—æžå…¶å¯†é›†ï¼Œç‰©ä½“çš„å½¢çŠ¶å·²ç»æ¸…æ™°å¯è¾¨ï¼Œåƒæ˜¯ç”±æ— æ•°ç»†å°çš„æ²™ç²’å †ç Œè€Œæˆçš„é›•å¡‘ã€‚</li>\n<li><strong>è¡¨é¢ç½‘æ ¼åŒ–ä¸Žçº¹ç†è´´å›¾ï¼ˆMeshing &amp; Texturingï¼‰ï¼š</strong><br />\nç®—æ³•åœ¨ç‚¹ä¸Žç‚¹ä¹‹é—´è¿žçº¿å½¢æˆä¸‰è§’é¢ç‰‡ï¼ˆMeshï¼‰ï¼Œå¹¶æŠŠç…§ç‰‡ä¸Šçš„é¢œè‰²â€œè´´â€ä¸ŠåŽ»ã€‚æ­¤æ—¶ï¼Œç‰©ä½“åœ¨å±å¹•ä¸Šçœ‹èµ·æ¥ä¸ŽçœŸå®žç…§ç‰‡æ— å¼‚ï¼Œä½†ä½ å¯ä»¥æ—‹è½¬ã€ç¼©æ”¾å®ƒã€‚</li>\n</ol>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /><br />\n<strong>æ¦‚å¿µè§£è¯»ï¼š</strong><br />\n<strong>1. Meshï¼ˆç½‘æ ¼ / ä¸‰è§’ç½‘æ ¼ï¼‰</strong><br />\nMesh æ˜¯ä¸‰ç»´æ¨¡åž‹çš„ â€œå‡ ä½•éª¨æž¶â€ï¼Œå®ƒç”±å¤§é‡ä¸‰è§’å½¢ï¼ˆæˆ–å¤šè¾¹å½¢ï¼‰é¢ç‰‡æ‹¼æŽ¥è€Œæˆï¼Œåªå®šä¹‰äº†ç‰©ä½“çš„ä¸‰ç»´å½¢çŠ¶ã€è½®å»“å’Œç»“æž„ï¼Œå°±åƒå»ºç­‘çš„é’¢ç­‹æ¡†æž¶ï¼Œæœ¬èº«æ²¡æœ‰é¢œè‰²å’Œçº¹ç†ã€‚<br />\n<strong>2. è´´å›¾ï¼ˆTexture Mappingï¼‰</strong><br />\nè´´å›¾æ˜¯ä¸€å¼ åŒ…å«é¢œè‰²ã€çº¹ç†ã€ç»†èŠ‚ä¿¡æ¯çš„äºŒç»´å›¾åƒï¼Œæ¯”å¦‚å¢™é¢çš„ç –çŸ³çº¹ç†ã€çª—æˆ·çš„çŽ»ç’ƒè´¨æ„Ÿã€‚å®ƒçš„ä½œç”¨æ˜¯æŠŠè¿™äº›è¡¨é¢ç»†èŠ‚ â€œè´´â€ åˆ° Mesh çš„å‡ ä½•æ¡†æž¶ä¸Šã€‚</p>\n<hr />\n<h1 id=\"3-ç›®å‰çš„ä¸»æµæ–¹æ³•\">3. ç›®å‰çš„ä¸»æµæ–¹æ³•</h1>\n<p>3Dé‡å»ºçš„æ–¹æ³•ä¸»è¦åˆ†ä¸º<strong>ä¸»åŠ¨è§†è§‰</strong>å’Œ<strong>è¢«åŠ¨è§†è§‰</strong>ä¸¤å¤§ç±»ï¼š</p>\n<h2 id=\"ä¸€è¢«åŠ¨è§†è§‰passive-vision-ä»…ä¾èµ–çŽ¯å¢ƒå…‰\">ä¸€ã€è¢«åŠ¨è§†è§‰ï¼ˆPassive Visionï¼‰â€”â€” ä»…ä¾èµ–çŽ¯å¢ƒå…‰</h2>\n<p>è¿™ç±»æ–¹æ³•ä¸å‘ç‰©ä½“å‘å°„èƒ½é‡ï¼Œä»…é€šè¿‡æ•æ‰çŽ¯å¢ƒå…‰æˆåƒã€‚</p>\n<h3 id=\"a--å•ç›®é‡å»ºmonocular-reconstruction\">A.  å•ç›®é‡å»ºï¼ˆMonocular Reconstructionï¼‰ï¼š</h3>\n<p><strong>åŽŸç†</strong>ï¼šåˆ©ç”¨<strong>è¿åŠ¨æ¢å¤ç»“æž„ï¼ˆSfM, Structure from Motionï¼‰</strong>ã€‚é€šè¿‡å•å°ç›¸æœºåœ¨ä¸åŒä½ç½®æ‹æ‘„çš„å¤šå¼ ç…§ç‰‡ï¼Œè®¡ç®—ç›¸æœºä½å§¿å˜åŒ–å¹¶é‡å»ºåœºæ™¯ã€‚</p>\n<p><strong>éš¾ç‚¹</strong>ï¼š å­˜åœ¨<strong>å°ºåº¦ä¸ç¡®å®šæ€§</strong>ï¼ˆä¸çŸ¥é“ç‰©ä½“åˆ°åº•æœ‰å¤šå¤§ï¼‰ã€‚</p>\n<p>ç®€å•æ¥è¯´ï¼ŒSfM å°±æ˜¯é€šè¿‡<strong>ç§»åŠ¨çš„ç›¸æœº</strong>æ‹æ‘„çš„ä¸€ç³»åˆ—<strong>2Då›¾åƒ</strong>ï¼Œæ¥æŽ¨ç®—å‡ºç‰©ä½“çš„<strong>3Då½¢çŠ¶</strong>å’Œç›¸æœºçš„<strong>ç©ºé—´è½¨è¿¹</strong>ã€‚</p>\n<p>ä¸‹é¢æˆ‘å°†éžå¸¸è¯¦ç»†åœ°ä¸ºä½ æ‹†è§£ SfM çš„åŽŸç†ã€æ­¥éª¤ä»¥åŠå®ƒèƒŒåŽçš„æ•°å­¦é€»è¾‘ã€‚</p>\n<hr />\n<h4 id=\"ä»€ä¹ˆæ˜¯-sfmstructure-from-motion\">ä»€ä¹ˆæ˜¯ SfMï¼ˆStructure from Motionï¼‰ï¼Ÿ</h4>\n<ul>\n<li><strong>Structureï¼ˆç»“æž„ï¼‰ï¼š</strong> æŒ‡çš„æ˜¯è¢«æ‹æ‘„ç‰©ä½“çš„ä¸‰ç»´åæ ‡ï¼ˆç‚¹äº‘ï¼‰ã€‚</li>\n<li><strong>Motionï¼ˆè¿åŠ¨ï¼‰ï¼š</strong> æŒ‡çš„æ˜¯ç›¸æœºåœ¨æ‹æ‘„æ¯ä¸€å¼ ç…§ç‰‡æ—¶çš„ä½ç½®ï¼ˆä½å§¿ï¼ŒPoseï¼‰ã€‚</li>\n<li><strong>Fromï¼ˆé€šè¿‡...æ¢å¤ï¼‰ï¼š</strong> å¼ºè°ƒäº†å› æžœå…³ç³»ã€‚</li>\n</ul>\n<p><strong>æ ¸å¿ƒæ€æƒ³ï¼š</strong><br />\næƒ³è±¡ä½ å›´ç€ä¸€ä¸ªé›•åƒè½¬åœˆæ‹ç…§ã€‚è™½ç„¶æ¯å¼ ç…§ç‰‡éƒ½æ˜¯å¹³é¢çš„ï¼ˆ2Dï¼‰ï¼Œä½†å½“ä½ ä»Žå·¦è¾¹ç§»åŠ¨åˆ°å³è¾¹æ—¶ï¼Œé›•åƒä¸Šçš„ç‰¹å¾ç‚¹ï¼ˆæ¯”å¦‚é¼»å­å°–ï¼‰åœ¨ç…§ç‰‡é‡Œçš„ä½ç½®ä¼šå‘ç”Ÿç§»åŠ¨ã€‚SfM ç®—æ³•å°±åƒä½ çš„å¤§è„‘ä¸€æ ·ï¼Œé€šè¿‡åˆ†æžè¿™äº›ç‚¹ç§»åŠ¨çš„å¹…åº¦ã€æ–¹å‘å’Œè§„å¾‹ï¼Œåè¿‡æ¥æŽ¨ç®—å‡ºï¼šâ€œå“¦ï¼ŒåŽŸæ¥é¼»å­å°–è·ç¦»æˆ‘è¿™ä¹ˆè¿œï¼Œè€Œæˆ‘åˆšæ‰å‘å³ç§»åŠ¨äº† 50 åŽ˜ç±³ã€‚â€</p>\n<hr />\n<h4 id=\"sfm-çš„å…¨æµç¨‹æžç®€æ­¥éª¤ç‰ˆ\">SfM çš„å…¨æµç¨‹ï¼ˆæžç®€æ­¥éª¤ç‰ˆï¼‰</h4>\n<p>ä¸€ä¸ªæ ‡å‡†çš„ SfM ç³»ç»Ÿé€šå¸¸åŒ…å«ä»¥ä¸‹äº”ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š</p>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></p>\n<h4 id=\"1-ç‰¹å¾æå–ä¸ŽåŒ¹é…-feature-extraction--matching\">1. ç‰¹å¾æå–ä¸ŽåŒ¹é… (Feature Extraction &amp; Matching)</h4>\n<ul>\n<li><strong>åšä»€ä¹ˆï¼š</strong> ç®—æ³•ä¼šåœ¨æ¯ä¸€å¼ ç…§ç‰‡ä¸­å¯»æ‰¾â€œåœ°æ ‡â€â€”â€”é‚£äº›æ— è®ºå…‰ç…§æ€Žä¹ˆå˜ã€è§’åº¦æ€Žä¹ˆå˜éƒ½èƒ½è®¤å‡ºæ¥çš„ç‚¹ï¼ˆé€šå¸¸ä½¿ç”¨ <strong>SIFT</strong> æˆ– <strong>ORB</strong> ç®—æ³•ï¼‰ã€‚</li>\n<li><strong>ç›®çš„ï¼š</strong> ç¡®å®šç¬¬ä¸€å¼ ç…§ç‰‡é‡Œçš„é‚£ä¸ªâ€œç‚¹â€å’Œç¬¬äºŒå¼ ç…§ç‰‡é‡Œçš„å“ªä¸ªâ€œç‚¹â€æ˜¯ç‰©ç†ä¸–ç•Œä¸­çš„åŒä¸€ä¸ªä½ç½®ã€‚</li>\n<li><strong>ç»“æžœï¼š</strong> å¾—åˆ°ä¸€å †é…å¯¹å¥½çš„åƒç´ ç‚¹åæ ‡ã€‚</li>\n</ul>\n<h4 id=\"2-è®¡ç®—å‡ ä½•å…³ç³»-estimating-epipolar-geometry\">2. è®¡ç®—å‡ ä½•å…³ç³» (Estimating Epipolar Geometry)</h4>\n<ul>\n<li><strong>åšä»€ä¹ˆï¼š</strong> åˆ©ç”¨<strong>å¯¹æžå‡ ä½•ï¼ˆEpipolar Geometryï¼‰</strong>ç†è®ºã€‚é€šè¿‡åŒ¹é…çš„ç‚¹å¯¹ï¼Œè®¡ç®—å‡ºä¸¤å¼ ç…§ç‰‡ä¹‹é—´çš„æ•°å­¦å˜æ¢å…³ç³»ï¼Œå³<strong>åŸºç¡€çŸ©é˜µï¼ˆFundamental Matrixï¼‰</strong>æˆ–<strong>æœ¬è´¨çŸ©é˜µï¼ˆEssential Matrixï¼‰</strong>ã€‚</li>\n<li><strong>åŽŸç†ï¼š</strong> å³ä½¿ä¸çŸ¥é“ç‰©ä½“çš„å½¢çŠ¶ï¼Œåªè¦æœ‰è¶³å¤Ÿçš„åŒ¹é…ç‚¹ï¼Œæˆ‘ä»¬å°±èƒ½ç®—å‡ºç›¸æœºä»Žä½ç½® A åˆ°ä½ç½® B åˆ°åº•æ—‹è½¬äº†å¤šå°‘åº¦ï¼Œå¹³ç§»äº†å¤šå°‘è·ç¦»ã€‚</li>\n</ul>\n<h4 id=\"3-ä¸‰è§’æµ‹é‡-triangulation\">3. ä¸‰è§’æµ‹é‡ (Triangulation)</h4>\n<ul>\n<li><strong>åšä»€ä¹ˆï¼š</strong> æœ‰äº†ç›¸æœºçš„ä½å§¿ï¼Œç®—æ³•ä¼šä»Žä¸¤ä¸ªç›¸æœºä¸­å¿ƒå‘åŒä¸€ä¸ªç‰¹å¾ç‚¹å‘å°„ä¸¤æ¡å°„çº¿ã€‚</li>\n<li><strong>åŽŸç†ï¼š</strong> è¿™ä¸¤æ¡å°„çº¿çš„äº¤ç‚¹ï¼Œå°±æ˜¯è¯¥ç‰¹å¾ç‚¹åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„çœŸå®žåæ ‡ï¼ˆ$X, Y, Z$ï¼‰ã€‚</li>\n<li><strong>ç»“æžœï¼š</strong> ç”Ÿæˆäº†æœ€åˆçš„ç¨€ç–ç‚¹äº‘ã€‚</li>\n</ul>\n<h4 id=\"4-å¢žé‡å¼é‡å»º-incremental-reconstruction\">4. å¢žé‡å¼é‡å»º (Incremental Reconstruction)</h4>\n<ul>\n<li><strong>åšä»€ä¹ˆï¼š</strong> ç®—æ³•ä¸ä¼šä¸€æ¬¡æ€§å¤„ç†å‡ ç™¾å¼ ç…§ç‰‡ï¼Œè€Œæ˜¯å…ˆä»Žä¸¤å¼ å¼€å§‹ï¼Œé‡å»ºå‡ºä¸€å°éƒ¨åˆ†ï¼Œç„¶åŽåƒæ‹¼å›¾ä¸€æ ·ï¼Œä¸æ–­åŠ å…¥ç¬¬ä¸‰å¼ ã€ç¬¬å››å¼ ç…§ç‰‡ã€‚</li>\n<li><strong>è¿‡ç¨‹ï¼š</strong> æ¯åŠ å…¥ä¸€å¼ æ–°ç…§ç‰‡ï¼Œå°±åˆ©ç”¨å·²æœ‰çš„ 3D ç‚¹äº‘æ¥åæŽ¨è¿™å¼ æ–°ç…§ç‰‡çš„ç›¸æœºä½ç½®ï¼ˆè¿™å« <strong>PnP</strong> ç®—æ³•ï¼‰ï¼Œç„¶åŽå†æŠŠæ–°çœ‹åˆ°çš„ç‚¹åŠ å…¥ç‚¹äº‘ã€‚</li>\n</ul>\n<h4 id=\"5-å…¨å±€ä¼˜åŒ–å…‰æŸæ³•å¹³å·®-bundle-adjustment-ba\">5. å…¨å±€ä¼˜åŒ–ï¼šå…‰æŸæ³•å¹³å·® (Bundle Adjustment, BA)</h4>\n<ul>\n<li><strong>è¿™æ˜¯ SfM çš„çµé­‚ï¼š</strong> éšç€ç…§ç‰‡è¶Šæ¥è¶Šå¤šï¼Œè¯¯å·®ä¼šç´¯ç§¯ï¼Œç‚¹äº‘ä¼šâ€œé£˜â€æŽ‰æˆ–å˜å½¢ã€‚</li>\n<li><strong>åšä»€ä¹ˆï¼š</strong> BA æ˜¯ä¸€é¡¹å¤§åž‹ä¼˜åŒ–å·¥ç¨‹ã€‚å®ƒä¼šåŒæ—¶è°ƒæ•´<strong>æ‰€æœ‰ç›¸æœºçš„ä½å§¿</strong>å’Œ<strong>æ‰€æœ‰ 3D ç‚¹çš„åæ ‡</strong>ï¼Œä½¿å¾—æ¯ä¸€ä¸ª 3D ç‚¹æŠ•å½±å›žç…§ç‰‡ä¸Šæ—¶ï¼Œä¸Žç…§ç‰‡é‡ŒåŽŸå§‹åƒç´ ç‚¹çš„è·ç¦»ï¼ˆ<strong>é‡æŠ•å½±è¯¯å·®</strong>ï¼‰æœ€å°ã€‚<br />\n<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></li>\n</ul>\n<hr />\n<h4 id=\"ä¸ºä»€ä¹ˆå•ç›®-sfm-æ— æ³•æ„ŸçŸ¥ç»å¯¹å°ºåº¦\">ä¸ºä»€ä¹ˆå•ç›® SfM æ— æ³•æ„ŸçŸ¥â€œç»å¯¹å°ºåº¦â€ï¼Ÿ</h4>\n<p>è¿™æ˜¯å•ç›®é‡å»ºæœ€è‡´å‘½çš„å¼±ç‚¹ï¼Œä¹Ÿæ˜¯é¢è¯•æˆ–å­¦æœ¯è®¨è®ºä¸­å¿…é—®çš„é—®é¢˜ã€‚</p>\n<p><strong>åŽŸç†ï¼š</strong><br />\nåœ¨æ•°å­¦ä¸Šï¼ŒSfM æ¢å¤å‡ºçš„ç©ºé—´ç»“æž„å…·æœ‰<strong>å°ºåº¦ç­‰æ¯”æ€§</strong>ã€‚</p>\n<ul>\n<li>å¦‚æžœä½ æ‹æ‘„ä¸€å¼ æ¡Œå­ä¸Šçš„æ°´æ¯ï¼Œé‡å»ºå‡ºæ¥çš„æ¨¡åž‹å¯èƒ½çœ‹èµ·æ¥å¾ˆå®Œç¾Žã€‚</li>\n<li>ä½†æ˜¯ï¼Œç®—æ³•æ— æ³•åˆ†è¾¨è¿™æ˜¯ä¸€ä¸ª<strong>çœŸå®žçš„é«˜ 10 åŽ˜ç±³çš„æ°´æ¯</strong>ï¼Œè¿˜æ˜¯ä¸€ä¸ª<strong>é«˜ 10 ç±³çš„å·¨åž‹æ°´æ¯æ¨¡åž‹</strong>ã€‚</li>\n</ul>\n<p><strong>åŽŸå› ï¼š</strong><br />\nåœ¨å•ç›®ç›¸æœºçœ‹æ¥ï¼Œä¸€ä¸ªå¾ˆå°çš„ç‰©ä½“ç¦»ç›¸æœºå¾ˆè¿‘ï¼Œå’Œä¸€ä¸ªå·¨å¤§çš„ç‰©ä½“ç¦»ç›¸æœºå¾ˆè¿œï¼Œåœ¨ç…§ç‰‡ä¸Šçš„åƒç´ è¡¨çŽ°æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚é™¤éžä½ é¢å¤–å‘Šè¯‰ç®—æ³•ä¸€ä¸ªå‚è€ƒå€¼ï¼ˆæ¯”å¦‚ï¼šç…§ç‰‡é‡Œé‚£ä¸ªç¡¬å¸ç›´å¾„æ˜¯ 25mmï¼Œæˆ–è€…ä¸¤ä¸ªç›¸æœºå…¶å®žè·ç¦» 10cmï¼‰ï¼Œå¦åˆ™å®ƒåªèƒ½å‘Šè¯‰ä½ ï¼šâ€œA ç‚¹åˆ° B ç‚¹çš„è·ç¦»æ˜¯ C ç‚¹åˆ° D ç‚¹è·ç¦»çš„ 2 å€â€ï¼Œè€Œä¸èƒ½å‘Šè¯‰ä½ åˆ°åº•æ˜¯å¤šå°‘ç±³ã€‚</p>\n<hr />\n<h4 id=\"sfm-çš„ä¸¤ç§ä¸»æµæµæ´¾\">SfM çš„ä¸¤ç§ä¸»æµæµæ´¾</h4>\n<ol>\n<li>\n<p><strong>å¢žé‡å¼ SfM (Incremental SfM)ï¼š</strong></p>\n<ul>\n<li>ä»£è¡¨ä½œï¼š<strong>COLMAP</strong>ï¼ˆç›®å‰å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæœ€å¸¸ç”¨çš„å¼€æºå·¥å…·ï¼‰ã€‚</li>\n<li>ä¼˜ç‚¹ï¼šéžå¸¸é²æ£’ï¼Œç²¾åº¦é«˜ã€‚</li>\n<li>ç¼ºç‚¹ï¼šæ…¢ï¼Œç…§ç‰‡å¤šäº†ä¹‹åŽä¼˜åŒ–éžå¸¸è€—æ—¶ã€‚<br />\n<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></li>\n</ul>\n</li>\n<li>\n<p><strong>å…¨å±€å¼ SfM (Global SfM)ï¼š</strong></p>\n<ul>\n<li>ä»£è¡¨ä½œï¼š<strong>TheiaSfM</strong>ã€‚</li>\n<li>ä¼˜ç‚¹ï¼šé€Ÿåº¦æžå¿«ï¼Œé€‚åˆå¤§è§„æ¨¡åœºæ™¯ã€‚</li>\n<li>ç¼ºç‚¹ï¼šå¯¹å¼‚å¸¸åŒ¹é…ç‚¹éžå¸¸æ•æ„Ÿï¼Œå®¹æ˜“å´©æºƒã€‚<br />\n<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></li>\n</ul>\n</li>\n</ol>\n<hr />\n<h4 id=\"å‚è€ƒæ–‡ä»¶\">å‚è€ƒæ–‡ä»¶</h4>\n<p>è‹¥è¦æ·±å…¥ç ”ç©¶ SfM çš„åº•å±‚æ•°å­¦ï¼Œä»¥ä¸‹æ–‡çŒ®æ˜¯å¿…è¯»çš„ï¼š</p>\n<ol>\n<li><strong>Agarwal, S., et al. (2011).</strong> <em>Building Rome in a Day</em>. Communications of the ACM.<br />\n(è¿™æ˜¯ SfM åŽ†å²ä¸Šçš„é‡Œç¨‹ç¢‘ï¼Œè¯æ˜Žäº†åˆ©ç”¨äº’è”ç½‘ä¸Šçš„æˆåƒä¸Šä¸‡å¼ ç…§ç‰‡é‡å»ºä¸€åº§åŸŽå¸‚æ˜¯å¯èƒ½çš„)ã€‚</li>\n<li><strong>SchÃ¶nberger, J. L., &amp; Frahm, J. M. (2016).</strong> <em>Structure-from-Motion Revisited</em>. CVPR.<br />\n(è¿™æ˜¯ <strong>COLMAP</strong> çš„æ ¸å¿ƒè®ºæ–‡ï¼Œç›®å‰è¢«å…¬è®¤ä¸ºå¢žé‡å¼ SfM çš„æœ€ä¼˜å®žè·µ)ã€‚</li>\n<li><strong>Triggs, B., et al. (1999).</strong> <em>Bundle Adjustment â€“ A Modern Synthesis</em>. Vision Algorithms: Theory and Practice.<br />\n(æ·±å…¥è®²è§£äº† SfM ä¸­æœ€å…³é”®çš„ä¼˜åŒ–ç®—æ³• BA)ã€‚</li>\n<li><strong>Snavely, N., Seitz, S. M., &amp; Szeliski, R. (2006).</strong> <em>Photo Tourism: Exploring image collections in 3D</em>. SIGGRAPH.<br />\n(é¦–æ¬¡å‘å¤§ä¼—å±•ç¤ºäº†å¦‚ä½•é€šè¿‡éžç»“æž„åŒ–ç…§ç‰‡é›†è¿›è¡Œ 3D å¯¼èˆª)ã€‚</li>\n<li><strong>Longuet-Higgins, H. C. (1981).</strong> <em>A computer algorithm for reconstructing a scene from two projections</em>. Nature.<br />\n(æå‡ºäº†è‘—åçš„å…«ç‚¹ç®—æ³•ï¼Œæ˜¯ SfM å‡ ä½•è®¡ç®—çš„åŸºçŸ³)ã€‚</li>\n</ol>\n<h3 id=\"b-åŒç›®å¤šç›®ç«‹ä½“è§†è§‰stereo-vision\">B. åŒç›®/å¤šç›®ç«‹ä½“è§†è§‰ï¼ˆStereo Visionï¼‰ï¼š</h3>\n<p><strong>åŽŸç†</strong>: æ¨¡æ‹Ÿäººç±»åŒçœ¼ã€‚é€šè¿‡è®¡ç®—å·¦å³ä¸¤ä¸ªç›¸æœºæ‹æ‘„å›¾åƒçš„<strong>è§†å·®ï¼ˆDisparityï¼‰</strong>ï¼Œåˆ©ç”¨ä¸‰è§’æµ‹é‡åŽŸç†è®¡ç®—æ·±åº¦ã€‚</p>\n<p>å¦‚æžœè¯´ SfMï¼ˆè¿åŠ¨æ¢å¤ç»“æž„ï¼‰æ˜¯æ¨¡ä»¿äººç±»â€œè¾¹èµ°è¾¹çœ‹â€æ¥è®¤çŸ¥ç©ºé—´ï¼Œé‚£ä¹ˆ<strong>åŒç›®ç«‹ä½“è§†è§‰ï¼ˆStereo Visionï¼‰</strong>åˆ™æ˜¯ç›´æŽ¥æ¨¡ä»¿äººç±»çš„<strong>åŒçœ¼ç»“æž„</strong>ã€‚</p>\n<p>å®ƒæ˜¯æœºå™¨è§†è§‰ä¸­æœ€ç»å…¸ã€åº”ç”¨æœ€å¹¿æ³›çš„æ·±åº¦æ„ŸçŸ¥æŠ€æœ¯ä¹‹ä¸€ã€‚ä¸‹é¢æˆ‘ä»¬æ·±åº¦æ‹†è§£åŒç›®ç«‹ä½“è§†è§‰çš„åŽŸç†ã€æ•°å­¦é€»è¾‘ä»¥åŠå®žçŽ°è¿‡ç¨‹ã€‚</p>\n<h4 id=\"1-æ ¸å¿ƒå®šä¹‰\">1. æ ¸å¿ƒå®šä¹‰</h4>\n<p>åŒç›®ç«‹ä½“è§†è§‰æ˜¯æŒ‡ä½¿ç”¨ä¸¤å°æˆåƒç‰¹æ€§ç›¸åŒï¼ˆç„¦è·ã€åƒç´ å°ºå¯¸ç­‰ä¸€è‡´ï¼‰çš„ç›¸æœºï¼Œå®‰è£…åœ¨åŒä¸€å¹³é¢ä¸Šï¼Œä¿æŒä¸€å®šçš„æ°´å¹³è·ç¦»ï¼ˆ<strong>åŸºçº¿ï¼ŒBaseline</strong>ï¼‰ï¼Œé€šè¿‡è®¡ç®—å·¦å³å›¾åƒä¸­åŒä¸€ç‰©ä½“åƒç´ ç‚¹çš„ç›¸å¯¹ä½å§¿å·®å¼‚ï¼Œæ¥èŽ·å–ç‰©ä½“ä¸‰ç»´ä¿¡æ¯çš„æ–¹æ³•ã€‚</p>\n<p><strong>ç›´è§‚ç†è§£ï¼š</strong><br />\nä¼¸å‡ºä½ çš„ä¸€æ ¹æ‰‹æŒ‡æ”¾åœ¨çœ¼å‰ï¼Œå…ˆé—­ä¸Šå·¦çœ¼çœ‹ï¼Œå†é—­ä¸Šå³çœ¼çœ‹ã€‚ä½ ä¼šå‘çŽ°æ‰‹æŒ‡ç›¸å¯¹äºŽèƒŒæ™¯çš„ä½ç½®å‘ç”Ÿäº†â€œè·³åŠ¨â€ã€‚</p>\n<ul>\n<li>æ‰‹æŒ‡ç¦»çœ¼ç›è¶Šè¿‘ï¼Œè·³åŠ¨å¹…åº¦ï¼ˆä½å§¿å·®å¼‚ï¼‰è¶Šå¤§ã€‚</li>\n<li>æ‰‹æŒ‡ç¦»çœ¼ç›è¶Šè¿œï¼Œè·³åŠ¨å¹…åº¦è¶Šå°ã€‚</li>\n<li>è¿™ç§è·³åŠ¨çš„ä½ç§»é‡ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­è¢«ç§°ä¸º<strong>è§†å·®ï¼ˆDisparityï¼‰</strong>ã€‚</li>\n</ul>\n<hr />\n<h4 id=\"2-æ ¸å¿ƒæ•°å­¦åŽŸç†ä¸‰è§’æµ‹é‡triangulation\">2. æ ¸å¿ƒæ•°å­¦åŽŸç†ï¼šä¸‰è§’æµ‹é‡ï¼ˆTriangulationï¼‰</h4>\n<p>åŒç›®è§†è§‰çš„ç²¾é«“å¯ä»¥ç”¨ä¸€ä¸ªæžå…¶ç®€å•çš„å‡ ä½•å…¬å¼æ¥æ¦‚æ‹¬ã€‚</p>\n<p>å‡è®¾ä¸¤å°ç›¸æœºå®Œå…¨å¹³è¡Œï¼Œç„¦è·ä¸º $f$ï¼Œä¸¤ç›¸æœºä¸­å¿ƒè·ç¦»ï¼ˆåŸºçº¿ï¼‰ä¸º $B$ã€‚ç©ºé—´ä¸­ä¸€ç‚¹ $P$ åœ¨å·¦ç›¸æœºæˆåƒé¢ä¸Šçš„æ¨ªåæ ‡æ˜¯ $x_L$ï¼Œåœ¨å³ç›¸æœºæˆåƒé¢ä¸Šçš„æ¨ªåæ ‡æ˜¯ $x_R$ã€‚</p>\n<ol>\n<li><strong>è§†å·®ï¼ˆDisparityï¼‰å®šä¹‰ï¼š</strong> $d = x_L - x_R$</li>\n<li><strong>æ·±åº¦ï¼ˆDepthï¼‰è®¡ç®—å…¬å¼ï¼š</strong><br />\n$$Z = \\frac{f \\cdot B}{d}$$</li>\n</ol>\n<p><strong>è¿™ä¸ªå…¬å¼å‘Šè¯‰æˆ‘ä»¬ä¸¤ä¸ªå…³é”®ä¿¡æ¯ï¼š</strong></p>\n<ul>\n<li><strong>åæ¯”å…³ç³»ï¼š</strong> æ·±åº¦ $Z$ ä¸Žè§†å·® $d$ æˆåæ¯”ã€‚è§†å·®è¶Šå¤§ï¼Œç‰©ä½“è¶Šè¿‘ã€‚</li>\n<li><strong>å°ºåº¦ç¡®å®šæ€§ï¼š</strong> ä¸Žå•ç›® SfM ä¸åŒï¼Œç”±äºŽåŸºçº¿ $B$ æ˜¯æå‰æµ‹é‡å¥½çš„ç‰©ç†å¸¸æ•°ï¼ˆæ¯”å¦‚ä¸¤ç›¸æœºé—´è· 6 åŽ˜ç±³ï¼‰ï¼Œæ‰€ä»¥åŒç›®è§†è§‰å¯ä»¥ç›´æŽ¥è®¡ç®—å‡ºç‰©ä½“çš„<strong>çœŸå®žç‰©ç†è·ç¦»</strong>ï¼ˆç±³æˆ–æ¯«ç±³ï¼‰ï¼Œä¸å­˜åœ¨å°ºåº¦ä¸ç¡®å®šæ€§ã€‚</li>\n</ul>\n<hr />\n<h4 id=\"3-æ ‡å‡†ç®—æ³•æµç¨‹stereo-pipeline\">3. æ ‡å‡†ç®—æ³•æµç¨‹ï¼ˆStereo Pipelineï¼‰</h4>\n<p>å®žçŽ°é«˜è´¨é‡çš„åŒç›®é‡å»ºé€šå¸¸éœ€è¦ç»è¿‡ä»¥ä¸‹å››ä¸ªä¸¥è°¨çš„æ­¥éª¤ï¼š</p>\n<p><strong>ç¬¬ä¸€æ­¥ï¼šç›¸æœºæ ‡å®šï¼ˆCamera Calibrationï¼‰</strong></p>\n<ul>\n<li><strong>ç›®çš„ï¼š</strong> ç¡®å®šç›¸æœºçš„å†…å‚ï¼ˆç„¦è·ã€ä¸­å¿ƒç‚¹ï¼‰å’Œå¤–å‚ï¼ˆä¸¤ç›¸æœºä¹‹é—´çš„ç²¾ç¡®æ—‹è½¬å’Œå¹³ç§»å…³ç³»ï¼‰ã€‚</li>\n<li><strong>æ„ä¹‰ï¼š</strong> å¦‚æžœç›¸æœºé•œå¤´æœ‰ç•¸å˜ï¼ˆæ¯”å¦‚é±¼çœ¼æ•ˆæžœï¼‰ï¼Œæˆ–è€…ä¸¤ä¸ªç›¸æœºæ²¡è£…æ­£ï¼ŒåŽç»­è®¡ç®—å…¨æ˜¯é”™çš„ã€‚</li>\n</ul>\n<p><strong>ç¬¬äºŒæ­¥ï¼šç«‹ä½“æ ¡æ­£ï¼ˆStereo Rectificationï¼‰â€”â€” æžå…³é”®</strong></p>\n<ul>\n<li><strong>åšä»€ä¹ˆï¼š</strong> é€šè¿‡æ•°å­¦å˜æ¢ï¼ŒæŠŠæ‹æ‘„æ—¶çš„ä¸¤å¼ ç…§ç‰‡å¯¹é½ï¼Œä½¿å¾—å·¦å³å›¾åƒçš„<strong>æžçº¿ï¼ˆEpipolar Lineï¼‰å¤„äºŽåŒä¸€æ°´å¹³çº¿ä¸Š</strong>ã€‚</li>\n<li><strong>æ„ä¹‰ï¼š</strong> æ ¡æ­£åŽï¼Œå·¦å›¾ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œåœ¨å³å›¾ä¸­ä¸€å®šèƒ½åœ¨<strong>åŒä¸€è¡Œ</strong>æ‰¾åˆ°ã€‚è¿™æŠŠä¸€ä¸ª 2D çš„æœç´¢é—®é¢˜é™ç»´æˆäº† 1D æœç´¢ï¼Œæžå¤§åœ°æé«˜äº†è¿ç®—é€Ÿåº¦ã€‚</li>\n</ul>\n<p><strong>ç¬¬ä¸‰æ­¥ï¼šç«‹ä½“åŒ¹é…ï¼ˆStereo Matchingï¼‰â€”â€” æœ€æ ¸å¿ƒã€æœ€éš¾</strong></p>\n<ul>\n<li><strong>åšä»€ä¹ˆï¼š</strong> ç®—æ³•è¦åœ¨å³å›¾çš„åŒä¸€è¡Œåƒç´ é‡Œï¼Œæ‰¾åˆ°é‚£ä¸ªå’Œå·¦å›¾ç‚¹â€œé•¿å¾—æœ€åƒâ€çš„ç‚¹ã€‚</li>\n<li><strong>ä¸»æµç®—æ³•ï¼š</strong>\n<ul>\n<li><strong>BM (Block Matching)ï¼š</strong> å¿«ï¼Œä½†ç²¾åº¦ä½Žï¼Œè¾¹ç¼˜ç²—ç³™ã€‚</li>\n<li><strong>SGM (Semi-Global Matching)ï¼š</strong> å·¥ä¸šç•Œçš„ä¸»æµï¼Œåœ¨é€Ÿåº¦å’Œç²¾åº¦é—´å–å¾—äº†æžä½³å¹³è¡¡ã€‚</li>\n<li><strong>æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼š</strong> å¦‚ PSMNetï¼Œåˆ©ç”¨ç¥žç»ç½‘ç»œç›´æŽ¥è¾“å‡ºè§†å·®å›¾ï¼Œåœ¨å¤„ç†æ— çº¹ç†åŒºåŸŸè¡¨çŽ°æ›´å¥½ã€‚</li>\n</ul>\n</li>\n</ul>\n<p><strong>ç¬¬å››æ­¥ï¼šè§†å·®è½¬æ·±åº¦ï¼ˆDisparity to Depthï¼‰</strong></p>\n<ul>\n<li><strong>åšä»€ä¹ˆï¼š</strong> åˆ©ç”¨å‰é¢æåˆ°çš„å…¬å¼ $Z = (f \\cdot B) / d$ï¼Œå°†åƒç´ çº§çš„è§†å·®å›¾è½¬åŒ–ä¸ºç‰©ç†ä¸–ç•Œçš„æ·±åº¦å›¾ã€‚</li>\n</ul>\n<hr />\n<h4 id=\"4-åŒç›®è§†è§‰-vs-å•ç›®-sfm-çš„åŒºåˆ«\">4. åŒç›®è§†è§‰ vs. å•ç›® SfM çš„åŒºåˆ«</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">ç‰¹æ€§</th>\n<th style=\"text-align: left;\">åŒç›®ç«‹ä½“è§†è§‰ (Stereo)</th>\n<th style=\"text-align: left;\">å•ç›® SfM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>ç›¸æœºæ•°é‡</strong></td>\n<td style=\"text-align: left;\">2å°æˆ–å¤šå°</td>\n<td style=\"text-align: left;\">1å°</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>æ‹æ‘„çŠ¶æ€</strong></td>\n<td style=\"text-align: left;\">å¯ä»¥é™æ€æ‹æ‘„ï¼ˆçž¬é—´æˆåƒï¼‰</td>\n<td style=\"text-align: left;\">å¿…é¡»ç§»åŠ¨ç›¸æœºï¼ˆåŠ¨æ€æ‹æ‘„ï¼‰</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>å°ºåº¦æ„ŸçŸ¥</strong></td>\n<td style=\"text-align: left;\">è‡ªåŠ¨èŽ·å¾—çœŸå®žç‰©ç†å°ºåº¦</td>\n<td style=\"text-align: left;\">æ— æ³•èŽ·å¾—çœŸå®žå°ºåº¦ï¼ˆä»…æœ‰æ¯”ä¾‹ï¼‰</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>å®žæ—¶æ€§</strong></td>\n<td style=\"text-align: left;\">é«˜ï¼ˆé€‚åˆé¿éšœã€æœºå™¨äººï¼‰</td>\n<td style=\"text-align: left;\">ä½Žï¼ˆè®¡ç®—é‡å¤§ï¼Œéœ€è¿åŠ¨åºåˆ—ï¼‰</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>é€‚ç”¨èŒƒå›´</strong></td>\n<td style=\"text-align: left;\">é€‚åˆè¿‘è·ç¦»ã€é«˜ç²¾åº¦æ„ŸçŸ¥</td>\n<td style=\"text-align: left;\">é€‚åˆå¤§åœºæ™¯ã€è¿œè·ç¦»å»ºæ¨¡</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h4 id=\"5-é€‚ç”¨åœºæ™¯ä¸Žå±€é™æ€§\">5. é€‚ç”¨åœºæ™¯ä¸Žå±€é™æ€§</h4>\n<p><strong>é€‚ç”¨åœºæ™¯ï¼š</strong></p>\n<ul>\n<li><strong>è½¦è½½è¾…åŠ©é©¾é©¶ï¼ˆADASï¼‰ï¼š</strong> è¯†åˆ«å‰æ–¹éšœç¢ç‰©è·ç¦»ã€‚</li>\n<li><strong>å·¥ä¸šæŠ“å–ï¼š</strong> æœºæ¢°è‡‚è¯†åˆ«é›¶ä»¶çš„ 3D ä½ç½®ã€‚</li>\n<li><strong>æ— äººæœºé¿éšœï¼š</strong> å®žæ—¶æ„ŸçŸ¥å‘¨å›´å¢™ä½“è·ç¦»ã€‚</li>\n</ul>\n<p><strong>å±€é™æ€§ï¼š</strong></p>\n<ol>\n<li><strong>çŽ¯å¢ƒçº¹ç†ä¾èµ–ï¼š</strong> å¦‚æžœå¢™é¢æ˜¯çº¯ç™½è‰²çš„ï¼Œç®—æ³•æ— æ³•åœ¨å³å›¾ä¸­æ‰¾åˆ°åŒ¹é…çš„ç‚¹ï¼ˆå› ä¸ºçœ‹èµ·æ¥éƒ½ä¸€æ ·ï¼‰ï¼Œä¼šå¯¼è‡´æ·±åº¦ä¸¢å¤±ã€‚</li>\n<li><strong>åŸºçº¿é™åˆ¶ï¼š</strong> æµ‹é‡è·ç¦»å—é™äºŽåŸºçº¿ $B$ã€‚å¦‚æžœè¦æµ‹ 100 ç±³å¤–çš„ç‰©ä½“ï¼ŒåŸºçº¿å¿…é¡»è¶³å¤Ÿå®½ï¼Œå¦åˆ™è§†å·® $d$ å¤ªå°ï¼Œä¼šå¯¼è‡´è¯¯å·®å‰§å¢žã€‚</li>\n<li><strong>é®æŒ¡é—®é¢˜ï¼š</strong> å·¦çœ¼çœ‹å¾—åˆ°çš„åœ°æ–¹ï¼Œå³çœ¼å¯èƒ½è¢«æŒ¡ä½äº†ï¼ˆæš—å½±åŒºï¼‰ï¼Œå¯¼è‡´æ— æ³•è®¡ç®—ã€‚</li>\n</ol>\n<hr />\n<h4 id=\"6-å‚è€ƒèµ„æ–™ä¸Žç»å…¸æ–‡çŒ®\">6. å‚è€ƒèµ„æ–™ä¸Žç»å…¸æ–‡çŒ®</h4>\n<ol>\n<li><strong>Scharstein, D., &amp; Szeliski, R. (2002).</strong> <em>A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</em>. International Journal of Computer Vision.<br />\n(ç«‹ä½“åŒ¹é…é¢†åŸŸçš„åŸºçŸ³è®ºæ–‡ï¼Œå®šä¹‰äº†è¯„ä¼°æ ‡å‡†)ã€‚</li>\n<li><strong>Hirschmuller, H. (2005).</strong> <em>Accurate and efficient stereo processing by semi-global matching and mutual information</em>. CVPR.<br />\n(æå‡ºäº†è‘—åçš„ <strong>SGM ç®—æ³•</strong>ï¼Œè‡³ä»Šä»æ˜¯è¯¥é¢†åŸŸçš„å·¥ä¸šæ ‡å‡†)ã€‚</li>\n<li><strong>Marr, D., &amp; Poggio, T. (1979).</strong> <em>A computational theory of human stereo vision</em>. Proceedings of the Royal Society of London.<br />\n(ä»Žç”Ÿç‰©å­¦å’Œè®¡ç®—è§†è§‰è§’åº¦è§£é‡Šäº†äººç±»åŒçœ¼å¦‚ä½•æ„ŸçŸ¥æ·±åº¦)ã€‚</li>\n<li><strong>Konolige, K. (1998).</strong> <em>Small Vision Systems: Hardware and Implementation</em>. International Symposium on Robotics Research.<br />\n(è®²è§£äº†å®žæ—¶åŒç›®è§†è§‰ç³»ç»Ÿçš„æ—©æœŸç¡¬ä»¶å®žçŽ°)ã€‚</li>\n<li><strong>Zhang, Z. (2000).</strong> <em>A flexible new technique for camera calibration</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence.<br />\n(è‘—åçš„â€œå¼ æ°æ ‡å®šæ³•â€ï¼ŒåŒç›®ç›¸æœºæ ‡å®šçš„å¿…å¤‡åŸºç¡€)ã€‚</li>\n</ol>\n<h3 id=\"c-ç¥žç»è¾å°„åœºnerf-neural-radiance-fields\">C. ç¥žç»è¾å°„åœºï¼ˆNeRF, Neural Radiance Fieldsï¼‰</h3>\n<pre><code>*   **å‰æ²¿ï¼š** 2020å¹´åŽçš„çˆ†å‘æ€§æŠ€æœ¯ã€‚åˆ©ç”¨æ·±åº¦å­¦ä¹ å°†åœºæ™¯è¡¨ç¤ºä¸ºä¸€ä¸ªè¿žç»­çš„ä½“ç§¯åœºï¼Œé€šè¿‡ä½“æ¸²æŸ“æŠ€æœ¯ç”Ÿæˆæžå…¶é€¼çœŸçš„3Dè§†å›¾ã€‚\n</code></pre>\n<p>å¦‚æžœè¯´ <strong>SfM</strong> æ˜¯åœ¨åšâ€œå‡ ä½•é¢˜â€ï¼Œ<strong>åŒç›®è§†è§‰</strong>æ˜¯åœ¨åšâ€œç‰©ç†é¢˜â€ï¼Œé‚£ä¹ˆ <strong>NeRFï¼ˆNeural Radiance Fieldsï¼Œç¥žç»è¾å°„åœºï¼‰</strong> å°±æ˜¯åœ¨ç”¨â€œäººå·¥æ™ºèƒ½ç®—æ³•â€è¿›è¡Œä¸€æ¬¡é©å‘½æ€§çš„<strong>è‰ºæœ¯åˆ›ä½œ</strong>ã€‚</p>\n<p>NeRF æ˜¯ 2020 å¹´è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€å…·é¢ è¦†æ€§çš„æŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒå½»åº•æ”¹å˜äº†æˆ‘ä»¬å­˜å‚¨å’Œè¡¨çŽ° 3D ä¸–ç•Œçš„æ–¹å¼ã€‚</p>\n<hr />\n<h4 id=\"1-ä»€ä¹ˆæ˜¯-nerf\">1. ä»€ä¹ˆæ˜¯ NeRFï¼Ÿ</h4>\n<p><strong>å®šä¹‰ï¼š</strong><br />\nNeRF æ˜¯ä¸€ç§åˆ©ç”¨<strong>æ·±åº¦ç¥žç»ç½‘ç»œï¼ˆé€šå¸¸æ˜¯å¤šå±‚æ„ŸçŸ¥æœº MLPï¼‰</strong>æ¥éšå¼åœ°è¡¨ç¤º 3D åœºæ™¯çš„æŠ€æœ¯ã€‚å®ƒä¸å­˜å‚¨ç‚¹äº‘ï¼Œä¹Ÿä¸å­˜å‚¨ä¸‰è§’é¢ç‰‡ï¼Œè€Œæ˜¯å°†æ•´ä¸ª 3D åœºæ™¯ç¼–ç è¿›ä¸€ä¸ªç¥žç»ç½‘ç»œçš„<strong>æƒé‡</strong>é‡Œã€‚</p>\n<p><strong>æ ¸å¿ƒç›´è§‰ï¼š</strong><br />\næƒ³è±¡ç©ºé—´ä¸­å……æ»¡äº†â€œå¸¦é¢œè‰²çš„çƒŸé›¾â€ã€‚åœ¨æ¯ä¸€ä¸ªåæ ‡ç‚¹ä¸Šï¼ŒçƒŸé›¾éƒ½æœ‰ç‰¹å®šçš„é¢œè‰²å’Œæµ“åº¦ã€‚NeRF çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ª AI åŠ©æ‰‹ï¼Œå½“ä½ é—®å®ƒï¼šâ€œåœ¨åæ ‡ $(x, y, z)$ è¿™ä¸ªç‚¹ï¼Œä»Ž $(\\theta, \\phi)$ è¿™ä¸ªè§’åº¦çœ‹è¿‡åŽ»æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿâ€å®ƒèƒ½ç«‹åˆ»è®¡ç®—å‡ºç»“æžœã€‚</p>\n<hr />\n<h4 id=\"2-nerf-çš„æ ¸å¿ƒåŽŸç†5d-å‡½æ•°\">2. NeRF çš„æ ¸å¿ƒåŽŸç†ï¼š5D å‡½æ•°</h4>\n<p>NeRF å°†åœºæ™¯è¡¨ç¤ºä¸ºä¸€ä¸ªè¿žç»­çš„ <strong>5D å‡½æ•°</strong>ï¼š</p>\n<ul>\n<li><strong>è¾“å…¥ (5ä¸ªå‚æ•°)ï¼š</strong> ç©ºé—´ä½ç½® $(x, y, z)$ + è§‚å¯Ÿè§†è§’ $(\\theta, \\phi)$ã€‚</li>\n<li><strong>è¾“å‡º (4ä¸ªå‚æ•°)ï¼š</strong> è¯¥ç‚¹çš„é¢œè‰² $RGB$ + è¯¥ç‚¹çš„ä½“ç§¯å¯†åº¦ $\\sigma$ï¼ˆå³å…‰çº¿é€šè¿‡è¿™é‡Œçš„é˜»åŠ›ï¼Œä»£è¡¨ç‰©ä½“æ˜¯å¦å­˜åœ¨ï¼‰ã€‚<br />\n<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></li>\n</ul>\n<ol>\n<li><strong>éšå¼è¡¨ç¤ºï¼ˆImplicit Representationï¼‰ï¼š</strong><br />\nä¼ ç»Ÿçš„ 3D æ¨¡åž‹åƒâ€œä¹é«˜ç§¯æœ¨â€ï¼ˆä½“ç´ ï¼‰æˆ–â€œæŠ˜çº¸â€ï¼ˆç½‘æ ¼ï¼‰ã€‚NeRF åƒâ€œä¸€æ®µä»£ç â€ï¼Œä½ ç»™å®ƒåæ ‡ï¼Œå®ƒåå‡ºé¢œè‰²ã€‚è¿™ä½¿å¾—å®ƒèƒ½è¡¨çŽ°æžå…¶ç²¾ç»†çš„ç»†èŠ‚ï¼Œè€Œä¸éœ€è¦æµ·é‡çš„å­˜å‚¨ç©ºé—´ã€‚</li>\n<li><strong>ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼š</strong><br />\nè¿™æ˜¯ NeRF èƒ½ç«çš„å…³é”®ã€‚ç¥žç»ç½‘ç»œå¤©ç”Ÿå€¾å‘äºŽå­¦ä¹ â€œå¹³æ»‘â€çš„ä¸œè¥¿ï¼Œå®¹æ˜“æŠŠå›¾åƒå˜æ¨¡ç³Šã€‚NeRF é€šè¿‡æŠŠåæ ‡è½¬æ¢æˆé«˜é¢‘æ­£å¼¦/ä½™å¼¦æ³¢ï¼Œå¼ºåˆ¶ç½‘ç»œè®°ä½ç»†èŠ‚ï¼ˆå¦‚ç‰©ä½“çš„è¾¹ç¼˜ã€ç»†å°çš„çº¹ç†ï¼‰ã€‚</li>\n<li><strong>å¯å¾®ä½“æ¸²æŸ“ï¼ˆDifferentiable Volume Renderingï¼‰ï¼š</strong><br />\nè¿™æ˜¯ NeRF çš„â€œæ¸²æŸ“å¼•æ“Žâ€ã€‚å®ƒæ²¿ç€ç›¸æœºå°„å‡ºçš„ä¸€æ¡å…‰çº¿è¿›è¡Œé‡‡æ ·ï¼ŒæŠŠè·¯å¾„ä¸Šæ‰€æœ‰ç‚¹çš„é¢œè‰²å’Œå¯†åº¦åŠ æƒç´¯åŠ ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªåƒç´ çš„é¢œè‰²ã€‚å› ä¸ºæ•´ä¸ªè¿‡ç¨‹æ˜¯<strong>æ•°å­¦å¯å¯¼</strong>çš„ï¼Œæ‰€ä»¥å¯ä»¥é€šè¿‡å¯¹æ¯”ç”Ÿæˆçš„å›¾åƒå’ŒçœŸå®žç…§ç‰‡çš„å·®å¼‚ï¼Œåˆ©ç”¨<strong>åå‘ä¼ æ’­</strong>æ¥è®­ç»ƒç½‘ç»œã€‚</li>\n</ol>\n<hr />\n<h4 id=\"3-nerf-çš„å·¥ä½œæµç¨‹\">3. NeRF çš„å·¥ä½œæµç¨‹</h4>\n<ol>\n<li><strong>å‡†å¤‡æ•°æ®ï¼š</strong> æ‹æ‘„å‡ åå¼ åˆ°ä¸Šç™¾å¼ ç‰©ä½“çš„ç…§ç‰‡ï¼Œå¹¶åˆ©ç”¨ <strong>SfM (å¦‚ COLMAP)</strong> ç®—å‡ºæ¯å¼ ç…§ç‰‡æ‹æ‘„æ—¶çš„ç²¾ç¡®ç›¸æœºä½ç½®ã€‚</li>\n<li><strong>å…‰çº¿æŠ•å°„ï¼š</strong> å¯¹äºŽå›¾åƒä¸Šçš„æ¯ä¸ªåƒç´ ï¼Œä»Žç›¸æœºä¸­å¿ƒå°„å‡ºä¸€æ¡ç©¿è¿‡è¯¥åƒç´ çš„å…‰çº¿ã€‚</li>\n<li><strong>é‡‡æ ·ä¸ŽæŸ¥è¯¢ï¼š</strong> åœ¨è¿™æ¡å…‰çº¿ä¸Šé‡‡æ ·æˆç™¾ä¸Šåƒä¸ªç‚¹ï¼ŒæŠŠè¿™äº›ç‚¹çš„åæ ‡ä¸¢è¿›ç¥žç»ç½‘ç»œé‡Œã€‚</li>\n<li><strong>åˆæˆé¢œè‰²ï¼š</strong> ç¥žç»ç½‘ç»œè¾“å‡ºè¿™äº›ç‚¹çš„é¢œè‰²å’Œå¯†åº¦ï¼Œé€šè¿‡ä½“æ¸²æŸ“å…¬å¼æŠŠå®ƒä»¬â€œæåˆâ€æˆä¸€ä¸ªåƒç´ é¢œè‰²ã€‚</li>\n<li><strong>ä¼˜åŒ–æƒé‡ï¼š</strong> æ¯”è¾ƒ AI ç”Ÿæˆçš„åƒç´ å’Œç…§ç‰‡é‡Œçš„çœŸå®žåƒç´ ã€‚å¦‚æžœä¸ä¸€æ ·ï¼Œå°±è°ƒæ•´ç¥žç»ç½‘ç»œçš„å‚æ•°ã€‚é‡å¤æˆåƒä¸Šä¸‡æ¬¡ï¼Œç›´åˆ°ç½‘ç»œèƒ½å®Œç¾Žé¢„æµ‹å‡ºä»»ä½•è§’åº¦çš„ç”»é¢ã€‚</li>\n</ol>\n<hr />\n<h4 id=\"4-ä¸ºä»€ä¹ˆ-nerf-æ•ˆæžœè¿™ä¹ˆéœ‡æ’¼\">4. ä¸ºä»€ä¹ˆ NeRF æ•ˆæžœè¿™ä¹ˆéœ‡æ’¼ï¼Ÿ</h4>\n<ul>\n<li><strong>ç…§ç‰‡çº§çš„çœŸå®žæ„Ÿï¼š</strong> å®ƒèƒ½å®Œç¾Žå¤„ç†å…‰æ³½è¡¨é¢ï¼ˆå¦‚é‡‘å±žçš„åå…‰ï¼‰ã€é€æ˜Žç‰©ä½“ï¼ˆå¦‚çŽ»ç’ƒæ¯ï¼‰ä»¥åŠæžå…¶å¤æ‚çš„å‡ ä½•ç»“æž„ï¼ˆå¦‚å¤´å‘ã€æ ‘å¶ï¼‰ï¼Œè¿™äº›éƒ½æ˜¯ä¼ ç»Ÿç‚¹äº‘æˆ– Mesh çš„å™©æ¢¦ã€‚</li>\n<li><strong>æ— é™çš„åˆ†è¾¨çŽ‡ï¼š</strong> å› ä¸ºæ˜¯è¿žç»­å‡½æ•°ï¼Œç†è®ºä¸Šä½ å¯ä»¥æ— é™æ”¾å¤§åœºæ™¯ã€‚</li>\n<li><strong>è§†è§’æ’å€¼ï¼š</strong> ä½ åªæ‹äº† 20 å¼ ç…§ç‰‡ï¼Œä½† NeRF å¯ä»¥ç”Ÿæˆç›¸æœºä»Žæœªç»è¿‡çš„ä½ç½®çš„å¹³æ»‘è§†é¢‘ï¼Œæ•ˆæžœå°±åƒåœ¨å¥½èŽ±åžç”µå½±é‡Œç©¿æ¢­ä¸€æ ·ã€‚</li>\n</ul>\n<hr />\n<h4 id=\"5-å±€é™æ€§ä¸Žç›®å‰çš„ç“¶é¢ˆ\">5. å±€é™æ€§ä¸Žç›®å‰çš„ç“¶é¢ˆ</h4>\n<p>å°½ç®¡æ•ˆæžœæƒŠè‰³ï¼Œä½†åŽŸç”Ÿ NeRF å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š</p>\n<ol>\n<li><strong>è®­ç»ƒæžæ…¢ï¼š</strong> 2020 å¹´åˆšå‡ºæ¥æ—¶ï¼Œè®­ç»ƒä¸€ä¸ªåœºæ™¯éœ€è¦å‡ å°æ—¶ç”šè‡³å‡ å¤©ã€‚</li>\n<li><strong>æ¸²æŸ“ï¼ˆæŽ¨ç†ï¼‰æžæ…¢ï¼š</strong> ç”Ÿæˆä¸€å¼ å›¾éœ€è¦å‡ ç§’é’Ÿï¼Œæ— æ³•åšåˆ°å®žæ—¶äº’åŠ¨ã€‚</li>\n<li><strong>åœºæ™¯æ˜¯æ­»æ¿çš„ï¼š</strong> åŽŸç”Ÿ NeRF åªèƒ½é‡å»ºé™æ€åœºæ™¯ï¼Œå¦‚æžœç”»é¢é‡Œæœ‰ä¸œè¥¿åœ¨åŠ¨ï¼Œæˆ–è€…å…‰çº¿å˜äº†ï¼Œæ¨¡åž‹å°±ä¼šå´©æºƒã€‚</li>\n<li><strong>éœ€è¦ç›¸æœºä½å§¿ï¼š</strong> å®ƒæžå…¶ä¾èµ– SfM æä¾›çš„ç²¾ç¡®åæ ‡ï¼Œå¦‚æžœç…§ç‰‡ä½å§¿ç®—é”™äº†ï¼ŒNeRF å‡ºæ¥çš„ç»“æžœä¼šæ˜¯ä¸€å›¢æµ†ç³Šã€‚</li>\n</ol>\n<hr />\n<h4 id=\"6-è¿›åŒ–ä¹‹è·¯nerf-çš„å­å­™ä»¬\">6. è¿›åŒ–ä¹‹è·¯ï¼ˆNeRF çš„å­å­™ä»¬ï¼‰</h4>\n<p>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œè¯žç”Ÿäº†ä¸€ç³»åˆ—å˜ä½“ï¼š</p>\n<ul>\n<li><strong>Instant-NGP (NVIDIA)ï¼š</strong> å°†è®­ç»ƒæ—¶é—´ä»Žå°æ—¶çº§ç¼©çŸ­åˆ°äº†<strong>ç§’çº§</strong>ã€‚</li>\n<li><strong>Mip-NeRFï¼š</strong> è§£å†³äº†ç‰©ä½“åœ¨è¿œè¿‘åˆ‡æ¢æ—¶çš„é”¯é½¿é—®é¢˜ã€‚</li>\n<li><strong>D-NeRF / NSVFï¼š</strong> å°è¯•å¤„ç†ä¼šåŠ¨çš„ç‰©ä½“ï¼ˆåŠ¨æ€åœºæ™¯ï¼‰ã€‚</li>\n<li><strong>3D Gaussian Splatting (2023)ï¼š</strong> è™½ç„¶ä¸å®Œå…¨æ˜¯ NeRFï¼Œä½†å®ƒå¸å–äº† NeRF çš„æ€æƒ³ï¼Œå®žçŽ°äº†<strong>çœŸæ­£å®žæ—¶</strong>çš„é«˜è´¨é‡æ¸²æŸ“ï¼Œæ˜¯ç›®å‰æœ€ç«çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>\n</ul>\n<hr />\n<h4 id=\"7-å‚è€ƒæ–‡çŒ®\">7. å‚è€ƒæ–‡çŒ®</h4>\n<ol>\n<li><strong>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. (2020).</strong> <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em>.<br />\nECCV. (NeRF çš„å¼€å±±ä¹‹ä½œ)ã€‚</li>\n<li><strong>MÃ¼ller, T., Evans, A., Schied, C., &amp; Keller, A. (2022).</strong> <em>Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</em>. ACM Transactions on Graphics.<br />\n(NVIDIA æå‡ºçš„ Instant-NGPï¼Œè®© NeRF èµ°å‘å®žç”¨åŒ–)ã€‚</li>\n<li><strong>Barron, J. T., et al. (2021).</strong> <em>Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</em>. ICCV.<br />\n(è§£å†³äº†æŠ—é”¯é½¿å’Œå¤šå°ºåº¦é—®é¢˜)ã€‚</li>\n<li><strong>Tancik, M., et al. (2022).</strong> <em>Block-NeRF: Scalable Large Scene Neural View Synthesis</em>. CVPR.<br />\n(è¯æ˜Žäº† NeRF å¯ä»¥ç”¨æ¥é‡å»ºæ•´ä¸ªåŸŽå¸‚è¡—é“)ã€‚</li>\n<li><strong>Martin-Brualla, R., et al. (2021).</strong> <em>NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</em>. CVPR.<br />\n(è§£å†³äº†åˆ©ç”¨äº’è”ç½‘ä¹±ä¸ƒå…«ç³Ÿçš„ç…§ç‰‡è¿›è¡Œé‡å»ºçš„é—®é¢˜)ã€‚</li>\n</ol>\n<h3 id=\"d-3d-gaussian-splatting-3dgs\">D. 3D Gaussian Splatting (3DGS)ï¼š</h3>\n<p><strong>æœ€æ–°ï¼š</strong> 2023å¹´å‡ºçŽ°çš„æŠ€æœ¯ï¼Œé€šè¿‡å¤§é‡3Dé«˜æ–¯æ¤­çƒä½“æ‹Ÿåˆåœºæ™¯ï¼Œå®žçŽ°äº†æ¯”NeRFæ›´å¿«çš„æ¸²æŸ“é€Ÿåº¦å’Œæžé«˜çš„é‡å»ºè´¨é‡ã€‚</p>\n<p>å¦‚æžœè¯´ <strong>NeRF</strong> æ˜¯ 3D é‡å»ºé¢†åŸŸçš„â€œæ•°å­—å¹»è§‰â€ï¼Œé‚£ä¹ˆ <strong>3D Gaussian Splatting (3DGS)</strong> å°±æ˜¯åœ¨ 2023 å¹´æ¨ªç©ºå‡ºä¸–çš„â€œæ•°å­—ç”»ç¬”â€ã€‚</p>\n<p>3DGS çš„å‡ºçŽ°ç›´æŽ¥è§£å†³äº† NeRF æ¸²æŸ“æ…¢ã€è®­ç»ƒä¹…çš„æ ¸å¿ƒç—›ç‚¹ï¼Œå°† 3D é‡å»ºæŽ¨å‘äº†<strong>å®žæ—¶ã€é«˜ç”»è´¨ã€å¯äº¤äº’</strong>çš„æ–°çºªå…ƒã€‚</p>\n<hr />\n<h4 id=\"1-ä»€ä¹ˆæ˜¯-3d-gaussian-splatting-3dgs\">1. ä»€ä¹ˆæ˜¯ 3D Gaussian Splatting (3DGS)ï¼Ÿ</h4>\n<p><strong>å®šä¹‰ï¼š</strong><br />\n3DGS æ˜¯ä¸€ç§åŸºäºŽ<strong>æ˜¾å¼è¾å°„åœºï¼ˆExplicit Radiance Fieldï¼‰</strong>çš„åœºæ™¯è¡¨ç¤ºæŠ€æœ¯ã€‚å®ƒä¸ä½¿ç”¨ç¥žç»ç½‘ç»œæ¥â€œçŒœâ€é¢œè‰²ï¼Œè€Œæ˜¯ç›´æŽ¥åœ¨ç©ºé—´ä¸­æŠ›æ´’æ•°ç™¾ä¸‡ä¸ªå¸¦æœ‰é¢œè‰²ã€å½¢çŠ¶å’Œé€æ˜Žåº¦çš„<strong>3D é«˜æ–¯æ¤­çƒä½“ï¼ˆGaussian Kernelsï¼‰</strong>ï¼Œé€šè¿‡è¿™äº›æ¤­çƒä½“çš„å åŠ æ¥æ‹¼å‡‘å‡ºæ•´ä¸ªä¸–ç•Œã€‚</p>\n<p><strong>å½¢è±¡æ¯”å–»ï¼š</strong></p>\n<ul>\n<li><strong>NeRF</strong> åƒæ˜¯ä¸€ä¸ªè®°å¿†åŠ›è¶…å¼ºçš„ç”»å¸ˆï¼šä½ é—®ä»–ç”»é‡ŒæŸä¸ªåæ ‡æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Œä»–æƒ³ä¸€ä¸‹å‘Šè¯‰ä½ ã€‚</li>\n<li><strong>3DGS</strong> åƒæ˜¯ä¸€ä¸ªè´´çº¸å¤§å¸ˆï¼šä»–ç›´æŽ¥åœ¨é€æ˜Žçš„ç©ºé—´é‡Œè´´äº†æ•°ç™¾ä¸‡å¼ åŠé€æ˜Žçš„å½©è‰²è´´çº¸ï¼ˆæ¤­çƒä½“ï¼‰ï¼Œä½ ä»Žä»»ä½•è§’åº¦çœ‹è¿‡åŽ»ï¼Œè¿™äº›è´´çº¸å åŠ åœ¨ä¸€èµ·å°±æ˜¯ä¸€å¼ å®Œç¾Žçš„ç…§ç‰‡ã€‚</li>\n</ul>\n<hr />\n<h4 id=\"2-æ ¸å¿ƒåŽŸç†3d-é«˜æ–¯æ¤­çƒä½“çš„åŸºå› \">2. æ ¸å¿ƒåŽŸç†ï¼š3D é«˜æ–¯æ¤­çƒä½“çš„â€œåŸºå› â€</h4>\n<p>æ¯ä¸€ä¸ª 3D é«˜æ–¯æ¤­çƒä½“éƒ½åŒ…å«äº†ä¸€ç»„æžå…¶ç²¾ç®€çš„æ•°å­¦å‚æ•°ï¼ˆåŸºå› ï¼‰ï¼š</p>\n<ol>\n<li><strong>ä¸­å¿ƒä½ç½® (Position, $X, Y, Z$)ï¼š</strong> æ¤­çƒä½“åœ¨å“ªã€‚</li>\n<li><strong>åæ–¹å·®çŸ©é˜µ (Covariance, $\\Sigma$)ï¼š</strong> å†³å®šæ¤­çƒä½“çš„<strong>å½¢çŠ¶å’Œæ–¹å‘</strong>ã€‚ä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œå®ƒè¢«åˆ†è§£ä¸ºï¼š\n<ul>\n<li><strong>ç¼©æ”¾ (Scaling)ï¼š</strong> å®ƒæ˜¯é•¿çš„ã€åœ†çš„è¿˜æ˜¯æ‰çš„ã€‚</li>\n<li><strong>æ—‹è½¬ (Rotation)ï¼š</strong> å®ƒæ˜¯æ¨ªç€çš„è¿˜æ˜¯æ–œç€çš„ã€‚</li>\n</ul>\n</li>\n<li><strong>ä¸é€æ˜Žåº¦ (Opacity, $\\alpha$)ï¼š</strong> è¿™ä¸ªæ¤­çƒä½“æœ‰å¤šé€æ˜Žã€‚</li>\n<li><strong>çƒè°å‡½æ•°é¢œè‰² (Spherical Harmonics, SH)ï¼š</strong> è¿™æ˜¯ä¸€ç§ç‰¹æ®Šçš„é¢œè‰²è¡¨ç¤ºæ³•ã€‚å®ƒä¸åªæ˜¯ä¸€ä¸ªå•ä¸€é¢œè‰²ï¼Œè€Œæ˜¯è®°å½•äº†<strong>ä»Žä¸åŒæ–¹å‘çœ‹è¿™ä¸ªç‚¹æ—¶é¢œè‰²çš„å˜åŒ–</strong>ï¼ˆæ¯”å¦‚é‡‘å±žçš„é«˜å…‰ï¼‰ã€‚</li>\n</ol>\n<hr />\n<h4 id=\"3-å·¥ä½œæµç¨‹ä»Žç‚¹äº‘åˆ°ç²¾ç¾Žå»ºæ¨¡\">3. å·¥ä½œæµç¨‹ï¼šä»Žç‚¹äº‘åˆ°ç²¾ç¾Žå»ºæ¨¡</h4>\n<p><strong>ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ– (Initialization)</strong><br />\n3DGS é€šå¸¸ä»¥ <strong>SfM (å¦‚ COLMAP)</strong> äº§ç”Ÿçš„ç¨€ç–ç‚¹äº‘ä½œä¸ºèµ·ç‚¹ã€‚æ¯ä¸ªç‚¹è¢«åˆå§‹åŒ–ä¸ºä¸€ä¸ªå°çš„ 3D é«˜æ–¯çƒã€‚</p>\n<p><strong>ç¬¬äºŒæ­¥ï¼šå¯å¾®æŠ•å½±ä¸Žæ¸²æŸ“ (Splatting &amp; Rasterization)</strong><br />\nè¿™æ˜¯ 3DGS é€Ÿåº¦æžå¿«çš„ç§˜è¯€ã€‚</p>\n<ul>\n<li>ç®—æ³•å°† 3D çš„æ¤­çƒä½“å¹³æ‰åŒ–ï¼ˆæŠ•å½±ï¼‰åˆ° 2D å±å¹•ä¸Šï¼Œå˜æˆä¸€ä¸ªä¸ª 2D æ¤­åœ†ã€‚</li>\n<li>é‡‡ç”¨<strong>åŸºäºŽåˆ†å—ï¼ˆTile-basedï¼‰çš„å¿«é€Ÿå…‰æ …åŒ–æ¸²æŸ“å™¨</strong>ã€‚å®ƒå°†å±å¹•åˆ†æˆè®¸å¤šå°æ ¼å­ï¼ˆTilesï¼‰ï¼Œåªæ¸²æŸ“å¯¹è¿™ä¸ªæ ¼å­æœ‰è´¡çŒ®çš„æ¤­çƒä½“ã€‚è¿™ä¸ŽçŽ°ä»£æ˜¾å¡ï¼ˆGPUï¼‰çš„å¤„ç†é€»è¾‘å®Œå…¨åŒ¹é…ã€‚<br />\n<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></li>\n</ul>\n<p><strong>ç¬¬ä¸‰æ­¥ï¼šè‡ªé€‚åº”å¯†åº¦æŽ§åˆ¶ (Adaptive Density Control)</strong><br />\nè¿™æ˜¯ 3DGS çš„â€œè¿›åŒ–â€è¿‡ç¨‹ã€‚åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œç®—æ³•ä¼šè§‚å¯Ÿï¼š</p>\n<ul>\n<li><strong>å…‹éš† (Clone)ï¼š</strong> å¦‚æžœæŸä¸ªåœ°æ–¹å¤ªå¹³æ»‘ï¼Œç‚¹ä¸å¤Ÿç”¨ï¼Œå°±åœ¨é‚£å¤åˆ¶ä¸€ä¸ªç‚¹ã€‚</li>\n<li><strong>åˆ†è£‚ (Split)ï¼š</strong> å¦‚æžœæŸä¸ªæ¤­çƒä½“å¤ªå¤§ã€å¤ªæ¨¡ç³Šï¼Œå°±æŠŠå®ƒæ‹†æˆä¸¤ä¸ªå°çš„ã€‚</li>\n<li><strong>ä¿®å‰ª (Culling)ï¼š</strong> å¦‚æžœæŸä¸ªæ¤­çƒä½“ä¸é€æ˜Žåº¦æžä½Žï¼ŒåŸºæœ¬çœ‹ä¸è§ï¼Œå°±æŠŠå®ƒåˆ æŽ‰ã€‚</li>\n</ul>\n<hr />\n<p><strong>ä¸ºä»€ä¹ˆ 3DGS èƒ½åœ¨ 2023 å¹´å¼•çˆ†æŠ€æœ¯åœˆï¼Ÿ</strong></p>\n<ol>\n<li><strong>æ¸²æŸ“é€Ÿåº¦èµ·é£žï¼š</strong> NeRF æ¸²æŸ“ä¸€å¼ å›¾éœ€è¦å‡ ç§’é’Ÿï¼Œè€Œ 3DGS å¯ä»¥åœ¨æ™®é€šçš„ GPU ä¸Šå®žçŽ° <strong>100+ FPS</strong>ï¼ˆæ¯ç§’å¸§æ•°ï¼‰çš„å®žæ—¶æ¸²æŸ“ã€‚</li>\n<li><strong>è®­ç»ƒæ—¶é—´æžçŸ­ï¼š</strong> ä¸€ä¸ªä¸­ç­‰å¤æ‚çš„åœºæ™¯ï¼ŒNeRF å¯èƒ½è¦è®­ç»ƒå‡ å°æ—¶ï¼Œ3DGS åªéœ€è¦ <strong>5-10 åˆ†é’Ÿ</strong>ã€‚</li>\n<li><strong>æ˜¾å¼å­˜å‚¨ï¼š</strong> å› ä¸ºå®ƒæ˜¯æ•°ç™¾ä¸‡ä¸ªç‚¹ï¼Œä½ å¯ä»¥ç›´æŽ¥åœ¨ 3D è½¯ä»¶é‡Œçœ‹åˆ°è¿™äº›ç‚¹ï¼Œç”šè‡³å¯ä»¥æ‰‹åŠ¨åˆ é™¤ã€ç§»åŠ¨æˆ–ç¼©æ”¾å…¶ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œè¿™åœ¨ NeRF çš„ç¥žç»ç½‘ç»œæƒé‡é‡Œæ˜¯å‡ ä¹Žåšä¸åˆ°çš„ã€‚</li>\n<li><strong>è§†è§‰æ•ˆæžœæƒŠè‰³ï¼š</strong> 3DGS æ•æ‰ç²¾ç»†ç»†èŠ‚ï¼ˆå¦‚æ ‘å¶ã€æ¯›å‘ã€è–„é›¾ï¼‰çš„èƒ½åŠ›æžå¼ºï¼Œç”»é¢è¾¹ç¼˜æ¯” NeRF æ›´åŠ é”åˆ©ã€‚</li>\n</ol>\n<hr />\n<h4 id=\"5-ç›®å‰çš„ç“¶é¢ˆä¸ŽæŒ‘æˆ˜\">5. ç›®å‰çš„ç“¶é¢ˆä¸ŽæŒ‘æˆ˜</h4>\n<ol>\n<li><strong>æ˜¾å­˜åŽ‹åŠ›å¤§ (VRAM Intensive)ï¼š</strong> æ¸²æŸ“è™½ç„¶å¿«ï¼Œä½†è¦åœ¨æ˜¾å­˜é‡Œå­˜å‚¨æ•°ç™¾ä¸‡ä¸ªé«˜æ–¯å‚æ•°ï¼Œéžå¸¸åƒæ˜¾å­˜ï¼ˆé€šå¸¸éœ€è¦ 8GB ä»¥ä¸Šçš„æ˜¾å­˜æ‰èƒ½æµç•…è¿è¡Œå¤§åž‹åœºæ™¯ï¼‰ã€‚</li>\n<li><strong>å­˜å‚¨ç©ºé—´å·¨å¤§ï¼š</strong> ä¸€ä¸ªé«˜è´¨é‡åœºæ™¯çš„æ¨¡åž‹æ–‡ä»¶å¯èƒ½é«˜è¾¾ <strong>æ•°ç™¾ MB ç”šè‡³å‡ ä¸ª GB</strong>ï¼Œè€Œ NeRF åªéœ€è¦å‡  MB çš„ç¥žç»ç½‘ç»œæƒé‡ã€‚</li>\n<li><strong>ä¾èµ–åˆå§‹åŒ–ï¼š</strong> å¦‚æžœ SfM å‡ºæ¥çš„åˆå§‹ç‚¹äº‘å¤ªå·®ï¼ˆæ¯”å¦‚åœ¨å…‰æ»‘å¹³é¢ä¸Šæ²¡å–åˆ°ç‚¹ï¼‰ï¼Œ3DGS å¾ˆéš¾å‡­ç©ºç”Ÿæˆé«˜è´¨é‡çš„æ¤­çƒä½“ï¼Œä¼šå‡ºçŽ°æ˜Žæ˜¾çš„â€œç ´æ´žâ€ã€‚</li>\n<li><strong>ä¼ªå½± (Artifacts)ï¼š</strong> åœ¨è§†è§’å˜åŒ–å‰§çƒˆæ—¶ï¼Œæœ‰æ—¶ä¼šçœ‹åˆ°æ¤­çƒä½“åƒâ€œå½©è‰²é›ªèŠ±â€ä¸€æ ·æ¼‚æµ®ã€‚</li>\n</ol>\n<hr />\n<h4 id=\"6-å‚è€ƒæ–‡çŒ®\">6. å‚è€ƒæ–‡çŒ®</h4>\n<ol>\n<li><strong>Kerbl, B., Kopanas, G., LeimkÃ¼hler, T., &amp; Drettakis, G. (2023).</strong> <em>3D Gaussian Splatting for Real-Time Radiance Field Rendering</em>. ACM Transactions on Graphics (SIGGRAPH).<br />\n(3DGS çš„å¼€å±±é¼»ç¥–ï¼Œç›®å‰è¯¥é¢†åŸŸå¼•ç”¨é‡æœ€é«˜çš„è®ºæ–‡)ã€‚</li>\n<li><strong>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001).</strong> <em>Surface Splatting</em>. SIGGRAPH.<br />\n(3DGS æ ¸å¿ƒæ€æƒ³â€œSplattingâ€çš„æ—©æœŸç†è®ºæ¥æº)ã€‚</li>\n<li><strong>Luan, F., et al. (2024).</strong> <em>GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis</em>.<br />\n(å°† 3DGS åº”ç”¨äºŽäººä½“åŠ¨æ€å»ºæ¨¡çš„å‰æ²¿å·¥ä½œ)ã€‚</li>\n<li><strong>Niedermayr, S., et al. (2023).</strong> <em>Compressed 3D Gaussian Splatting for Highly Accessible Novel View Synthesis</em>.<br />\n(é’ˆå¯¹ 3DGS å­˜å‚¨ä½“ç§¯è¿‡å¤§é—®é¢˜çš„æ—©æœŸä¼˜åŒ–ç ”ç©¶)ã€‚</li>\n<li><strong>Chen, Z., et al. (2023).</strong> <em>MCMC-GS: Multi-Scale 3D Gaussian Splatting for Large-Scale Scene Reconstruction</em>.<br />\n(é’ˆå¯¹å¤§è§„æ¨¡åŸŽå¸‚åœºæ™¯çš„ 3DGS æ”¹è¿›ç®—æ³•)ã€‚</li>\n</ol>\n<h2 id=\"äºŒ-ä¸»åŠ¨è§†è§‰active-vision-è‡ªå¸¦å…‰æº\">äºŒ. ä¸»åŠ¨è§†è§‰ï¼ˆActive Visionï¼‰â€”â€” è‡ªå¸¦å…‰æº</h2>\n<p>è¿™ç±»æ–¹æ³•é€šè¿‡å‘ç‰©ä½“å‘å°„ç‰¹å®šå…‰çº¿æ¥è¾…åŠ©æµ‹é‡ã€‚</p>\n<ol>\n<li><strong>ç»“æž„å…‰ï¼ˆStructured Lightï¼‰ï¼š</strong>\n<ul>\n<li><strong>åŽŸç†ï¼š</strong> æŠ•å½±ä»ªå‘ç‰©ä½“æŠ•å°„ç‰¹å®šçš„å…‰æ …å›¾æ¡ˆï¼Œç›¸æœºæ‹æ‘„è¢«ç‰©ä½“è¡¨é¢ç•¸å˜åŽçš„å›¾æ¡ˆï¼Œé€šè¿‡æ•°å­¦è®¡ç®—å¾—å‡ºæ·±åº¦ã€‚</li>\n<li><strong>ä»£è¡¨ï¼š</strong> iPhoneçš„å‰ç½®FaceIDã€‚<br />\nå¦‚æžœè¯´ <strong>SfM</strong> å’Œ<strong>åŒç›®è§†è§‰</strong>æ˜¯åˆ©ç”¨â€œè‡ªç„¶çš„çœ¼ç›â€åŽ»è§‚å¯Ÿä¸–ç•Œï¼Œé‚£ä¹ˆ<strong>ç»“æž„å…‰ï¼ˆStructured Lightï¼‰</strong>åˆ™æ˜¯ç»™æœºå™¨è£…ä¸Šäº†ä¸€å°â€œä¸»åŠ¨æµ‹é‡å°ºâ€ã€‚</li>\n</ul>\n</li>\n</ol>\n<p>å®ƒæ˜¯ç›®å‰é«˜ç²¾åº¦ 3D æ‰«æã€äººè„¸è¯†åˆ«ï¼ˆå¦‚ iPhone FaceIDï¼‰å’Œå·¥ä¸šç²¾å¯†æ£€æµ‹é¢†åŸŸæœ€æ ¸å¿ƒçš„æŠ€æœ¯ã€‚</p>\n<hr />\n<h3 id=\"a-ç»“æž„å…‰æŠ€æœ¯\">A. ç»“æž„å…‰æŠ€æœ¯</h3>\n<h4 id=\"å®šä¹‰-1\"><strong>å®šä¹‰ï¼š</strong></h4>\n<p>ç»“æž„å…‰æ˜¯ä¸€ç§<strong>ä¸»åŠ¨è§†è§‰</strong>æµ‹é‡æŠ€æœ¯ã€‚å®ƒé€šè¿‡æŠ•å½±ä»ªå‘ç‰©ä½“è¡¨é¢æŠ•å°„ç‰¹å®šçš„<strong>å·²çŸ¥å›¾æ¡ˆ</strong>ï¼ˆå¦‚æ ¼ç‚¹ã€æ¡çº¹ã€ç¼–ç å›¾ï¼‰ï¼Œå†ç”±ç›¸æœºæ•æ‰è¿™äº›å›¾æ¡ˆåœ¨ç‰©ä½“è¡¨é¢å‘ç”Ÿçš„<strong>å‡ ä½•ç•¸å˜</strong>ï¼Œé€šè¿‡ä¸‰è§’æµ‹é‡åŽŸç†è®¡ç®—å‡ºç‰©ä½“çš„ä¸‰ç»´ç©ºé—´åæ ‡ã€‚</p>\n<p><strong>å½¢è±¡æ¯”å–»ï¼š</strong><br />\næƒ³è±¡ä½ åœ¨é»‘æš—ä¸­é¢å¯¹ä¸€é¢å‡¹å‡¸ä¸å¹³çš„å¢™ã€‚ä½ æ‰“å¼€ä¸€æ”¯æ‰‹ç”µç­’ï¼Œæ‰‹ç”µç­’çš„å…‰ç½©ä¸Šç”»ç€æ•´é½çš„æ–¹æ ¼ã€‚</p>\n<ul>\n<li>å¦‚æžœå¢™æ˜¯å¹³çš„ï¼Œä½ åœ¨å¢™ä¸Šçœ‹åˆ°çš„æ–¹æ ¼ä¹Ÿæ˜¯æ•´é½çš„ã€‚</li>\n<li>å¦‚æžœå¢™ä¸Šæœ‰ä¸ªå‘æˆ–è€…å‡¸èµ·ï¼Œæ–¹æ ¼çº¿å°±ä¼šå˜å¾—å¼¯æ›²ã€‚</li>\n<li>é€šè¿‡è§‚å¯Ÿæ–¹æ ¼çº¿â€œå¼¯äº†å¤šå°‘â€ï¼Œä½ å°±èƒ½æŽ¨ç®—å‡ºå¢™é¢çš„èµ·ä¼ã€‚è¿™å°±æ˜¯ç»“æž„å…‰ã€‚</li>\n</ul>\n<p><img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></p>\n<hr />\n<h4 id=\"2-æ ¸å¿ƒæ•°å­¦åŽŸç†å˜å½¢çš„ä¸‰è§’æµ‹é‡\">2. æ ¸å¿ƒæ•°å­¦åŽŸç†ï¼šå˜å½¢çš„ä¸‰è§’æµ‹é‡</h4>\n<p>ç»“æž„å…‰æœ¬è´¨ä¸Šæ˜¯<strong>åŒç›®è§†è§‰çš„å˜ç§</strong>ã€‚</p>\n<ul>\n<li>åœ¨åŒç›®è§†è§‰ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªç›¸æœºä»Žä¸åŒè§’åº¦çœ‹åŒä¸€ä¸ªç‚¹ã€‚</li>\n<li>åœ¨ç»“æž„å…‰ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ª<strong>æŠ•å½±ä»ª</strong>æ›¿ä»£äº†å…¶ä¸­ä¸€ä¸ªç›¸æœºã€‚</li>\n</ul>\n<p><strong>æ•°å­¦é€»è¾‘ï¼š</strong></p>\n<ol>\n<li><strong>å·²çŸ¥æ¡ä»¶ï¼š</strong> æŠ•å½±ä»ªæŠ•å°„å‡ºçš„å…‰æŸè§’åº¦æ˜¯å·²çŸ¥çš„ï¼Œç›¸æœºä¸ŽæŠ•å½±ä»ªä¹‹é—´çš„è·ç¦»ï¼ˆåŸºçº¿ $B$ï¼‰æ˜¯å›ºå®šçš„ã€‚</li>\n<li><strong>å¯»æ‰¾åŒ¹é…ï¼š</strong> æŠ•å½±ä»ªæŠ•å‡ºçš„æ¯ä¸€ä¸ªå…‰ç‚¹ï¼ˆæˆ–çº¿æ¡ï¼‰éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„â€œç¼–ç â€ã€‚ç›¸æœºæ‹åˆ°è¿™ä¸ªç‚¹åŽï¼Œç«‹åˆ»å°±èƒ½çŸ¥é“å®ƒæ˜¯ä»ŽæŠ•å½±ä»ªçš„å“ªä¸ªè§’åº¦å°„å‡ºæ¥çš„ã€‚</li>\n<li><strong>å‡ ä½•æ±‚è§£ï¼š</strong> æŠ•å½±ä»ªå‘å‡ºçš„å°„çº¿ä¸Žç›¸æœºæŽ¥æ”¶åˆ°çš„å°„çº¿åœ¨ç©ºé—´ä¸­ç›¸äº¤ã€‚æ ¹æ®<strong>æ­£å¼¦å®šç†</strong>æˆ–ç®€å•çš„<strong>ä¸‰è§’å‡½æ•°</strong>ï¼Œå³å¯è§£ç®—å‡ºè¯¥äº¤ç‚¹çš„ä¸‰ç»´åæ ‡ $(X, Y, Z)$ã€‚</li>\n</ol>\n<hr />\n<h4 id=\"3-ç¼–ç æ–¹å¼ç»“æž„å…‰çš„è¯­è¨€\">3. ç¼–ç æ–¹å¼ï¼šç»“æž„å…‰çš„â€œè¯­è¨€â€</h4>\n<p>ç»“æž„å…‰æœ€æ ¸å¿ƒçš„ç«žäº‰åŠ›åœ¨äºŽå¦‚ä½•è®¾è®¡æŠ•å°„çš„â€œå›¾æ¡ˆâ€ã€‚ç›®å‰ä¸»æµæœ‰ä¸‰ç§ï¼š</p>\n<p><strong>A. ç©ºé—´ç¼–ç  (Spatial Coding) â€”â€” ä»£è¡¨ï¼šiPhone FaceID</strong></p>\n<ul>\n<li><strong>åšæ³•ï¼š</strong> ä¸€æ¬¡æ€§æŠ•å°„ä¸€ä¸ªå¤æ‚çš„æ•£æ–‘å›¾æ¡ˆï¼ˆç±»ä¼¼æ»¡å¤©æ˜Ÿï¼‰ã€‚</li>\n<li><strong>ç‰¹ç‚¹ï¼š</strong> åªéœ€æ‹æ‘„ä¸€å¼ å›¾ç‰‡å³å¯å®Œæˆé‡å»ºï¼Œé€Ÿåº¦æžå¿«ï¼Œé€‚åˆåŠ¨æ€ç‰©ä½“ï¼ˆå¦‚äººè„¸ï¼‰ã€‚<br />\n<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></li>\n</ul>\n<p><strong>B. æ—¶é—´ç¼–ç  (Temporal Coding) â€”â€” ä»£è¡¨ï¼šå·¥ä¸š 3D æ‰«æä»ª</strong></p>\n<ul>\n<li><strong>åšæ³•ï¼š</strong> è¿žç»­æŠ•å°„å¤šå¼ ä¸åŒé¢‘çŽ‡çš„é»‘ç™½æ¡çº¹ï¼ˆå¦‚æ ¼é›·ç  + æ­£å¼¦æ¡çº¹ï¼‰ã€‚</li>\n<li><strong>ç‰¹ç‚¹ï¼š</strong> ç²¾åº¦æžé«˜ï¼ˆå¯è¾¾å¾®ç±³çº§ï¼‰ï¼Œä½†ç‰©ä½“å¿…é¡»ä¿æŒé™æ­¢ï¼Œå› ä¸ºéœ€è¦æ‹æ‘„å¤šå¼ ç…§ç‰‡ã€‚<br />\n<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></li>\n</ul>\n<p><strong>C. ç›¸ç§»æ³• (Phase Shifting)</strong></p>\n<ul>\n<li><strong>åšæ³•ï¼š</strong> æŠ•å°„å‘¨æœŸæ€§çš„æ­£å¼¦å…‰æ …ï¼Œé€šè¿‡åˆ†æžå…‰æ³¢ç›¸ä½çš„åç§»æ¥æå–é«˜åº¦ä¿¡æ¯ã€‚<br />\n<img alt=\"åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°\" class=\"lazyload\" /></li>\n</ul>\n<hr />\n<h4 id=\"4-æ·±åº¦è§£æžiphone-faceid-æ˜¯å¦‚ä½•å·¥ä½œçš„\">4. æ·±åº¦è§£æžï¼šiPhone FaceID æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ</h4>\n<p>FaceID æ˜¯ç»“æž„å…‰æŠ€æœ¯å°åž‹åŒ–ã€æ¶ˆè´¹çº§åº”ç”¨çš„å·…å³°ã€‚å®ƒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š</p>\n<ol>\n<li><strong>ç‚¹é˜µæŠ•å½±å™¨ (Dot Projector)ï¼š</strong> å‘ä½ çš„è„¸éƒ¨æŠ•å°„è¶…è¿‡ <strong>30,000 ä¸ª</strong>è‚‰çœ¼ä¸å¯è§çš„çº¢å¤–æ•£æ–‘ç‚¹ã€‚</li>\n<li><strong>çº¢å¤–æ‘„åƒå¤´ (Infrared Camera)ï¼š</strong> è¯»å–è¿™äº›æ•£æ–‘åœ¨è„¸éƒ¨èµ·ä¼ä¸‹å½¢æˆçš„ç•¸å˜å›¾åƒã€‚</li>\n<li><strong>æ³›å…‰æ„Ÿåº”å…ƒä»¶ (Flood Illuminator)ï¼š</strong> ç¡®ä¿åœ¨å…¨é»‘çŽ¯å¢ƒä¸‹ä¹Ÿèƒ½ç…§äº®è„¸éƒ¨ï¼Œä¾›çº¢å¤–ç›¸æœºè¯†åˆ«ã€‚</li>\n</ol>\n<p><strong>ä¸ºä»€ä¹ˆå®ƒå®‰å…¨ï¼Ÿ</strong><br />\nå› ä¸ºå®ƒä¸æ˜¯åœ¨å¯¹æ¯”â€œç…§ç‰‡â€ï¼Œè€Œæ˜¯åœ¨å¯¹æ¯”ä½ è„¸éƒ¨çš„<strong>ä¸‰ç»´åœ°å½¢å›¾</strong>ã€‚ç…§ç‰‡æ˜¯å¹³é¢çš„ï¼Œæ— æ³•äº§ç”Ÿç»“æž„å…‰çš„ç•¸å˜æ•ˆæžœï¼Œå› æ­¤æ— æ³•æ¬ºéª— FaceIDã€‚</p>\n<hr />\n<h4 id=\"5-ç»“æž„å…‰çš„ä¼˜ç¼ºç‚¹\">5. ç»“æž„å…‰çš„ä¼˜ç¼ºç‚¹</h4>\n<p><strong>ä¼˜ç‚¹ï¼š</strong></p>\n<ol>\n<li><strong>æ— éœ€çŽ¯å¢ƒçº¹ç†ï¼š</strong> å³ä½¿æ˜¯çº¯ç™½è‰²çš„å¢™ï¼Œç»“æž„å…‰ä¹Ÿèƒ½é‡å»ºï¼ˆå› ä¸ºå®ƒè‡ªå·±æŠ•å°„äº†çº¹ç†ï¼‰ï¼Œè¿™å¼¥è¡¥äº†åŒç›®è§†è§‰çš„çŸ­æ¿ã€‚</li>\n<li><strong>ç²¾åº¦æžé«˜ï¼š</strong> åœ¨è¿‘è·ç¦»ï¼ˆ1ç±³ä»¥å†…ï¼‰ï¼Œç»“æž„å…‰çš„ç²¾åº¦å¯ä»¥è½»æ¾è¾¾åˆ°æ¯«ç±³ç”šè‡³å¾®ç±³çº§ã€‚</li>\n<li><strong>ä¸»åŠ¨è¡¥å…‰ï¼š</strong> åœ¨å…¨é»‘çš„çŽ¯å¢ƒä¸‹ä¾ç„¶èƒ½å·¥ä½œã€‚</li>\n</ol>\n<p><strong>ç¼ºç‚¹ï¼š</strong></p>\n<ol>\n<li><strong>æ€•å¼ºå…‰ï¼š</strong> åœ¨å®¤å¤–å¼ºå…‰ï¼ˆå¦‚é˜³å…‰ï¼‰ä¸‹ï¼ŒæŠ•å½±ä»ªçš„å…‰ä¼šè¢«é˜³å…‰æ·¹æ²¡ï¼Œå¯¼è‡´å¤±æ•ˆã€‚</li>\n<li><strong>è·ç¦»é™åˆ¶ï¼š</strong> æŠ•å½±ä»ªåŠŸçŽ‡æœ‰é™ï¼Œé€šå¸¸åªé€‚ç”¨äºŽè¿‘è·ç¦»ï¼ˆå¦‚æ‰‹æœºè§£é”ã€æ¡Œé¢æ‰«æï¼‰ã€‚</li>\n<li><strong>æè´¨æ•æ„Ÿï¼š</strong> é¢å¯¹é•œé¢åå…‰ç‰©ä½“æˆ–å…¨é»‘å¸å…‰ç‰©ä½“ï¼ŒæŠ•å°„çš„å…‰ä¼šæ¶ˆå¤±æˆ–ä¹±å°„ï¼Œå¯¼è‡´æ— æ³•æˆåƒã€‚</li>\n</ol>\n<hr />\n<h3 id=\"6-å‚è€ƒæ–‡çŒ®-1\">6. å‚è€ƒæ–‡çŒ®</h3>\n<ol>\n<li><strong>Geng, J. (2011).</strong> <em>Structured-light 3D surface imaging: a tutorial</em>. Advances in Optics and Photonics.<br />\n(æœ€ç»å…¸çš„ç»“æž„å…‰ç»¼è¿°æ€§æ•™ç¨‹)ã€‚</li>\n<li><strong>Scharstein, D., &amp; Szeliski, R. (2003).</strong> <em>High-accuracy stereo depth maps using structured light</em>. CVPR.<br />\n(æŽ¢è®¨äº†å¦‚ä½•åˆ©ç”¨ç»“æž„å…‰èŽ·å–æžé«˜ç²¾åº¦çš„æ·±åº¦å›¾)ã€‚</li>\n<li><strong>Salvi, J., et al. (2004).</strong> <em>Pattern codification strategies in structured light systems</em>. Pattern Recognition.<br />\n(è¯¦ç»†å¯¹æ¯”äº†å„ç§ç¼–ç å›¾æ¡ˆçš„ä¼˜åŠ£)ã€‚</li>\n<li><strong>Apple Inc.</strong> <em>About Face ID advanced technology</em>. Support Documentation.<br />\n(è™½ç„¶ä¸æ˜¯å­¦æœ¯è®ºæ–‡ï¼Œä½†æ­ç¤ºäº†æ¶ˆè´¹çº§ç»“æž„å…‰ç³»ç»Ÿçš„ç¡¬ä»¶æž¶æž„)ã€‚</li>\n<li><strong>Zuo, C., et al. (2016).</strong> <em>Micro-structured-light 3D surface profiling: A review</em>. Measurement.<br />\n(èšç„¦äºŽå¾®è§‚é¢†åŸŸé«˜ç²¾åº¦ç»“æž„å…‰æµ‹é‡çš„ç»¼è¿°)ã€‚</li>\n</ol>\n<h3 id=\"é£žè¡Œæ—¶é—´æ³•tof-time-of-flight\">é£žè¡Œæ—¶é—´æ³•ï¼ˆToF, Time of Flightï¼‰</h3>\n<p><strong>åŽŸç†ï¼š</strong> å‘å°„çº¢å¤–è„‰å†²ï¼Œæµ‹é‡å…‰çº¿åå°„å›žæ¥çš„æ—¶é—´ã€‚<br />\n<strong>ä»£è¡¨ï¼š</strong> æ¿€å…‰é›·è¾¾ï¼ˆLiDARï¼‰ã€Kinect 2.0ã€‚<br />\nå¦‚æžœè¯´<strong>ç»“æž„å…‰</strong>æ˜¯åˆ©ç”¨â€œå‡ ä½•ç•¸å˜â€æ¥æµ‹é‡ï¼Œé‚£ä¹ˆ<strong>é£žè¡Œæ—¶é—´æ³•ï¼ˆToF, Time of Flightï¼‰</strong>å°±æ˜¯åˆ©ç”¨â€œç»å¯¹é€Ÿåº¦â€æ¥æµ‹é‡ã€‚</p>\n<p>ToF æ˜¯ç›®å‰ 3D æ„ŸçŸ¥é¢†åŸŸä¸­é€Ÿåº¦æœ€å¿«ã€å»¶è¿Ÿæœ€ä½Žçš„æŠ€æœ¯ã€‚å®ƒå¹¿æ³›åº”ç”¨äºŽè‡ªåŠ¨é©¾é©¶çš„æ¿€å…‰é›·è¾¾ï¼ˆLiDARï¼‰ã€æ‰‹æœºåŽç½®æ·±åº¦æ‘„åƒå¤´ï¼ˆå¦‚ iPhone çš„ LiDAR æ‰«æä»ªï¼‰ä»¥åŠæ—©æœŸçš„ä½“æ„Ÿæ¸¸æˆè®¾å¤‡ã€‚</p>\n<hr />\n<h4 id=\"1-ä»€ä¹ˆæ˜¯-tof-é£žè¡Œæ—¶é—´æ³•\">1. ä»€ä¹ˆæ˜¯ ToF (é£žè¡Œæ—¶é—´æ³•)ï¼Ÿ</h4>\n<p><strong>å®šä¹‰ï¼š</strong><br />\nToF æ˜¯ä¸€ç§é€šè¿‡æµ‹é‡å…‰æ³¢åœ¨ä¼ æ„Ÿå™¨ä¸Žç›®æ ‡ç‰©ä½“ä¹‹é—´â€œé£žè¡Œâ€çš„æ—¶é—´ï¼Œæ¥è®¡ç®—ç›®æ ‡è·ç¦»çš„æŠ€æœ¯ã€‚å› ä¸ºå…‰çš„ä¼ æ’­é€Ÿåº¦ $c$ æ˜¯å·²çŸ¥çš„å¸¸æ•°ï¼Œåªè¦çŸ¥é“äº†æ—¶é—´ $t$ï¼Œè·ç¦» $d$ ä¹Ÿå°±è¿Žåˆƒè€Œè§£ã€‚</p>\n<p><strong>å½¢è±¡æ¯”å–»ï¼š</strong><br />\næƒ³è±¡ä½ å¯¹ç€ä¸€å£æ·±äº•å¤§å–Šä¸€å£°ï¼Œç„¶åŽå¼€å§‹è®¡æ—¶ã€‚å½“ä½ å¬åˆ°å›žå£°æ—¶ï¼Œåœæ­¢è®¡æ—¶ã€‚</p>\n<ul>\n<li><strong>å£°çº³/é›·è¾¾ï¼š</strong> åˆ©ç”¨å£°æ³¢æˆ–æ— çº¿ç”µæ³¢çš„åå°„ã€‚</li>\n<li><strong>ToFï¼š</strong> åˆ©ç”¨å…‰æ³¢ï¼ˆé€šå¸¸æ˜¯ 850nm-940nm çš„è¿‘çº¢å¤–å…‰ï¼‰ã€‚ç”±äºŽå…‰é€Ÿæžå¿«ï¼ˆæ¯ç§’ 30 ä¸‡å…¬é‡Œï¼‰ï¼ŒToF èŠ¯ç‰‡éœ€è¦å…·å¤‡â€œçš®ç§’çº§â€ï¼ˆä¸‡äº¿åˆ†ä¹‹ä¸€ç§’ï¼‰çš„è®¡æ—¶ç²¾åº¦ã€‚</li>\n</ul>\n<hr />\n<h4 id=\"2-æ ¸å¿ƒæ•°å­¦åŽŸç†æžé€Ÿå…¬å¼\">2. æ ¸å¿ƒæ•°å­¦åŽŸç†ï¼šæžé€Ÿå…¬å¼</h4>\n<p>ToF çš„åŸºæœ¬åŽŸç†éžå¸¸ç›´è§‚ï¼Œä½†å®žçŽ°æ–¹æ³•åˆ†ä¸ºä¸¤ç§ä¸»è¦æµæ´¾ï¼š<strong>dToF</strong> å’Œ <strong>iToF</strong>ã€‚</p>\n<p><strong>A. dToF (Direct ToFï¼Œç›´æŽ¥æµ‹é‡) â€”â€” ä»£è¡¨ï¼šæ¿€å…‰é›·è¾¾ (LiDAR)</strong><br />\nè¿™æ˜¯æœ€ç¡¬æ ¸çš„æ–¹å¼ã€‚ä¼ æ„Ÿå™¨ç›´æŽ¥å‘å°„ä¸€ä¸ªæžçŸ­çš„å…‰è„‰å†²ï¼Œå¹¶å¯åŠ¨ä¸€ä¸ªè¶…é«˜ç²¾åº¦çš„â€œç§’è¡¨â€ï¼Œè®°å½•å…‰å­é£žå‡ºåŽ»å¹¶è·³å›žæ¥çš„ç¡®åˆ‡æ—¶é—´ $\\Delta t$ã€‚</p>\n<ul>\n<li><strong>å…¬å¼ï¼š</strong> $d = \\frac{c \\cdot \\Delta t}{2}$</li>\n<li><strong>éš¾ç‚¹ï¼š</strong> å…‰é€Ÿå¤ªå¿«ï¼Œå…‰èµ° 1 åŽ˜ç±³åªéœ€è¦çº¦ 33 çš®ç§’ã€‚è¿™å°±è¦æ±‚ä¼ æ„Ÿå™¨ï¼ˆé€šå¸¸æ˜¯ SPADï¼Œå•å…‰å­é›ªå´©äºŒæžç®¡ï¼‰å…·æœ‰æžå…¶ææ€–çš„å“åº”é€Ÿåº¦ã€‚</li>\n</ul>\n<p><strong>B. iToF (Indirect ToFï¼Œé—´æŽ¥æµ‹é‡) â€”â€” ä»£è¡¨ï¼šKinect 2.0</strong><br />\nç”±äºŽç²¾ç¡®è®¡æ—¶å¤ªéš¾ä¸”è´µï¼ŒiToF é‡‡ç”¨äº†â€œæ›²çº¿æ•‘å›½â€çš„æ–¹æ³•ï¼šå®ƒå‘å°„çš„æ˜¯<strong>è¿žç»­çš„æ­£å¼¦è°ƒåˆ¶å…‰æ³¢</strong>ï¼Œé€šè¿‡æµ‹é‡å‘å°„æ³¢ä¸Žåå°„æ³¢ä¹‹é—´çš„<strong>ç›¸ä½å·®ï¼ˆPhase Shiftï¼‰</strong>æ¥æŽ¨ç®—æ—¶é—´ã€‚</p>\n<ul>\n<li><strong>é€»è¾‘ï¼š</strong> ç›¸ä½åç§»äº†å¤šå°‘åº¦ï¼Œå°±ä»£è¡¨æ—¶é—´è¿‡åŽ»äº†å¤šä¹…ã€‚</li>\n<li><strong>ä¼˜ç‚¹ï¼š</strong> ç¡¬ä»¶æˆæœ¬è¾ƒä½Žï¼Œåƒç´ åˆ†è¾¨çŽ‡å¯ä»¥åšå¾—å¾ˆé«˜ï¼ˆç±»ä¼¼æ‰‹æœºæ‘„åƒå¤´ï¼‰ã€‚</li>\n</ul>\n<hr />\n<h4 id=\"3-tof-çš„å·¥ä½œæµç¨‹\">3. ToF çš„å·¥ä½œæµç¨‹</h4>\n<ol>\n<li><strong>å‘å°„å•å…ƒ (Emitter)ï¼š</strong> å‘å‡ºç»è¿‡è°ƒåˆ¶çš„çº¢å¤–æ¿€å…‰æˆ–è„‰å†²ã€‚</li>\n<li><strong>åå°„ (Reflection)ï¼š</strong> å…‰çº¿è§¦ç¢°ç‰©ä½“è¡¨é¢åŽå‘ç”Ÿå¼¥æ•£åå°„ã€‚</li>\n<li><strong>æŽ¥æ”¶å•å…ƒ (Sensor)ï¼š</strong> ä¸€ä¸ªç‰¹æ®Šçš„ CMOS å›¾åƒä¼ æ„Ÿå™¨æ•æ‰è¿”å›žçš„å…‰ä¿¡å·ã€‚</li>\n<li><strong>å¤„ç†å•å…ƒ (Processor)ï¼š</strong>\n<ul>\n<li>å¯¹äºŽ dToFï¼šç»Ÿè®¡å…‰å­åˆ°è¾¾çš„ç›´æ–¹å›¾ï¼Œç¡®å®šå³°å€¼æ—¶é—´ã€‚</li>\n<li>å¯¹äºŽ iToFï¼šè®¡ç®—æ¯ä¸ªåƒç´ ç‚¹çš„ç”µè·å·®å¼‚ï¼Œè½¬æ¢æˆç›¸ä½ï¼Œå†æ¢ç®—æˆæ·±åº¦å€¼ã€‚</li>\n</ul>\n</li>\n<li><strong>è¾“å‡ºï¼š</strong> ç”Ÿæˆä¸€å¼ <strong>æ·±åº¦å›¾ï¼ˆDepth Mapï¼‰</strong>ï¼Œæ¯ä¸ªåƒç´ ä»£è¡¨è¯¥ç‚¹çš„ç‰©ç†è·ç¦»ã€‚</li>\n</ol>\n<hr />\n<h4 id=\"4-tof-vs-ç»“æž„å…‰æœ‰ä»€ä¹ˆåŒºåˆ«\">4. ToF vs. ç»“æž„å…‰ï¼šæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">ç‰¹æ€§</th>\n<th style=\"text-align: left;\">ToF (é£žè¡Œæ—¶é—´æ³•)</th>\n<th style=\"text-align: left;\">ç»“æž„å…‰ (Structured Light)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>è®¡ç®—é‡</strong></td>\n<td style=\"text-align: left;\"><strong>æžä½Ž</strong>ï¼ˆç¡¬ä»¶ç›´æŽ¥è¾“å‡ºç»“æžœï¼‰</td>\n<td style=\"text-align: left;\"><strong>é«˜</strong>ï¼ˆéœ€è¦å¤æ‚çš„å›¾åƒå¯¹æ¯”åˆ†æžï¼‰</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>å·¥ä½œè·ç¦»</strong></td>\n<td style=\"text-align: left;\"><strong>è¿œ</strong>ï¼ˆæ¿€å…‰é›·è¾¾å¯è¾¾æ•°ç™¾ç±³ï¼‰</td>\n<td style=\"text-align: left;\"><strong>çŸ­</strong>ï¼ˆé€šå¸¸ 1 ç±³ä»¥å†…ï¼‰</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>æŠ—å¹²æ‰°æ€§</strong></td>\n<td style=\"text-align: left;\">è¾ƒå¼ºï¼ˆå°¤å…¶ dToF é€‚åˆå®¤å¤–ï¼‰</td>\n<td style=\"text-align: left;\">è¾ƒå¼±ï¼ˆå¼ºå…‰ä¸‹å›¾æ¡ˆä¼šè¢«æ·¹æ²¡ï¼‰</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>åˆ†è¾¨çŽ‡</strong></td>\n<td style=\"text-align: left;\">è¾ƒä½Žï¼ˆé€šå¸¸æ˜¯ QVGA çº§ï¼‰</td>\n<td style=\"text-align: left;\">è¾ƒé«˜ï¼ˆå¯ä»¥è¾¾åˆ°ç™¾ä¸‡åƒç´ ï¼‰</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>åº”ç”¨åœºæ™¯</strong></td>\n<td style=\"text-align: left;\">æ‰«åœ°æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ã€AR å»ºæ¨¡</td>\n<td style=\"text-align: left;\">æ‰‹æœºé¢éƒ¨è§£é”ã€ç²¾å¯†æ‰«æã€æ´»ä½“æ£€æµ‹</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h4 id=\"5-ç›®å‰çš„ç“¶é¢ˆä¸ŽæŒ‘æˆ˜-1\">5. ç›®å‰çš„ç“¶é¢ˆä¸ŽæŒ‘æˆ˜</h4>\n<ol>\n<li><strong>å¤šå¾„å¹²æ‰° (Multi-path Interference)ï¼š</strong><br />\nå¦‚æžœå…‰çº¿åœ¨å¢™è§’ç»è¿‡å¤šæ¬¡åå°„æ‰å›žåˆ°ä¼ æ„Ÿå™¨ï¼ŒToF ä¼šè¯¯ä»¥ä¸ºç‰©ä½“æ¯”å®žé™…æ›´è¿œï¼Œäº§ç”Ÿâ€œæ·±åº¦é‡å½±â€ã€‚</li>\n<li><strong>é£žç‚¹æ•ˆåº” (Flying Pixels)ï¼š</strong><br />\nåœ¨ç‰©ä½“çš„è¾¹ç¼˜ï¼Œä¸€ä¸ªåƒç´ å¯èƒ½åŒæ—¶æŽ¥æ”¶åˆ°ç‰©ä½“è¡¨é¢å’ŒèƒŒæ™¯å¢™çš„åå°„ä¿¡å·ï¼Œå¯¼è‡´è¾¹ç¼˜å‡ºçŽ°ä¸€åœˆèŽ«é¡»æœ‰çš„â€œé£žç‚¹â€ã€‚</li>\n<li><strong>çŽ¯å¢ƒå…‰å¹²æ‰°ï¼š</strong><br />\nè™½ç„¶ iToF åœ¨å®¤å†…è¡¨çŽ°ä¼˜å¼‚ï¼Œä½†åœ¨å¤ªé˜³å…‰ä¸‹ï¼Œçº¢å¤–å…‰å™ªå£°å·¨å¤§ï¼Œä¼šå¯¼è‡´ç²¾åº¦å¤§å¹…ä¸‹é™ï¼ˆdToF é€šè¿‡å•å…‰å­æŠ€æœ¯ç¼“è§£äº†è¿™ä¸€é—®é¢˜ï¼‰ã€‚</li>\n<li><strong>åŠŸè€—é—®é¢˜ï¼š</strong><br />\nå› ä¸ºè¦ä¸æ–­åœ°ä¸»åŠ¨å‘å°„å…‰ï¼ŒToF ä¼ æ„Ÿå™¨é€šå¸¸æ¯”æ™®é€šæ‘„åƒå¤´æ›´è€—ç”µã€‚</li>\n</ol>\n<hr />\n<h4 id=\"6-å‚è€ƒæ–‡çŒ®-2\">6. å‚è€ƒæ–‡çŒ®</h4>\n<ol>\n<li><strong>Hansard, M., Lee, S., Choi, O., &amp; Horaud, R. P. (2012).</strong> <em>Time-of-Flight Cameras: Principles, Methods and Applications</em>. Springer Briefs in Computer Science.<br />\n(æœ€ç³»ç»Ÿè®²è§£ ToF åŽŸç†çš„ç»å…¸ä¹¦ç±)ã€‚</li>\n<li><strong>Li, L. (2014).</strong> <em>Time-of-flight camera â€“ An introduction</em>. Technical report, Texas Instruments.<br />\n(TI å‘å¸ƒçš„å®˜æ–¹æŠ€æœ¯æŠ¥å‘Šï¼Œæ·±å…¥è®²è§£äº† iToF çš„ç”µå­å­¦å®žçŽ°)ã€‚</li>\n<li><strong>Horaud, R., et al. (2016).</strong> <em>An overview of gradient-based 3D reconstruction methods for Time-of-Flight RGB-D cameras</em>. Journal of Mathematical Imaging and Vision.<br />\n(æŽ¢è®¨äº†å¦‚ä½•ä¼˜åŒ– ToF çš„æ·±åº¦å›¾è´¨é‡)ã€‚</li>\n<li><strong>Bamji, C. S., et al. (2015).</strong> <em>A 0.13Î¼m CMOS System-on-Chip for a 512Ã—424 Time-of-Flight Image Sensor with Multi-Frequency Photo-Demodulation</em>. ISSCC.<br />\n(Kinect 2.0 èƒŒåŽèŠ¯ç‰‡çš„æŠ€æœ¯ç»†èŠ‚è®ºæ–‡)ã€‚</li>\n<li><strong>Niclass, C., et al. (2005).</strong> <em>A CMOS single photon avalanche diode array for 3D imaging</em>. IEEE International Solid-State Circuits Conference.<br />\n(dToF æ ¸å¿ƒç»„ä»¶ SPAD ä¼ æ„Ÿå™¨çš„å¥ åŸºæ€§ç ”ç©¶)ã€‚</li>\n</ol>\n<hr />\n<h1 id=\"4-é€‚ç”¨åœºæ™¯\">4. é€‚ç”¨åœºæ™¯</h1>\n<p>3Dé‡å»ºæŠ€æœ¯å·²ç»æ·±å…¥åˆ°å„è¡Œå„ä¸šï¼š</p>\n<ul>\n<li><strong>å·¥ä¸šè‡ªåŠ¨åŒ–ï¼š</strong> é«˜ç²¾åº¦é›¶ä»¶çš„å°ºå¯¸æ£€æµ‹ã€ç¼ºé™·æ‰«æã€æœºå™¨äººæŠ“å–å¼•å¯¼ã€‚</li>\n<li><strong>è‡ªåŠ¨é©¾é©¶ä¸Žæœºå™¨äººï¼š</strong> é¿éšœã€çŽ¯å¢ƒå»ºæ¨¡ï¼ˆSLAMï¼‰ã€é«˜ç²¾åœ°å›¾åˆ¶ä½œã€‚</li>\n<li><strong>æ–‡åŒ–é—äº§ä¿æŠ¤ï¼š</strong> å¯¹å¤å»ºç­‘ç‰©ã€æ–‡ç‰©è¿›è¡Œæ•°å­—åŒ–å»ºæ¨¡ï¼Œå®žçŽ°æ°¸æ’ä¿å­˜ã€‚</li>\n<li><strong>åŒ»ç–—æˆåƒï¼š</strong> ç‰™ç§‘æ‰«æå»ºæ¨¡ã€æ‰‹æœ¯å‰3Dæ¨¡æ‹Ÿã€‚</li>\n<li><strong>æ•°å­—å­ªç”Ÿä¸Žå…ƒå®‡å®™ï¼š</strong> å°†çŽ°å®žä¸–ç•Œçš„ç‰©ä½“å¿«é€Ÿå»ºæ¨¡å¹¶å¯¼å…¥æ¸¸æˆæˆ–è™šæ‹Ÿç©ºé—´ã€‚</li>\n<li><strong>ç”µå­å•†åŠ¡ï¼š</strong> æ‰‹æœºæ‰«æå•†å“å®žçŽ°3Dé¢„è§ˆï¼ˆå¦‚å®œå®¶å®¶å…·è¯•æ‘†ï¼‰ã€‚</li>\n</ul>\n<hr />\n<h1 id=\"5-ç›®å‰çš„ç“¶é¢ˆä¸Žé—®é¢˜\">5. ç›®å‰çš„ç“¶é¢ˆä¸Žé—®é¢˜</h1>\n<p>å°½ç®¡æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œä½†3Dé‡å»ºä»é¢ä¸´ä»¥ä¸‹ä¸¥å³»æŒ‘æˆ˜ï¼š</p>\n<ol>\n<li><strong>å¼±çº¹ç†ä¸Žé‡å¤çº¹ç†ï¼š</strong><br />\né¢å¯¹æ´ç™½çš„å¢™å£ã€çº¯è‰²æ¡Œé¢æˆ–çŽ»ç’ƒè¡¨é¢ï¼Œç®—æ³•å¾ˆéš¾æ‰¾åˆ°å¯é çš„ç‰¹å¾åŒ¹é…ç‚¹ï¼Œå¯¼è‡´é‡å»ºå¤±è´¥æˆ–å‡ºçŽ°â€œç ´æ´žâ€ã€‚</li>\n<li><strong>åå…‰ä¸Žé€æ˜Žç‰©ä½“ï¼š</strong><br />\né‡‘å±žè¡¨é¢çš„é«˜å…‰å’ŒçŽ»ç’ƒçš„æŠ˜å°„ä¼šè¯¯å¯¼ç›¸æœºï¼Œä½¿ç®—æ³•è®¡ç®—å‡ºé”™è¯¯çš„æ·±åº¦ã€‚</li>\n<li><strong>è®¡ç®—å¤æ‚åº¦ä¸Žå®žæ—¶æ€§ï¼š</strong><br />\né«˜ç²¾åº¦çš„ç¨ å¯†é‡å»ºéœ€è¦å·¨å¤§çš„ç®—åŠ›ï¼Œåœ¨ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼å¹³å°ä¸Šéš¾ä»¥å®žçŽ°è¶…é«˜ç²¾åº¦çš„å®žæ—¶é‡å»ºã€‚</li>\n<li><strong>é®æŒ¡é—®é¢˜ï¼š</strong><br />\nå½“ç‰©ä½“çš„ä¸€éƒ¨åˆ†è¢«æŒ¡ä½æ—¶ï¼Œç®—æ³•éœ€è¦é€šè¿‡å…ˆéªŒçŸ¥è¯†ï¼ˆAIçŒœæƒ³ï¼‰æ¥è¡¥å…¨ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´ç²¾åº¦ä¸‹é™ã€‚</li>\n<li><strong>å…‰ç…§æ•æ„Ÿæ€§ï¼š</strong><br />\nè¿‡å¼ºæˆ–è¿‡æš—çš„å…‰çº¿éƒ½ä¼šä¸¥é‡å½±å“è¢«åŠ¨è§†è§‰æ–¹æ³•çš„ç‰¹å¾æå–è´¨é‡ã€‚</li>\n</ol>\n<hr />\n<h1 id=\"æ€»ç»“\">æ€»ç»“</h1>\n<p>é€šè¿‡æœºå™¨è§†è§‰å®žçŽ°3Dé‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„â€œåœ£æ¯â€ä¹‹ä¸€ã€‚ä»Žç»å…¸çš„ä¸‰è§’å‡ ä½•åˆ°çŽ°ä»£çš„æ·±åº¦å­¦ä¹ ï¼ˆNeRF/3DGSï¼‰ï¼Œæˆ‘ä»¬æ­£å¤„äºŽä»Žâ€œæ‹å‡ºå¥½çœ‹çš„ç…§ç‰‡â€å‘â€œæž„å»ºå®Œç¾Žçš„æ•°å­—ä¸–ç•Œâ€è·¨è¶Šçš„æ—¶ä»£ã€‚éšç€ç®—åŠ›çš„æå‡å’Œç®—æ³•çš„ä¼˜åŒ–ï¼Œæœªæ¥çš„3Dé‡å»ºå°†å˜å¾—åƒæ‹ç…§ä¸€æ ·ç®€å•ä¸”ç²¾å‡†ã€‚</p>\n<hr />\n<h1 id=\"å‚è€ƒæ–‡çŒ®\">å‚è€ƒæ–‡çŒ®</h1>\n<p>ä¸ºäº†ä¿è¯ä¿¡æ¯çš„çœŸå®žæ€§ä¸Žå­¦æœ¯ä¸¥è°¨æ€§ï¼Œä»¥ä¸‹æ˜¯æœ¬åšå®¢å‚è€ƒçš„æ ¸å¿ƒæ–‡çŒ®ä¸Žç»å…¸æ•™æï¼š</p>\n<ol>\n<li><strong>Hartley, R., &amp; Zisserman, A. (2004).</strong> <em>Multiple View Geometry in Computer Vision</em>. Cambridge University Press.<br />\n(è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åœ£ç»ï¼Œè¯¦ç»†è®²è§£äº†å¤šè§†å›¾å‡ ä½•å’Œç›¸æœºæ¨¡åž‹)ã€‚</li>\n<li><strong>Mildenhall, B., et al. (2020).</strong> <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</em>. ECCV.<br />\n(ç¥žç»è¾å°„åœºå¥ åŸºä¹‹ä½œ)ã€‚</li>\n<li><strong>Kerbl, B., et al. (2023).</strong> <em>3D Gaussian Splatting for Real-Time Radiance Field Rendering</em>. ACM Transactions on Graphics.<br />\n(å½“å‰æœ€å‰æ²¿çš„å®žæ—¶3Dé‡å»ºç®—æ³•)ã€‚</li>\n<li><strong>Schonberger, J. L., &amp; Frahm, J. M. (2016).</strong> <em>Structure-from-Motion Revisited</em>. CVPR.<br />\n(COLMAPç®—æ³•çš„ç†è®ºåŸºç¡€)ã€‚</li>\n<li><strong>Newcombe, R. A., et al. (2011).</strong> <em>KinectFusion: Real-time 3D reconstruction and interaction using a moving depth camera</em>. ISMAR.<br />\n(ç»“æž„å…‰ä¸ŽToFå®žæ—¶é‡å»ºçš„ç»å…¸æ–‡çŒ®)ã€‚</li>\n</ol>\n<hr />\n<p><strong>ä½œè€…æç¤ºï¼š</strong> å¦‚æžœä½ å¯¹å…·ä½“çš„æŸä¸ªç®—æ³•ï¼ˆå¦‚å¦‚ä½•é…ç½®COLMAPæˆ–å¦‚ä½•è®­ç»ƒNeRFï¼‰æ„Ÿå…´è¶£ï¼Œæ¬¢è¿Žåœ¨è¯„è®ºåŒºç•™è¨€ï¼Œæˆ‘å°†åœ¨ä¸‹ä¸€æœŸè¯¦ç»†è®²è§£ï¼</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 09:04</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ChenAI-TGF\">TTGF</a>&nbsp;\né˜…è¯»(<span id=\"post_view_count\">0</span>)&nbsp;\nè¯„è®º(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">æ”¶è—</a>&nbsp;\n<a href=\"\">ä¸¾æŠ¥</a>\n</div>"
    },
    {
      "title": "别再只做 “点点点”！AI测试的六种不同玩法，附带Midscene详细教程！",
      "link": "https://www.cnblogs.com/jinjiangongzuoshi/p/19582572",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jinjiangongzuoshi/p/19582572\" id=\"cb_post_title_url\" title=\"发布于 2026-02-06 09:02\">\n    <span>别再只做 “点点点”！AI测试的六种不同玩法，附带Midscene详细教程！</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>零基础新手，最大的学习障碍是\"恐惧感\"！</p>\n<p>上一篇，我们以AI编程切入，手把手带大家如何在10分钟内做出第一个应用，并且如何将它部署到互联网上。</p>\n<p>不需要你有任何编程基础，只要你会打字、会上网，就能跟着做。通过第一个项目，帮助大家建立<code>Vibe Coding</code> 的思维方式：<strong>关注 \"要做什么\" 而不是 \"怎么做\"</strong>。</p>\n<p>今天是牛刀小试的第二篇，我们来聊一聊AI测试，带大家感受一下AI在测试领域带来的魅力。</p>\n<p>同样，在这篇教程中，我们不聊AI测试的相关理论、晦涩难懂的技术，聊聊AI在UI自动化测试的一些新玩法、思路。</p>\n<blockquote>\n<p>本文为试读，原文发布到「狂师.AI进化社」教程专栏中，会有更加详细的AI测试教程，阅读体验更佳~</p>\n</blockquote>\n<h2 id=\"1传统ui测试工具是怎么做自动化的\">1、传统UI测试工具是怎么做自动化的？</h2>\n<p>传统 UI 自动化测试，核心是<strong>通过代码/脚本对页面元素做 “定位 -&gt; 操作 -&gt; 断言” 的程序化模拟</strong>，本质是用机器替代人工的手工点击、输入、校验操作，实现回归测试、重复场景测试的自动化执行，整体围绕 “元素定位” 这个核心展开，步骤固定且对页面稳定性要求极高。</p>\n<p>而传统 UI 自动化测试工具（如 Selenium、Appium、Playwright、Robot Framework 等），无论开源还是商用，实现思路基本一致，大体可以分为几步：</p>\n<h3 id=\"11-搭环境与工具适配\">1.1 搭环境与工具适配</h3>\n<p>根据被测应用的类型（Web 端 / 移动端 APP / 小程序）选择对应工具，比如：</p>\n<ul>\n<li>Web 端，一般首选用 <code>Selenium/Playwright</code></li>\n<li>移动端用 <code>Appium</code>，</li>\n<li>低代码封装用 <code>Robot Framework</code>；</li>\n</ul>\n<p>同时搭建配套环境，比如 Web 端需要配置浏览器驱动（ChromeDriver/GeckoDriver）、移动端需要配置模拟器/真机+ADB环境，再结合 Python/Java/JavaScript等开发语言，搭建基础的自动化测试框架。</p>\n<h3 id=\"12-定位页面元素\">1.2 定位页面元素</h3>\n<p>工具无法像人一样 “看” 到页面上的按钮、输入框、下拉框，只能通过<strong>页面元素的唯一标识</strong>来识别，这是所有自动化操作的前提。测试人员需要通过浏览器开发者工具（F12）/APP元素探查工具（UIAutomatorViewer），提取元素的专属属性，常用定位方式按优先级排序：</p>\n<ul>\n<li>\n<p>精准定位：id、name（唯一属性，优先级最高，最稳定）；</p>\n</li>\n<li>\n<p>通用定位：xpath、css selector（支持路径 / 属性匹配，适配无 id/name 的元素，如<code>//input[@placeholder='请输入账号']</code>）；</p>\n</li>\n<li>\n<p>兜底定位：class name、tag name、link text（易重复，仅用于无其他属性的场景）。</p>\n</li>\n</ul>\n<p><strong>核心要求</strong>：元素必须有稳定的标识，一旦元素属性被开发修改（如 id 从<code>btn-login</code>改成<code>btn-login-new</code>），脚本就会 “找不到元素”，直接执行失败。</p>\n<h3 id=\"13-开发自动化执行脚本\">1.3 开发自动化执行脚本</h3>\n<p>基于选定的语言和工具，编写脚本模拟人工操作，核心逻辑是 <strong>“操作链 + 校验点”</strong>，主流有两种开发模式：</p>\n<ul>\n<li>线性脚本：按手工测试的步骤依次编写，如 “打开浏览器→输入网址→点击账号输入框→输入账号→点击密码框→输入密码→点击登录按钮”，适合简单的单流程场景；</li>\n<li>模块化/关键字驱动：将重复操作封装成公共模块（如 “登录模块”“退出模块”），用例直接调用模块，减少代码冗余，适合大型项目的批量用例开发。</li>\n</ul>\n<p>并且，因为页面加载、接口请求会存在延迟和偶发性，为了确保脚本的稳定性，脚本中除了会包含<strong>操作指令</strong>（click/input/clear/select）等，必须加等待避免工具 “提前操作” 导致的失败—— 实际则需要，根据不同的场景引入不同的等待机制（强制等待/显式等待/隐式等待）。</p>\n<h3 id=\"14-执行测试\">1.4 执行测试</h3>\n<p>运行编写好的脚本，工具会自动驱动浏览器/模拟器，按脚本步骤执行操作；执行到关键节点时，会通过<strong>断言语句</strong>校验实际结果是否符合预期，比如 “登录后是否跳转到首页”“点击提交后是否显示‘提交成功’提示”“列表是否加载出指定数据”，断言通过则用例成功，失败则终止并记录错误。</p>\n<h3 id=\"15-生成测试报告与问题排查\">1.5 生成测试报告与问题排查</h3>\n<p>执行完成后，可以是由工具来自动生成报告，也可以是人工来整理报告。</p>\n<p>测试人员需要根据报告来排查问题，如区分是<strong>脚本问题</strong>（定位方式错误）、<strong>页面问题</strong>（元素属性修改）还是<strong>工具环境问题</strong>（驱动版本不兼容），再针对性修改。</p>\n<h2 id=\"2-传统ui自动化的核心痛点\">2. 传统UI自动化的核心痛点</h2>\n<p>正因为传统UI自动化工具，主要是依赖 “固定元素定位” 和 “死板脚本逻辑”，导致在如今快敏捷开发、高频迭代的场景中，暴露了诸多难以解决的问题：</p>\n<ol>\n<li>\n<p><strong>元素定位依赖“硬编码”</strong>：传统的UI自动化测试工具，比如Selenium、Appium、Playwright等，通常需要通过ID、Class、Xpath等固定的元素属性来定位元素，但前端开发常因需求调整修改DOM结构（比如组件库升级、样式优化、布局调整），导致元素ID、Class这类属性可能会出现动态变化，脚本极易因为这些变化，导致“找不到元素”，据统计，企业级项目中约<code>40%-60%</code>的UI自动化用例失败源于元素定位失效，而非真实功能缺陷。</p>\n</li>\n<li>\n<p><strong>维护成本极高</strong>：开发迭代会频繁修改页面元素属性、调整页面布局，测试人员需要同步修改所有相关脚本的定位方式，迭代越快，维护工作量越大，甚至出现 “维护脚本的时间超过开发脚本的时间”；</p>\n</li>\n<li>\n<p><strong>对非标准元素无能为力</strong>：无法识别图片、验证码、动态生成的元素（如随机 id、动态弹窗），遇到这类场景只能通过人工介入或额外的破解方案，自动化覆盖率受限；</p>\n</li>\n<li>\n<p><strong>学习门槛不低</strong>：需要掌握开发语言（Python/Java）、工具 API、框架封装，非技术背景的测试人员难以上手；</p>\n</li>\n<li>\n<p><strong>无法像人一样 “认知” 页面</strong>：工具只能识别元素属性，无法理解页面的语义和逻辑，比如 “红色的提交按钮”“位于页面底部的分页栏”，一旦元素位置变化，即使属性不变，也可能因脚本逻辑问题执行失败。</p>\n</li>\n</ol>\n<h2 id=\"3-ai技术在ui自动化测试的六种玩法\">3. AI技术在UI自动化测试的六种玩法</h2>\n<p>针对上述问题，AI 在 UI 自动化测试中的应用，可以有很多种组合玩法。</p>\n<p>核心是通过<strong>大语言模型（LLM）</strong>、<strong>计算机视觉（CV）、自然语言处理（NLP）、机器学习（ML）</strong> 等技术，解决传统方案中“元素定位难、脚本维护成本高、跨端兼容性差、人力依赖强”等问题。</p>\n<p>有了AI的融入，自动化测试，不再是依赖 “固定元素属性”，而是让测试工具像<strong>人一样 “看” 页面、“理解” 页面、“推理” 操作逻辑</strong>。</p>\n<p>以<strong>计算机视觉（CV）</strong>和<strong>自然语言处理（NLP）</strong> 为代表的AI技术为例，可以通过“感知界面内容+理解用户意图”的方式，推动UI自动化测试从“硬编码规则驱动”向“认知驱动”转型，其核心理念可概括为：<strong>让测试系统像人类一样“看懂界面”“听懂需求”，并自主完成操作与验证</strong>。</p>\n<h4 id=\"玩法-1利用ai-视觉定位--替代传统元素定位\">玩法 1：利用AI 视觉定位 —— 替代传统元素定位</h4>\n<p>基于<strong>计算机视觉（CV）+ 图像识别</strong>技术，让测试工具不再依赖元素的 <code>id/xpath</code> 等固定属性，而是像人一样通过 “视觉特征” 识别元素，比如 “页面顶部的蓝色登录按钮”“中间的白色输入框，占位符是请输入手机号”“右下角的红色提交按钮”。</p>\n<ul>\n<li>实现方式：测试人员只需用自然语言描述元素（或直接在页面上框选元素），AI 会提取元素的视觉特征（颜色、形状、位置、文字内容），生成唯一的视觉定位标识，即使元素属性被修改、位置被调整，只要视觉特征不变，工具就能精准识别；</li>\n<li>适配场景：高频迭代的页面、动态生成元素的场景、无标准属性的小众页面，彻底解决传统方案 “元素定位失效” 的核心痛点，大幅降低脚本维护成本。</li>\n</ul>\n<h4 id=\"玩法-2自然语言生成自动化用例--让人人都能做自动化\">玩法 2：自然语言生成自动化用例 —— 让人人都能做自动化</h4>\n<p>基于<strong>大语言模型（LLM）</strong>，实现 <strong>“自然语言描述需求→AI 自动生成自动化脚本 / 用例”</strong>，彻底降低自动化的学习门槛，非技术背景的测试人员、产品经理甚至开发，都能快速制作自动化用例。</p>\n<ul>\n<li>实现方式：测试人员只需用日常语言描述测试场景，比如 “打开 XX 网站，输入账号 admin、密码 123456，点击登录，校验是否跳转到首页，是否显示用户名 admin”，AI 会自动识别场景中的操作步骤、校验点，结合对应的测试工具（Selenium/Playwright），生成可直接运行的自动化脚本，还能自动添加等待机制、断言语句；</li>\n<li>延伸能力：还可以通过 “上传手工测试用例文档→AI 批量解析生成自动化用例”，实现用例的批量转化，大幅提升自动化覆盖率的转化效率。</li>\n</ul>\n<h4 id=\"玩法-3ai-智能维护脚本--自动化适配页面变化\">玩法 3：AI 智能维护脚本 —— 自动化适配页面变化</h4>\n<p>这是针对传统 “脚本维护成本高” 的核心优化，基于<strong>LLM + 视觉对比</strong>技术，让测试脚本具备 “自我修复” 能力，页面发生变化时，AI 会自动识别并修改脚本，无需人工介入。</p>\n<ul>\n<li>实现方式 1：脚本执行失败时，AI 会自动分析失败原因（如 “元素属性修改”“元素位置变化”），对比新旧页面的元素特征，自动更新脚本中的定位方式，再重新执行用例；</li>\n<li>实现方式 2：开发提交代码后，AI 会自动对比迭代前后的页面差异，识别出被修改的元素，批量更新所有相关的自动化脚本，实现 “页面变，脚本自动变”。</li>\n</ul>\n<h4 id=\"玩法-4ai-驱动的探索式测试--全自动覆盖页面场景\">玩法 4：AI 驱动的探索式测试 —— 全自动覆盖页面场景</h4>\n<p>传统自动化只能执行 “预设好的脚本”，覆盖的场景有限，而基于<strong>多模态 AI + 强化学习</strong>的探索式测试，让工具能像<strong>资深测试工程师一样，自主探索页面的所有功能场景</strong>，自动执行操作、校验结果，发现人工测试和传统自动化容易遗漏的 BUG。</p>\n<ul>\n<li>实现方式：AI 先通过视觉识别和语义理解，分析页面的所有元素（按钮、输入框、下拉框、链接等）和业务逻辑，生成合理的测试路径；然后自主执行操作（如随机点击、输入不同类型的数据、触发不同的交互组合），同时实时校验页面的响应结果（如是否报错、是否跳转到错误页面、数据是否显示异常），发现问题后自动截图、记录日志并标记 BUG 等级；</li>\n<li>核心价值：适合在新版本上线前，快速做全量场景的探索式测试，覆盖人工难以考虑到的边界场景、组合场景，提升测试的完整性。</li>\n</ul>\n<h4 id=\"玩法-5ai-生成测试数据--自动化适配场景造数据\">玩法 5：AI 生成测试数据 —— 自动化适配场景造数据</h4>\n<p>传统自动化测试中，测试数据（如账号、手机号、身份证号、订单号）需要人工准备或通过代码生成，而 AI 能根据测试场景，<strong>智能生成贴合业务的真实测试数据</strong>，还能自动适配边界值、异常值场景。</p>\n<ul>\n<li>实现方式：测试人员只需描述数据需求，比如 “生成 10 个有效的手机号”“生成 5 个不符合格式的身份证号”“生成 3 个符合电商平台规则的订单号，包含字母和数字”，AI 会自动生成对应的测试数据，还能直接将数据注入到自动化脚本中，实现 “数据 + 脚本” 的一体化执行。</li>\n</ul>\n<h4 id=\"玩法-6多模态-ai-测试--覆盖跨端--非标准-ui-场景\">玩法 6：多模态 AI 测试 —— 覆盖跨端 / 非标准 UI 场景</h4>\n<p>传统工具对<strong>图片、视频、小程序、H5 混合页面</strong>等非标准 UI 场景的覆盖率极低，而基于<strong>多模态大模型</strong>的 AI 测试，能融合 “视觉、文本、语音” 等多种信息，实现跨端、非标准 UI 场景的自动化测试。</p>\n<ul>\n<li>适配场景：识别页面中的图片验证码并自动破解、测试小程序中的图文混合模块、测试 APP 中的语音交互功能、测试电商平台的商品图片展示和点击功能等，彻底打破传统 UI 自动化的场景覆盖限制。</li>\n</ul>\n<p>除了上述提到的六种AI玩法外，其实还有很多，这里就不一一列举了，后面将会在「<strong>狂师. AI进化社</strong>」中，逐步分享。</p>\n<h2 id=\"4-ai测试工具\">4. AI测试工具</h2>\n<p>讲到这里，你可能会说，上面这些AI测试的玩法是不是都还是停留在理论层面，属于理想测试的状态吧，但现实是，你或许还停留在“井”里，事实是，已经有很多先辈，利用上面的这些AI玩法，已经在2倍、10倍、提升工作效能了。</p>\n<p>要实现上述的这些玩法，有两种方式：</p>\n<ul>\n<li>自己利用AI技术，自己开发，自己定制。（灵活度高，对技术要求也高）</li>\n<li>用现有的AI测试工具。（缺点有什么，用什么，优点是快，开箱即用）</li>\n</ul>\n<p>目前，市面上的AI测试工具有很多，在我之前公众号技术推文分享中，也给大家推荐过一些，比如<code>Testim</code>、<code>Applitools</code>、<code>Browser Use</code>、<code>Midscene</code>、<code>Playwright MCP</code>等。</p>\n<p>如果是你是AI零基础新手，此前，从来没有接触过AI测试工具，那我强烈你先使用<code>Midscene</code>。</p>\n<p><code>Midscene</code>作为AI测试工具中的典型代表，它更接近真实用户操作视角，对视觉变化适应性强，减少了对DOM的绝对依赖，相比有些AI测试工具来说，它在定位动态元素时更稳定，尤其在桥接模式下，还能复用浏览器Cookie，避免重复登录的问题。</p>\n<p>接下来，我就以<code>Midscene</code> 为例，来带大家感受一下Midscen`做UI自动化测试的魅力，不需要你写一行代码，不需要你定位元素，你只需要描述需求，指拿打拿，任何人都可以用它来开展自动化测试。</p>\n<blockquote>\n<p>其它的AI测试工具，后续会逐步分享到<strong>狂师.AI 进化社</strong>-&gt;<strong>AI工具百宝箱</strong>对应的版块中。</p>\n</blockquote>\n<h2 id=\"5-带你快速体验-midscens\">5. 带你快速体验 Midscens</h2>\n<p>由于本篇教程，并不是专门讲解<code>Midscens</code>的特性，主要是让大家提前感受一下AI测试所带来的实际效果。</p>\n<p>所以不会，花太多内容篇符来介绍<code>Midscens</code>特性，具体关于<code>Midscens</code>的工具介绍，后续在<strong>AI进化社</strong>中，会有专门工具教程分享。</p>\n<p>为了照顾新手，还是简单科普一下：</p>\n<blockquote>\n<p>Midscens，更具体一点的叫法，是<code>Midscene.js</code>，由字节跳动的Web Infra团队开发（一个负责字节内部Web基础设施效能提升的团队）。</p>\n<p>定位：是一款 AI驱动的自动化测试工具，且是开源的。</p>\n<p>它通过多模态大语言模型（LLM）实现对用户界面的“理解”，并自动执行端到端（e2e）测试任务，核心特点是能够直接解析 UI 元素的视觉和语义信息，模拟用户操作行为。</p>\n</blockquote>\n<p><strong>简单来说</strong>：<code>Midscene.js</code> 是一个让使用者，直接用自然语言的方式描述 UI 操作，由 AI帮你自动定位元素、生成脚本、执行测试”的智能 UI 自动化工具。人提需求，AI帮你自动执行测试，本质上和AI 编程的思想是一样的。</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260131214243942.png\" /></p>\n<p>更进一步的介绍，感兴趣的话，可以访问它的<a href=\"https://midscenejs.com/zh/\" rel=\"noopener nofollow\" target=\"_blank\">官网</a>来学习：<code>https://midscenejs.com/</code></p>\n<ul>\n<li>帮助文档：<a href=\"https://midscenejs.com/zh/introduction\" rel=\"noopener nofollow\" target=\"_blank\">https://midscenejs.com/zh/introduction</a></li>\n<li>源码地址：<a href=\"https://github.com/web-infra-dev/midscene\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/web-infra-dev/midscene</a></li>\n</ul>\n<p>这里，值得再提一点的是，<code>Midscene.js</code>虽最初是为Web自动化测试而设计的，但它也同样支持Android、iOS等多端应用，任意界面都可以。</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260131214515527.png\" /></p>\n<p>说了，这么多，新手该如何使用呢？概括起来，分几步：</p>\n<h3 id=\"51-环境准备\">5.1 环境准备</h3>\n<p><code>Midscene.js</code>，提供了多种集成使用方式：</p>\n<ul>\n<li>浏览器插件</li>\n<li>与 Playwright的SDK集成（又分为两种：一种直接用脚本方式集成和调用<code>Midscene Agent</code>，另外一种在 <code>Playwright</code> 的测试用例中集成<code>Midscene</code>模块）</li>\n<li>与Puppeteer的SDK集成 (也有两种集成方式，可参考Playwright，同上)</li>\n<li>命令行调用</li>\n<li>API调用</li>\n</ul>\n<p>需要注意，不同端，需要准备的环境会有所不一样，新手优先推荐以<strong>Web端+浏览器插件</strong>使用方式为主，通过使用<code>Midscene.js Chrome</code>插件，你可以快速在任意网页上体验 <code>Midscene</code> 的主要功能，而无需编写任何代码。</p>\n<p>其它的集成方式，以及<code>Android</code>、<code>iOS</code>端如何使用<code>Midscene</code>，后面会根据MVP迭代的安排，逐步在<code>AI 工具百宝箱</code>版块内更新，大家感兴趣的话，也可以提前查看<a href=\"https://midscenejs.com/zh/android-introduction.html\" rel=\"noopener nofollow\" target=\"_blank\">官方帮助文档</a>，官方文档里也提供了很多实战案例。</p>\n<h3 id=\"52-安装方法\">5.2 安装方法</h3>\n<p>如果想通过 <code>Midscene.js Chrome</code> 插件快速体验AI测试的话，我们需要先前往 Chrome 扩展商店安装 <code>Midscene 扩展</code></p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201124737578.png\" /></p>\n<p>或者直接在<code>github</code>中下载releaese压缩包，直接拖到chrome浏览器扩展程序页中，即可安装</p>\n<pre><code>https://github.com/web-infra-dev/midscene/releases\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201101301468.png\" /></p>\n<p>插件安装完成后，启动扩展（可能默认折叠在 Chrome 扩展列表中），你应该能在浏览器右侧看到名为 “Midscene” 的侧边栏。</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201125014786.png\" /></p>\n<h3 id=\"53-ai模型选择\">5.3 AI模型选择</h3>\n<p>安装完<code>Midscene</code>插件之后，你还无法立即正常使用它来开展自动化测试，因为你此时只是安装了一个躯壳，还没有指挥大脑，而这个大脑，就是我们常说的<strong>大模型</strong>。</p>\n<p>讲到这里，就不得不提一句 AI 工具的核心本质：<strong>几乎所有的 AI 测试工具，乃至各类 AI 编程工具，本质上都只是做了一层面向用户的交互应用层封装</strong>，其核心能力的实现，底层终究需要依托特定的大模型来驱动。而一款 AI 测试工具的实际使用效果好坏，也正与它背后选用的大模型能力强弱、适配性高低息息相关。</p>\n<p>而使用 AI 大模型驱动 UI 自动化的有两个关键点：规划合理的操作路径，以及准确找到需要交互的元素。其中“元素定位”能力的强弱，会直接影响到自动化任务的成功率。</p>\n<p>为了完成元素定位工作，UI 自动化框架一般有两种技术路线：</p>\n<ul>\n<li>基于 DOM + 截图标注：提前提取页面的 DOM 结构，结合截图做好标注，请模型“挑选”其中的内容。</li>\n<li>纯视觉：利用模型的视觉定位能力，基于截图完成所有分析工作，即模型收到的只有图片，没有 DOM，也没有标注信息。</li>\n</ul>\n<h4 id=\"1-midscene-支持纯视觉元素定位\">1. Midscene 支持纯视觉元素定位</h4>\n<p><code>Midscene</code> 早期同时兼容「DOM 定位」和「纯视觉」两种技术路线。</p>\n<p>但DOM 定位方案的稳定性不足，经常会在 Canvas 元素、CSS background-image 绘制的控件、跨域 iframe 中的内容、没有充分被辅助技术标注的元素等情况下出现定位偏差。</p>\n<p>与此同时，采用「纯视觉」方案，相对于DOM定位方案来说，优势明显，主要体现在：</p>\n<ul>\n<li><strong>效果稳定</strong>：在 UI 操作规划、组件定位、界面理解等领域的综合表现较好，能够帮助使用者更快上手。</li>\n<li><strong>适用于任意系统</strong>：自动化框架不再依赖 UI 渲染的技术栈。无论是 Android、iOS、桌面应用，还是浏览器中的 canvas 标签，只要能获取截图，Midscene 即可完成交互操作。</li>\n<li><strong>易于编写</strong>：抛弃各类 selector 和 DOM 之后，使用者与模型的“磨合”会变得更简单，不熟悉渲染技术的新人也能很快上手。</li>\n<li><strong>token 量显著下降</strong>：相较于 DOM 方案，视觉方案的 token 使用量最多可以减少 80%，成本更低，且本地运行速度也变得更快。</li>\n</ul>\n<p>因此，<strong>从 1.0 版本开始，Midscene 只支持纯视觉方案</strong>，在UI操作与元素定位方面，不再提供“提取 DOM”的兼容模式。</p>\n<h4 id=\"2-midscene推荐使用的视觉模型\">2. Midscene推荐使用的视觉模型</h4>\n<p>使用<code>Midscene</code>，选择模型时，也是有讲究的，并不是随便找个大模型都可以用，必须使用多模型模型，也就是需要支持图像输入，视觉能力的模型才可以。</p>\n<p>官方，推荐使用 <code>Midscene</code> 的默认模型有：</p>\n<ul>\n<li><a href=\"https://midscenejs.com/zh/model-common-config.html#doubao-seed-model\" rel=\"noopener nofollow\" target=\"_blank\">豆包 Seed 模型</a></li>\n<li><a href=\"https://midscenejs.com/zh/model-common-config.html#qwen3-vl\" rel=\"noopener nofollow\" target=\"_blank\">千问 Qwen3-VL</a></li>\n<li><a href=\"https://midscenejs.com/zh/model-common-config.html#qwen25-vl\" rel=\"noopener nofollow\" target=\"_blank\">千问 Qwen2.5-VL</a></li>\n<li><a href=\"https://midscenejs.com/zh/model-common-config.html#glm-v\" rel=\"noopener nofollow\" target=\"_blank\">智谱 GLM-V</a></li>\n<li><a href=\"https://midscenejs.com/zh/model-common-config.html#auto-glm\" rel=\"noopener nofollow\" target=\"_blank\">智谱 AutoGLM</a></li>\n<li><a href=\"https://midscenejs.com/zh/model-common-config.html#gemini-3-pro\" rel=\"noopener nofollow\" target=\"_blank\">Gemini-3-Pro / Gemini-3-Flash</a></li>\n<li><a href=\"https://midscenejs.com/zh/model-common-config.html#ui-tars\" rel=\"noopener nofollow\" target=\"_blank\">UI-TARS</a></li>\n</ul>\n<p>这些模型都具备良好的“元素定位”能力，且在任务规划、界面理解等场景上也有不错的表现。如果你不知道从哪里开始，选用你眼下最容易获得的模型即可。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">模型系列</th>\n<th style=\"text-align: left;\">部署</th>\n<th style=\"text-align: left;\">Midscene 评价</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">豆包 Seed 模型 <br /><a href=\"https://midscenejs.com/zh/model-common-config.html#doubao-seed-model\" rel=\"noopener nofollow\" target=\"_blank\">快速配置</a></td>\n<td style=\"text-align: left;\">火山引擎版本： <a href=\"https://www.volcengine.com/docs/82379/1799865\" rel=\"noopener nofollow\" target=\"_blank\">Doubao-Seed-1.6-Vision</a></td>\n<td style=\"text-align: left;\">⭐⭐⭐⭐                         UI 操作规划、定位能力较强 速度略慢</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">千问 Qwen3-VL<br /> <a href=\"https://midscenejs.com/zh/model-common-config.html#qwen3-vl\" rel=\"noopener nofollow\" target=\"_blank\">快速配置</a></td>\n<td style=\"text-align: left;\"><a href=\"https://help.aliyun.com/zh/model-studio/vision\" rel=\"noopener nofollow\" target=\"_blank\">阿里云</a> <a href=\"https://openrouter.ai/qwen\" rel=\"noopener nofollow\" target=\"_blank\">OpenRouter</a> <a href=\"https://ollama.com/library/qwen3-vl\" rel=\"noopener nofollow\" target=\"_blank\">Ollama(开源)</a></td>\n<td style=\"text-align: left;\">⭐⭐⭐⭐ <br />复杂场景断言能力不够稳定 性能超群，操作准确 有开源版本（<a href=\"https://huggingface.co/Qwen\" rel=\"noopener nofollow\" target=\"_blank\">HuggingFace</a> / <a href=\"https://github.com/QwenLM/\" rel=\"noopener nofollow\" target=\"_blank\">Github</a>）</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">千问 Qwen2.5-VL <br /><a href=\"https://midscenejs.com/zh/model-common-config.html#qwen25-vl\" rel=\"noopener nofollow\" target=\"_blank\">快速配置</a></td>\n<td style=\"text-align: left;\"><a href=\"https://help.aliyun.com/zh/model-studio/vision\" rel=\"noopener nofollow\" target=\"_blank\">阿里云</a> <a href=\"https://openrouter.ai/qwen\" rel=\"noopener nofollow\" target=\"_blank\">OpenRouter</a></td>\n<td style=\"text-align: left;\">⭐⭐⭐ <br />综合效果不如 Qwen3-VL</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">智谱 GLM-4.6V <br /><a href=\"https://midscenejs.com/zh/model-common-config.html#glm-v\" rel=\"noopener nofollow\" target=\"_blank\">快速配置</a></td>\n<td style=\"text-align: left;\"><a href=\"https://docs.z.ai/guides/vlm/glm-4.6v\" rel=\"noopener nofollow\" target=\"_blank\">Z.AI (Global)</a> <a href=\"https://docs.bigmodel.cn/cn/guide/models/vlm/glm-4.6v\" rel=\"noopener nofollow\" target=\"_blank\">BigModel (CN)</a></td>\n<td style=\"text-align: left;\">全新接入，欢迎体验 模型权重开源<a href=\"https://huggingface.co/zai-org/GLM-4.6V\" rel=\"noopener nofollow\" target=\"_blank\">HuggingFace</a></td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Gemini-3-Pro / Gemini-3-Flash <br /><a href=\"https://midscenejs.com/zh/model-common-config.html#gemini-3-pro\" rel=\"noopener nofollow\" target=\"_blank\">快速配置</a></td>\n<td style=\"text-align: left;\"><a href=\"https://ai.google.dev/gemini-api/docs/models/gemini\" rel=\"noopener nofollow\" target=\"_blank\">Google Cloud</a></td>\n<td style=\"text-align: left;\">⭐⭐⭐ <br />支持 Gemini-3-Flash 价格高于豆包和千问</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">UI-TARS<br /> <a href=\"https://midscenejs.com/zh/model-common-config.html#ui-tars\" rel=\"noopener nofollow\" target=\"_blank\">快速配置</a></td>\n<td style=\"text-align: left;\"><a href=\"https://www.volcengine.com/docs/82379/1536429\" rel=\"noopener nofollow\" target=\"_blank\">火山引擎</a></td>\n<td style=\"text-align: left;\">⭐⭐ <br />有探索能力，但在不同场景表现可能差异较大 有开源版本（<a href=\"https://huggingface.co/bytedance-research/UI-TARS-72B-SFT\" rel=\"noopener nofollow\" target=\"_blank\">HuggingFace</a> / <a href=\"https://github.com/bytedance/ui-tars\" rel=\"noopener nofollow\" target=\"_blank\">Github</a>）</td>\n</tr>\n</tbody>\n</table>\n<p>如果你完全没有头绪，我推荐选择<code>千问 Qwen3-VL</code>或智谱<code> GLM-4.6V</code>。</p>\n<h3 id=\"54-模型配置\">5.4 模型配置</h3>\n<h4 id=\"1-申请大模型api-key\">1. 申请大模型API KEY</h4>\n<p>以<code>千问 Qwen3-VL</code>为例。</p>\n<p>1、访问<code>DashScope</code>管理<a href=\"https://dashscope.console.aliyun.com/\" rel=\"noopener nofollow\" target=\"_blank\">控制台</a></p>\n<pre><code>https://dashscope.console.aliyun.com/\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201122300613.png\" /></p>\n<p>2、点击开通服务（DashScope 模型服务），跳转到阿里云百炼平台</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201122342667.png\" /></p>\n<p>3、开通之后，新用户每个模型都会赚送100w的免费体验额度</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201122509525.png\" /></p>\n<p>在阿里云百炼模型广场中，可以查看到所有支持的模型</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201122837281.png\" /></p>\n<p>比如我们要选用的<code>qwen3-vl-plus</code>，这个是通义千问推出来的新版视觉理解模型，如果你不清楚，你能不能用这个模型，可以点击进入看一下还有没有免费额度</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201123211973.png\" /></p>\n<p>如果免费额度用完了，还想用，也可以再花点小钱，购买它的token套餐。</p>\n<p>4、选择<code>密钥管理</code>-&gt;创建<code>API KEY</code></p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201123547539.png\" /></p>\n<p>复制<code>API Key</code>，不要泄露给别人，自己拿个小本本记下来就行</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201123629460.png\" /></p>\n<h4 id=\"2-配置环境变量\">2. 配置环境变量</h4>\n<p>有了API Key之后，需要将模型配置参数项放置在系统环境变量中，Midscene 会自动读取这些环境变量。</p>\n<p>配置环境变量常用的有两种方式：</p>\n<ul>\n<li>在 <code>Midscene Chrome</code> 插件中，你也可以使用这种 <code>export KEY=\"value\"</code> 配置格式</li>\n</ul>\n<pre><code class=\"language-bash\"># 替换为你自己的 API Key\nexport MIDSCENE_MODEL_BASE_URL=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\nexport MIDSCENE_MODEL_API_KEY=\"sk-abcdefghijklmnopqrstuvwxyz\"\nexport MIDSCENE_MODEL_NAME=\"qwen3-vl-plus\"\nexport MIDSCENE_MODEL_FAMILY=\"qwen3-vl\"\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201125206521.png\" /></p>\n<ul>\n<li>在项目的运行路径下创建一个 <code>.env</code> 文件，并添加以下内容，Midscene 的命令行工具默认会读取这个文件。（该种方式适用于命令行工具）</li>\n</ul>\n<pre><code class=\"language-BASH\">MIDSCENE_MODEL_BASE_URL=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\nMIDSCENE_MODEL_API_KEY=\"sk-abcdefghijklmnopqrstuvwxyz\"\nMIDSCENE_MODEL_NAME=\"qwen3-vl-plus\"\nMIDSCENE_MODEL_FAMILY=\"qwen3-vl\"\n</code></pre>\n<blockquote>\n<p>请注意：这里不需要在每一行前添加 <code>export</code>，且只有 Midscene 命令行工具会默认读取这个文件</p>\n</blockquote>\n<h3 id=\"55-快速体验\">5.5 快速体验</h3>\n<p>讲到这里，所有的准备工作，都已经做好了，想必此时的你，已经迫不及待，要体验一下Midscene了。</p>\n<p>1、在Midscene输入框中，输入提示词指令：<code>打开浏览器，访问百度页面，在输入框中Python，点击搜索</code>，选择第一个Action类型，可以理解，该类型会根据你的需求自动帮你规划，执行，有点类似Agent一样。</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260202084359423.png\" /></p>\n<p>几个常用的操作类型：</p>\n<ul>\n<li>Action：AI自动规划执行任务</li>\n<li>Query：常用于提取数据</li>\n<li>Assert：常用于断言</li>\n<li>Tap（点击）：用于点击操作</li>\n</ul>\n<p>2、当点击运行后，Midscene会根据我们的指令意图，帮我们自动规则任务，执行操作，比如打开百度，自动找到百度页面的输入框，并输入Python，点击搜索等的操作。所有的控制过程，都不需要我们自己写代码，找元素定位，你只需要将你的要求用自然语言描述出来就可以了。</p>\n<p>具体执行效果，大家可以看这个动画视频，也可自行在本地尝试运行。</p>\n<blockquote>\n<p>这里面有一个小坑，如果你打开的是一个空网页，即地址栏为空时，发送指令时，会报错Cannot access a chrome:// URL</p>\n</blockquote>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201132348598.png\" /></p>\n<p>也就意味着，大家用Midscene浏览器插件进行AI自动化时，要先停留在某个页面，至少要确保地址栏不能为空，这个有点小鸡肋。当然影响不大，看官方后续会不会优化一下吧。</p>\n<p>3、除了这种常规用法，<code>Midscene</code>还支持桥接模式连接，<code>Midscene Chrome</code> 插件的桥接模式允许你使用本地脚本来控制桌面版 Chrome。脚本既能连接新标签页，也可以附着到当前激活的标签页。这种方式能复用本地浏览器的 cookies、插件和页面状态，与自动化脚本协作完成任务；在自动化领域也被称作 “man-in-the-loop”。</p>\n<p><strong>为什么需要桥接模式呢？</strong> 因为正常情况下，<code>Midscene</code>每次执行任务都会先打开一个全新的浏览器窗口，但很多场景下，操作页面都需要进行用户登录认证，通过桥接模式连接，可以直接复用本地已有的Cookie和页面状态，不必在执行AI测试任务期间手工来完成登录操作。</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201133546151.png\" /></p>\n<p>运行任务前，先点击 “Allow connection” ，执行后你会看到插件状态变为 “connected”</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201133639128.png\" /></p>\n<p>讲到这里，你已经基本掌握了如何利用插件的方式来简单使用<code>Midscene</code>了，但还不够。</p>\n<h3 id=\"56-进阶使用\">5.6 进阶使用</h3>\n<p>前面我们已经学习了，如何通过Chrome浏览器插件的方式来使用Midscene了，但这种方式比较鸡肋，不太适合<strong>规划化</strong>、<strong>持久化</strong>、<strong>资产化</strong>。</p>\n<p><strong>怎么理解呢？</strong></p>\n<p>你想，我们利用Midscene开展自动化时，总不能每次在执行自动化测试之前，还要人工手动打开浏览器插件，把指令手工喂给它吧，那这不叫自动化测试！</p>\n<p>自动化测试，最起码的一点要求，前期只要准备好了测试脚本或AI指令提示词，后续的所有执行动作，都应该是不需要人为介入的。</p>\n<p>还有更重要的一点，浏览器插件的这种方式，我们很难在企业中，将业务测试用例持久化、资产化复用起来。</p>\n<p>所以接下来，我再花一点点时间，分享一下，Midscene另外的一种进阶用法：<strong>命令行+配置文件</strong>的方式。</p>\n<p>简单来说，Midscene 定义了一种 YAML 格式的脚本，可以让开发者快速编写自动化脚本，并提供了对应的命令行工具来快速执行这些脚本。</p>\n<blockquote>\n<p>如果说插件的方式，只是为了提供给使用者简单体验一下，那<strong>命令行+配置文件</strong>方式，就比较适合企业内正式使用了。</p>\n</blockquote>\n<h4 id=\"1-如何上手\">1. 如何上手</h4>\n<p>1、全局安装 <code>@midscene/cli</code> （推荐新手使用）：</p>\n<pre><code>npm i -g @midscene/cli\n</code></pre>\n<p>或在项目中按需安装</p>\n<pre><code>npm i @midscene/cli --save-dev\n</code></pre>\n<p>2、新建一个项目目录，在目录下编写YAML配置文件，可以理解为YAML配置文件就是midscene的测试脚本，比如创建一个名为 <code>bing-search.yaml</code> 的文件：</p>\n<pre><code class=\"language-YAML\">web:\n  url: https://www.bing.com\n\ntasks:\n  - name: 搜索天气\n    flow:\n      - ai: 搜索 \"今日天气\"\n      - sleep: 3000\n      - aiAssert: 结果显示天气信息\n</code></pre>\n<p>3、在项目目录，创建.env文件，配置AI模型驱动的环境变量信息</p>\n<pre><code class=\"language-bash\">MIDSCENE_MODEL_BASE_URL=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n# 替换成自己的API KEY\nMIDSCENE_MODEL_API_KEY=\"sk-abcdefghijklmnopqrstuvwxyz\"\nMIDSCENE_MODEL_NAME=\"qwen3-vl-plus\"\nMIDSCENE_MODEL_FAMILY=\"qwen3-vl\"\n</code></pre>\n<p>4、通过命令行来执行它：</p>\n<pre><code class=\"language-bash\">midscene ./bing-search.yaml\n# 如果在项目中安装了 Midscene\nnpx midscene ./bing-search.yaml\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201141958373.png\" /></p>\n<p>5、在配置文件中，如果涉及到一些动态变量，可以通过 <code>${variable-name}</code> 引用环境变量来实现，如：</p>\n<pre><code>topic=weather today\n</code></pre>\n<pre><code># ...\n- ai: 在输入框中输入 ${topic}\n# ...\n</code></pre>\n<p>6、<code>@midscene/cli</code> 支持使用通配符匹配多个脚本来批量执行脚本，这相当于 <code>--files</code> 参数的简写。</p>\n<pre><code># 运行单个脚本\nmidscene ./bing-search.yaml\n\n# 使用通配符模式运行所有匹配的脚本\nmidscene './scripts/**/*.yaml'\n</code></pre>\n<p>7、执行完成后，输出目录会包含：</p>\n<ul>\n<li><code>--summary</code> 指定的 JSON 报告（默认 <code>index.json</code>），记录所有脚本的执行状态与统计数据。</li>\n<li>每个 YAML 文件对应的独立执行结果（JSON 格式）。</li>\n<li>每个脚本生成的可视化报告（HTML 格式）。</li>\n</ul>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260201142119212.png\" /></p>\n<p>8、默认情况下脚本是在无头模式下运行，如果想打开浏览器窗口，可以带上<code>Headed</code>参数</p>\n<pre><code class=\"language-bash\"># 运行在 headed 模式\nmidscene /path/to/yaml --headed\n\n# 运行在 headed 模式并在结束后保留窗口\nmidscene /path/to/yaml --keep-window\n</code></pre>\n<p>9、如果你想在脚本中，使用桥接模式让 YAML 脚本驱动现有的桌面浏览器，复用 Cookies、插件或已有状态。先安装 Chrome 扩展，然后在 <code>web</code> 配置中加入：</p>\n<pre><code>web:\n  url: https://www.bing.com\n+ bridgeMode: newTabWithUrl\n</code></pre>\n<h4 id=\"2-命令行参数\">2. 命令行参数</h4>\n<p>命令行工具提供了多项参数，用于控制脚本的执行行为：</p>\n<ul>\n<li><code>--files &lt;file1&gt; &lt;file2&gt; ...</code>：指定脚本文件列表。默认按顺序执行（<code>--concurrent</code> 为 <code>1</code>），可通过 <code>--concurrent</code> 设置并发数量。支持 <a href=\"https://www.npmjs.com/package/glob\" rel=\"noopener nofollow\" target=\"_blank\">glob</a> 通配符语法。</li>\n<li><code>--concurrent &lt;number&gt;</code>：设置并发执行的数量，默认 <code>1</code>。</li>\n<li><code>--continue-on-error</code>：启用后，即使某个脚本失败也会继续执行后续脚本。默认关闭。</li>\n<li><code>--share-browser-context</code>：在多个脚本之间共享浏览器上下文（Cookies、<code>localStorage</code> 等），适合需要连续登录态的场景。默认关闭。</li>\n<li><code>--summary &lt;filename&gt;</code>：指定生成的 JSON 总结报告路径。</li>\n<li><code>--headed</code>：在带界面的浏览器中运行脚本，而非默认的无头模式。</li>\n<li><code>--keep-window</code>：脚本执行完成后保持浏览器窗口，会自动开启 <code>--headed</code> 模式。</li>\n<li><code>--config &lt;filename&gt;</code>：指定配置文件，文件中的参数会作为命令行参数的默认值。</li>\n<li><code>--web.userAgent &lt;ua&gt;</code>：设置浏览器 UA，覆盖所有脚本中的 <code>web.userAgent</code>。</li>\n<li><code>--web.viewportWidth &lt;width&gt;</code>：设置浏览器视口宽度，覆盖所有脚本中的 <code>web.viewportWidth</code>。</li>\n<li><code>--web.viewportHeight &lt;height&gt;</code>：设置浏览器视口高度，覆盖所有脚本中的 <code>web.viewportHeight</code>。</li>\n<li><code>--android.deviceId &lt;device-id&gt;</code>：设置安卓设备 ID，覆盖所有脚本中的 <code>android.deviceId</code>。</li>\n<li><code>--ios.wdaPort &lt;port&gt;</code>：设置 WebDriverAgent 端口，覆盖所有脚本中的 <code>ios.wdaPort</code>。</li>\n<li><code>--ios.wdaHost &lt;host&gt;</code>：设置 WebDriverAgent 主机地址，覆盖所有脚本中的 <code>ios.wdaHost</code>。</li>\n<li><code>--dotenv-debug</code>：开启 dotenv 的调试日志，默认关闭。</li>\n<li><code>--dotenv-override</code>：允许 dotenv 覆盖同名的全局环境变量，默认关闭。</li>\n</ul>\n<p>比如，使用 <code>--files</code> 指定执行顺序：</p>\n<pre><code class=\"language-bash\">midscene --files ./login.yaml ./buy/*.yaml ./checkout.yaml\n</code></pre>\n<p><code>midscene</code>支持并发运行，如以 4 个并发执行所有脚本，并在出错时继续运行：</p>\n<pre><code class=\"language-bash\">midscene --files './scripts/**/*.yaml' --concurrent 4 --continue-on-error\n</code></pre>\n<p>还可以把参数写到 YAML 配置文件中，并通过 <code>--config</code> 引用，命令行传入的参数优先级高于配置文件。</p>\n<pre><code class=\"language-bash\">files:\n  - './scripts/login.yaml'\n  - './scripts/search.yaml'\n  - './scripts/**/*.yaml'\n\nconcurrent: 4\ncontinueOnError: true\nshareBrowserContext: true\n</code></pre>\n<p>运行方式：</p>\n<pre><code class=\"language-bash\">midscene --config ./config.yaml\n</code></pre>\n<p>好了，通过学习Chrome插件和命令行的这两种方式，我相信你，已经可以完全能上手操作<code>midscene</code>，还在等什么🤣，赶紧体验一下AI测试的乐趣吧。</p>\n<blockquote>\n<p>除了文中提到的，midscene还有很多高级的进阶玩法，后面会统一分享到AI进化社内。</p>\n</blockquote>\n<h2 id=\"6-最在最后\">6. 最在最后</h2>\n<p>在牛刀小试版块，特意挑选了<strong>AI编程</strong>和<strong>AI测试</strong>两个场景下的小案例，通过在10分钟内完成第一个AI应用并上线，体验AI编程（Vibe Coding）、AI测试的核心魅力，帮助大家建立对AI能力的直观认知。（当然，目前你所看到的也才只是AI能力的冰山一角）</p>\n<p>回归标题，别再只做点点点的重复劳动！AI+LLM 测试的强大，核心就在于重构了测试的核心逻辑！不管AI技术发展如何强大，但技术终究要回归于人。</p>\n<p>而以 <code>Midscene</code> 为代表的 AI 自动化工具，正彻底改写传统测试、传统编程的游戏规则。它让测试人员、开发人员从纠结 “<strong>怎么做</strong>” 的技术细节里抽离，聚焦于 “<strong>要做什么</strong>” 的核心需求，真正从重复低效的手工操作中解放出来，将精力投入到更高价值的业务场景探索、产品质量深度洞察中，而这恰恰才是 AI+LLM 测试的真正魅力。</p>\n<p>我的实践表明：仅初步使用Midscene，脚本编写效率就提升了3倍以上，维护成本大幅下降，非技术人员也能快速上手写用例。这已不仅是工具升级，更是一场测试理念的进化。</p>\n<p>如果你还在苦于UI自动化，不妨从今天开始：</p>\n<ul>\n<li>安装Midscene插件，用自然语言写第一个测试脚本；</li>\n<li>从小场景验证，逐步扩展到核心业务流程；</li>\n<li>建立团队内的提示词库与最佳实践。</li>\n</ul>\n<p>AI不会取代测试工程师，但会用AI的测试人，必将淘汰那些仍停留在“<strong>手工+硬编码</strong>”时代的团队。如果您有更好的想法或实践，欢迎一起探讨！也欢迎板砖！😄</p>\n\n\n</div>\n<div id=\"MySignature\">\n    技术改变世界！\n         --狂诗绝剑\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 09:02</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jinjiangongzuoshi\">狂师</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Unsafe魔法类深度解析：Java底层操作的终极指南",
      "link": "https://www.cnblogs.com/sevencoding/p/19559023",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/sevencoding/p/19559023\" id=\"cb_post_title_url\" title=\"发布于 2026-02-06 09:00\">\n    <span>Unsafe魔法类深度解析：Java底层操作的终极指南</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"介绍\">介绍</h2>\n<p>Unsafe是位于sun.misc包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等，这些方法在提升Java运行效率、增强Java语言底层资源操作能力方面起到了很大的作用。但由于Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。</p>\n<p>这个类尽管里面的方法都是 public 的，但是并没有办法使用它们，JDK API 文档也没有提供任何关于这个类的方法的解释。总而言之，对于 Unsafe 类的使用都是受限制的，只有授信的代码才能获得该类的实例，当然 JDK 库里面的类是可以随意使用的。</p>\n<p>先来看下这张图，对UnSafe类总体功能：</p>\n<p><img alt=\"\" src=\"https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202404251040445.jpg\" /></p>\n<p>如上图所示，Unsafe提供的API大致可分为内存操作、CAS、Class相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类，下面将对其相关方法和应用场景进行详细介绍。</p>\n<h2 id=\"内存操作\">内存操作</h2>\n<h3 id=\"介绍-1\">介绍</h3>\n<p>如果你是一个写过 C 或者 C++ 的程序员，一定对内存操作不会陌生，而在 Java 中是不允许直接对内存进行操作的，对象内存的分配和回收都是由 JVM 自己实现的。但是在 <code>Unsafe</code> 中，提供的下列接口可以直接进行内存操作：</p>\n<pre><code class=\"language-java\">//分配新的本地空间\npublic native long allocateMemory(long bytes);\n//重新调整内存空间的大小\npublic native long reallocateMemory(long address, long bytes);\n//将内存设置为指定值\npublic native void setMemory(Object o, long offset, long bytes, byte value);\n//内存拷贝\npublic native void copyMemory(Object srcBase, long srcOffset,Object destBase, long destOffset,long bytes);\n//清除内存\npublic native void freeMemory(long address);\n</code></pre>\n<p>使用下面的代码进行测试：</p>\n<pre><code class=\"language-java\">private void memoryTest() {\n    int size = 4;\n    long addr = unsafe.allocateMemory(size);\n    long addr3 = unsafe.reallocateMemory(addr, size * 2);\n    System.out.println(\"addr: \"+addr);\n    System.out.println(\"addr3: \"+addr3);\n    try {\n        unsafe.setMemory(null,addr ,size,(byte)1);\n        for (int i = 0; i &lt; 2; i++) {\n            unsafe.copyMemory(null,addr,null,addr3+size*i,4);\n        }\n        System.out.println(unsafe.getInt(addr));\n        System.out.println(unsafe.getLong(addr3));\n    }finally {\n        unsafe.freeMemory(addr);\n        unsafe.freeMemory(addr3);\n    }\n}\n</code></pre>\n<p>先看结果输出：</p>\n<pre><code class=\"language-plain\">addr: 2433733895744\naddr3: 2433733894944\n16843009\n72340172838076673\n</code></pre>\n<p>分析一下运行结果，首先使用<code>allocateMemory</code>方法申请 4 字节长度的内存空间，调用<code>setMemory</code>方法向每个字节写入内容为<code>byte</code>类型的 1，当使用 Unsafe 调用<code>getInt</code>方法时，因为一个<code>int</code>型变量占 4 个字节，会一次性读取 4 个字节，组成一个<code>int</code>的值，对应的十进制结果为 16843009。</p>\n<p>你可以通过下图理解这个过程：</p>\n<p><img alt=\"\" src=\"https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406202130396.png\" /></p>\n<p>在代码中调用<code>reallocateMemory</code>方法重新分配了一块 8 字节长度的内存空间，通过比较<code>addr</code>和<code>addr3</code>可以看到和之前申请的内存地址是不同的。在代码中的第二个 for 循环里，调用<code>copyMemory</code>方法进行了两次内存的拷贝，每次拷贝内存地址<code>addr</code>开始的 4 个字节，分别拷贝到以<code>addr3</code>和<code>addr3+4</code>开始的内存空间上：</p>\n<p><img alt=\"\" src=\"https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406202130404.png\" /></p>\n<p>拷贝完成后，使用<code>getLong</code>方法一次性读取 8 个字节，得到<code>long</code>类型的值为 72340172838076673。</p>\n<p>需要注意，通过这种方式分配的内存属于 堆外内存 ，是无法进行垃圾回收的，需要我们把这些内存当做一种资源去手动调用<code>freeMemory</code>方法进行释放，否则会产生内存泄漏。通用的操作内存方式是在<code>try</code>中执行对内存的操作，最终在<code>finally</code>块中进行内存的释放。</p>\n<p><strong>为什么要使用堆外内存？</strong></p>\n<ul>\n<li>对垃圾回收停顿的改善。由于堆外内存是直接受操作系统管理而不是 JVM，所以当我们使用堆外内存时，即可保持较小的堆内内存规模。从而在 GC 时减少回收停顿对于应用的影响。</li>\n<li>提升程序 I/O 操作的性能。通常在 I/O 通信过程中，会存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。</li>\n</ul>\n<h3 id=\"典型应用\">典型应用</h3>\n<p><code>DirectByteBuffer</code> 是 Java 用于实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在 Netty、MINA 等 NIO 框架中应用广泛。<code>DirectByteBuffer</code> 对于堆外内存的创建、使用、销毁等逻辑均由 Unsafe 提供的堆外内存 API 来实现。</p>\n<p>下图为 <code>DirectByteBuffer</code> 构造函数，创建 <code>DirectByteBuffer</code> 的时候，通过 <code>Unsafe.allocateMemory</code> 分配内存、<code>Unsafe.setMemory</code> 进行内存初始化，而后构建 <code>Cleaner</code> 对象用于跟踪 <code>DirectByteBuffer</code> 对象的垃圾回收，以实现当 <code>DirectByteBuffer</code> 被垃圾回收时，分配的堆外内存一起被释放。</p>\n<pre><code class=\"language-java\">DirectByteBuffer(int cap) {                   // package-private\n\n    super(-1, 0, cap, cap);\n    boolean pa = VM.isDirectMemoryPageAligned();\n    int ps = Bits.pageSize();\n    long size = Math.max(1L, (long)cap + (pa ? ps : 0));\n    Bits.reserveMemory(size, cap);\n\n    long base = 0;\n    try {\n        // 分配内存并返回基地址\n        base = unsafe.allocateMemory(size);\n    } catch (OutOfMemoryError x) {\n        Bits.unreserveMemory(size, cap);\n        throw x;\n    }\n    // 内存初始化\n    unsafe.setMemory(base, size, (byte) 0);\n    if (pa &amp;&amp; (base % ps != 0)) {\n        // Round up to page boundary\n        address = base + ps - (base &amp; (ps - 1));\n    } else {\n        address = base;\n    }\n    // 跟踪 DirectByteBuffer 对象的垃圾回收，以实现堆外内存释放\n    cleaner = Cleaner.create(this, new Deallocator(base, size, cap));\n    att = null;\n}\n</code></pre>\n<h2 id=\"内存屏障\">内存屏障</h2>\n<h3 id=\"介绍-2\">介绍</h3>\n<p>在介绍内存屏障前，需要知道编译器和 CPU 会在保证程序输出结果一致的情况下，会对代码进行重排序，从指令优化角度提升性能。而指令重排序可能会带来一个不好的结果，导致 CPU 的高速缓存和内存中数据的不一致，而内存屏障（<code>Memory Barrier</code>）就是通过阻止屏障两边的指令重排序从而避免编译器和硬件的不正确优化情况。</p>\n<p>在硬件层面上，内存屏障是 CPU 为了防止代码进行重排序而提供的指令，不同的硬件平台上实现内存屏障的方法可能并不相同。在 Java8 中，引入了 3 个内存屏障的函数，它屏蔽了操作系统底层的差异，允许在代码中定义、并统一由 JVM 来生成内存屏障指令，来实现内存屏障的功能。</p>\n<p><code>Unsafe</code> 中提供了下面三个内存屏障相关方法：</p>\n<pre><code class=\"language-java\">//内存屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能被重排序到屏障前\npublic native void loadFence();\n//内存屏障，禁止store操作重排序。屏障前的store操作不能被重排序到屏障后，屏障后的store操作不能被重排序到屏障前\npublic native void storeFence();\n//内存屏障，禁止load、store操作重排序\npublic native void fullFence();\n</code></pre>\n<p>内存屏障可以看做对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作。以<code>loadFence</code>方法为例，它会禁止读操作重排序，保证在这个屏障之前的所有读操作都已经完成，并且将缓存数据设为无效，重新从主存中进行加载。</p>\n<p>看到这估计很多小伙伴们会想到<code>volatile</code>关键字了，如果在字段上添加了<code>volatile</code>关键字，就能够实现字段在多线程下的可见性。基于读内存屏障，我们也能实现相同的功能。下面定义一个线程方法，在线程中去修改<code>flag</code>标志位，注意这里的<code>flag</code>是没有被<code>volatile</code>修饰的：</p>\n<pre><code class=\"language-java\">@Getter\nclass ChangeThread implements Runnable{\n    /**volatile**/ boolean flag=false;\n    @Override\n    public void run() {\n        try {\n            Thread.sleep(3000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n        System.out.println(\"subThread change flag to:\" + flag);\n        flag = true;\n    }\n}\n</code></pre>\n<p>在主线程的<code>while</code>循环中，加入内存屏障，测试是否能够感知到<code>flag</code>的修改变化：</p>\n<pre><code class=\"language-java\">public static void main(String[] args){\n    ChangeThread changeThread = new ChangeThread();\n    new Thread(changeThread).start();\n    while (true) {\n        boolean flag = changeThread.isFlag();\n        unsafe.loadFence(); //加入读内存屏障\n        if (flag){\n            System.out.println(\"detected flag changed\");\n            break;\n        }\n    }\n    System.out.println(\"main thread end\");\n}\n</code></pre>\n<p>运行结果：</p>\n<pre><code class=\"language-plain\">subThread change flag to:false\ndetected flag changed\nmain thread end\n</code></pre>\n<p>而如果删掉上面代码中的<code>loadFence</code>方法，那么主线程将无法感知到<code>flag</code>发生的变化，会一直在<code>while</code>中循环。可以用图来表示上面的过程：</p>\n<p><img alt=\"\" src=\"https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406202130484.png\" /></p>\n<p>了解 Java 内存模型（<code>JMM</code>）的小伙伴们应该清楚，运行中的线程不是直接读取主内存中的变量的，只能操作自己工作内存中的变量，然后同步到主内存中，并且线程的工作内存是不能共享的。上面的图中的流程就是子线程借助于主内存，将修改后的结果同步给了主线程，进而修改主线程中的工作空间，跳出循环。</p>\n<h3 id=\"典型应用-1\">典型应用</h3>\n<p>在 Java 8 中引入了一种锁的新机制——<code>StampedLock</code>，它可以看成是读写锁的一个改进版本。<code>StampedLock</code> 提供了一种乐观读锁的实现，这种乐观读锁类似于无锁的操作，完全不会阻塞写线程获取写锁，从而缓解读多写少时写线程“饥饿”现象。由于 <code>StampedLock</code> 提供的乐观读锁不阻塞写线程获取读锁，当线程共享变量从主内存 load 到线程工作内存时，会存在数据不一致问题。</p>\n<p>为了解决这个问题，<code>StampedLock</code> 的 <code>validate</code> 方法会通过 <code>Unsafe</code> 的 <code>loadFence</code> 方法加入一个 <code>load</code> 内存屏障。</p>\n<pre><code class=\"language-java\">public boolean validate(long stamp) {\n   U.loadFence();\n   return (stamp &amp; SBITS) == (state &amp; SBITS);\n}\n</code></pre>\n<h2 id=\"对象操作\">对象操作</h2>\n<h3 id=\"介绍-3\">介绍</h3>\n<p><strong>例子</strong></p>\n<pre><code class=\"language-java\">import sun.misc.Unsafe;\nimport java.lang.reflect.Field;\n\npublic class Main {\n\n    private int value;\n\n    public static void main(String[] args) throws Exception{\n        Unsafe unsafe = reflectGetUnsafe();\n        assert unsafe != null;\n        long offset = unsafe.objectFieldOffset(Main.class.getDeclaredField(\"value\"));\n        Main main = new Main();\n        System.out.println(\"value before putInt: \" + main.value);\n        unsafe.putInt(main, offset, 42);\n        System.out.println(\"value after putInt: \" + main.value);\n  System.out.println(\"value after putInt: \" + unsafe.getInt(main, offset));\n    }\n\n    private static Unsafe reflectGetUnsafe() {\n        try {\n            Field field = Unsafe.class.getDeclaredField(\"theUnsafe\");\n            field.setAccessible(true);\n            return (Unsafe) field.get(null);\n        } catch (Exception e) {\n            e.printStackTrace();\n            return null;\n        }\n    }\n\n}\n</code></pre>\n<p>输出结果：</p>\n<pre><code class=\"language-plain\">value before putInt: 0\nvalue after putInt: 42\nvalue after putInt: 42\n</code></pre>\n<p><strong>对象属性</strong></p>\n<p>对象成员属性的内存偏移量获取，以及字段属性值的修改，在上面的例子中我们已经测试过了。除了前面的<code>putInt</code>、<code>getInt</code>方法外，Unsafe 提供了全部 8 种基础数据类型以及<code>Object</code>的<code>put</code>和<code>get</code>方法，并且所有的<code>put</code>方法都可以越过访问权限，直接修改内存中的数据。阅读 openJDK 源码中的注释发现，基础数据类型和<code>Object</code>的读写稍有不同，基础数据类型是直接操作的属性值（<code>value</code>），而<code>Object</code>的操作则是基于引用值（<code>reference value</code>）。下面是<code>Object</code>的读写方法：</p>\n<pre><code class=\"language-java\">//在对象的指定偏移地址获取一个对象引用\npublic native Object getObject(Object o, long offset);\n//在对象指定偏移地址写入一个对象引用\npublic native void putObject(Object o, long offset, Object x);\n</code></pre>\n<p>除了对象属性的普通读写外，<code>Unsafe</code> 还提供了 <strong>volatile 读写</strong>和<strong>有序写入</strong>方法。<code>volatile</code>读写方法的覆盖范围与普通读写相同，包含了全部基础数据类型和<code>Object</code>类型，以<code>int</code>类型为例：</p>\n<pre><code class=\"language-java\">//在对象的指定偏移地址处读取一个int值，支持volatile load语义\npublic native int getIntVolatile(Object o, long offset);\n//在对象指定偏移地址处写入一个int，支持volatile store语义\npublic native void putIntVolatile(Object o, long offset, int x);\n</code></pre>\n<p>相对于普通读写来说，<code>volatile</code>读写具有更高的成本，因为它需要保证可见性和有序性。在执行<code>get</code>操作时，会强制从主存中获取属性值，在使用<code>put</code>方法设置属性值时，会强制将值更新到主存中，从而保证这些变更对其他线程是可见的。</p>\n<p>有序写入的方法有以下三个：</p>\n<pre><code class=\"language-java\">public native void putOrderedObject(Object o, long offset, Object x);\npublic native void putOrderedInt(Object o, long offset, int x);\npublic native void putOrderedLong(Object o, long offset, long x);\n</code></pre>\n<p>有序写入的成本相对<code>volatile</code>较低，因为它只保证写入时的有序性，而不保证可见性，也就是一个线程写入的值不能保证其他线程立即可见。为了解决这里的差异性，需要对内存屏障的知识点再进一步进行补充，首先需要了解两个指令的概念：</p>\n<ul>\n<li><code>Load</code>：将主内存中的数据拷贝到处理器的缓存中</li>\n<li><code>Store</code>：将处理器缓存的数据刷新到主内存中</li>\n</ul>\n<p>顺序写入与<code>volatile</code>写入的差别在于，在顺序写时加入的内存屏障类型为<code>StoreStore</code>类型，而在<code>volatile</code>写入时加入的内存屏障是<code>StoreLoad</code>类型，如下图所示：</p>\n<p><img alt=\"\" src=\"https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406202130467.png\" /></p>\n<p>在有序写入方法中，使用的是<code>StoreStore</code>屏障，该屏障确保<code>Store1</code>立刻刷新数据到内存，这一操作先于<code>Store2</code>以及后续的存储指令操作。而在<code>volatile</code>写入中，使用的是<code>StoreLoad</code>屏障，该屏障确保<code>Store1</code>立刻刷新数据到内存，这一操作先于<code>Load2</code>及后续的装载指令，并且，<code>StoreLoad</code>屏障会使该屏障之前的所有内存访问指令，包括存储指令和访问指令全部完成之后，才执行该屏障之后的内存访问指令。</p>\n<p>综上所述，在上面的三类写入方法中，在写入效率方面，按照<code>put</code>、<code>putOrder</code>、<code>putVolatile</code>的顺序效率逐渐降低。</p>\n<p><strong>对象实例化</strong></p>\n<p>使用 <code>Unsafe</code> 的 <code>allocateInstance</code> 方法，允许我们使用非常规的方式进行对象的实例化，首先定义一个实体类，并且在构造函数中对其成员变量进行赋值操作：</p>\n<pre><code class=\"language-java\">@Data\npublic class A {\n    private int b;\n    public A(){\n        this.b =1;\n    }\n}\n</code></pre>\n<p>分别基于构造函数、反射以及 <code>Unsafe</code> 方法的不同方式创建对象进行比较：</p>\n<pre><code class=\"language-java\">public void objTest() throws Exception{\n    A a1=new A();\n    System.out.println(a1.getB());\n    A a2 = A.class.newInstance();\n    System.out.println(a2.getB());\n    A a3= (A) unsafe.allocateInstance(A.class);\n    System.out.println(a3.getB());\n}\n</code></pre>\n<p>打印结果分别为 1、1、0，说明通过<code>allocateInstance</code>方法创建对象过程中，不会调用类的构造方法。使用这种方式创建对象时，只用到了<code>Class</code>对象，所以说如果想要跳过对象的初始化阶段或者跳过构造器的安全检查，就可以使用这种方法。在上面的例子中，如果将 A 类的构造函数改为<code>private</code>类型，将无法通过构造函数和反射创建对象（可以通过构造函数对象 setAccessible 后创建对象），但<code>allocateInstance</code>方法仍然有效。</p>\n<h3 id=\"典型应用-2\">典型应用</h3>\n<ul>\n<li><strong>常规对象实例化方式</strong>：我们通常所用到的创建对象的方式，从本质上来讲，都是通过 new 机制来实现对象的创建。但是，new 机制有个特点就是当类只提供有参的构造函数且无显示声明无参构造函数时，则必须使用有参构造函数进行对象构造，而使用有参构造函数时，必须传递相应个数的参数才能完成对象实例化。</li>\n<li><strong>非常规的实例化方式</strong>：而 Unsafe 中提供 allocateInstance 方法，仅通过 Class 对象就可以创建此类的实例对象，而且不需要调用其构造函数、初始化代码、JVM 安全检查等。它抑制修饰符检测，也就是即使构造器是 private 修饰的也能通过此方法实例化，只需提类对象即可创建相应的对象。由于这种特性，allocateInstance 在 java.lang.invoke、Objenesis（提供绕过类构造器的对象生成方式）、Gson（反序列化时用到）中都有相应的应用。</li>\n</ul>\n<h2 id=\"数组操作\">数组操作</h2>\n<h3 id=\"介绍-4\">介绍</h3>\n<p><code>arrayBaseOffset</code> 与 <code>arrayIndexScale</code> 这两个方法配合起来使用，即可定位数组中每个元素在内存中的位置。</p>\n<pre><code class=\"language-java\">//返回数组中第一个元素的偏移地址\npublic native int arrayBaseOffset(Class&lt;?&gt; arrayClass);\n//返回数组中一个元素占用的大小\npublic native int arrayIndexScale(Class&lt;?&gt; arrayClass);\n</code></pre>\n<h3 id=\"典型应用-3\">典型应用</h3>\n<p>这两个与数据操作相关的方法，在 <code>java.util.concurrent.atomic</code> 包下的 <code>AtomicIntegerArray</code>（可以实现对 <code>Integer</code> 数组中每个元素的原子性操作）中有典型的应用，如下图 <code>AtomicIntegerArray</code> 源码所示，通过 <code>Unsafe</code> 的 <code>arrayBaseOffset</code>、<code>arrayIndexScale</code> 分别获取数组首元素的偏移地址 <code>base</code> 及单个元素大小因子 <code>scale</code> 。后续相关原子性操作，均依赖于这两个值进行数组中元素的定位，如下图二所示的 <code>getAndAdd</code> 方法即通过 <code>checkedByteOffset</code> 方法获取某数组元素的偏移地址，而后通过 CAS 实现原子性操作。</p>\n<p><img alt=\"\" src=\"https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406202130522.png\" /></p>\n<h2 id=\"cas-操作\">CAS 操作</h2>\n<h3 id=\"介绍-5\">介绍</h3>\n<p>这部分主要为 CAS 相关操作的方法。</p>\n<pre><code class=\"language-java\">/**\n  *  CAS\n  * @param o         包含要修改field的对象\n  * @param offset    对象中某field的偏移量\n  * @param expected  期望值\n  * @param update    更新值\n  * @return          true | false\n  */\npublic final native boolean compareAndSwapObject(Object o, long offset,  Object expected, Object update);\n\npublic final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);\n\npublic final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);\n</code></pre>\n<p><strong>什么是 CAS?</strong> CAS 即比较并替换（Compare And Swap)，是实现并发算法时常用到的一种技术。CAS 操作包含三个操作数——内存位置、预期原值及新值。执行 CAS 操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。我们都知道，CAS 是一条 CPU 的原子指令（cmpxchg 指令），不会造成所谓的数据不一致问题，<code>Unsafe</code> 提供的 CAS 方法（如 <code>compareAndSwapXXX</code>）底层实现即为 CPU 指令 <code>cmpxchg</code> 。</p>\n<h3 id=\"典型应用-4\">典型应用</h3>\n<p>在 JUC 包的并发工具类中大量地使用了 CAS 操作，像在前面介绍<code>synchronized</code>和<code>AQS</code>的文章中也多次提到了 CAS，其作为乐观锁在并发工具类中广泛发挥了作用。在 <code>Unsafe</code> 类中，提供了<code>compareAndSwapObject</code>、<code>compareAndSwapInt</code>、<code>compareAndSwapLong</code>方法来实现的对<code>Object</code>、<code>int</code>、<code>long</code>类型的 CAS 操作。以<code>compareAndSwapInt</code>方法为例：</p>\n<pre><code class=\"language-java\">public final native boolean compareAndSwapInt(Object o, long offset,int expected,int x);\n</code></pre>\n<p>参数中<code>o</code>为需要更新的对象，<code>offset</code>是对象<code>o</code>中整形字段的偏移量，如果这个字段的值与<code>expected</code>相同，则将字段的值设为<code>x</code>这个新值，并且此更新是不可被中断的，也就是一个原子操作。下面是一个使用<code>compareAndSwapInt</code>的例子：</p>\n<pre><code class=\"language-java\">private volatile int a;\npublic static void main(String[] args){\n    CasTest casTest=new CasTest();\n    new Thread(()-&gt;{\n        for (int i = 1; i &lt; 5; i++) {\n            casTest.increment(i);\n            System.out.print(casTest.a+\" \");\n        }\n    }).start();\n    new Thread(()-&gt;{\n        for (int i = 5 ; i &lt;10 ; i++) {\n            casTest.increment(i);\n            System.out.print(casTest.a+\" \");\n        }\n    }).start();\n}\n\nprivate void increment(int x){\n    while (true){\n        try {\n            long fieldOffset = unsafe.objectFieldOffset(CasTest.class.getDeclaredField(\"a\"));\n            if (unsafe.compareAndSwapInt(this,fieldOffset,x-1,x))\n                break;\n        } catch (NoSuchFieldException e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre>\n<p>运行代码会依次输出：</p>\n<pre><code class=\"language-plain\">1 2 3 4 5 6 7 8 9\n</code></pre>\n<p>在上面的例子中，使用两个线程去修改<code>int</code>型属性<code>a</code>的值，并且只有在<code>a</code>的值等于传入的参数<code>x</code>减一时，才会将<code>a</code>的值变为<code>x</code>，也就是实现对<code>a</code>的加一的操作。流程如下所示：</p>\n<p><img alt=\"\" src=\"https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406202130526.png\" /></p>\n<p>需要注意的是，在调用<code>compareAndSwapInt</code>方法后，会直接返回<code>true</code>或<code>false</code>的修改结果，因此需要我们在代码中手动添加自旋的逻辑。在<code>AtomicInteger</code>类的设计中，也是采用了将<code>compareAndSwapInt</code>的结果作为循环条件，直至修改成功才退出死循环的方式来实现的原子性的自增操作。</p>\n<h2 id=\"线程调度\">线程调度</h2>\n<h3 id=\"介绍-6\">介绍</h3>\n<p><code>Unsafe</code> 类中提供了<code>park</code>、<code>unpark</code>、<code>monitorEnter</code>、<code>monitorExit</code>、<code>tryMonitorEnter</code>方法进行线程调度。</p>\n<pre><code class=\"language-java\">//取消阻塞线程\npublic native void unpark(Object thread);\n//阻塞线程\npublic native void park(boolean isAbsolute, long time);\n//获得对象锁（可重入锁）\n@Deprecated\npublic native void monitorEnter(Object o);\n//释放对象锁\n@Deprecated\npublic native void monitorExit(Object o);\n//尝试获取对象锁\n@Deprecated\npublic native boolean tryMonitorEnter(Object o);\n</code></pre>\n<p>方法 <code>park</code>、<code>unpark</code> 即可实现线程的挂起与恢复，将一个线程进行挂起是通过 <code>park</code> 方法实现的，调用 <code>park</code> 方法后，线程将一直阻塞直到超时或者中断等条件出现；<code>unpark</code> 可以终止一个挂起的线程，使其恢复正常。</p>\n<p>此外，<code>Unsafe</code> 源码中<code>monitor</code>相关的三个方法已经被标记为<code>deprecated</code>，不建议被使用：</p>\n<pre><code class=\"language-java\">//获得对象锁\n@Deprecated\npublic native void monitorEnter(Object var1);\n//释放对象锁\n@Deprecated\npublic native void monitorExit(Object var1);\n//尝试获得对象锁\n@Deprecated\npublic native boolean tryMonitorEnter(Object var1);\n</code></pre>\n<p><code>monitorEnter</code>方法用于获得对象锁，<code>monitorExit</code>用于释放对象锁，如果对一个没有被<code>monitorEnter</code>加锁的对象执行此方法，会抛出<code>IllegalMonitorStateException</code>异常。<code>tryMonitorEnter</code>方法尝试获取对象锁，如果成功则返回<code>true</code>，反之返回<code>false</code>。</p>\n<h3 id=\"典型应用-5\">典型应用</h3>\n<p>Java 锁和同步器框架的核心类 <code>AbstractQueuedSynchronizer</code> (AQS)，就是通过调用<code>LockSupport.park()</code>和<code>LockSupport.unpark()</code>实现线程的阻塞和唤醒的，而 <code>LockSupport</code> 的 <code>park</code>、<code>unpark</code> 方法实际是调用 <code>Unsafe</code> 的 <code>park</code>、<code>unpark</code> 方式实现的。</p>\n<pre><code class=\"language-java\">public static void park(Object blocker) {\n    Thread t = Thread.currentThread();\n    setBlocker(t, blocker);\n    UNSAFE.park(false, 0L);\n    setBlocker(t, null);\n}\npublic static void unpark(Thread thread) {\n    if (thread != null)\n        UNSAFE.unpark(thread);\n}\n</code></pre>\n<p><code>LockSupport</code> 的<code>park</code>方法调用了 <code>Unsafe</code> 的<code>park</code>方法来阻塞当前线程，此方法将线程阻塞后就不会继续往后执行，直到有其他线程调用<code>unpark</code>方法唤醒当前线程。下面的例子对 <code>Unsafe</code> 的这两个方法进行测试：</p>\n<pre><code class=\"language-java\">public static void main(String[] args) {\n    Thread mainThread = Thread.currentThread();\n    new Thread(()-&gt;{\n        try {\n            TimeUnit.SECONDS.sleep(5);\n            System.out.println(\"subThread try to unpark mainThread\");\n            unsafe.unpark(mainThread);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }).start();\n\n    System.out.println(\"park main mainThread\");\n    unsafe.park(false,0L);\n    System.out.println(\"unpark mainThread success\");\n}\n</code></pre>\n<p>程序输出为：</p>\n<pre><code class=\"language-plain\">park main mainThread\nsubThread try to unpark mainThread\nunpark mainThread success\n</code></pre>\n<p>程序运行的流程也比较容易看懂，子线程开始运行后先进行睡眠，确保主线程能够调用<code>park</code>方法阻塞自己，子线程在睡眠 5 秒后，调用<code>unpark</code>方法唤醒主线程，使主线程能继续向下执行。整个流程如下图所示：</p>\n<p><img alt=\"\" src=\"https://seven97-blog.oss-cn-hangzhou.aliyuncs.com/imgs/202406202130484.png\" /></p>\n<h2 id=\"class-操作\">Class 操作</h2>\n<h3 id=\"介绍-7\">介绍</h3>\n<p><code>Unsafe</code> 对<code>Class</code>的相关操作主要包括类加载和静态变量的操作方法。</p>\n<p><strong>静态属性读取相关的方法</strong></p>\n<pre><code class=\"language-java\">//获取静态属性的偏移量\npublic native long staticFieldOffset(Field f);\n//获取静态属性的对象指针\npublic native Object staticFieldBase(Field f);\n//判断类是否需要初始化（用于获取类的静态属性前进行检测）\npublic native boolean shouldBeInitialized(Class&lt;?&gt; c);\n</code></pre>\n<p>创建一个包含静态属性的类，进行测试：</p>\n<pre><code class=\"language-java\">@Data\npublic class User {\n    public static String name=\"Hydra\";\n    int age;\n}\nprivate void staticTest() throws Exception {\n    User user=new User();\n    // 也可以用下面的语句触发类初始化\n    // 1.\n    // unsafe.ensureClassInitialized(User.class);\n    // 2.\n    // System.out.println(User.name);\n    System.out.println(unsafe.shouldBeInitialized(User.class));\n    Field sexField = User.class.getDeclaredField(\"name\");\n    long fieldOffset = unsafe.staticFieldOffset(sexField);\n    Object fieldBase = unsafe.staticFieldBase(sexField);\n    Object object = unsafe.getObject(fieldBase, fieldOffset);\n    System.out.println(object);\n}\n</code></pre>\n<p>运行结果：</p>\n<pre><code class=\"language-plain\">false\nHydra\n</code></pre>\n<p>在 <code>Unsafe</code> 的对象操作中，我们学习了通过<code>objectFieldOffset</code>方法获取对象属性偏移量并基于它对变量的值进行存取，但是它不适用于类中的静态属性，这时候就需要使用<code>staticFieldOffset</code>方法。在上面的代码中，只有在获取<code>Field</code>对象的过程中依赖到了<code>Class</code>，而获取静态变量的属性时不再依赖于<code>Class</code>。</p>\n<p>在上面的代码中首先创建一个<code>User</code>对象，这是因为如果一个类没有被初始化，那么它的静态属性也不会被初始化，最后获取的字段属性将是<code>null</code>。所以在获取静态属性前，需要调用<code>shouldBeInitialized</code>方法，判断在获取前是否需要初始化这个类。如果删除创建 User 对象的语句，运行结果会变为：</p>\n<pre><code class=\"language-plain\">true\nnull\n</code></pre>\n<p><strong>使用<code>defineClass</code>方法允许程序在运行时动态地创建一个类</strong></p>\n<pre><code class=\"language-java\">public native Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len, ClassLoader loader,ProtectionDomain protectionDomain);\n</code></pre>\n<p>在实际使用过程中，可以只传入字节数组、起始字节的下标以及读取的字节长度，默认情况下，类加载器（<code>ClassLoader</code>）和保护域（<code>ProtectionDomain</code>）来源于调用此方法的实例。下面的例子中实现了反编译生成后的 class 文件的功能：</p>\n<pre><code class=\"language-java\">private static void defineTest() {\n    String fileName=\"F:\\\\workspace\\\\unsafe-test\\\\target\\\\classes\\\\com\\\\cn\\\\model\\\\User.class\";\n    File file = new File(fileName);\n    try(FileInputStream fis = new FileInputStream(file)) {\n        byte[] content=new byte[(int)file.length()];\n        fis.read(content);\n        Class clazz = unsafe.defineClass(null, content, 0, content.length, null, null);\n        Object o = clazz.newInstance();\n        Object age = clazz.getMethod(\"getAge\").invoke(o, null);\n        System.out.println(age);\n    } catch (Exception e) {\n        e.printStackTrace();\n    }\n}\n</code></pre>\n<p>在上面的代码中，首先读取了一个<code>class</code>文件并通过文件流将它转化为字节数组，之后使用<code>defineClass</code>方法动态的创建了一个类，并在后续完成了它的实例化工作，流程如下图所示，并且通过这种方式创建的类，会跳过 JVM 的所有安全检查。</p>\n<p><img alt=\"\" src=\"https://oss.javaguide.cn/github/javaguide/java/basis/unsafe/image-20220717145000710.png\" /></p>\n<p>除了<code>defineClass</code>方法外，Unsafe 还提供了一个<code>defineAnonymousClass</code>方法：</p>\n<pre><code class=\"language-java\">public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; hostClass, byte[] data, Object[] cpPatches);\n</code></pre>\n<p>使用该方法可以用来动态的创建一个匿名类，在<code>Lambda</code>表达式中就是使用 ASM 动态生成字节码，然后利用该方法定义实现相应的函数式接口的匿名类。在 JDK 15 发布的新特性中，在隐藏类（<code>Hidden classes</code>）一条中，指出将在未来的版本中弃用 <code>Unsafe</code> 的<code>defineAnonymousClass</code>方法。</p>\n<h3 id=\"典型应用-6\">典型应用</h3>\n<p>Lambda 表达式实现需要依赖 <code>Unsafe</code> 的 <code>defineAnonymousClass</code> 方法定义实现相应的函数式接口的匿名类。</p>\n<h2 id=\"系统信息\">系统信息</h2>\n<h3 id=\"介绍-8\">介绍</h3>\n<p>这部分包含两个获取系统相关信息的方法。</p>\n<pre><code class=\"language-java\">//返回系统指针的大小。返回值为4（32位系统）或 8（64位系统）。\npublic native int addressSize();\n//内存页的大小，此值为2的幂次方。\npublic native int pageSize();\n</code></pre>\n<h3 id=\"典型应用-7\">典型应用</h3>\n<p>这两个方法的应用场景比较少，在<code>java.nio.Bits</code>类中，在使用<code>pageCount</code>计算所需的内存页的数量时，调用了<code>pageSize</code>方法获取内存页的大小。另外，在使用<code>copySwapMemory</code>方法拷贝内存时，调用了<code>addressSize</code>方法，检测 32 位系统的情况。</p>\n<h2 id=\"unsafe底层\">Unsafe底层</h2>\n<p>再看看Unsafe的compareAndSwap 方法来实现CAS操作，它是一个本地方法，实现位于unsafe.cpp中。</p>\n<pre><code class=\"language-java\">UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x))\n  UnsafeWrapper(\"Unsafe_CompareAndSwapInt\");\n  oop p = JNIHandles::resolve(obj);\n  jint* addr = (jint *) index_oop_from_field_offset_long(p, offset);\n  return (jint)(Atomic::cmpxchg(x, addr, e)) == e;\nUNSAFE_END\n</code></pre>\n<p>可以看到它通过 Atomic::cmpxchg 来实现比较和替换操作。其中参数x是即将更新的值，参数e是原内存的值。</p>\n<p>如果是Linux的x86，Atomic::cmpxchg方法的实现如下：</p>\n<pre><code class=\"language-java\">inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) {\n  int mp = os::is_MP();\n  __asm__ volatile (LOCK_IF_MP(%4) \"cmpxchgl %1,(%3)\"\n                    : \"=a\" (exchange_value)\n                    : \"r\" (exchange_value), \"a\" (compare_value), \"r\" (dest), \"r\" (mp)\n                    : \"cc\", \"memory\");\n  return exchange_value;\n}\n</code></pre>\n<p>而windows的x86的实现如下：</p>\n<pre><code class=\"language-java\">inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) {\n    int mp = os::isMP(); //判断是否是多处理器\n    _asm {\n        mov edx, dest\n        mov ecx, exchange_value\n        mov eax, compare_value\n        LOCK_IF_MP(mp)\n        cmpxchg dword ptr [edx], ecx\n    }\n}\n\n// Adding a lock prefix to an instruction on MP machine\n// VC++ doesn't like the lock prefix to be on a single line\n// so we can't insert a label after the lock prefix.\n// By emitting a lock prefix, we can define a label after it.\n#define LOCK_IF_MP(mp) __asm cmp mp, 0  \\\n                       __asm je L0      \\\n                       __asm _emit 0xF0 \\\n                       __asm L0:\n</code></pre>\n<p>如果是多处理器，为cmpxchg指令添加lock前缀。反之，就省略lock前缀(单处理器会不需要lock前缀提供的内存屏障效果)。这里的lock前缀就是使用了处理器的总线锁(最新的处理器都使用缓存锁代替总线锁来提高性能)。</p>\n<blockquote>\n<p>cmpxchg(void* ptr, int old, int new)，如果ptr和old的值一样，则把new写到ptr内存，否则返回ptr的值，整个操作是原子的。在Intel平台下，会用lock cmpxchg来实现，使用lock触发缓存锁，这样另一个线程想访问ptr的内存，就会被block住。</p>\n</blockquote>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自在线网站：seven的菜鸟成长之路，作者：seven，转载请注明原文链接：www.seven97.top</p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 09:00</span>&nbsp;\n<a href=\"https://www.cnblogs.com/sevencoding\">程序员Seven</a>&nbsp;\n阅读(<span id=\"post_view_count\">4</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "通过 OpenSpec + OpenCode 实践 AI Specs",
      "link": "https://www.cnblogs.com/whuanle/p/19581835",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/whuanle/p/19581835\" id=\"cb_post_title_url\" title=\"发布于 2026-02-06 08:33\">\n    <span>通过 OpenSpec + OpenCode 实践 AI Specs</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p></p><div class=\"toc\"><div class=\"toc-container-header\">目录</div><ul><li><a href=\"#规划项目\" rel=\"noopener nofollow\">规划项目</a></li><li><a href=\"#设计-ui\" rel=\"noopener nofollow\">设计 UI</a></li><li><a href=\"#了解-openspec\" rel=\"noopener nofollow\">了解 OpenSpec</a></li><li><a href=\"#体验-openspec\" rel=\"noopener nofollow\">体验 OpenSpec</a><ul><li><a href=\"#第-1-阶段创建变更-提案\" rel=\"noopener nofollow\">第 1 阶段：创建变更 (提案)</a></li><li><a href=\"#第二阶段实施\" rel=\"noopener nofollow\">第二阶段：实施</a></li><li><a href=\"#第-3-阶段归档-集成\" rel=\"noopener nofollow\">第 3 阶段：归档 (集成)</a></li></ul></li></ul></div><p></p>\n<p>前段时间写了 《万字长文讲解：团队落地 AI 辅助编程和 AI Specs 实战》，核心内容是讨论公司落地 AI 辅助编程的一些常见问题，通过使用 Kiro 引入 Spec 实现规范驱动开发，也讲解了实践过程。 不过这篇文章太长了，而且强依赖了 Kiro，对使用 Clauda Code、Cursor、OpenCode 的小伙伴来说，迁移成本太大了。</p>\n<blockquote>\n<p>文章地址：<a href=\"https://www.cnblogs.com/whuanle/p/19469026\" target=\"_blank\">https://www.cnblogs.com/whuanle/p/19469026</a></p>\n</blockquote>\n<br />\n<p>基于这个背景，所以找时间写这篇基于 OpenSpec + OpenCode 的实践教程。</p>\n<blockquote>\n<p>《万字长文讲解：团队落地 AI 辅助编程和 AI Specs 实战》主要是讲解思路，Kiro 并不重要，本篇主要讲解实现案例、怎么从零开发出一个程序，两篇文章并不冲突，推荐先看万字长文，再看本文。</p>\n</blockquote>\n<br />\n<p>为什么说 OpenSpec + OpenCode 适合落地呢？</p>\n<br />\n<p>现在市面上的辅助编程工具非常多，以标新立异的 Clauda Code 为首，还有 Codex、 Kimi Code CLI、Roo Code 等命令行工具，以 IDE 交互方式的有 Kiro、VS Code、Cursor 等。所以 Spec 工具不能强依赖一个辅助编程工具，就像 Kiro Spec 只能在 Kiro 下使用，后续迁移到其它辅助编程工具的成本大。</p>\n<br />\n<p>OpenSpec 是纯粹的 Spec 工具，它是开源项目，由社区驱动开发，并且支持集成到 22 种工具中。</p>\n<p>由于辅助编程工具的交互方式、智能程度、交付能力差异大，每个开发者的使用体验也不一样，适应习惯也有很大差异，所以团队内没必要限制只能使用某个工具，就像笔者喜欢多个工具同时搭配使用。而 OpenSpec 支持非常多的工具，可以同时在这些工具中开发一个项目，避免因为切换另一个工具开发代码，需要对 Spec 进行迁移。</p>\n<p><img alt=\"image-20260205210740234\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233608810-602785156.png\" /></p>\n<br />\n<p>本文选用 OpenCode 的原因，是因为 OpenCode 也是开源的，它本身支持非常多的模型，可以自定义接入各家的模型商，不像 Kiro、Cursor 要花钱买官方的套餐，所以也很适合用于案例讲解，也适合团队落地做主力工具。</p>\n<br />\n<p>接下来，将以开发一个基金实时估值程序为例，讲解如何实现 AI Specs，但是本文不会围绕团队落地去讲解原理和技术，纯粹是讲解 OpenSpec + OpenCode 怎么用。</p>\n<br />\n<h3 id=\"规划项目\">规划项目</h3>\n<p>开发项目要有一个明确说明，不能说一句 ”我要一个基金实时估值的程序“，当然，开发个人项目，也没必要写一堆需求文档说明，因为并没有太多业务场景，开发个人项目的目的是满足个人需求，而不是专业领域业务，所以可以借助 OpenCode 的 plan 功能或 OpenSpec 的 <code>expore</code> 功能帮助我们生成任务规划。</p>\n<br />\n<p>虽然 AI 很强大，但是还是得想想，实现基金实时估计的程序，我需要什么功能？抄！</p>\n<p>找一个觉得不错的程序，根据观察别人的程序，然后整理自己想要的功能。</p>\n<img alt=\"4285c043c2bac9c39658be66217f5ce6\" src=\"images/4285c043c2bac9c39658be66217f5ce6.jpg\" />\n<blockquote>\n<p>本文写于公元 2026 年 2 月 5 号 周四，可谓是疯狂星期四，亏损了这么多。</p>\n</blockquote>\n<br />\n<p>以下是笔者手敲的一些需要。</p>\n<pre><code>app 定位或目标：做一个能够实时查看基金涨跌幅、估值的 app，以及可以查看当前 A股、美股、港股的行情信息。\napp 有两个菜单，“刷新”、“基金”、“行情”\n刷新菜单是基金管理功能：\n1，能够手动添加基金代码到列表，修改持仓，支持支付宝截图后识别图片同步基金。\n2，可以修改基金持仓信息或删除基金\n3，界面要能够显示当前涨幅、单日收益、持有收益\n4，下拉刷新数据（本身要默认自动1s刷新一次数据）\n5，点击具体的基金后，跳转到 “基金” 菜单\n\n基金功能是查看基金详细信息的：\n目的是看到一个基金的各类信息，以便让我评估行情和涨跌，以便调仓。\n信息显示有三个主要功能：\n实时估值，要画出走向图；\n业绩走势，显示历史涨跌\n我的收益，显示收益信息\n接着界面下面显示这个基金的持仓股票信息和份额，然后显示每个股票的实时涨跌幅\n\n行情功能是看不同交易所和指数的：\n目的是查看当前 A股、美股、港股的行情信息。\n功能按一般证券app实现，该 app 是 桌面 app，不是移动 app。\n</code></pre>\n<br />\n<p>有了想要做的功能后，还需要列出技术方案。</p>\n<p>有了 AI 还需要自己想方案？</p>\n<p>笔者认为还是要的，比如基金估值数据从哪里来？如果你自己没有主意，没有提前调研技术方案，那么 AI 自己给了一个通过网络爬取证券公司页面整理信息的方案，这个方案不仅数据十分落后，会消耗大量处理时间，还可能因为爬虫写得好、牢房吃得早。所以，有必要自己去学习、研究技术方案，评估不同技术方案的实现难度、性能、带来的风险。</p>\n<p>不过可以利用 AI 的规划能力，大大简少技术调研和编写技术方案的时间，后面会说到。</p>\n<br />\n<p>举个例子，这个程序最重要的数据源，但是花钱买数据源是不可能的，只能白嫖。</p>\n<p>研究了一些资料之后，笔者采用 akshare 这个开源项目，它已经内置了多种数据源和采集方案，接口非常齐全。</p>\n<blockquote>\n<p>项目地址： <a href=\"https://github.com/akfamily/akshare\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/akfamily/akshare</a></p>\n</blockquote>\n<p><img alt=\"image-20260205111201148\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233611242-552245019.png\" /></p>\n<br />\n<p>需要想的东西已经想好了，现在通过 tauri 创建一个项目模板，后面的开发工作都在这个项目内进行。</p>\n<pre><code> npm create tauri-app@latest\n</code></pre>\n<p><img alt=\"image-20260205105136557\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610608-161142391.png\" /></p>\n<h3 id=\"设计-ui\">设计 UI</h3>\n<p>为了避免在编写界面时花费太多时间调教 UI，反复处理细节，我们可以利用一些支持 MCP 的 UI 工具平台，使用 AI 提前设计页面，这里选择 Pencil。</p>\n<p>在 VS Code 里面安装插件 Pencil 插件，然后根据提示使用邮箱注册账号。</p>\n<p><img alt=\"image-20260205093015893\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233612395-1865506485.png\" /></p>\n<p><img alt=\"image-20260205093119543\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233611045-469202573.png\" /></p>\n<br />\n<p>Pencli 支持自动配置到多种辅助编程工具上，你可以在扩展的设置面板配置开启 MCP 功能。</p>\n<p><img alt=\"3e7b50c4-ae7b-46b1-beaa-10b01b010a62\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610649-834051531.png\" /></p>\n<br />\n<p>重启 OpenCode 后发现已自动连接 Pencil 的 MCP 服务器。</p>\n<p><img alt=\"image-20260205112131607\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233609401-1792420871.png\" /></p>\n<p><br />复制 “我需要什么功能” 里面列出的需求，贴到 OpenCode 里面，使用 Plan 模式，然后开始规划设计，跟 AI 不断确认细节。</p>\n<p><img alt=\"image-20260205112040196\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610350-29351082.png\" /></p>\n<p><img alt=\"image-20260205112847778\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233608500-1556561665.png\" /></p>\n<br />\n<p>觉得设计没问题之后，将模式切换为 <code>Build</code>，开始真正设计页面。</p>\n<p><img alt=\"image-20260205113037233\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610704-1270917691.png\" /></p>\n<br />\n<p>在 VS Code 里面可以看到预览。</p>\n<p><img alt=\"image-20260205113905182\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233609553-1744404008.png\" /></p>\n<p><img alt=\"image-20260205113354777\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610728-1057267105.png\" /></p>\n<h3 id=\"了解-openspec\">了解 OpenSpec</h3>\n<p>安装 OpenSpec 。</p>\n<pre><code class=\"language-bash\">npm install -g @fission-ai/openspec@latest\n</code></pre>\n<br />\n<p>在 VSCode 安装 OpenSpecCodeExplorer 插件，便于后续维护 Spec 相关文件。</p>\n<p><img alt=\"image-20260205115153739\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610497-25878844.png\" /></p>\n<br />\n<p>在项目里面执行 <code>openspec init</code> 初始化 OpenSpec 文件，绑定使用 OpenCode，设置完成后需要重启 OpenCode。</p>\n<blockquote>\n<p>可以多选，同时在一个项目里面使用不同的工具。</p>\n</blockquote>\n<p><img alt=\"image-20260205115443385\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233608197-1788876487.png\" /></p>\n<br />\n<p>OpenSpec 会跟项目创建 <code>.openspec</code> 目录和 <code>AGENTS.md</code> 文件，第一次初始化时，<code>.openspec</code> 目录结构比较简单，但是后面随着迭代，目录会越来越复杂。</p>\n<p><img alt=\"image-20260205115646611\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610213-397718838.png\" /></p>\n<br />\n<p>OpenSpec 的目录结构规划说明如下：</p>\n<pre><code class=\"language-bash\">openspec/\n├── AGENTS.md               # 人工智能助手使用指南（自动生成）\n├── project.md              # 项目专属上下文信息（技术栈、编码/项目规范）\n├── specs/                  # 权威基准：当前已部署的功能能力\n│   └── [capability]/       # 示例：用户认证、支付处理\n│       ├── spec.md         # 需求说明与场景描述\n│       └── design.md       # （可选）技术实现模式\n├── changes/                # 提案集：正在进行中的活跃工作\n│   ├── [change-name]/      # 示例：新增谷歌登录功能\n│   │   ├── proposal.md     # 提案背景、核心内容与影响评估\n│   │   ├── tasks.md        # 实现任务清单（检查项）\n│   │   ├── design.md       # （可选）架构决策文档\n│   │   └── specs/          # 增量规格说明（对权威基准的变更内容）\n│       └── [capability]/\n│           └── spec.md     # 包含 ## 新增/修改/移除 的需求内容\n│   └── archive/            # 已完成变更的历史归档\n</code></pre>\n<br />\n<p>因为关联了 OpenCode，所以 OpenCode 会在 <code>.opencode/command</code>&nbsp;下生成一些命令文件。</p>\n<p><img alt=\"image-20260205140615775\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233608940-1837730502.png\" /></p>\n<br />\n<p>按开发的阶段排序，这些命令的说明如下：</p>\n<table>\n<thead>\n<tr>\n<th>命令</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>/opsx:explore</code></td>\n<td><strong>规划用的</strong>。进入探索模式——思考各种想法，研究问题，明确需求</td>\n</tr>\n<tr>\n<td><code>/opsx:new</code></td>\n<td><strong>创建 spec 用的</strong>。使用实验性工件工作流开始新的变更操作</td>\n</tr>\n<tr>\n<td><code>/opsx:continue</code></td>\n<td>恢复上一个功能点，然后执行下一步用的，继续进行变更工作——创建下一个成果（实验性成果）</td>\n</tr>\n<tr>\n<td><code>/opsx:ff</code></td>\n<td>以上步骤一步到位，一次性创建变更并生成实施所需的所有相关文件</td>\n</tr>\n<tr>\n<td><code>/opsx:apply</code></td>\n<td>执行 spec，开始干活！执行来自 OpenSpec 更改的任务（实验性）</td>\n</tr>\n<tr>\n<td><code>/opsx:verify</code></td>\n<td>验收！在归档之前，要核实实施情况与变更工件是否一致</td>\n</tr>\n<tr>\n<td><code>/opsx:sync</code></td>\n<td>改需求了，同步到 spec 文件。将变更后的详细规格（spec）同步至主规格中</td>\n</tr>\n<tr>\n<td><code>/opsx:archive</code></td>\n<td>干完活了，标记完成！在实验工作流程中归档已完成的变更</td>\n</tr>\n<tr>\n<td><code>/opsx:bulk-archive</code></td>\n<td>一次归档多个已完成的更改</td>\n</tr>\n<tr>\n<td><code>/opsx:onboard</code></td>\n<td>启动引导流程</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p><code>/opsx:explore</code> 、<code>/opsx-explore</code> 是一样的。</p>\n</blockquote>\n<br />\n<p>为了便于理解 OpenSpec 的工作原理，可以把这些命令分为 CLI 、Command、Skill 三部分。</p>\n<p>上面提到的 <code>/opsx:explore</code> 就是 Command，是跟 AI 协作的时候使用的，可以<strong>通过 Command 来引导 AI 规划和执行 Spec</strong>。</p>\n<p>而 CLI 是管理配置或项目用的，功能比较多，但是本文使用不到，这里只列出使用说明。</p>\n<table>\n<thead>\n<tr>\n<th>类型</th>\n<th>命令</th>\n<th>Purpose目的</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Setup</td>\n<td><code>init</code>, <code>update</code></td>\n<td>在项目中初始化和更新 OpenSpec</td>\n</tr>\n<tr>\n<td>Browsing</td>\n<td><code>list</code>, <code>view</code>, <code>show</code></td>\n<td>探索变更和规格</td>\n</tr>\n<tr>\n<td>Validation</td>\n<td><code>validate</code></td>\n<td>检查问题的变更和规格</td>\n</tr>\n<tr>\n<td>Lifecycle</td>\n<td><code>archive</code></td>\n<td>完成已完成的更改</td>\n</tr>\n<tr>\n<td>Workflow</td>\n<td><code>status</code>, <code>instructions</code>, <code>templates</code>, <code>schemas</code></td>\n<td>支持构件驱动工作流程</td>\n</tr>\n<tr>\n<td>Schemasschemas</td>\n<td><code>schema init</code>, <code>schema fork</code>, <code>schema validate</code>, <code>schema which</code></td>\n<td>创建和管理自定义工作流</td>\n</tr>\n<tr>\n<td>Config</td>\n<td><code>config</code></td>\n<td>查看和修改设置</td>\n</tr>\n<tr>\n<td>Utility</td>\n<td><code>feedback</code>, <code>completion</code></td>\n<td>Feedback and shell integration(我翻译不出来)</td>\n</tr>\n</tbody>\n</table>\n<br />\n<p>Command 引导 AI 干活，而 Skill 则是告诉 AI 怎么做，具体要做什么。</p>\n<p><img alt=\"image-20260205223926980\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233609693-1271354578.png\" /></p>\n<h3 id=\"体验-openspec\">体验 OpenSpec</h3>\n<p>在做了前期规划和了解 OpenSpec 后，我们正式开始进入生成 Spec 和编码的过程。</p>\n<p>OpenSpec 强制实施严格的三阶段工作流程，以防止需求漂移，并确保 AI 助手始终与人类意图保持一致，所以开发也是按三步来走。</p>\n<br />\n<p>所谓万丈高楼平地起，做项目的第一步就是把基础架构做好，把架子搭起来，让程序有个基本的页面，所以这里我们不做具体的功能，而且规划一个 Spec 把项目先跑起来。</p>\n<br />\n<p>前面写了 “我需要什么功能” ，但是这种描述并不完善，在 AI 编码之前，要仔细思考想法，调查问题，并明确要求。所以这一步跟 OpenCode 的 plan 一样，先跟 AI 对话，逐步明确细节和步骤。</p>\n<p>在对话框输入：</p>\n<pre><code>/opsx:explore 创建项目架构\n</code></pre>\n<blockquote>\n<p><code>/opsx:explore</code> 后面可以不填主题，也可以直接把 “我需要什么功能” 内容写到里面，这个命令比较灵活。</p>\n</blockquote>\n<br />\n<p>其实，OpenSpec Command 的本质就是提示词，当你发出 <code>/opsx-explore 创建项目架构</code>&nbsp;的命令时，实际上是把 opsx-explore.md 文件的内容输入做提示词了。</p>\n<p><img alt=\"image-20260205144928202\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233612321-761053920.png\" /></p>\n<br />\n<p>接着把  “我需要什么功能” 贴到对话框，<strong>让 AI 帮助你把想法变为具体的需求说明</strong>。你可以在对话中继续跟 AI 探讨明确目标，确认架构规划，然后进入下一阶段。</p>\n<p><img alt=\"image-20260205144451993\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610849-1210072118.png\" /></p>\n<br />\n<p>常见问题：上下文太多，需要新开对话怎么办。</p>\n<p>因为需要加载很多提示词、Spec 文件还有代码，导致后面每次对话的 token 会爆炸，为了减少 token 消耗和对话性能，不得不新开一个对话，但是又怕对话上下文丢失。</p>\n<p>基于 OpenSpec 做任务时，因为每个功能都会在 <code>openspec/changes</code> 目录，并且会根据任务进度在 tasks.md 里打标记，所以执行命令时指定续写某个功能即可，会自动恢复要做的事情，例如：</p>\n<pre><code>/opsx-continue [change-name]\n</code></pre>\n<h4 id=\"第-1-阶段创建变更-提案\">第 1 阶段：创建变更 (提案)</h4>\n<p>”变更“ 指在原有需要和代码下，需要做改动。</p>\n<p>”提案“ 指要做新的功能。</p>\n<br />\n<p>OpenSpec 要求在这个阶段，在将资源投入到 “如何” 之前，强制要求在 “做什么” 和 “为什么” 上保持一致。</p>\n<p>这个阶段要做的事情（准确来说是 OpenSpec 会做的事情）：</p>\n<ol>\n<li>识别需求：新功能、重构或架构转变。</li>\n<li>脚手架：在 <code>openspec/changes/&lt;change-id&gt;/</code> 下创建唯一目录。</li>\n<li>定义规范：为相关功能编写增量 (添加、修改、删除)。</li>\n<li>计划：创建 tasks.md 文件，将工作分解成可验证的步骤。</li>\n<li>验证（验收）：使用 openspec validation 确保提案在结构上是合理的。</li>\n</ol>\n<br />\n<p>执行命令开始创建 Spec：</p>\n<pre><code class=\"language-bash\">/opsx-new\n# 或者\n/opsx-new {功能名称}\n</code></pre>\n<p><img alt=\"image-20260205150053070\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233608813-1679168930.png\" /></p>\n<br />\n<p>Spec 是不能一下子直接创建的，需要按照流程一个个创建，最后组合成一个 Spec，Spec 每个部分都由一些文件组成，后续 AI 编码时会按照这些文件完成工作。</p>\n<pre><code>[ ] proposal\n[ ] design\n[ ] specs\n[ ] tasks\n</code></pre>\n<br />\n<p>你可以使用 <code>/opsx-continue</code>&nbsp;进入下一个阶段，或者参考笔者的方式，输入文字说明让 AI 自动流转步骤。</p>\n<br />\n<p>先创建 proposal：</p>\n<pre><code>描述这个变更的内容，我来帮你起草 proposal.md\n</code></pre>\n<blockquote>\n<p>或者输入 <code>/opsx-continue</code>&nbsp;自动进入此步骤。</p>\n</blockquote>\n<p><img alt=\"image-20260205150839593\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233609698-76468035.png\" /></p>\n<br />\n<p>design、specs 可以一起并行工作，所以可以输入：</p>\n<pre><code>同时创建 design、specs\n</code></pre>\n<br />\n<p>等 AI 完成任务后，<code>openspec/changes/{功能名称}</code>&nbsp;下会创建一批文件。</p>\n<p><img alt=\"image-20260205151456733\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233609578-33837835.png\" /></p>\n<p><br />当前阶段最后一步创建任务清单，规划每个环节要做的步骤和目的、验收标准。</p>\n<pre><code>创建任务清单\n</code></pre>\n<p><img alt=\"image-20260205151415959\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233608994-1403053363.png\" /></p>\n<br />\n<p>不要急着进入第二阶段，笔者建议好好研究一下 <code>openspec/changes</code>目录下的文件。</p>\n<h4 id=\"第二阶段实施\">第二阶段：实施</h4>\n<p>这是编码阶段，AI 会根据前面创建的 Spec 去执行编码任务。</p>\n<p>OpenSpec 的工作步骤：</p>\n<ol>\n<li>上下文加载：人工智能读取 proposal.md 和规范/以理解需求。</li>\n<li>执行:AI 按顺序处理 tasks.md。</li>\n<li>跟踪：任务实时标记为已完成。</li>\n</ol>\n<br />\n<p>前面提到，如果上下文太长，需要新开对话；或者今天偷懒不写代码，第二天写的时候忘记了进度，那么可以使用 <code>/opsx-continue [change-name]</code> 恢复进度。</p>\n<p><img alt=\"image-20260205153010435\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233609473-1542596676.png\" /></p>\n<br />\n<p>如下图，第一阶段的四个步骤已经完成了，提示我们下一步可以使用 <code>/opsx-apply</code>、<code>/opsx-archive</code>&nbsp;执行编码任务了。</p>\n<p><img alt=\"image-20260205153042693\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233609691-1175318801.png\" /></p>\n<p><br /><code>/opsx-apply</code> 按<code>tasks.md</code> 规划一步步实现功能，实现时无需监管。完成任务后，使用<code>/opsx-archive</code> 存档已完成的更改。</p>\n<br />\n<p>所以先执行 <code>/opsx-apply</code> ，刷一会儿抖音，等十几二十分钟，任务执行完毕后。</p>\n<p>但是还不能直接存档，需要执行 <code>/opsx-verify [change-name]</code>&nbsp;检查完整性、正确性和一致性，如果有问题，让 AI 接着改。</p>\n<p><img alt=\"image-20260205160114683\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233610648-912983200.png\" /></p>\n<p><br />如果发现有地方完成得不好，需要调整，可以直接在当前对话中要求 AI 完善设计、修改代码。如果改动比较大，涉及到逻辑和设计的变更，则需要使用 <code>/opsx-sync</code> 把变更同步到 <code>changes</code> 里面。</p>\n<h4 id=\"第-3-阶段归档-集成\">第 3 阶段：归档 (集成)</h4>\n<p>这一步要使用 <code>/opsx-archive</code> 归档已完成的更改，也就是标记这个功能已经结束了。</p>\n<br />\n<p>一旦代码部署并验证，变更提案就不再是 “提案”，而是现实代码了，为了减少后续检索成本等，OpenSpec 会做一些操作。</p>\n<ol>\n<li>归档命令：运行 <code>openspec archive &lt;change-id&gt;</code>将 change 文件夹移动到 <code>openspec/changes/archive/</code>。</li>\n<li>规范合并: CLI 自动将规范增量 (ADDED/MODIFIED 要求) 应用于<code>openspec/specs/</code>&nbsp;目录。</li>\n</ol>\n<br />\n<p>因为 OpenSpec 没有数据，也不存数据库，所以主要是根据目录文件来判断工作状态，如果不整理文件，会导致检索时的文件内容很多，把没必要的文件也读取进去了。</p>\n<p><code>openspec/specs</code> 里面存储了程序当前的架构、功能说明等文件和信息，所以每次完成 Spec 后，都需要更新这里的文件，</p>\n<br />\n<p>基础架构搭建起来后，界面太丑了，忘记了让引入 ant design 框架了，也忘记了安装 pencli 的设计走，不过问题不大，后面创建新的 Spec 专门优化即可。当然你也可以先继续完善一些基础功能，然后才归档。</p>\n<br />\n<p><img alt=\"image-20260205162649717\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233608256-14045611.png\" /></p>\n<br />\n<p>目前界面只是有个架子，功能还是不可用的，我们可以新开一个 <code>change</code>&nbsp;按一个模块去增加功能，不要一个 <code>change</code>&nbsp;干一堆事情，小步快跑、逐步积累。</p>\n<p><img alt=\"image-20260205164224697\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233608651-976846377.png\" /></p>\n<p><img alt=\"image-20260205164956608\" src=\"https://img2024.cnblogs.com/blog/1315495/202602/1315495-20260205233609495-1644838055.png\" /></p>\n<br />\n<p>接下来，我们可以继续使用 <code>/opsx-expore</code>&nbsp;或 <code>/opsx-new</code>&nbsp;逐步实现其它功能。</p>\n<p>目前，该项目还需要实现：</p>\n<ul>\n<li>界面按照 pencli 的设计图实现界面</li>\n<li>基金管理功能</li>\n<li>基金信息查看功能</li>\n<li>行情功能</li>\n</ul>\n<br />\n<p>可以逐个创建 Spec，逐步实现功能。</p>\n\n\n</div>\n<div id=\"MySignature\">\n    痴者工良(https://whuanle.cn)\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 08:33</span>&nbsp;\n<a href=\"https://www.cnblogs.com/whuanle\">痴者工良</a>&nbsp;\n阅读(<span id=\"post_view_count\">42</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Claude Opus 4.6 发布：Agent 能力暴涨，上下文窗口翻五倍！",
      "link": "https://www.cnblogs.com/zh94/p/19582489",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/zh94/p/19582489\" id=\"cb_post_title_url\" title=\"发布于 2026-02-06 08:23\">\n    <span>Claude Opus 4.6 发布：Agent 能力暴涨，上下文窗口翻五倍！</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>Claude 本次共发布了两个更新：</p>\n<ol>\n<li>\n<p>模型层面从原先的 Opus4.5 升级到了 Opus4.6，相关指标有显著提升！</p>\n</li>\n<li>\n<p>Claude Code 的升级，新增了 agent-teams 的功能！</p>\n</li>\n</ol>\n<h1 id=\"-先看模型升级-\">/ 先看模型升级 /</h1>\n<p><img alt=\"opuspingfeng\" src=\"https://img2024.cnblogs.com/blog/1104472/202602/1104472-20260206081830251-1624303846.png\" /></p>\n<p>上述 Opus4.6 模型最新的评分当中，相比于上一代 Opus4.5 有显著提升的指标分别是：</p>\n<ol>\n<li>\n<p>Agentic terminal coding（Terminal-Bench 2.0）：该指标是评估 AI 在终端编程环境中自主解决问题能力的评分，主要考察复杂指令解析、环境交互和工作流自动化三大能力。Opus 4.6 是 65.4%，相比于 Opus4.5 59.8% 有相应提升。</p>\n</li>\n<li>\n<p>Agentic computer use(OSWorld)： 是评估 AI 在真实操作系统中自主完成任务的能力，Opus 4.6 拿了 72.7%，比 Opus 4.5 的 66.3% 有明显提升。</p>\n</li>\n</ol>\n<p>这意味着 Claude 通过图形界面或命令行与电脑交互，解决实际问题的能力明显提高，未来操作系统就是 Claude 的基本操作工具，全面 Agent 化有了非常大的可能。</p>\n<ol start=\"3\">\n<li>\n<p>Agentic search(BrowserComp) ：是测试 AI 在浏览器环境中自主搜索、筛选和整合信息的能力，比如根据模糊指令找到准确答案，或者对比多个网页内容，进行复杂信息的整合能力。Opus 4.6 是 84.0%，相比于与 Opus4.5 的 67.8 有较高提升。</p>\n</li>\n<li>\n<p>Novel problem-solving（ARC AGI 2）：是评估 AI 解决全新、未见过的复杂问题的能力，比如逻辑推理、数学证明或抽象概念理解，它测试的是 AI 能否像人类一样，通过已有知识灵活组合，找到创新解法。</p>\n</li>\n</ol>\n<p>简单来说，该指标的提升意味着 AI 未来陪你一起“开脑洞”的能力更强了，创新能力找 AI 也没有任何问题。</p>\n<p>该指标从原本的 37.6% 升高到了 68.8% ，提升显著！</p>\n<h1 id=\"opus46-支持-1m-token-的上下文窗口\">Opus4.6 支持 1M Token 的上下文窗口</h1>\n<p><img alt=\"opus2\" src=\"https://img2024.cnblogs.com/blog/1104472/202602/1104472-20260206081922277-1854598102.png\" /></p>\n<p>除了上述模型指标有较高提示外，原本的 Opus4.5 上下文窗口是支持 200K，本次直接升级到了 1M，足足翻了五倍！</p>\n<p>上下文窗口对于 AI Coding 是有非常重要的含义，上下文不足容易导致 AI Coding 质量下降，本次直接升级到 1M 上下文窗口，AI Coding 开发者的福音！</p>\n<h1 id=\"claude-code-升级\">Claude Code 升级</h1>\n<p>本次 Claude Code 升级了 agent-teams 的功能。</p>\n<p>以前，我们想让 Claude Code 并行跑多个任务的时候，比较简陋的做法是，每次都打开一个新的 CLI 终端，然后 Claude Code 在不同的终端进行执行。</p>\n<p>再或者就是直接采用 sub agent，但 sub agent 的问题是，这些程序在单一会话内运行，只能向主代理汇报。</p>\n<p>而本次更新的 agent teams 功能，则是各个 Agent 队友各自独立工作，各自在自己的上下文窗口中，并直接相互沟通！</p>\n<p>是的，并不是各个 Agent 向主代理汇报，而是各个 Agent 在自己的上下文窗口中，独自运行，且各个 Agent 之间可以直接沟通，互相协作！完全并行！</p>\n<p>不过，由于该功能还是一个实验功能，所以默认在 Claude Code 中是被禁用的，我们可以直接调整 Claude Code 的 setting.json 配置来开启它：</p>\n<pre><code>{\n  \"env\": {\n    \"CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS\": \"1\"\n  }\n}\n</code></pre>\n<p>开启该配置后，多个Claude Code实例之间进行团队协作，共享任务的能力将大大加强！</p>\n<p>Claude 团队针对 Claude Code 并行处理能力也单独录制了一部视频，感兴趣的可以直接在这里查看：</p>\n<p><a href=\"https://www.youtube.com/watch?v=vNeIQS9GsZ8\" rel=\"noopener nofollow\" target=\"_blank\">https://www.youtube.com/watch?v=vNeIQS9GsZ8</a></p>\n<p>除此之外，关于Agent teams 能力的详细说明，也可以查看 Claude 官网的最新更新，文档地址是：</p>\n<p><a href=\"https://code.claude.com/docs/en/agent-teams\" rel=\"noopener nofollow\" target=\"_blank\">https://code.claude.com/docs/en/agent-teams</a></p>\n<p>今年极大可能是Agent 的元年，无论是前段时间爆火的OpenClaw，还是最近 Claude 和 GPT 的模型更新，都直指模型Agent能力的提升，无论是写代码，操作浏览器，还是控制操作系统，模型现在都有了较大升级！</p>\n<p>跟上节奏！还没用过 Claude Code 今年一定要体验一下！这是入局的最好时机！</p>\n<p>入局早了你会觉得 Claude Code 垃圾，入局晚了你会跟不上 Claude Code 的节奏，现在入局正当时。💪</p>\n<h1 id=\"欢迎日常交流\">欢迎日常交流</h1>\n<p>AI 驱动团队开发是这个时代的新命题，欢迎大家加微信互相交流心得。</p>\n<p>👉 想要进群的朋友，扫码时备注 “AI实验群”，看到消息后会第一时间拉你进群。</p>\n<p>群定位：AI工具提效/实战经验互助</p>\n<p>群规则：不水群、不广告、干货优先</p>\n<p>欢迎访问该链接获取群信息：<a href=\"https://zhaozhihao.com/archives/KRMxDLo4\" rel=\"noopener nofollow\" target=\"_blank\">https://zhaozhihao.com/archives/KRMxDLo4</a></p>\n<p>好文章值得被更多人看见！既然看到这里了，随手点个赞👍和关注，并转发给更多的朋友吧！感谢。</p>\n<p>作者：贾克斯的平行世界、V：x_h886688</p>\n<hr />\n<p><a href=\"https://mp.weixin.qq.com/s/dKvltmgdSk9uGIZ4ZuHkQg\" rel=\"noopener nofollow\" target=\"_blank\">原文地址：Claude Opus 4.6 发布：Agent 能力暴涨，上下文窗口翻五倍！</a></p>\n\n\n</div>\n<div id=\"MySignature\">\n    <br />\n<fieldset style=\"padding: 10px; margin: 10px; background-color: #fff; width: 850px; border: 1px solid black; font-size: blod;\">\n<p><span style=\"color: black;\">版权声明</span></p>\n<hr style=\"color: black;\" />\n<p><span style=\"color: black;\">作者：陈咬金</span></p>\n<p><span style=\"color: black;\">出处：</span><a href=\"https://www.cnblogs.com/zh94/\" style=\"color: black;\" target=\"_blank\">陈咬金的技术博客--https://www.cnblogs.com/zh94/</a></p>\n<p><span style=\"color: black;\">您的支持是对博主最大的鼓励，感谢您的认真阅读。</span></p>\n<p><span style=\"color: black;\">本文版权归作者所有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留任何追究法律责任的权利。</span></p>\n</fieldset>\n\n<div style=\"padding: 10px; margin: 10px; background-color: #fff; width: 850px; border: 0px solid black; font-size: blod;\">\n     <img src=\"https://images.cnblogs.com/cnblogs_com/zh94/1586631/o_211225012748_weixin_saoma.png\" />&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://images.cnblogs.com/cnblogs_com/zh94/1586631/o_211225012748_weixin_saoma.png\" />\n</div>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 08:23</span>&nbsp;\n<a href=\"https://www.cnblogs.com/zh94\">贾克斯的平行世界</a>&nbsp;\n阅读(<span id=\"post_view_count\">29</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Laravel AI SDK 正式发布",
      "link": "https://www.cnblogs.com/catchadmin/p/19582488",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/catchadmin/p/19582488\" id=\"cb_post_title_url\" title=\"发布于 2026-02-06 08:23\">\n    <span>Laravel AI SDK 正式发布</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"laravel-ai-sdk-正式发布\">Laravel AI SDK 正式发布</h1>\n<p>Laravel AI SDK 今天正式发布了。这个由 Taylor Otwell 开发数月的官方包，为 Laravel 应用提供了一套统一的 AI 交互接口，覆盖文本对话、图像生成、语音合成、语音转录、向量嵌入等场景，支持 OpenAI、Anthropic、Gemini、Groq、xAI 等主流服务商。</p>\n<p>安装方式和其他 Laravel 官方包一样简单：</p>\n<pre><code class=\"language-shell\">composer require laravel/ai\n</code></pre>\n<h2 id=\"agent核心交互单元\">Agent：核心交互单元</h2>\n<p>SDK 的核心概念是 Agent。每个 Agent 是一个 PHP 类，封装了系统指令、对话上下文、工具和输出格式。可以把它理解为一个专用助手——销售教练、文档分析器、客服机器人——配置一次，随处调用。</p>\n<p>通过 Artisan 命令创建：</p>\n<pre><code class=\"language-shell\">php artisan make:agent SalesCoach\n</code></pre>\n<p>生成的类实现 <code>Agent</code> 接口，定义 <code>instructions()</code> 方法提供系统提示词，然后调用 <code>prompt()</code> 发起对话：</p>\n<pre><code class=\"language-php\">$response = SalesCoach::make(user: $user)\n    -&gt;prompt('分析这段销售录音...');\n\nreturn (string) $response;\n</code></pre>\n<p><code>prompt()</code> 方法支持在调用时切换服务商和模型：</p>\n<pre><code class=\"language-php\">$response = (new SalesCoach)-&gt;prompt(\n    '分析这段销售录音...',\n    provider: 'anthropic',\n    model: 'claude-haiku-4-5-20251001',\n    timeout: 120,\n);\n</code></pre>\n<p>如果不想创建专门的类，也可以用匿名 Agent 快速调用：</p>\n<pre><code class=\"language-php\">use function Laravel\\Ai\\{agent};\n\n$response = agent(\n    instructions: 'You are an expert at software development.',\n)-&gt;prompt('Tell me about Laravel');\n</code></pre>\n<h2 id=\"结构化输出\">结构化输出</h2>\n<p>Agent 可以返回结构化数据，而不仅仅是纯文本。实现 <code>HasStructuredOutput</code> 接口，定义 <code>schema()</code> 方法即可：</p>\n<pre><code class=\"language-php\">public function schema(JsonSchema $schema): array\n{\n    return [\n        'feedback' =&gt; $schema-&gt;string()-&gt;required(),\n        'score' =&gt; $schema-&gt;integer()-&gt;min(1)-&gt;max(10)-&gt;required(),\n    ];\n}\n</code></pre>\n<p>调用后直接当数组用：</p>\n<pre><code class=\"language-php\">$response = (new SalesCoach)-&gt;prompt('分析这段录音...');\n\nreturn $response['score']; // 8\n</code></pre>\n<h2 id=\"对话记忆\">对话记忆</h2>\n<p>Agent 支持自动持久化对话历史。使用 <code>RemembersConversations</code> trait 后，SDK 会自动将对话存入数据库，后续可以通过 <code>continue()</code> 方法继续之前的对话：</p>\n<pre><code class=\"language-php\">// 开始新对话\n$response = (new SalesCoach)-&gt;forUser($user)-&gt;prompt('你好！');\n$conversationId = $response-&gt;conversationId;\n\n// 继续对话\n$response = (new SalesCoach)\n    -&gt;continue($conversationId, as: $user)\n    -&gt;prompt('接着刚才的话题...');\n</code></pre>\n<h2 id=\"工具系统\">工具系统</h2>\n<p>Agent 可以使用工具来扩展能力。通过 <code>make:tool</code> 命令创建工具类，定义输入 schema 和 <code>handle()</code> 方法：</p>\n<pre><code class=\"language-php\">class RandomNumberGenerator implements Tool\n{\n    public function description(): string\n    {\n        return '生成加密安全的随机数。';\n    }\n\n    public function handle(Request $request): string\n    {\n        return (string) random_int($request['min'], $request['max']);\n    }\n\n    public function schema(JsonSchema $schema): array\n    {\n        return [\n            'min' =&gt; $schema-&gt;integer()-&gt;min(0)-&gt;required(),\n            'max' =&gt; $schema-&gt;integer()-&gt;required(),\n        ];\n    }\n}\n</code></pre>\n<p>SDK 还内置了几个服务商级别的工具：</p>\n<ul>\n<li><strong>WebSearch</strong> — 让 Agent 搜索网页，支持 Anthropic、OpenAI、Gemini</li>\n<li><strong>WebFetch</strong> — 让 Agent 抓取网页内容，支持 Anthropic、Gemini</li>\n<li><strong>FileSearch</strong> — 在向量存储中搜索文件，支持 OpenAI、Gemini</li>\n<li><strong>SimilaritySearch</strong> — 基于 Eloquent 模型的向量相似度搜索，用于 RAG 场景</li>\n</ul>\n<h2 id=\"流式响应与广播\">流式响应与广播</h2>\n<p>对于需要实时输出的场景，Agent 支持流式响应。返回值可以直接作为路由响应，自动发送 SSE：</p>\n<pre><code class=\"language-php\">Route::get('/coach', function () {\n    return (new SalesCoach)-&gt;stream('分析这段录音...');\n});\n</code></pre>\n<p>流式事件还可以通过 Laravel Broadcasting 广播到前端频道，或者使用 Vercel AI SDK 协议与前端框架对接：</p>\n<pre><code class=\"language-php\">return (new SalesCoach)\n    -&gt;stream('分析这段录音...')\n    -&gt;usingVercelDataProtocol();\n</code></pre>\n<h2 id=\"队列处理\">队列处理</h2>\n<p>耗时的 AI 请求可以推入队列在后台处理：</p>\n<pre><code class=\"language-php\">(new SalesCoach)\n    -&gt;queue($request-&gt;input('transcript'))\n    -&gt;then(function (AgentResponse $response) {\n        // 处理响应...\n    })\n    -&gt;catch(function (Throwable $e) {\n        // 处理异常...\n    });\n</code></pre>\n<h2 id=\"图像生成\">图像生成</h2>\n<p><code>Image</code> 类提供了简洁的图像生成接口，支持 OpenAI、Gemini 和 xAI：</p>\n<pre><code class=\"language-php\">use Laravel\\Ai\\Image;\n\n$image = Image::of('厨房台面上的甜甜圈')\n    -&gt;quality('high')\n    -&gt;landscape()\n    -&gt;generate();\n\n$path = $image-&gt;store();\n</code></pre>\n<p>支持附加参考图像进行风格迁移，也可以推入队列异步生成。</p>\n<h2 id=\"音频与转录\">音频与转录</h2>\n<p>语音合成（TTS）和语音转录（STT）同样被纳入 SDK：</p>\n<pre><code class=\"language-php\">use Laravel\\Ai\\Audio;\nuse Laravel\\Ai\\Transcription;\n\n// 文字转语音\n$audio = Audio::of('I love coding with Laravel.')\n    -&gt;female()\n    -&gt;instructions('用海盗的语气说')\n    -&gt;generate();\n\n// 语音转文字\n$transcript = Transcription::fromStorage('audio.mp3')\n    -&gt;diarize() // 按说话人分段\n    -&gt;generate();\n</code></pre>\n<p>TTS 支持 OpenAI 和 ElevenLabs，STT 同样支持这两个服务商。</p>\n<h2 id=\"embeddings-与向量搜索\">Embeddings 与向量搜索</h2>\n<p>生成向量嵌入变得非常直观。Laravel 的 <code>Stringable</code> 类新增了 <code>toEmbeddings()</code> 方法：</p>\n<pre><code class=\"language-php\">$embeddings = Str::of('Napa Valley has great wine.')-&gt;toEmbeddings();\n</code></pre>\n<p>配合 PostgreSQL 的 pgvector 扩展，可以在数据库中直接进行向量相似度查询：</p>\n<pre><code class=\"language-php\">$documents = Document::query()\n    -&gt;whereVectorSimilarTo('embedding', '纳帕谷最好的酒庄')\n    -&gt;limit(10)\n    -&gt;get();\n</code></pre>\n<p>传入字符串时，Laravel 会自动生成嵌入向量再进行查询，不需要手动处理。Embedding 还支持缓存，避免重复调用 API。</p>\n<h2 id=\"reranking\">Reranking</h2>\n<p>Reranking 可以对搜索结果按语义相关性重新排序，支持 Cohere 和 Jina：</p>\n<pre><code class=\"language-php\">$posts = Post::all()-&gt;rerank('body', 'Laravel 教程');\n</code></pre>\n<p>这个功能直接以 Collection 宏的形式提供，可以对 Eloquent 集合按指定字段做语义重排。</p>\n<h2 id=\"文件与向量存储\">文件与向量存储</h2>\n<p>SDK 提供了文件管理和向量存储的完整方案。文件可以上传到服务商存储后反复引用，向量存储则用于 RAG 场景下的文件检索：</p>\n<pre><code class=\"language-php\">use Laravel\\Ai\\Files\\Document;\nuse Laravel\\Ai\\Stores;\n\n// 上传文件\n$stored = Document::fromPath('/path/to/report.pdf')-&gt;put();\n\n// 创建向量存储并添加文件\n$store = Stores::create('知识库');\n$store-&gt;add($stored);\n</code></pre>\n<h2 id=\"failover\">Failover</h2>\n<p>调用时传入服务商数组，SDK 会在主服务商不可用时自动切换到备用服务商：</p>\n<pre><code class=\"language-php\">$response = (new SalesCoach)-&gt;prompt(\n    '分析这段录音...',\n    provider: ['openai', 'anthropic'],\n);\n</code></pre>\n<h2 id=\"agent-配置\">Agent 配置</h2>\n<p>Agent 支持通过 PHP Attribute 配置参数，包括最大步数、最大 token 数、温度、超时时间等：</p>\n<pre><code class=\"language-php\">#[MaxSteps(10)]\n#[MaxTokens(4096)]\n#[Provider('anthropic')]\n#[Temperature(0.7)]\n#[Timeout(120)]\nclass SalesCoach implements Agent\n{\n    use Promptable;\n}\n</code></pre>\n<p><code>UseCheapestModel</code> 和 <code>UseSmartestModel</code> 两个 Attribute 可以自动选择服务商最便宜或最强的模型，不需要记具体的模型名。</p>\n<h2 id=\"中间件\">中间件</h2>\n<p>Agent 支持中间件机制，可以在请求发送前后插入自定义逻辑，比如日志记录：</p>\n<pre><code class=\"language-php\">class LogPrompts\n{\n    public function handle(AgentPrompt $prompt, Closure $next)\n    {\n        Log::info('Prompting agent', ['prompt' =&gt; $prompt-&gt;prompt]);\n\n        return $next($prompt)-&gt;then(function (AgentResponse $response) {\n            Log::info('Agent responded', ['text' =&gt; $response-&gt;text]);\n        });\n    }\n}\n</code></pre>\n<h2 id=\"测试支持\">测试支持</h2>\n<p>SDK 为每个功能都提供了 <code>fake()</code> 方法和断言 API，测试时不需要真实调用 AI 服务商：</p>\n<pre><code class=\"language-php\">SalesCoach::fake(['第一条响应', '第二条响应']);\n\n// 执行业务逻辑...\n\nSalesCoach::assertPrompted('分析这段...');\nSalesCoach::assertNeverPrompted();\n</code></pre>\n<p>图像、音频、转录、Embeddings、Reranking、文件操作、向量存储都有对应的 fake 和断言方法。</p>\n<h2 id=\"服务商支持一览\">服务商支持一览</h2>\n<table>\n<thead>\n<tr>\n<th>功能</th>\n<th>支持的服务商</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>文本对话</td>\n<td>OpenAI、Anthropic、Gemini、Groq、xAI</td>\n</tr>\n<tr>\n<td>图像生成</td>\n<td>OpenAI、Gemini、xAI</td>\n</tr>\n<tr>\n<td>语音合成</td>\n<td>OpenAI、ElevenLabs</td>\n</tr>\n<tr>\n<td>语音转录</td>\n<td>OpenAI、ElevenLabs</td>\n</tr>\n<tr>\n<td>向量嵌入</td>\n<td>OpenAI、Gemini、Cohere、Jina</td>\n</tr>\n<tr>\n<td>重排序</td>\n<td>Cohere、Jina</td>\n</tr>\n<tr>\n<td>文件管理</td>\n<td>OpenAI、Anthropic、Gemini</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"小结\">小结</h2>\n<p>Laravel AI SDK 把 AI 集成做成了 Laravel 开发者熟悉的样子：Artisan 命令生成类、接口约束行为、trait 复用逻辑、队列异步处理、fake 方法写测试。如果你的 Laravel 项目需要接入 AI 能力，这个包值得尝试。</p>\n<p><a href=\"https://catchadmin.com/post/2026-02/laravel-ai-sdk-released\" rel=\"noopener nofollow\" target=\"_blank\">🎉Laravel AI SDK 正式发布</a></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 08:23</span>&nbsp;\n<a href=\"https://www.cnblogs.com/catchadmin\">JaguarJack</a>&nbsp;\n阅读(<span id=\"post_view_count\">7</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": ".NET AI 核心构建块：重塑智能应用开发的架构范式与生态",
      "link": "https://www.cnblogs.com/shanyou/p/19582324",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/shanyou/p/19582324\" id=\"cb_post_title_url\" title=\"发布于 2026-02-06 07:30\">\n    <span>.NET AI 核心构建块：重塑智能应用开发的架构范式与生态</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        随着.NET 10 的正式发布，微软通过一系列深思熟虑的架构革新，将人工智能从一种附加的外部能力，彻底转化为.NET 生态系统中的一等公民。这一转型的核心在于提供一套统一、现代且高性能的构建块，旨在解决开发者在构建智能应用程序时面临的碎片化 SDK、复杂的私有数据集成以及日益增长的智能体化（Agentic）需求\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>随着.NET 10 的正式发布，微软通过一系列深思熟虑的架构革新，将人工智能从一种附加的外部能力，彻底转化为.NET 生态系统中的一等公民。这一转型的核心在于提供一套统一、现代且高性能的构建块，旨在解决开发者在构建智能应用程序时面临的碎片化 SDK、复杂的私有数据集成以及日益增长的智能体化（Agentic）需求。.NET其战略布局不仅涵盖了生成式 AI 的基础接口标准，还延伸到了向量数据管理、多智能体协作编排以及跨平台的协议互操作性 。</p>\n<h2 id=\"统一生成式-ai-的基石microsoftextensionsai-的深度演进\"><strong>统一生成式 AI 的基石：Microsoft.Extensions.AI 的深度演进</strong></h2>\n<p>在生成式 AI 爆发的初期，开发者不得不面对不同大型语言模型（LLM）供应商之间迥异的接入标准。无论是 OpenAI、Azure OpenAI、Google Gemini 还是本地部署的 Ollama，每个平台都拥有独立的身份验证机制、请求参数结构和响应处理流程。这种碎片化现状不仅增加了开发成本，也导致了应用代码与特定供应商的深度耦合。</p>\n<h3 id=\"统一抽象ichatclient-与模型无关性\"><strong>统一抽象：IChatClient 与模型无关性</strong></h3>\n<p>为了打破这一僵局，Microsoft.Extensions.AI（简称 MEAI）引入了以 IChatClient 为核心的一套标准接口。这一设计的宏观目标是提供一个类似于.NET 中 ILogger 或 HttpClient 的通用抽象层，使开发者能够编写与具体模型供应商无关的代码 。</p>\n<p>通过 IChatClient 接口，应用程序可以无缝切换后端模型。例如，在开发环境下，开发者可能倾向于使用本地运行的 Ollama 模型以节省成本并确保隐私；而在生产环境下，则可以仅通过修改配置，将底层实现替换为高性能的 Azure OpenAI 服务。这种灵活性源于 MEAI 对请求和响应对象的标准化处理，即 ChatOptions 和 ChatResponse。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">接口与类名</th>\n<th style=\"text-align: left;\">核心职责</th>\n<th style=\"text-align: left;\">技术价值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">IChatClient</td>\n<td style=\"text-align: left;\">提供文本及多模态交互的基础接口 。</td>\n<td style=\"text-align: left;\">实现模型供应商的解耦，支持热切换。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">ChatOptions</td>\n<td style=\"text-align: left;\">定义温度（Temperature）、最大令牌（MaxTokens）等模型参数 。</td>\n<td style=\"text-align: left;\">跨模型标准化推理行为的微调。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">ChatMessage</td>\n<td style=\"text-align: left;\">封装用户、助手或系统角色的消息内容。</td>\n<td style=\"text-align: left;\">提供一致的对话历史表示方式。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">IEmbeddingGenerator</td>\n<td style=\"text-align: left;\">负责将文本或多模态数据转换为向量。</td>\n<td style=\"text-align: left;\">为语义搜索和 RAG 模式提供标准向量化支持。</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"中间件模式企业级治理的切入点\"><strong>中间件模式：企业级治理的切入点</strong></h3>\n<p>MEAI 架构中最具洞察力的设计之一是引入了与 ASP.NET Core 类似的中间件（Middleware）模式。传统的 AI 集成往往将治理逻辑（如安全性检查、性能监控、成本控制）硬编码在业务逻辑中，而 MEAI 允许开发者通过拦截器流水线来透明地注入这些横切关注点。</p>\n<p>中间件的存在使得企业能够构建一套健壮的 AI 治理体系。例如，恶意内容过滤中间件可以在提示词发送至模型前进行合规性审查；限流与配额中间件可以防止单个用户消耗过多的 API 令牌；而基于 Redis 的缓存中间件则可以存储高频问题的回答，从而降低推理成本并提升响应速度。</p>\n<p>此外，MEAI 深度集成了 OpenTelemetry（OTEL），这意味着开发者无需额外配置即可通过.NET Aspire 或 Azure Monitor 观察到每一条 AI 调用的详细链路轨迹。这种“可观测性内置”对于生产环境下的 AI 故障排查和性能优化至关重要，它能清晰地展示出模型延迟分布、令牌消耗统计以及推理过程中的任何异常中断。</p>\n<h3 id=\"结构化输出与类型安全\"><strong>结构化输出与类型安全</strong></h3>\n<p>在传统的 AI 应用开发中，处理模型返回的非结构化文本数据是一项充满挑战的任务。开发者通常需要编写大量的正则表达式或不可靠的字符串解析逻辑 11。MEAI 通过 GetResponseAsync&lt;T&gt; 等扩展方法，极大地简化了这一流程。</p>\n<p>该机制允许开发者定义一个普通的 C# 类或记录（Record），框架会自动根据该类型的定义生成 JSON 架构（Schema），并将其作为提示词的一部分发送给支持结构化输出的模型。模型生成的 JSON 响应会被自动反序列化为 C# 对象，从而保证了业务逻辑在处理 AI 产出时的类型安全性。这种“代码优先”的方法不仅提高了开发效率，还显著增强了自动化工作流的稳健性，避免了因模型输出格式微调而导致的生产故障。</p>\n<h2 id=\"数据与语义搜索的革新microsoftextensionsvectordata\"><strong>数据与语义搜索的革新：Microsoft.Extensions.VectorData</strong></h2>\n<p>生成式 AI 的一个显著局限性是其知识存在截止日期，且无法直接感知组织的私有数据。检索增强生成（RAG）已成为解决这一问题的标准范式，而向量数据库则是 RAG 架构的灵魂。Microsoft.Extensions.VectorData 库的推出，旨在为.NET 开发者提供一套统一且符合直觉的向量操作抽象。</p>\n<h3 id=\"统一向量存储抽象层\"><strong>统一向量存储抽象层</strong></h3>\n<p>与数据库访问中的 ORM 理念类似，Microsoft.Extensions.VectorData 并不绑定于特定的向量数据库实现。它为 Azure AI Search、Pinecone、Milvus、Qdrant 以及 SQL Server 的向量扩展提供了一个通用的编程模。这种抽象使得开发者能够以面向对象的方式管理向量数据，而无需深入研究每种数据库特定的 API。</p>\n<p>框架通过一组精心设计的 C# 属性（Attributes），将普通的 POCO 对象映射到向量数据库的 Schema 中。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">映射属性</th>\n<th style=\"text-align: left;\">核心功能</th>\n<th style=\"text-align: left;\">适用场景</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">`VectorStoreRecordKey`</td>\n<td style=\"text-align: left;\">标记记录的唯一标识符。</td>\n<td style=\"text-align: left;\">主键索引与记录更新。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">`VectorStoreRecordData`</td>\n<td style=\"text-align: left;\">标记需要存储的元数据或正文内容 。</td>\n<td style=\"text-align: left;\">存储供 RAG 检索的原始上下文。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">`VectorStoreRecordVector`</td>\n<td style=\"text-align: left;\">标记存储向量嵌入的属性 。</td>\n<td style=\"text-align: left;\">定义向量维度、距离函数（如余弦相似度）。</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"语义搜索的实现机制\"><strong>语义搜索的实现机制</strong></h3>\n<p>在.NET 10 的体系下，实现语义搜索变得异常简洁。其核心逻辑在于将用户的自然语言查询通过 IEmbeddingGenerator 转换为高维向量，然后利用向量数据库的 VectorizedSearchAsync 能力进行相似度匹配 。</p>\n<p>这种搜索方式超越了传统关键字匹配的限制。即使查询词与文档中的词汇没有重叠，只要其语义意图相近（例如“适合家庭观看的电影”与“迪士尼经典动画”），向量检索系统就能精准地召回相关记录。在微软提供的 eShop 演示项目中，这种能力被用于提升商品搜索的鲁棒性，有效解决了用户拼写错误、使用近义词或描述性语言搜索商品的问题 。</p>\n<h2 id=\"从聊天到智能体microsoft-agent-framework-与多智能体编排\"><strong>从聊天到智能体：Microsoft Agent Framework 与多智能体编排</strong></h2>\n<p>随着 AI 应用从简单的问答演变为具备自主行动能力的智能体系统，开发者需要更加强大的编排框架 。微软智能体框架（Microsoft Agent Framework，简称 MAF）应运而生，它标志着微软在智能体化（Agentic）开发领域迈出了关键一步。</p>\n<h3 id=\"框架定位与传承\"><strong>框架定位与传承</strong></h3>\n<p>MAF 是对 Semantic Kernel（语义内核）和 AutoGen 项目经验的集大成者。它保留了 Semantic Kernel 在企业级状态管理、会话持久化和遥测方面的严谨性，同时引入了 AutoGen 广受赞誉的多智能体协作模式和灵活的对话拓扑。</p>\n<p>在 MAF 的视角下，“智能体”不仅仅是一个 LLM，而是一个拥有身份（Identity）、指令（Instructions）、工具（Tools）和记忆（Memory）的自治实体。智能体能够通过调用外部函数、访问数据库或操作特定协议（如 MCP）来主动改变系统状态。</p>\n<h3 id=\"基于图的工作流编排\"><strong>基于图的工作流编排</strong></h3>\n<p>MAF 的一大亮点是引入了基于图（Graph-based）的工作流编排系统。它允许开发者显式定义多智能体协作的逻辑路径，而不再仅仅依赖于 LLM 模糊的自我决策。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">协作模式</th>\n<th style=\"text-align: left;\">运作逻辑</th>\n<th style=\"text-align: left;\">应用场景示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>顺序模式 (Sequential)</strong></td>\n<td style=\"text-align: left;\">智能体按固定序列执行，前者的产出作为后者的输入。</td>\n<td style=\"text-align: left;\">内容生产线：研究者 -&gt; 撰稿者 -&gt; 校对者。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>并发模式 (Concurrent)</strong></td>\n<td style=\"text-align: left;\">多个智能体并行处理任务的不同侧面，最后汇总结果。</td>\n<td style=\"text-align: left;\">软件测试：功能测试智能体与性能测试智能体同步运行。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>交接模式 (Handoff)</strong></td>\n<td style=\"text-align: left;\">根据对话上下文，动态地将控制权转移给更专业的智能体 。</td>\n<td style=\"text-align: left;\">客服系统：分类智能体 -&gt; 财务专家智能体或技术支持智能体。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>磁性模式 (Magentic)</strong></td>\n<td style=\"text-align: left;\">智能体基于特定的拓扑结构自主进行多轮讨论。</td>\n<td style=\"text-align: left;\">复杂问题的头脑风暴与决策分析。</td>\n</tr>\n</tbody>\n</table>\n<p>MAF 还特别强调了“人机协同”（Human-in-the-loop）的重要性，支持在工作流的关键节点设置检查点（Checkpointing），允许暂停执行以待人工审批或干预，这对于处理金融交易、代码部署等高风险任务至关重要。</p>\n<h2 id=\"互操作性的新标准模型上下文协议-mcp\"><strong>互操作性的新标准：模型上下文协议 (MCP)</strong></h2>\n<p>在构建复杂的 AI 插件和工具集成时，开发者长期受困于“每集成一个新工具就需要编写一套新适配器”的困境。模型上下文协议（Model Context Protocol，简称 MCP）的出现，被业内形象地比喻为 AI 时代的“USB-C 接口” 。</p>\n<h3 id=\"mcp-的架构与价值\"><strong>MCP 的架构与价值</strong></h3>\n<p>MCP 是由微软与 Anthropic 共同推动的开放协议，旨在标准化 AI 模型与外部数据源、工具集之间的交互方式 5。在.NET 生态中，官方发布的 MCP C# SDK 让开发者能够以极低的成本构建符合规范的 MCP 客户端与服务器。</p>\n<p>该协议的核心优势在于其供应商中立性 。一旦开发者构建了一个公开组织内部文档、数据库或操作逻辑的 MCP 服务器，任何支持 MCP 的智能体（如 Visual Studio 的 GitHub Copilot、Claude 或是开发者自建的智能体）都可以立即与其通信，无需进行二次开发 。</p>\n<h3 id=\"协议原语资源工具与提示词\"><strong>协议原语：资源、工具与提示词</strong></h3>\n<p>MCP 协议定义了一组简洁而强大的原语，通过 JSON-RPC 进行通信。</p>\n<ul>\n<li><strong>资源 (Resources):</strong> 提供只读的上下文数据，如数据库行、本地文件内容或实时日志流。</li>\n<li><strong>工具 (Tools):</strong> 模型可以调用的具有副作用的操作函数，如“创建 Jira 工单”、“发送 Slack 消息”或“重启服务器”。</li>\n<li><strong>提示词 (Prompts):</strong> 服务器托管的预定义模板，帮助客户端以最优化的方式利用服务器提供的能力。</li>\n</ul>\n<p>在企业级部署中，MCP 服务器可以被赋予严格的 OAuth 2.1 安全边界和文件系统沙箱，确保 AI 智能体在“能干活”的同时不会逾越安全底线 。目前，Visual Studio 2026 已将 MCP 作为其 AI 治理的核心，支持对 MCP 服务器的身份识别和策略控制。</p>\n<h2 id=\"数据摄取的现代化data-ingestion-building-blocks\"><strong>数据摄取的现代化：Data Ingestion Building Blocks</strong></h2>\n<p>高效的 RAG 系统不仅取决于推理阶段的算法，更取决于高质量的离线数据摄取流程。微软发布的“数据摄取构建块”（Data Ingestion Building Blocks）通过标准化的 ETL 流程，解决了数据碎片化的问题 。</p>\n<h3 id=\"etl-的-ai-化转型\"><strong>ETL 的 AI 化转型</strong></h3>\n<p>这套构建块将传统的数据处理抽象为三个关键阶段，并提供了一套流式 API 来简化编排 。</p>\n<ol>\n<li><strong>提取 (Extract):</strong> IngestionDocumentReader 将多种格式（PDF、Word、Markdown）统一转化为内部标准化表示 。</li>\n<li><strong>转换 (Transform):</strong> 包含文档级和分块级的处理。例如，利用 AI 对文档中的图像生成替代文本（Alt Text），或是将长文档切割为符合模型窗口限制的语义块（Chunks）。</li>\n<li><strong>加载 (Load):</strong> VectorStoreWriter 将处理后的数据块及其对应的向量嵌入存储到指定的向量数据库中 。</li>\n</ol>\n<p>这种工程化的方法极大地减少了开发者在构建 RAG 管道时的重复性劳动，使他们能够更专注于如何通过更好的分块策略或语义增强来提升检索的精度 。</p>\n<h2 id=\"底层性能支撑systemnumericstensors-与硬件加速\"><strong>底层性能支撑：System.Numerics.Tensors 与硬件加速</strong></h2>\n<p>为了确保.NET 在 AI 时代的竞争力，微软对底层数值计算能力进行了深度重构。System.Numerics.Tensors 库的更新，标志着.NET 具备了处理大规模高维数据的原生高性能支持 。</p>\n<h3 id=\"simd-驱动的向量运算\"><strong>SIMD 驱动的向量运算</strong></h3>\n<p>AI 模型的核心是张量（Tensors）和向量运算 。在.NET 10 中，TensorPrimitives 类利用单指令多数据（SIMD）技术，充分压榨现代 CPU 的硬件潜力 。无论是余弦相似度计算、向量点积还是归一化操作，现在都能以接近 C++ 的原生性能运行 。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/510/202602/510-20260206072935765-504222877.png\" /><br />\n通过在运行时自动选择最优的指令集（如 AVX-512、AVX2 或 ARM NEON），.NET 能够确保在不同硬件架构上都能获得最佳的推理速度 。</p>\n<h3 id=\"内存管理与惰性变换\"><strong>内存管理与惰性变换</strong></h3>\n<p>处理大型嵌入模型时，内存带宽往往成为瓶颈。System.Numerics.Tensors 引入了“视图”（Views）和“惰性变换”（Lazy Transformations）的概念 。例如，对一个包含数百万个元素的张量进行切片或转置操作时，框架不再创建昂贵的内存副本，而是返回一个指向原始内存的新视图 。这种极致的内存效率对于在受限环境下（如边缘计算或高并发服务器）运行 AI 逻辑至关重要。</p>\n<h2 id=\"行业趋势与开发者生态2025-2026-深度观察\"><strong>行业趋势与开发者生态：2025-2026 深度观察</strong></h2>\n<p>.NET AI 构建块的发布并非孤立事件，它深深植根于全球开发者生态的剧烈变迁中。</p>\n<h3 id=\"生产力飞跃与职业重塑\"><strong>生产力飞跃与职业重塑</strong></h3>\n<p>根据 2025 年 JetBrains 和 Stack Overflow 的联合调查，AI 已不再是可选技能，而是成为开发者日常工作的核心工具。</p>\n<ul>\n<li><strong>普及率:</strong> 85% 的受访开发者定期使用 AI 辅助开发，51% 的专业开发者每天都会与其协作 。</li>\n<li><strong>效率收益:</strong> 近 90% 的开发者每周至少节省 1 小时，五分之一的开发者每周能节省 8 小时甚至更多，这相当于每周节省出了一个完整的工作日 。</li>\n<li><strong>关键应用点:</strong> 开发者最乐意交给 AI 的任务是编写样板代码、生成文档和总结代码变更（PR Summary） 。</li>\n</ul>\n<p>然而，这种依赖也带来了新的风险。约 46% 的开发者对 AI 输出的准确性表示担忧，特别是高级开发者在面对复杂逻辑时表现出了极高的谨慎度 34。这种“近乎正确但并不完全正确”的 AI 解法，已成为目前开发流程中最大的瓶颈之一 。</p>\n<h3 id=\"全球-ai-鸿沟与开源模型的崛起\"><strong>全球 AI 鸿沟与开源模型的崛起</strong></h3>\n<p>2025 年下半年的全球 AI 采用报告显示，AI 的普及呈现出明显的地区不均衡性。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">指标</th>\n<th style=\"text-align: left;\">全球北方 (Global North)</th>\n<th style=\"text-align: left;\">全球南方 (Global South)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">劳动人口采用率</td>\n<td style=\"text-align: left;\">24.7%</td>\n<td style=\"text-align: left;\">14.1%</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">增长速度</td>\n<td style=\"text-align: left;\">约全球南方两倍</td>\n<td style=\"text-align: left;\">相对滞后</td>\n</tr>\n</tbody>\n</table>\n<p>在这一背景下，以 DeepSeek 为代表的开源模型（Open-source Models）正在改变游戏规则。通过提供完全免费且采用 MIT 协议的模型，这些平台打破了传统闭源模型供应商设置的财务和技术壁垒。这种趋势不仅在非洲等市场引发了强烈反响，也促使微软等大厂通过 Microsoft.Extensions.AI 加速对各类开源模型的支持，以确保开发者能够灵活应对地缘政治和供应链的变化。</p>\n<h2 id=\"总结与未来展望迈向智能体原生的开发时代\"><strong>总结与未来展望：迈向“智能体原生”的开发时代</strong></h2>\n<p>.NET 10 所确立的 AI 核心构建块，标志着软件开发范式的根本性转变。从 Microsoft.Extensions.AI 的统一抽象，到 Microsoft.Extensions.VectorData 的语义搜索，再到 Microsoft Agent Framework 的多智能体协作，微软正在为开发者构建一个标准化、高性能且治理友好的全栈 AI 工具箱 。</p>\n<p>随着.NET 11 路标的清晰化，我们可以预见到 Visual Studio 将不仅仅是一个代码编辑器，而是一个由“规划智能体”、“调试智能体”和“测试智能体”组成的智能协作中心。通过 MCP 协议，开发者的私有工具和数据将与这些智能体无缝连接，真正实现“代码自愈”和“需求到代码”的高效转化 。</p>\n<p>对于企业而言，这意味着能够以更低的成本、更高的安全性构建具有深厚垂直领域知识的智能应用。对于开发者而言，虽然 AI 辅助工具极大地减轻了繁琐劳动，但同时也对架构设计、提示词工程（Prompt Engineering）以及对 AI 输出的审慎把控提出了更高的要求。在这个“智能体原生”的新时代，.NET 生态系统无疑已做好了充足的准备，为智能软件的无限可能提供坚实的基石。</p>\n<h4 id=\"引用的文章\"><strong>引用的文章</strong></h4>\n<ol>\n<li>NET AI Essentials - The Core Building Blocks Explained - Microsoft Dev Blogs, <a href=\"https://devblogs.microsoft.com/dotnet/dotnet-ai-essentials-the-core-building-blocks-explained/\" rel=\"noopener nofollow\" target=\"_blank\">https://devblogs.microsoft.com/dotnet/dotnet-ai-essentials-the-core-building-blocks-explained/</a></li>\n<li>Bridging the Gap: Microsoft.Extensions.AI vs. Semantic Kernel - Medium <a href=\"https://medium.com/@monsterking10/bridging-the-gap-microsoft-extensions-ai-vs-semantic-kernel-b4e332105181\" rel=\"noopener nofollow\" target=\"_blank\">https://medium.com/@monsterking10/bridging-the-gap-microsoft-extensions-ai-vs-semantic-kernel-b4e332105181</a></li>\n<li>Semantic Kernel and Microsoft.Extensions.AI: Better Together, Part 1 ..., <a href=\"https://devblogs.microsoft.com/semantic-kernel/semantic-kernel-and-microsoft-extensions-ai-better-together-part-1/\" rel=\"noopener nofollow\" target=\"_blank\">https://devblogs.microsoft.com/semantic-kernel/semantic-kernel-and-microsoft-extensions-ai-better-together-part-1/</a></li>\n<li>Semantic Kernel and Microsoft.Extensions.AI: Better Together, Part 2 <a href=\"https://devblogs.microsoft.com/semantic-kernel/semantic-kernel-and-microsoft-extensions-ai-better-together-part-2/\" rel=\"noopener nofollow\" target=\"_blank\">https://devblogs.microsoft.com/semantic-kernel/semantic-kernel-and-microsoft-extensions-ai-better-together-part-2/</a></li>\n<li>Introducing Microsoft Agent Framework | Microsoft Azure Blog <a href=\"https://azure.microsoft.com/en-us/blog/introducing-microsoft-agent-framework/\" rel=\"noopener nofollow\" target=\"_blank\">https://azure.microsoft.com/en-us/blog/introducing-microsoft-agent-framework/</a></li>\n<li>eShop infused with AI - a comprehensive intelligent app sample  <a href=\"https://devblogs.microsoft.com/dotnet/e-shop-infused-with-ai-comprehensive-intelligent-dotnet-app-sample/\" rel=\"noopener nofollow\" target=\"_blank\">https://devblogs.microsoft.com/dotnet/e-shop-infused-with-ai-comprehensive-intelligent-dotnet-app-sample/</a></li>\n<li>Exploring How to Produce Structured Output with AI Agents | by Sai Nitesh Palamakula,  <a href=\"https://medium.com/@sainitesh/exploring-how-to-produce-structured-output-with-ai-agents-5d35a0b0d195\" rel=\"noopener nofollow\" target=\"_blank\">https://medium.com/@sainitesh/exploring-how-to-produce-structured-output-with-ai-agents-5d35a0b0d195</a></li>\n<li><a href=\"https://learn.microsoft.com/en-us/agent-framework/tutorials/agents/structured-output\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/agent-framework/tutorials/agents/structured-output</a></li>\n<li>Introducing Data Ingestion Building Blocks (Preview) - .NET Blog,  <a href=\"https://devblogs.microsoft.com/dotnet/introducing-data-ingestion-building-blocks-preview/\" rel=\"noopener nofollow\" target=\"_blank\">https://devblogs.microsoft.com/dotnet/introducing-data-ingestion-building-blocks-preview/</a></li>\n<li>Introducing Microsoft.Extensions.VectorData Preview - .NET Blog, <a href=\"https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-vector-data/\" rel=\"noopener nofollow\" target=\"_blank\">https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-vector-data/</a></li>\n<li>Semantic Search in 50 Lines of Code — AI in .NET | by Stefan Đokić | Medium <a href=\"https://medium.com/@thecodeman/semantic-search-in-50-lines-of-code-ai-in-net-f6bd98b6e92b\" rel=\"noopener nofollow\" target=\"_blank\">https://medium.com/@thecodeman/semantic-search-in-50-lines-of-code-ai-in-net-f6bd98b6e92b</a></li>\n<li>Top Tech Trends of 2025: AI-powered everything - Capgemini,  <a href=\"https://www.capgemini.com/wp-content/uploads/2025/01/Top-Tech-Trends-2025_Report.pdf\" rel=\"noopener nofollow\" target=\"_blank\">https://www.capgemini.com/wp-content/uploads/2025/01/Top-Tech-Trends-2025_Report.pdf</a></li>\n<li>MCP Deep Dive (Part 1): Building the Hands and Eyes of an AI Agent in C# | by Alon Fliess  <a href=\"https://medium.com/@alonfliess/mcp-deep-dive-part-1-building-the-hands-and-eyes-of-an-ai-agent-in-c-b26e71ebe102\" rel=\"noopener nofollow\" target=\"_blank\">https://medium.com/@alonfliess/mcp-deep-dive-part-1-building-the-hands-and-eyes-of-an-ai-agent-in-c-b26e71ebe102</a></li>\n<li>Roadmap for AI in Visual Studio (November) - Microsoft Dev Blogs, <a href=\"https://devblogs.microsoft.com/visualstudio/roadmap-for-ai-in-visual-studio-november/\" rel=\"noopener nofollow\" target=\"_blank\">https://devblogs.microsoft.com/visualstudio/roadmap-for-ai-in-visual-studio-november/</a></li>\n<li>In-depth: Tensors, ndarrays for .NET in C# - Numerics.NET,   <a href=\"https://numerics.net/documentation/in-depth-tensors\" rel=\"noopener nofollow\" target=\"_blank\">https://numerics.net/documentation/in-depth-tensors</a></li>\n<li>A generic tensor library for .NET | Antão Almada's Blog  <a href=\"https://aalmada.github.io/posts/A-generic-tensor-library-for-dotnet/\" rel=\"noopener nofollow\" target=\"_blank\">https://aalmada.github.io/posts/A-generic-tensor-library-for-dotnet/</a></li>\n<li>The State of Developer Ecosystem 2025: Coding in the Age of AI, New Productivity Metrics, and Changing Realities - The JetBrains Blog <a href=\"https://blog.jetbrains.com/research/2025/10/state-of-developer-ecosystem-2025/\" rel=\"noopener nofollow\" target=\"_blank\">https://blog.jetbrains.com/research/2025/10/state-of-developer-ecosystem-2025</a></li>\n<li>2025: io.net Year in Review <a href=\"https://io.net/blog/2025-io-net-year-in-review\" rel=\"noopener nofollow\" target=\"_blank\">https://io.net/blog/2025-io-net-year-in-review</a></li>\n</ol>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>欢迎大家扫描下面二维码成为我的客户，扶你上云</p>\n<img src=\"https://images.cnblogs.com/cnblogs_com/shanyou/57459/o_220125090408_%E9%82%80%E8%AF%B7%E4%BA%8C%E7%BB%B4%E7%A0%81-258px.jpeg\" width=\"170\" />\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 07:30</span>&nbsp;\n<a href=\"https://www.cnblogs.com/shanyou\">张善友</a>&nbsp;\n阅读(<span id=\"post_view_count\">23</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "LLVM的混淆之旅(五)-手动实现控制流平坦化混淆",
      "link": "https://www.cnblogs.com/ClownLMe/p/19581710",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ClownLMe/p/19581710\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 22:54\">\n    <span>LLVM的混淆之旅(五)-手动实现控制流平坦化混淆</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"简介\">简介</h1>\n<p>之前的教学中，简单的演示了LLVM的基本用法，下面，展示一个实战项目。</p>\n<h1 id=\"编译目标\">编译目标</h1>\n<p>本次的实验编译样例是下面判断正数，负数，和零的代码</p>\n<pre><code class=\"language-c\">#include &lt;stdio.h&gt;\n\nint main() {\n    int a = 9;\n    scanf_s(\"%d\", &amp;a);\n    if (a &lt; 0) {\n        printf(\"Negative number!\\n\");\n    } else if(a &gt; 0) {\n        printf(\"Positive number!\\n\");\n    } else {\n        printf(\"Zero!\\n\");\n    }\n    \n    printf(\"Done.\\n\");\n    return 0;\n} \n</code></pre>\n<h1 id=\"一控制流混淆平坦化\">一，控制流混淆平坦化</h1>\n<h3 id=\"简介-1\">简介</h3>\n<p>什么是控制流平坦化？简单来说，就是让原本垂直的流程分支平摊到水平方向上，使用这种方法可以提高逆向难度，软件更耐造。</p>\n<pre><code>            +-----------------------+\n            |        [开始]          |\n            |    设定初始状态 = 1     |\n            +-----------|-----------+\n                        |\n      +----------------&gt;V&lt;----------------+\n      |       +-------------------+       |\n      |       |    循环控制中心     |       | \n      |       |     (分发器)       |       |\n      |       +---------|---------+       |\n      |                 |                 |\n      |        _________V_________        |\n      |       |                   |       |\n      |       |  switch(状态变量)  |       | \n      |       |___________________|       |\n      |          /      |      \\          |\n      |         /       |       \\         |\n      |   [状态 1]   [状态 2]    [退出]     |\n      |   +-----+    +-----+    +-----+   |\n      |   | 块 1|    | 块 2 |    |结束 |   |\n      |   |     |    |     |    +-----+   |\n      |   |更新 |    | 更新  |             |\n      |   |状态 |    | 状态  |             |\n      |   +--|--+    +--|--+              |\n      |      |          |                 |\n      +------+----------+-----------------+\n</code></pre>\n<h3 id=\"实现控制流平坦化pass的核心代码\">实现控制流平坦化Pass的核心代码</h3>\n<p><strong>代码流程：</strong><br />\n识别与收集分支路径（代码块）-&gt; 构建控制骨架 -&gt; 初始化状态变量 -&gt; 配置分发器 -&gt; 重构跳转逻辑<br />\n我将代码的解释都标注在注释中</p>\n<pre><code class=\"language-cpp\">namespace{\n    struct mypass : public PassInfoMixin&lt;mypass&gt;{\n        PreservedAnalyses run(Function &amp;F, FunctionAnalysisManager &amp;AM){\n\t        //这里为了演示只混淆main函数\n            if(F.getName() != \"main\"){\n                return PreservedAnalyses::all();\n            }\n\n            errs() &lt;&lt; \"My Flattening Function:\" &lt;&lt; F.getName() &lt;&lt; \"\\n\";\n\t\t\t\n\t\t\t//通过遍历，获取main函数中的所有代码块\n            std::vector&lt;BasicBlock*&gt; OrigBBs;\n            DenseMap&lt;BasicBlock*, int&gt; BBtoID;\n            BasicBlock &amp;EntryBB = F.getEntryBlock();\n            int id_counter = 0;\n\n            for(BasicBlock &amp;BB : F){\n\t            //这里去除首代码块\n                if(&amp;BB == &amp;EntryBB) continue;\n                //记录代码块并给代码块标序号\n                //这里主要方便case中使用，可以是很复杂的独一无二的数字\n                OrigBBs.push_back(&amp;BB);\n                BBtoID[&amp;BB] = id_counter++;\n            }\n\t\t\t\n\t\t\t//判断收集到的分支是否存在，如果没有分支，就\n            if(OrigBBs.empty()){\n                return PreservedAnalyses::all();\n            }\n\t\t\t\n\t\t\t//创建循环控制代码块\n            BasicBlock *LoopEntry = BasicBlock::Create(F.getContext(), \"loop_entry\", &amp;F);\n            //控制流分发器代码块\n            BasicBlock *SwitchBB = BasicBlock::Create(F.getContext(), \"switch_block\", &amp;F);\n            //循环分发器尾部（用于兜底，可以不使用）\n            BasicBlock *LoopEnd = BasicBlock::Create(F.getContext(), \"loop_end\", &amp;F);\n\t\t\t\n\t\t\t//在首代码块中，创建初始状态变量，用于获取刚开始跳转的状态量\n            IRBuilder&lt;&gt; builderEntry(&amp;EntryBB);\n            Instruction *EntryTerm = EntryBB.getTerminator();\n            if (EntryTerm) builderEntry.SetInsertPoint(EntryTerm);\n            AllocaInst *SwitchVar = builderEntry.CreateAlloca(builderEntry.getInt32Ty(), nullptr, \"switchVar\");\n\t\t\t//获取跳转指令\n            if(BranchInst *BI = dyn_cast_or_null&lt;BranchInst&gt;(EntryTerm)){\n\t            //判断是否是条件跳转\n\t            //如果是，则直接获取跳转后的代码块序号\n                if(BI-&gt;isUnconditional()){\n                    BasicBlock *Target = BI-&gt;getSuccessor(0);\n                    if(BBtoID.count(Target)){\n                        builderEntry.CreateStore(builderEntry.getInt32(BBtoID[Target]), SwitchVar);\n                    }\n                    //如果是条件跳转\n                    //创建一条条件判断指令\n                }else if(BI-&gt;isConditional()){\n                    Value *Cond = BI-&gt;getCondition();\n                    BasicBlock *TrueBB = BI-&gt;getSuccessor(0);\n                    BasicBlock *FalseBB = BI-&gt;getSuccessor(1);\n                    if(BBtoID.count(TrueBB) &amp;&amp; BBtoID.count(FalseBB)){\n                        Value*Select = builderEntry.CreateSelect(\n                            Cond, \n                            builderEntry.getInt32(BBtoID[TrueBB]), \n                            builderEntry.getInt32(BBtoID[FalseBB]),\n                            \"init_state\"\n                        );\n                        builderEntry.CreateStore(Select, SwitchVar);\n                    }\n                }\n            }\n\t\t\t\n\t\t\t//首部创建跳转-&gt;跳转到循环控制入口\n            builderEntry.CreateBr(LoopEntry);\n            //删除旧的跳转指令\n            if(EntryTerm) EntryTerm-&gt;eraseFromParent();\n\t\t\t\n\t\t\t//创建状态变量，用于控制分发器的走向\n            IRBuilder&lt;&gt; builderLoop(LoopEntry);\n            LoadInst *CurrState = builderLoop.CreateLoad(builderLoop.getInt32Ty(), SwitchVar, \"curr_state\");\n            builderLoop.CreateBr(SwitchBB);\n\t\t\t\n\t\t\t//创建switch指令\n            IRBuilder&lt;&gt; builderSwitch(SwitchBB);\n            SwitchInst *SwitchI = builderSwitch.CreateSwitch(CurrState, LoopEnd, OrigBBs.size());\n            //创建case\n            //其中的标识符是上面收集到的代码块的序号\n            for(BasicBlock *BB :OrigBBs){\n\t            //所有都跳转到尾部\n                BB-&gt;moveBefore(LoopEnd);\n                SwitchI-&gt;addCase(builderSwitch.getInt32(BBtoID[BB]), BB);\n            }\n\t\t\t\n\t\t\t//用于兜底，跳转回循环控制头部\n            IRBuilder&lt;&gt; builderEnd(LoopEnd);\n            builderEnd.CreateBr(LoopEntry);\n\t\t\t\n\t\t\t//遍历每个代码块，插入修改状态变量的指令\n\t\t\t//这里根据下一个跳转到的代码块来标识状态变量\n\t\t\t//这里的代码跟获取初次跳转数值一模一样\n            for(BasicBlock *BB : OrigBBs){\n                Instruction *Term = BB-&gt;getTerminator();\n                IRBuilder&lt;&gt; builder(BB);\n\t\t\t\t//由于跳转分为有条件跳转和无条件跳转\n\t\t\t\t//这里照样要做个判断\n                if(BranchInst*BI = dyn_cast_or_null&lt;BranchInst&gt;(Term)){\n\t                \n                    if(BI-&gt;isUnconditional()){\n                        BasicBlock *Target = BI-&gt;getSuccessor(0);\n                        if(BBtoID.count(Target)){\n                            builder.SetInsertPoint(Term);\n                            builder.CreateStore(builderSwitch.getInt32(BBtoID[Target]), SwitchVar);\n                            builder.CreateBr(LoopEntry);\n                            Term-&gt;eraseFromParent();\n                        }\n                    }\n                    else if(BI-&gt;isConditional()){\n                        BasicBlock *TrueBB = BI-&gt;getSuccessor(0);\n                        BasicBlock *FalseBB = BI-&gt;getSuccessor(1);\n                        if(BBtoID.count(TrueBB) &amp;&amp; BBtoID.count(FalseBB)){\n                            builder.SetInsertPoint(Term);\n                            Value *Select = builder.CreateSelect(\n                                BI-&gt;getCondition(),\n                                builder.getInt32(BBtoID[TrueBB]),\n                                builder.getInt32(BBtoID[FalseBB]),\n                                \"next_state\"\n                            );\n                            builder.CreateStore(Select, SwitchVar);\n                            builder.CreateBr(LoopEntry);\n                            Term-&gt;eraseFromParent();\n                        }\n                    }\n                }\n            }\n        }\n    };\n}\n</code></pre>\n<h1 id=\"编译和使用\">编译和使用</h1>\n<p><strong>这部分之前文章讲过，这里就不浪费篇幅，不懂的可以翻看</strong></p>\n<h1 id=\"使用效果演示\">使用效果演示</h1>\n<h3 id=\"未使用时\">未使用时</h3>\n<p><img alt=\"控制流3\" class=\"lazyload\" /><br />\n<img alt=\"控制流4\" class=\"lazyload\" /></p>\n<h3 id=\"使用后\">使用后</h3>\n<p><img alt=\"控制流1\" class=\"lazyload\" /><br />\n<img alt=\"控制流2\" class=\"lazyload\" /></p>\n<p><strong>至此，我们成功实现了控制流平坦化</strong></p>\n<p><strong>如果❤喜欢❤本系列教程，就点个关注吧，后续不定期更新~</strong></p>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/ClownLMe/\" target=\"_blank\">ClownLMe</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/ClownLMe/p/19581710\" target=\"_blank\">https://www.cnblogs.com/ClownLMe/p/19581710</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 22:54</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ClownLMe\">ClownLMe</a>&nbsp;\n阅读(<span id=\"post_view_count\">27</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战",
      "link": "https://www.cnblogs.com/algieba/p/19581327",
      "published": "",
      "description": "<div class=\"postcontent\">\n\t\t\t    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260205193318638-616281954.png\" />\n        会跑代码还不够，我们要懂原理。本文从 Transformer 的底层视角出发，图解从位置编码到注意力机制的全流程；并基于 Kaggle 平台，深入拆解 HuggingFace Transformers 库的“铁三角”组件与生成参数的玄机。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"大模型实战-03-拆解-transformers从原理图解到-huggingface-transformers-实战\">[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战</h1>\n<blockquote>\n<p><strong>核心摘要 (TL;DR)</strong></p>\n<ul>\n<li><strong>原理</strong>：图解 Transformer 是如何通过“注意力机制”和“位置编码”来理解人类语言的。</li>\n<li><strong>实战</strong>：在 <strong>Kaggle (双 T4 GPU)</strong> 环境下，拆解 HuggingFace 代码的“铁三角”（Config, Tokenizer, Model）。</li>\n<li><strong>技巧</strong>：掌握 <code>Temperature</code> 和 <code>Top_p</code>，学会控制 AI 的“创造力”。</li>\n</ul>\n</blockquote>\n<h2 id=\"前言\">前言</h2>\n<p>各位友人们，大家好，这里是阿尔。在上一节的“炼丹”环境搭建中，咱们成功地将Qwen2.5模型运行了起来，跑通了。 但是相信大家对运行的时候的那些参数都代表着什么，都还是懵的。 这篇博客就是为此准备的，我们打算先快速大概地了解一下当前的大模型的底层原理，再结合在一起介绍通用的transformers库, 去看看代码和如何对应这些理论的。</p>\n<h2 id=\"1-transformer-极简原理大模型是怎么思考的\">1. Transformer 极简原理：大模型是怎么思考的？</h2>\n<p>大模型和普通的机器学习模型一样，本质也是一个函数，不同的是，传统机器学习可能输入的是一些整理好的数据，比如房子的尺寸，地段，购买时长，和市中心的距离等等数据，输出一个预测的放假，而大模型输入的是我们的问题，拿到的是大模型给出的回答。但，咱们究其根本，它们都可以看作是一个函数，我们输入一些东西，经过运算之后，输出给我们一些东西。</p>\n<p>对于大模型，它的本质就是一个<strong>下一个字符预测器</strong>，我们输入一些文字，它只负责根据它所训练的海量数据，输出最符合，最有可能的下一个字符，就像一个<strong>文字接龙机器</strong>，其核心任务只有一个：<strong>根据上文，猜下一个字是什么。</strong></p>\n<p>为了实现这个目标，Transformer 架构经历了一个精密的流水线：<br />\n<img alt=\"包含Transformer的5个步骤的总览图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/01_cover.png\" /><br />\n下面，我们会稍微详细一点地介绍一下各个步骤。</p>\n<h3 id=\"11-第一步token-化-tokenization--查字典\">1.1 第一步：Token 化 (Tokenization) —— 查字典</h3>\n<p>大模型既然是一个函数，那么肯定是针对数字进行处理的，所以，我们就需要一个法子，去将我们的文字字符（甚至是图片）变成大模型认识的数字（或者说张量，向量）。 这就是Token化，去做这一步操作的函数，或者模块就是<strong>分词器（Tokenizer）</strong>。</p>\n<p>那么<strong>Tokenizer</strong>具体是如何工作的？它实际上就是将一个个的字符，对应成一个个数字，或者说ID，就像ASCII码一样，不过它做得更高级。</p>\n<p>把每一个字都找一个数字对应，有点奢侈，特别是对于英语,act, acting,action,actor其实都有相同的词根，主要的语义来源于其词根act，所以分词器是按照词元（token）去拆分的，能把有些词拆成词根、前缀和后缀等等，当然具体如何拆，取决于字典如何定义，字典有多大。</p>\n<p>这里我们给一个简单的例子</p>\n<ul>\n<li><strong>输入</strong>：“我爱 AI”</li>\n<li><strong>动作</strong>：切分 -&gt; <code>[\"我\", \"爱\", \"AI\"]</code> -&gt; 查表 -&gt; <code>[2301, 452, 1083]</code><br />\n<img alt=\"Token化的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.1_tokenization.png\" /></li>\n</ul>\n<h3 id=\"12-第二步embedding--位置编码--赋予含义与顺序\">1.2 第二步：Embedding &amp; 位置编码 —— 赋予含义与顺序</h3>\n<p>有了token之后，我们想知道词和词的关系，我们想要通过一个可以量化的量去判断两个词是否是有关系的，关系多大。这就引入了我们的下一个模块，词嵌入模块（Embedding）。</p>\n<ul>\n<li><strong>Embedding (词向量)</strong>：把每个 ID 变成一个长长的向量（比如 4096 维的数组）。这个向量在模型训练之前是随机的，其后随着海量的训练数据洗礼，越相关的词向量越靠近，越不相关的词越远离。这个向量代表了词的<strong>含义</strong>。比如“猫”和“狗”的向量在空间里距离很近，“苹果”和“手机”在某种语境下也更近。</li>\n</ul>\n<p>光知道词和词的关系还不够，“我爱你”和“你爱我”的每一个词都是相同的，但是它们确实可以不相关的两个句子，“小狗咬了我”和“我咬了小狗”，也会被人视作“可以正常理解”和“这人好像不对劲”两种完全不同的理解。显而易见，词语在句中的位置是一个非常重要的信息，我们不能弄丢它，也需要将这一部分信息传递给模型训练时候去学习。</p>\n<ul>\n<li><strong>Positional Encoding (位置编码)</strong>：Transformer 是并行计算的（它一眼看完所有词），这导致它不知道“我爱你”和“你爱我”的区别。所以，我们需要给每个词贴上一个“座位号”，告诉模型谁在前面，谁在后面。<br />\n<img alt=\"Embedding &amp; 位置编码 —— 赋予含义与顺序示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.2_ebedding.png\" /></li>\n</ul>\n<h3 id=\"13-第三步self-attention-自注意力--寻找关系\">1.3 第三步：Self-Attention (自注意力) —— 寻找关系</h3>\n<p>接下来就是大模型的灵魂，我们想知道一句话里的每一个词元和其他词元的<strong>关联度</strong>，其实就是上下文的联系。当模型处理“苹果”这个词时，如果上下文里有“手机”、“发布会”，注意力机制会告诉模型：“嘿，这里的‘苹果’指的是科技公司，不是水果！”,自注意力机制，让模型能理解上下文。<br />\n<img alt=\"Self-Attention 示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.3-self-attention.png\" /></p>\n<h3 id=\"14-第四步mlp-前馈神经网络--消化吸收\">1.4 第四步：MLP (前馈神经网络) —— 消化吸收</h3>\n<p>如果说注意力机制是“看”，那么 MLP 就是“想”。它包含多层神经元，负责对提取到的信息进行复杂的非线性变换和逻辑推理,也就是传统的深度学习。<br />\n<img alt=\"MLP (前馈神经网络)示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.4-mlp.png\" /></p>\n<h3 id=\"15-第五步decoder--softmax--输出概率\">1.5 第五步：Decoder &amp; Softmax —— 输出概率</h3>\n<p>经过层层计算，模型最终会输出一个包含了所有词汇（比如 15 万个词）的<strong>概率列表</strong>。</p>\n<ul>\n<li><code>AI</code> (80%)</li>\n<li><code>吃</code> (10%)</li>\n<li><code>睡</code> (5%)<br />\n...<br />\n最后，我们根据这个概率列表，选择下一个词。<br />\n<img alt=\"输出概率的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.5-decoder&amp;softmax.png\" /></li>\n</ul>\n<hr />\n<p>这就是大模型词语接龙的原理了。</p>\n<h2 id=\"2-拆解模型文件夹下载下来的到底是什么\">2. 拆解模型文件夹：下载下来的到底是什么？</h2>\n<p>理论讲完了，我们来看看在 <strong>Kaggle</strong> 的文件系统里，这些理论变成了什么文件。</p>\n<p>当你下载一个模型（以 Qwen2.5-7B 为例）时，文件夹结构如下：</p>\n<p><img alt=\"safetensor格式的文件组织示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/safetensor.png\" /></p>\n<h3 id=\"21-核心架构configjson\">2.1 核心架构：config.json</h3>\n<p><strong>config.json</strong>: 是大模型的身份证，也可以说是体检表，它其中就是真正的我们的模型，具体由哪些层构成，是什么架构类型，隐藏层有多深，注意力头的数量有几个，词表的大小是多少。对于大模型而言，它就是模型本身，也是<strong>骨架</strong>，因为大模型重要的是训练完的参数，模型本身是很小的。<br />\n<img alt=\"config.json的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/config.json.png\" /></p>\n<h3 id=\"22-行为预设generation_configjson\">2.2 行为预设：generation_config.json</h3>\n<p><strong>generation_config.json</strong>:是模型的出厂默认设置。<br />\n<img alt=\"generation_config的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/generation_config_.png\" /></p>\n<h3 id=\"23-大脑本身safetensors和indexjson\">2.3 大脑本身：*.safetensors和*.index.json</h3>\n<p>有了config.json中的躯体，我们再从<em>.safetensors和</em>.index.json载入灵魂，这才是我们能说会道的大模型。</p>\n<ul>\n<li><strong><code>model-xxxxx.safetensors</code></strong>：这里面存的是实打实的<strong>张量（Tensor）数据</strong>，即数十亿个参数的浮点数。为了方便存储和加载，通常会被切分成多个 2GB-5GB 的小文件（Shard）。</li>\n<li><strong><code>model.safetensors.index.json</code></strong>：这是一张<strong>藏宝图</strong>。因为权重被切分了，模型需要知道“第 5 层的权重”到底藏在哪个文件里。\n<ul>\n<li><strong>内部长这样</strong>：<pre><code class=\"language-json\">{\n  \"metadata\": { \"total_size\": 15423653888 },\n  \"weight_map\": {\n    \"model.layers.0.self_attn.q_proj.weight\": \"model-00001-of-00004.safetensors\",\n    \"model.layers.20.mlp.gate_proj.weight\": \"model-00003-of-00004.safetensors\"\n  }\n}\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"数字和文字的翻译官tokenizer相关文件\">数字和文字的翻译官：tokenizer相关文件</h3>\n<ul>\n<li><strong><code>vocab.json</code> / <code>merges.txt</code></strong>：这是最原始的<strong>生词表</strong>。记录了所有字、词根对应的 ID。</li>\n<li><strong><code>tokenizer.json</code></strong>：这是一个<strong>编译后</strong>的高效字典文件，包含了分词的所有逻辑（Pre-tokenization, Normalization 等），加载速度比读原始文本快得多。</li>\n<li><strong><code>tokenizer_config.json</code> (至关重要)</strong>：这是分词器的<strong>配置文件</strong>。\n<ul>\n<li>它定义了<strong>特殊符号</strong>（Special Tokens）：比如哪个 ID 代表“开始”，哪个代表“结束”。</li>\n<li>它包含了 <strong>Chat Template (聊天模板)</strong>：这是一段 Jinja2 代码，决定了 <code>apply_chat_template</code> 如何工作。</li>\n<li><strong>内部长这样</strong>：<pre><code class=\"language-json\">{\n  \"chat_template\": \"{% for message in messages %}...&lt;|im_start|&gt;...\",\n  \"eos_token\": \"&lt;|im_end|&gt;\",\n  \"pad_token\": \"&lt;|endoftext|&gt;\"\n}\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-transformers库实战代码中的铁三角\">3. Transformers库实战：代码中的“铁三角”</h2>\n<p>在Transforms库中，我们永远绕不开三个核心类。</p>\n<p><strong>环境准备：</strong><br />\n在 Kaggle 右侧设置中，确保 <strong>Internet: On</strong> 且 <strong>Accelerator: GPU T4 x2</strong>。</p>\n<pre><code class=\"language-python\">!pip install -U transformers accelerate bitsandbytes\n</code></pre>\n<h3 id=\"31-autotokenizer-翻译官\">3.1 AutoTokenizer (翻译官)</h3>\n<p>对应理论中的 <strong>Token 化</strong> 步骤,和模型文件中的tokenizer相关文件。</p>\n<pre><code class=\"language-python\">from transformers import AutoTokenizer\n\nmodel_path = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1/\"\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\ntext = \"Transformer is amazing\"\n# 1. 编码 (Encode): 文本 -&gt; 数字 ID\ninput_ids = tokenizer.encode(text)\nprint(f\"原文: {text}\")\nprint(f\"数字 ID: {input_ids}\")\n\n# 2. 解码 (Decode): 数字 ID -&gt; 文本\ndecoded_text = tokenizer.decode(input_ids)\nprint(f\"还原: {decoded_text}\")\n</code></pre>\n<p>得到的结果将会是这样</p>\n<pre><code class=\"language-bash\">原文: Transformer is amazing\n数字 ID: [46358, 374, 7897]\n还原: Transformer is amazing\n</code></pre>\n<p><strong>对话格式：Chat Template</strong><br />\n大模型需要特定的对话格式（Prompt）。来将模型的回答，和用户的问题做区分, 我们一般都可以通过载入语言模型对应模板（不同家的模型，可能模板会有不同），甚至去拼装历史记录。</p>\n<pre><code class=\"language-python\">messages = [\n    {\"role\": \"system\", \"content\": \"你是一个物理学家。\"},\n    {\"role\": \"user\", \"content\": \"用一句话解释相对论。\"}\n]\n\n# 自动应用聊天模板\nformatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nprint(\"模型实际看到的输入:\\n\", formatted_prompt)\n</code></pre>\n<p>运行结果如下</p>\n<pre><code class=\"language-bash\">模型实际看到的输入:\n &lt;|im_start|&gt;system\n你是一个物理学家。&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n用一句话解释相对论。&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></pre>\n<p>看到这里，我们也能更好地理解，为什么一个本质是词语接龙的模型，能够区分问题，然后做出回答。因为在训练的过程中，我们会让他知道&lt;|im_start&gt;表示一个message的开始，其中会告诉模型，这句话是谁说的，这句话在什么位置结束，它接龙的时候也会带上开始和结束符号，在推理模型中，甚至会带上思考的标签。</p>\n<h3 id=\"32-automodel-大脑本体\">3.2 AutoModel (大脑本体)</h3>\n<p>对应理论中的 <strong>Embedding -&gt; Attention -&gt; MLP</strong> 计算过程,通过从config.json中加载模型躯体，在加载上模型的safetensor灵魂数据以及generation_config.json的默认初始化,<br />\n在 Kaggle 上，我们拥有 <strong>双 T4 (15GB x 2)</strong>，一定要利用 <code>device_map=\"auto\"</code> 让库自动分配显存。</p>\n<pre><code class=\"language-python\">from transformers import AutoModelForCausalLM\nimport torch\n\n# 加载模型\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",      # 关键！自动将模型切分到两张 T4 显卡上\n    torch_dtype=torch.float16, # 使用半精度，节省显存\n    trust_remote_code=True\n)\n\nprint(f\"模型加载成功！显存分布: {model.hf_device_map}\")\n</code></pre>\n<p>输出结果为</p>\n<pre><code class=\"language-bash\">模型加载成功！显存分布: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n</code></pre>\n<h2 id=\"4-掌控生成的调节旋钮\">4. 掌控生成的“调节旋钮”</h2>\n<p>在模型输出的过程中，我们有一些参数可以对输出结果进行调节，对应于我们讲理论部分的中的 <strong>第五步 (Softmax 概率输出)</strong>, 我们有一堆下一个词元的概率分布了，但是我们应该如何去选择呢？</p>\n<h3 id=\"41-temperature-温度\">4.1 Temperature (温度)</h3>\n<p>我们可以设定的Temperature参数，我们在generation_config.json中也看见过它</p>\n<p>值越大，更热，更具创造性，更容易输出各种天马行空的词, 会缩小所有词元的差距，<strong>雨露均沾</strong>，以达到创造性，让低概率的词，也有机会被选中，当然,也更容易胡说八道，出现幻觉.<br />\n值越小，更冷，更严肃，在温度为0的时候甚至会固定输出最高的那个词，它会拉大高概率和低概率的差距，<strong>赢家通吃</strong>，让模型的回答更稳定，严谨。</p>\n<h3 id=\"42-top_p-核采样\">4.2 Top_p (核采样)</h3>\n<p>和温度不同，我们还有另一种方式，这种方式更类似于拉网，我们只要可能性前80%的词，在那些词里进行挑选，这个可能性就是P，这个选词（采样）方法又叫top_p(核采样).<br />\n简而言之：其只在累积概率达到 P (e.g., 0.9) 的前几个词里选。直接切掉尾部那些极低概率的离谱词。</p>\n<h3 id=\"43-实验一下\">4.3 实验一下</h3>\n<p>我们这里先定义一个函数，以温度和top_p为参数去测试不同的参数对回答的影响</p>\n<pre><code class=\"language-python\"># 定义一个测试函数\ndef test_generation(temp, top_p, prompt_text):\n    messages = [\n        {\"role\": \"system\", \"content\": \"你是一个前卫的科幻小说家。\"},\n        {\"role\": \"user\", \"content\": prompt_text}\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    print(f\"\\n======== 设置: Temperature={temp}, Top_p={top_p} ========\")\n\n    try:\n        generated_ids = model.generate(\n            **inputs,\n            max_new_tokens=100,  # 限制长度，方便快速看结果\n            temperature=temp,\n            top_p=top_p,\n            do_sample=True,      # 必须开启采样\n            pad_token_id=tokenizer.eos_token_id\n        )\n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        # 只打印回答部分\n        print(response.split('assistant')[-1].strip())\n    except Exception as e:\n        print(f\"生成出错: {e}\")\n</code></pre>\n<h3 id=\"温度实验\">温度实验</h3>\n<p>然后再其下新建code block去测试，看看效果</p>\n<pre><code class=\"language-python\">prompt = \"请用这三个词写一个微故事：量子、失恋、炒饭。\"\n\n# 1. 低温模式 (0.1)：严谨、死板\ntest_generation(temp=0.1, top_p=0.9, prompt_text=prompt)\n\n# 2. 适中模式 (0.7)：正常、流畅\ntest_generation(temp=0.7, top_p=0.9, prompt_text=prompt)\n\n# 3. 高温模式 (1.5)：疯狂、混乱\n# 注意：可能会输出乱码或完全不通顺的句子\ntest_generation(temp=1.5, top_p=0.9, prompt_text=prompt)\n</code></pre>\n<p>这是我的运行结果</p>\n<pre><code class=\"language-bash\">======== 设置: Temperature=0.1, Top_p=0.9 ========\n在量子世界里，时间与空间的概念变得模糊不清，而李明的世界也因为一段失败的恋情而变得一片混沌。他尝试着用量子纠缠理论来修复自己破碎的心灵，却意外地将自己从现实世界送入了一个平行宇宙。\n\n在这个平行宇宙中，李明发现了一家特别的餐馆，这里的厨师是一位曾经的恋人，她正在为一位顾客准备一道特别的炒饭。这道炒饭不仅色香味俱全，还\n\n======== 设置: Temperature=0.7, Top_p=0.9 ========\n在量子世界的边缘，李明独自一人坐在一家不起眼的小餐馆里，面前是一碗普通的炒饭。他和女朋友分手的原因，是因为她沉迷于虚拟现实中的量子世界，而他却对现实世界充满了留恋。\n\n就在几天前，他们最后一次争吵后，她告诉他：“我找到了真正的自我——一个穿梭在量子世界的探索者。”她留下了一盘未吃完的炒饭，然后消失在了虚拟现实中。\n\n李明望着那盘炒饭，\n\n======== 设置: Temperature=1.5, Top_p=0.9 ========\n标题：时空泡饭\n\n李明最近失恋了，每天只能煮一大锅泡饭来消磨时间。\n\n这天李明正在做饭，他忽然接收到一条量子信号。他惊奇地发现那是自己失恋前女友的坐标位置。想到能与自己相爱过的人共度时光是多么美妙的事情，他便将自己煮了一锅泡饭送进了坐标传送器。\n\n结果却是一锅炒饭。\n\n李明百思不解。\n</code></pre>\n<h3 id=\"top_p实验\">top_p实验</h3>\n<p>接下来是top_p</p>\n<pre><code class=\"language-python\">prompt_2 = \"请给一种不存在的颜色起个名字，并描述它的样子。\"\n\n# 1. 极窄采样 (0.01)：只选概率最高的那个词（近似贪婪搜索）\ntest_generation(temp=0.8, top_p=0.01, prompt_text=prompt_2)\n\n# 2. 宽广采样 (0.95)：允许罕见词出现\ntest_generation(temp=0.8, top_p=0.95, prompt_text=prompt_2)\n</code></pre>\n<p>输出结果为</p>\n<pre><code class=\"language-bash\">======== 设置: Temperature=0.8, Top_p=0.01 ========\n这种不存在的颜色我命名为“星尘紫”。它是一种梦幻般的颜色，介于紫色和银色之间，仿佛是宇宙中无数微小的星辰碎片在闪烁时所散发出的光芒。在不同的光线下，星尘紫会呈现出不同的色调，有时偏紫，有时偏银，有时又像是掺杂了点点星光的淡蓝色。它既神秘又优雅，仿佛能让人感受到宇宙的浩瀚与深邃。\n\n======== 设置: Temperature=0.8, Top_p=0.95 ========\n这种不存在的颜色我称之为“星际幻彩”（Stellar Mirage）。在视觉上，它并非单一色调，而是一种动态变化的色彩组合，像是无数微小的光点在眼前闪烁变幻，这些光点包含了所有可见光谱的颜色，同时又带着一种神秘的、不可名状的色彩。\n\n当观察者注视着“星际幻彩”的时候，他可以看到蓝、绿、紫等颜色快速地在眼前切换和混合，它们以一种几\n</code></pre>\n<h2 id=\"5-完整-transformers-代码实战\">5. 完整 Transformers 代码实战</h2>\n<p>把所有积木搭在一起，这就是一段标准的模型推理代码：</p>\n<pre><code class=\"language-python\">import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 1. 设置模型 ID\nmodel_id = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1/\"\n\n# 2. 加载翻译官\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n\n# 3. 加载大脑 (双卡模式)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\n\n# 4. 准备输入\nprompt = \"请用这三个词写一个微小说：Kaggle、深夜、爆显存\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是一个幽默的程序员。\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# 5. 生成 (调节参数！)\nprint(\" 正在生成...\")\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512,\n    temperature=0.8,  # 稍微有点创意\n    top_p=0.9,        # 剔除离谱词\n    do_sample=True    # 必须开启采样，温度才生效\n)\n\n# 6. 解码并输出\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(\"-\" * 20)\nprint(f\" 回答:\\n{response}\")\nprint(\"-\" * 20)\n</code></pre>\n<p>这是输出结果</p>\n<pre><code class=\"language-bash\">Loading checkpoint shards: 100%\n 4/4 [00:13&lt;00:00,  3.27s/it]\nThe module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\nWARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n 正在生成...\n--------------------\n 回答:\n在一个寒冷的深夜，李雷坐在他那间堆满咖啡罐和代码文件的房间里，屏幕上是Kaggle竞赛的数据集。他正尝试训练一个复杂的深度学习模型。然而，就在他认为胜利在望的时候，“嘶~”的一声，显示器瞬间变成了一片漆黑，伴随着一声悲壮的“爆显存了”。\n\n李雷揉了揉眼睛，看着眼前一片空白的屏幕，心中充满了挫败感，但他转念一想：“还好不是‘爆内存了’，否则我这台老旧电脑可能就要彻底退休了。”于是，他又开始调整参数，希望能在这个深夜里找到那个隐藏在数据海洋中的宝藏。\n--------------------\n</code></pre>\n<h3 id=\"hint\">Hint:</h3>\n<p><strong>所有的代码，都可以在 <a href=\"https://www.kaggle.com/code/thaodinhoio/llm03-transformers\" rel=\"noopener nofollow\" target=\"_blank\">这个笔记本</a>中直接获取运行哦</strong></p>\n<h2 id=\"6-常见问题-qa\">6. 常见问题 (Q&amp;A)</h2>\n<p><strong>Q: 在 Kaggle 上 <code>device_map=\"auto\"</code> 是必须的吗？</strong><br />\n<strong>A:</strong> 如果你使用单卡 T4 (15GB) 跑 7B 模型（约 14GB），勉强能塞进一张卡。但如果你开启了 Kaggle 的 <strong>T4 x2</strong>，为了利用全部 30GB 显存，<strong>必须</strong>加这个参数，否则模型只会塞进第一张卡，导致第一张爆满，第二张围观。</p>\n<p><strong>Q: 为什么生成的每一句话都不一样？</strong><br />\n<strong>A:</strong> 因为我们开启了 <code>do_sample=True</code> 并且设置了 <code>temperature &gt; 0</code>。模型在选择下一个词时是<strong>按概率随机抽取</strong>的。如果你想让结果每次都一样（比如做数学题），请设置 <code>do_sample=False</code>（此时温度失效，变为贪婪解码）。</p>\n<p><strong>Q: 什么是 Logits？</strong><br />\n<strong>A:</strong> 在代码深处，模型输出的那个“概率表”在变成百分比之前，叫 Logits（未归一化的数值）。Softmax 函数的作用就是把 Logits 变成概率。你可以把 Logits 理解为模型对每个词的“原始打分”。</p>\n<p><strong>Q: Token 和字是一一对应的吗？</strong><br />\n<strong>A:</strong> 不一定。</p>\n<ul>\n<li>英文：通常一个单词是一个 Token，长单词可能被切分。</li>\n<li>中文：通常一个汉字是一个 Token，但常见词（如“你好”）可能会合并为一个 Token。</li>\n<li>平均来说，<strong>0.75 个英文单词 ≈ 1 Token</strong>，<strong>1 个汉字 ≈ 1.5 - 2 Token</strong>（取决于分词器效率，Qwen 的中文压缩率很高）。</li>\n</ul>\n<hr />\n<p><strong>本文作者：</strong> Algieba<br />\n<strong>本文链接：</strong> <a href=\"https://blog.algieba12.cn/llm03-know-more-about-transformer-lib/\" rel=\"noopener nofollow\" target=\"_blank\">https://blog.algieba12.cn/llm03-know-more-about-transformer-lib/</a><br />\n<strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</p>\n<pre><code>\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"itemdesc\">\n\t\t\t发表于 \n<span id=\"post-date\">2026-02-05 19:33</span>&nbsp;\n<a href=\"https://www.cnblogs.com/algieba\">阿尔的代码屋</a>&nbsp;\n阅读(<span id=\"post_view_count\">60</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</div>"
    }
  ]
}