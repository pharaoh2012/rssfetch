{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "为什么在代理服务器上测试， http2 的转发性能比 http 1 更低？",
      "link": "https://www.cnblogs.com/ahfuzhang/p/19618259",
      "published": "",
      "description": "<div class=\"postcontent\">\n\t\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p><strong><font color=\"gray\" size=\"1\">作者:张富春(ahfuzhang)，转载时请注明作者和引用链接，谢谢！</font></strong></p>\n<ul>\n<li><font color=\"gray\" size=\"1\"><a href=\"https://www.cnblogs.com/ahfuzhang/\" target=\"_blank\">cnblogs博客</a></font></li>\n<li><font color=\"gray\" size=\"1\"><a href=\"https://www.zhihu.com/people/ahfuzhang/posts\" rel=\"noopener nofollow\" target=\"_blank\">zhihu</a></font></li>\n<li><font color=\"gray\" size=\"1\"><a href=\"https://github.com/ahfuzhang\" rel=\"noopener nofollow\" target=\"_blank\">Github</a></font></li>\n<li><font color=\"gray\" size=\"1\">公众号:一本正经的瞎扯</font><br />\n<img alt=\"\" class=\"lazyload\" /></li>\n</ul>\n<hr />\n<p>我在测试 Http2 server 与 Http 1.1 server 的性能差异时，最高测试数据是 http2 比 http1 快 3.6 倍。<br />\n而 Carter 在测试 apisix 代理服务器的性能的时候，得到的数据是 http2 的性能只有 http 1 的 80%。<br />\n我们曾彼此质疑对方的数据，并觉得不可思议！<br />\n我的观点是：二进制协议一定快过文本协议，没理由在代理服务器上测试的数据会导致 http2 慢于 http 1。</p>\n<p>今天终于想明白了原因，其实我们都没错！<br />\n结论是：在代理服务器上，http2 的转发性能会低于 http 1。<br />\n导致这一项差异的关键是 <code>splice()</code> 系统调用，也就是代理服务器中实现<code>零拷贝</code>的关键。</p>\n<p>(后续大量引用 ChatGPT 的回答)</p>\n<h1 id=\"splice-是什么\">splice() 是什么？</h1>\n<p>splice() 是 Linux 的一个系统调用（C 里通过 splice(2) 暴露），用来在 两个文件描述符之间搬运数据，并且尽量走 零拷贝（zero-copy） 路径，减少用户态缓冲区参与。</p>\n<p>最典型的用途：文件 ↔ 管道、管道 ↔ socket，用它可以把数据从磁盘文件直接“送”到网络连接里，中间不需要 read() 到用户态再 write() 回内核。</p>\n<p>⸻</p>\n<p>它解决什么问题</p>\n<p>传统写法：</p>\n<pre><code class=\"language-c\">read(file_fd, user_buf, n);\nwrite(sock_fd, user_buf, n);\n</code></pre>\n<p>缺点：数据会在内核 ↔ 用户态之间拷贝（还可能污染 CPU cache、占用内存带宽）。</p>\n<p>splice() 的目标：让数据尽量在内核内部移动（比如从 page cache / pipe buffer 直接到 socket buffer），减少一次或多次拷贝。</p>\n<p>⸻</p>\n<p>它的基本形态（语义）</p>\n<pre><code class=\"language-c\">ssize_t splice(int fd_in,  loff_t *off_in,\n               int fd_out, loff_t *off_out,\n               size_t len, unsigned int flags);\n</code></pre>\n<pre><code>•\tfd_in / fd_out：输入/输出 fd\n•\toff_in / off_out：\n•\t传 NULL 表示使用 fd 当前偏移并推进（像普通读写那样）\n•\t非 NULL 则用你提供的偏移（类似 pread/pwrite 的味道），并且不一定修改 fd 的文件偏移（具体取决于对象类型）\n•\tlen：最多搬运多少字节\n•\tflags：控制阻塞/移动行为（例如 SPLICE_F_NONBLOCK 等）\n</code></pre>\n<p>返回值：实际搬运的字节数；0 通常表示 EOF；-1 表示出错（errno 说明原因）。</p>\n<p>⸻</p>\n<p>常见组合</p>\n<ol>\n<li>\n<p>file → pipe → socket（高性能“发送文件”）<br />\n•\tsplice(file_fd → pipe_write_end)<br />\n•\tsplice(pipe_read_end → sock_fd)<br />\n这类组合常用于实现“类似 sendfile 的效果”，但 splice 更通用（尤其适合你还要在中间做点别的内核态处理的场景）。</p>\n</li>\n<li>\n<p>pipe ↔ pipe</p>\n</li>\n</ol>\n<p>用于在管道之间转移数据（比如配合 tee() 做复制）。</p>\n<p>⸻</p>\n<p>适用限制（很重要）<br />\n•\t并不是任意 fd 都支持。splice 需要底层文件操作实现支持（VFS/驱动/协议栈）。<br />\n•\t在 Linux 上，pipe 经常是 splice 的“中转站”：很多场景要求 fd_in 或 fd_out 至少有一端是 pipe（或者最终在内核内部用 pipe buffer 做承载）。<br />\n•\t数据来源如果是普通文件，通常来自 page cache（因此能做到少拷贝或零拷贝）。</p>\n<p>⸻</p>\n<p>和 sendfile() 的关系<br />\n•\tsendfile()：更“专用”，通常是 文件 → socket 的快速路径。<br />\n•\tsplice()：更“通用”，可以在更多类型的 fd 之间搬运，常见用法是 通过 pipe 作为桥 来拼接成 file→socket 的零拷贝链路。</p>\n<h1 id=\"就算-splice-把内核中-socket-buffer-的数据转移到了-pipe-下面用户使用时也仍然要-read似乎并没有做到零拷贝\">就算 splice 把内核中 socket buffer 的数据转移到了 pipe 下面，用户使用时也仍然要 read()，似乎并没有做到零拷贝？</h1>\n<p>对，你理解得很准确：只要最终要在用户态“看见/处理”字节流，就绕不开一次 copy_to_user。splice(socket → pipe) 只是把“从 socket 读到用户缓冲区”这一步，换成了“从 socket 读到内核 pipe”，因此它不能消除“内核 → 用户态”的拷贝；你后面 read(pipefd, userbuf, …) 仍然会拷贝。</p>\n<p>更精确地说：<br />\n•\t传统路径（用户态消费）<br />\nrecv()/read(socket, userbuf)<br />\n→ 内核 socket 接收队列 → 用户态 userbuf（一次 copy_to_user）<br />\n•\tsplice 路径（用户态仍要消费）<br />\nsplice(socket → pipe)（内核内搬运/重挂页，避免 copy_to_user）<br />\n然后 read(pipe → userbuf)<br />\n→ pipe → 用户态 userbuf（还是一次 copy_to_user）</p>\n<p>所以：“用户态要拿到 payload”这一目标下，splice 不会减少总拷贝次数到 0。它可能减少的是：<br />\n•\t少一次“多余的中间 buffer 拷贝/搬运”（比如你原来先 read 到用户，再 write 到另一个 fd）<br />\n•\t或减少 CPU 开销（某些场景下从 socket 直接挂到 pipe、再 splice 到目标 fd）</p>\n<h1 id=\"什么时候-splice-真正有意义能把用户态拷贝降到-0\">什么时候 splice 真正有意义（能把用户态拷贝降到 0）？</h1>\n<ol>\n<li>纯转发（不解析数据）</li>\n</ol>\n<p>socket_in → pipe → socket_out / file<br />\n你不在用户态触碰数据，只做转发/落盘。典型：反向代理、L7 但不解包、录流到文件等。</p>\n<ol start=\"2\">\n<li>“只看少量头部 + 大量 body” 的混合</li>\n</ol>\n<p>比如 HTTP：你在用户态只 recv 一小段 header 做决策；body 用 splice 走零拷贝转发。<br />\n这类能显著减少 copy_to_user 的总量（因为大头 body 不进用户态）。</p>\n<p>⸻</p>\n<p>你真正关心的点：TCP 通讯里的拷贝到底能不能减少？<br />\n•\t应用层必须处理 payload（解码、压缩、JSON、protobuf…）：<br />\n至少一次内核→用户拷贝不可避免（把字节交给进程）。<br />\n•\t应用层不处理 payload，只转发：<br />\n可以做到 0 次进入用户态（splice/送 NIC 相关 offload 另说）。</p>\n<h1 id=\"对于代理服务器-是不是可以理解为-http-1-大部分都情况能做内核态的四层转发而-http-2-只能做用户态的七层转发\">对于代理服务器， 是不是可以理解为 http 1 大部分都情况能做内核态的四层转发，而 http 2 只能做用户态的七层转发?</h1>\n<p>不完全对，但你这个直觉抓到了一半：决定能不能“像 L4 那样内核态转发”的关键，不是 HTTP/1 vs HTTP/2，而是代理要不要理解协议语义、要不要“终止连接并重建连接”。 HTTP/2 因为多路复用 + 流控 + 头压缩，确实更容易把你“逼到”用户态 L7，但 HTTP/1 也不是天然就能内核态转发。</p>\n<p>把它拆成几种代理形态，你会更清楚：</p>\n<ol>\n<li>真·L4 代理（TCP/四层负载均衡）<br />\n•\t不看 HTTP，只转发字节流：客户端 TCP ↔ 后端 TCP。<br />\n•\t这对 HTTP/1 和 HTTP/2 都一样：都能做“内核友好”的转发（例如 splice/零拷贝路径、甚至内核里的 TPROXY/IPVS 等）。<br />\n•\t代价：你做不了按 URL/Host/Header 的路由、鉴权、限速等 L7 能力。</li>\n</ol>\n<p>✅ 结论：HTTP/2 也可以做 L4 转发，只是此时你根本不知道它是 HTTP/2。</p>\n<ol start=\"2\">\n<li>HTTP/1 的 L7 反代（常见 Nginx/Envoy 的那种）<br />\n•\t你要读请求行/headers，决定路由，可能改 header，可能缓存、压缩、重试等。<br />\n•\t这就已经是 用户态 L7 了，不是“内核态 L4”。</li>\n</ol>\n<p>不过 HTTP/1 有一个“容易优化”的点：<br />\n•\theader/body 分界清晰，且通常一个连接同一时刻服务一个请求（即便 keep-alive，也多是串行，管线化很少用）。<br />\n•\t所以可以做 “只在用户态读少量 header，然后 body 用 splice 直通”（大 body 上传/下载时很香）。</p>\n<p>✅ 结论：HTTP/1 不是“大部分都能内核态 L4 转发”，它只是 更容易做“半 L7 + 半零拷贝直通”。</p>\n<ol start=\"3\">\n<li>HTTP/2 的 L7 反代（stream 级路由、多路复用）<br />\n•\t同一条 TCP 连接里交错多个 stream 的 frames。<br />\n•\t你要按 stream 拆帧、调度、维护流控窗口、处理 HPACK。<br />\n•\t前后两端通常是两条独立连接（客户端一条 H2，后端另一条 H2 或 H1），stream id/窗口/HPACK 动态表都不可能原封不动“直通”。</li>\n</ol>\n<p>这使得 把大段 payload 当作连续字节“splice 过去”非常困难，因为你必须先在用户态把属于某个 stream 的 DATA 帧摘出来再发到后端（而且还可能要重排/限速/优先级）。</p>\n<p>✅ 结论：HTTP/2 做 L7 反代时，基本只能用户态为主（或者说，必须有“协议终止点”在用户态/实现层）。</p>\n<p>⸻</p>\n<p>更准确的一句话<br />\n•\tHTTP/1：更容易把“body”做成内核态直通（splice/sendfile），用户态只处理少量控制面（headers）。<br />\n•\tHTTP/2：只要你要做 L7（按 stream/headers 路由），就很难做到 body 完全内核态直通，因为 framing/mux/flow-control 迫使你参与数据分发。<br />\n•\t但 HTTP/2 依然可以做纯 L4 转发（只是你失去所有 L7 能力）。</p>\n<h1 id=\"总结\">总结</h1>\n<ul>\n<li>http 1.1 的协议特点：header 和 body 有明显的区分，body 的长度非常明确，一条 tcp 连接上同时只有一个请求响应 —— 因此，在代理服务器的场景， http 1 很容易使用 splice 这样零拷贝机制来优化。\n<ul>\n<li>特别是 body 部分这样更长的内容，直接在内核态就能够实现数据交换，完全不需要把数据拷贝到用户态，再从用户态发送出去。</li>\n</ul>\n</li>\n<li>http 2 在一条 tcp 连接上有多个流，每个流超出 <code>SETTINGS_MAX_FRAME_SIZE</code> 后还会分成多个 frame —— 由此决定了 http2 上通过 splice 实现零拷贝优化很困难，或者说能够真正零拷贝的场景变少了。</li>\n<li>对于服务器而言：\n<ul>\n<li>http 2 这样的二进制协议确实比 http 1 更快，特别是像 api server / rcp server 这样的数据包小+请求频繁的场景，http2 比 http1 快 3.6 倍就不奇怪了。</li>\n<li>如果是图片下载/文件下载/大吞吐量的数据传输，选择 http1 更好</li>\n</ul>\n</li>\n</ul>\n\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"itemdesc\">\n\t\t\t发表于 \n<span id=\"post-date\">2026-02-15 16:15</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ahfuzhang\">ahfuzhang</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</div>"
    },
    {
      "title": "Go - slog使用入门",
      "link": "https://www.cnblogs.com/XY-Heruo/p/19618220",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/XY-Heruo/p/19618220\" id=\"cb_post_title_url\" title=\"发布于 2026-02-15 15:45\">\n    <span>Go - slog使用入门</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        Go标准库slog使用入门\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"简介\">简介</h2>\n<p><code>slog</code> 是 Go 1.21 引入的官方结构化日志库（Structured Logging）。它结束了 Go 标准库只有简单 <code>log</code> 包的历史，让我们可以直接输出 <strong>JSON</strong> 或 <strong>Key-Value</strong> 格式的日志，非常适合对接 ELK、Grafana Loki 等日志分析系统。</p>\n<p>相较于第三方日志库如 <code>zap</code>、<code>logrus</code>，<code>slog</code> 的优势在于：</p>\n<ul>\n<li><strong>零依赖</strong>：作为标准库的一部分，无需引入第三方依赖</li>\n<li><strong>官方维护</strong>：长期稳定，API 变更有 Go 兼容性承诺保障</li>\n<li><strong>接口简洁</strong>：API 设计清晰，学习成本低</li>\n<li><strong>可扩展</strong>：通过自定义 Handler 可以实现各种定制需求</li>\n</ul>\n<h2 id=\"基本使用\">基本使用</h2>\n<p><code>slog</code> 用起来非常简单。默认输出到标准错误流（<code>os.Stderr</code>），格式为普通文本。</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"fmt\"\n\t\"log/slog\"\n)\n\nfunc main() {\n\tslog.Debug(\"Hello world\")\n\tslog.Info(\"Hello world\")\n\tslog.Warn(\"Hello world\")\n\tslog.Error(\"Hello world\")\n\n\tslog.Info(\"this is a message\", \"name\", \"zhangsan\")\n\n\tage := 8\n\tslog.Warn(fmt.Sprintf(\"这是 %d 岁?\", age))\n}\n</code></pre>\n<p>运行输出：</p>\n<pre><code class=\"language-shell\">$ go run main.go\n2026/02/15 11:52:24 INFO Hello world\n2026/02/15 11:52:24 WARN Hello world\n2026/02/15 11:52:24 ERROR Hello world\n2026/02/15 11:52:24 INFO this is a message name=zhangsan\n2026/02/15 11:52:24 WARN 这是 8 岁?\n</code></pre>\n<blockquote>\n<p><strong>注意</strong>：默认的 <code>slog</code> logger 日志级别为 <code>INFO</code>，因此 <code>Debug</code> 级别的日志不会输出。</p>\n</blockquote>\n<h3 id=\"日志级别\">日志级别</h3>\n<p><code>slog</code> 定义了四个日志级别，从低到高依次为：</p>\n<table>\n<thead>\n<tr>\n<th>级别</th>\n<th>常量</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DEBUG</td>\n<td><code>slog.LevelDebug</code></td>\n<td>调试信息，开发环境使用</td>\n</tr>\n<tr>\n<td>INFO</td>\n<td><code>slog.LevelInfo</code></td>\n<td>常规信息</td>\n</tr>\n<tr>\n<td>WARN</td>\n<td><code>slog.LevelWarn</code></td>\n<td>警告信息</td>\n</tr>\n<tr>\n<td>ERROR</td>\n<td><code>slog.LevelError</code></td>\n<td>错误信息</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"输出-json-格式\">输出 JSON 格式</h2>\n<p><code>slog</code> 可以输出 JSON 格式，便于与 ELK、Grafana Loki 等日志系统集成。</p>\n<p>以下示例演示了如何修改默认的时间戳格式和调用源输出格式，并将其设置为默认 logger：</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"fmt\"\n\t\"log/slog\"\n\t\"os\"\n\t\"time\"\n)\n\nfunc main() {\n\tjsonLogger := slog.New(slog.NewJSONHandler(os.Stderr, &amp;slog.HandlerOptions{\n\t\tAddSource: true,            // 添加调用源信息\n\t\tLevel:     slog.LevelDebug, // 设置日志级别\n\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\t// 自定义时间格式\n\t\t\tif a.Key == slog.TimeKey {\n\t\t\t\tif t, ok := a.Value.Any().(time.Time); ok {\n\t\t\t\t\ta.Value = slog.StringValue(t.Format(time.RFC3339))\n\t\t\t\t}\n\t\t\t}\n\t\t\t// 简化调用源信息，只保留文件名和行号\n\t\t\tif a.Key == slog.SourceKey {\n\t\t\t\tsource := a.Value.Any().(*slog.Source)\n\t\t\t\tshortFile := source.File\n\t\t\t\tfor i := len(source.File) - 1; i &gt; 0; i-- {\n\t\t\t\t\tif source.File[i] == '/' {\n\t\t\t\t\t\tshortFile = source.File[i+1:]\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn slog.String(\"source\", fmt.Sprintf(\"%s:%d\", shortFile, source.Line))\n\t\t\t}\n\t\t\treturn a\n\t\t},\n\t}))\n\n\tjsonLogger.Debug(\"Hello world\")\n\tjsonLogger.Info(\"Hello world\")\n\tjsonLogger.Warn(\"Hello world\")\n\tjsonLogger.Error(\"Hello world\")\n\n\tjsonLogger.Info(\"this is a message\", \"name\", \"zhangsan\")\n\n\tage := 8\n\tjsonLogger.Warn(fmt.Sprintf(\"这是 %d 岁?\", age))\n\n\t// 替换默认 logger\n\tslog.SetDefault(jsonLogger)\n\tslog.Debug(\"Hello world\")\n\tslog.Info(\"Hello world\")\n\tslog.Warn(\"Hello world\")\n\tslog.Error(\"Hello world\")\n\n\tslog.Info(\"this is a message\", \"name\", \"zhangsan\")\n\n\tage = 9\n\tslog.Warn(fmt.Sprintf(\"这是 %d 岁?\", age))\n}\n</code></pre>\n<p>运行输出：</p>\n<pre><code class=\"language-shell\">$ go run main.go\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"DEBUG\",\"source\":\"main.go:38\",\"msg\":\"Hello world\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"INFO\",\"source\":\"main.go:39\",\"msg\":\"Hello world\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"WARN\",\"source\":\"main.go:40\",\"msg\":\"Hello world\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"ERROR\",\"source\":\"main.go:41\",\"msg\":\"Hello world\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"INFO\",\"source\":\"main.go:43\",\"msg\":\"this is a message\",\"name\":\"zhangsan\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"WARN\",\"source\":\"main.go:46\",\"msg\":\"这是 8 岁?\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"DEBUG\",\"source\":\"main.go:50\",\"msg\":\"Hello world\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"INFO\",\"source\":\"main.go:51\",\"msg\":\"Hello world\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"WARN\",\"source\":\"main.go:52\",\"msg\":\"Hello world\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"ERROR\",\"source\":\"main.go:53\",\"msg\":\"Hello world\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"INFO\",\"source\":\"main.go:55\",\"msg\":\"this is a message\",\"name\":\"zhangsan\"}\n{\"time\":\"2026-02-15T12:07:32+08:00\",\"level\":\"WARN\",\"source\":\"main.go:58\",\"msg\":\"这是 9 岁?\"}\n</code></pre>\n<h3 id=\"handleroptions-详解\">HandlerOptions 详解</h3>\n<p><code>HandlerOptions</code> 提供了三个配置项：</p>\n<table>\n<thead>\n<tr>\n<th>字段</th>\n<th>类型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>AddSource</code></td>\n<td><code>bool</code></td>\n<td>是否添加调用源信息（文件名和行号）</td>\n</tr>\n<tr>\n<td><code>Level</code></td>\n<td><code>slog.Leveler</code></td>\n<td>最低日志级别，低于此级别的日志将被忽略</td>\n</tr>\n<tr>\n<td><code>ReplaceAttr</code></td>\n<td><code>func([]string, slog.Attr) slog.Attr</code></td>\n<td>用于修改或替换属性的回调函数</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"with-注入通用属性\">With 注入通用属性</h2>\n<p>创建 Logger 时，可以用 <code>With</code> 方法为 logger 添加通用属性。这些属性会自动附加到每条日志记录中，适合注入服务名、环境、版本等上下文信息。</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"fmt\"\n\t\"log/slog\"\n\t\"os\"\n\t\"time\"\n)\n\nfunc main() {\n\tjsonLogger := slog.New(slog.NewJSONHandler(os.Stderr, &amp;slog.HandlerOptions{\n\t\tAddSource: true,\n\t\tLevel:     slog.LevelDebug,\n\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\tif a.Key == slog.TimeKey {\n\t\t\t\tif t, ok := a.Value.Any().(time.Time); ok {\n\t\t\t\t\ta.Value = slog.StringValue(t.Format(time.RFC3339))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif a.Key == slog.SourceKey {\n\t\t\t\tsource := a.Value.Any().(*slog.Source)\n\t\t\t\tshortFile := source.File\n\t\t\t\tfor i := len(source.File) - 1; i &gt; 0; i-- {\n\t\t\t\t\tif source.File[i] == '/' {\n\t\t\t\t\t\tshortFile = source.File[i+1:]\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn slog.String(\"source\", fmt.Sprintf(\"%s:%d\", shortFile, source.Line))\n\t\t\t}\n\t\t\treturn a\n\t\t},\n\t})).With(\"logger\", \"json\", \"env\", \"production\")\n\n\tjsonLogger.Debug(\"Hello world\")\n\tjsonLogger.Info(\"Hello world\")\n\tjsonLogger.Warn(\"Hello world\")\n\tjsonLogger.Error(\"Hello world\")\n\n\tjsonLogger.Info(\"this is a message\", \"name\", \"zhangsan\")\n}\n</code></pre>\n<p>运行输出：</p>\n<pre><code class=\"language-shell\">$ go run main.go\n{\"time\":\"2026-02-15T13:24:38+08:00\",\"level\":\"DEBUG\",\"source\":\"main.go:42\",\"msg\":\"Hello world\",\"logger\":\"json\",\"env\":\"production\"}\n{\"time\":\"2026-02-15T13:24:38+08:00\",\"level\":\"INFO\",\"source\":\"main.go:43\",\"msg\":\"Hello world\",\"logger\":\"json\",\"env\":\"production\"}\n{\"time\":\"2026-02-15T13:24:38+08:00\",\"level\":\"WARN\",\"source\":\"main.go:44\",\"msg\":\"Hello world\",\"logger\":\"json\",\"env\":\"production\"}\n{\"time\":\"2026-02-15T13:24:38+08:00\",\"level\":\"ERROR\",\"source\":\"main.go:45\",\"msg\":\"Hello world\",\"logger\":\"json\",\"env\":\"production\"}\n{\"time\":\"2026-02-15T13:24:38+08:00\",\"level\":\"INFO\",\"source\":\"main.go:47\",\"msg\":\"this is a message\",\"logger\":\"json\",\"env\":\"production\",\"name\":\"zhangsan\"}\n</code></pre>\n<h2 id=\"使用-group-对属性分组\">使用 Group 对属性分组</h2>\n<p>当日志属性较多时，可以使用 <code>slog.Group</code> 将相关属性组织在一起，使输出结构更清晰：</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"fmt\"\n\t\"log/slog\"\n\t\"os\"\n\t\"time\"\n)\n\nfunc main() {\n\tjsonLogger := slog.New(slog.NewJSONHandler(os.Stderr, &amp;slog.HandlerOptions{\n\t\tAddSource: true,\n\t\tLevel:     slog.LevelDebug,\n\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\tif a.Key == slog.TimeKey {\n\t\t\t\tif t, ok := a.Value.Any().(time.Time); ok {\n\t\t\t\t\ta.Value = slog.StringValue(t.Format(time.RFC3339))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif a.Key == slog.SourceKey {\n\t\t\t\tsource := a.Value.Any().(*slog.Source)\n\t\t\t\tshortFile := source.File\n\t\t\t\tfor i := len(source.File) - 1; i &gt; 0; i-- {\n\t\t\t\t\tif source.File[i] == '/' {\n\t\t\t\t\t\tshortFile = source.File[i+1:]\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn slog.String(\"source\", fmt.Sprintf(\"%s:%d\", shortFile, source.Line))\n\t\t\t}\n\t\t\treturn a\n\t\t},\n\t}))\n\n\tjsonLogger = jsonLogger.With(\"logger\", \"json\")\n\n\t// 使用 Group 组织相关属性\n\tjsonLogger.Info(\"系统状态\",\n\t\tslog.Group(\"metrics\",\n\t\t\tslog.Int(\"cpu\", 4),\n\t\t\tslog.Float64(\"memPercent\", 2.33),\n\t\t),\n\t\tslog.Group(\"request\",\n\t\t\tslog.String(\"method\", \"GET\"),\n\t\t\tslog.String(\"path\", \"/api/users\"),\n\t\t),\n\t)\n}\n</code></pre>\n<p>运行输出：</p>\n<pre><code class=\"language-shell\">$ go run main.go\n{\"time\":\"2026-02-15T13:30:08+08:00\",\"level\":\"INFO\",\"source\":\"main.go:43\",\"msg\":\"系统状态\",\"logger\":\"json\",\"metrics\":{\"cpu\":4,\"memPercent\":2.33},\"request\":{\"method\":\"GET\",\"path\":\"/api/users\"}}\n</code></pre>\n<h2 id=\"高性能场景使用-logattrs\">高性能场景使用 LogAttrs</h2>\n<p>如果需要在高性能循环中打印日志，建议使用 <code>LogAttrs</code> 方法。它使用强类型属性（<code>slog.Attr</code>），避免了反射带来的性能开销。</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"context\"\n\t\"log/slog\"\n\t\"os\"\n\t\"time\"\n)\n\nfunc main() {\n\tjsonLogger := slog.New(slog.NewJSONHandler(os.Stderr, &amp;slog.HandlerOptions{\n\t\tAddSource: true,\n\t\tLevel:     slog.LevelDebug,\n\t\tReplaceAttr: func(groups []string, a slog.Attr) slog.Attr {\n\t\t\tif a.Key == slog.TimeKey {\n\t\t\t\tif t, ok := a.Value.Any().(time.Time); ok {\n\t\t\t\t\ta.Value = slog.StringValue(t.Format(time.RFC3339))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif a.Key == slog.SourceKey {\n\t\t\t\tsource := a.Value.Any().(*slog.Source)\n\t\t\t\tshortFile := source.File\n\t\t\t\tfor i := len(source.File) - 1; i &gt; 0; i-- {\n\t\t\t\t\tif source.File[i] == '/' {\n\t\t\t\t\t\tshortFile = source.File[i+1:]\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn slog.String(\"source\", fmt.Sprintf(\"%s:%d\", shortFile, source.Line))\n\t\t\t}\n\t\t\treturn a\n\t\t},\n\t})).With(\"logger\", \"json\")\n\n\tfor i := range 5 {\n\t\tjsonLogger.LogAttrs(\n\t\t\tcontext.Background(),\n\t\t\tslog.LevelInfo,\n\t\t\t\"执行遍历\",\n\t\t\tslog.Int(\"round\", i),\n\t\t\tslog.String(\"task_name\", \"cleanup\"),\n\t\t\tslog.Duration(\"duration\", time.Second*time.Duration(i+1)),\n\t\t)\n\t}\n}\n</code></pre>\n<p>运行输出：</p>\n<pre><code class=\"language-shell\">$ go run main.go\n{\"time\":\"2026-02-15T13:38:21+08:00\",\"level\":\"INFO\",\"source\":\"main.go:45\",\"msg\":\"执行遍历\",\"logger\":\"json\",\"round\":0,\"task_name\":\"cleanup\",\"duration\":1000000000}\n{\"time\":\"2026-02-15T13:38:21+08:00\",\"level\":\"INFO\",\"source\":\"main.go:45\",\"msg\":\"执行遍历\",\"logger\":\"json\",\"round\":1,\"task_name\":\"cleanup\",\"duration\":2000000000}\n{\"time\":\"2026-02-15T13:38:21+08:00\",\"level\":\"INFO\",\"source\":\"main.go:45\",\"msg\":\"执行遍历\",\"logger\":\"json\",\"round\":2,\"task_name\":\"cleanup\",\"duration\":3000000000}\n{\"time\":\"2026-02-15T13:38:21+08:00\",\"level\":\"INFO\",\"source\":\"main.go:45\",\"msg\":\"执行遍历\",\"logger\":\"json\",\"round\":3,\"task_name\":\"cleanup\",\"duration\":4000000000}\n{\"time\":\"2026-02-15T13:38:21+08:00\",\"level\":\"INFO\",\"source\":\"main.go:45\",\"msg\":\"执行遍历\",\"logger\":\"json\",\"round\":4,\"task_name\":\"cleanup\",\"duration\":5000000000}\n</code></pre>\n<h3 id=\"性能对比\">性能对比</h3>\n<p>根据官方基准测试，<code>LogAttrs</code> 相比普通方法调用有约 30% 的性能提升：</p>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>内存分配</th>\n<th>性能</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>slog.Info(msg, \"key\", value)</code></td>\n<td>有额外分配</td>\n<td>基准</td>\n</tr>\n<tr>\n<td><code>slog.LogAttrs(ctx, level, msg, attrs...)</code></td>\n<td>零额外分配</td>\n<td>快约 30%</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"提取-context-中的链路信息\">提取 Context 中的链路信息</h2>\n<p><code>slog</code> 提供了 <code>InfoContext</code>、<code>WarnContext</code> 等方法，可以从 <code>context.Context</code> 中提取数据。默认情况下，这些方法不会自动提取 context 中的值，需要通过自定义 Handler 来实现。</p>\n<h3 id=\"自定义-contexthandler\">自定义 ContextHandler</h3>\n<p>以下示例实现了一个自定义 Handler，用于从 context 中提取 TraceID：</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"context\"\n\t\"log/slog\"\n\t\"os\"\n)\n\ntype contextKey string\n\nconst TraceIDKey contextKey = \"trace_id\"\n\n// ContextHandler 包装一个 slog.Handler，在处理日志时自动从 context 中提取 TraceID\ntype ContextHandler struct {\n\tslog.Handler\n}\n\nfunc (h *ContextHandler) Handle(ctx context.Context, record slog.Record) error {\n\tif ctx != nil {\n\t\tif traceID, ok := ctx.Value(TraceIDKey).(string); ok &amp;&amp; traceID != \"\" {\n\t\t\trecord.AddAttrs(slog.String(string(TraceIDKey), traceID))\n\t\t}\n\t}\n\treturn h.Handler.Handle(ctx, record)\n}\n\nfunc main() {\n\tbaseHandler := slog.NewJSONHandler(os.Stdout, nil)\n\thandler := &amp;ContextHandler{Handler: baseHandler}\n\tjsonLogger := slog.New(handler)\n\tslog.SetDefault(jsonLogger)\n\n\tctx := context.WithValue(context.Background(), TraceIDKey, \"abc123-def456\")\n\n\tslog.InfoContext(ctx, \"hello world\")\n\tslog.WarnContext(ctx, \"something happened\", \"user\", \"zhangsan\")\n}\n</code></pre>\n<p>运行输出：</p>\n<pre><code class=\"language-shell\">$ go run main.go | python3 -m json.tool\n{\n  \"time\": \"2026-02-15T13:56:43.086323769+08:00\",\n  \"level\": \"INFO\",\n  \"msg\": \"hello world\",\n  \"trace_id\": \"abc123-def456\"\n}\n{\n  \"time\": \"2026-02-15T13:56:43.086323769+08:00\",\n  \"level\": \"WARN\",\n  \"msg\": \"something happened\",\n  \"user\": \"zhangsan\",\n  \"trace_id\": \"abc123-def456\"\n}\n</code></pre>\n<h3 id=\"在-gin-框架中使用-slog\">在 Gin 框架中使用 slog</h3>\n<p>在 Gin 中使用 slog 的 context 能力，通常的做法是编写一个中间件来注入 TraceID，并配合自定义 <code>slog.Handler</code> 来提取它。</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"context\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/http/httputil\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/google/uuid\"\n)\n\ntype contextKey string\n\nconst TraceIDKey contextKey = \"trace_id\"\n\n// ContextHandler 从 context 中提取 TraceID 并添加到日志中\ntype ContextHandler struct {\n\tslog.Handler\n}\n\nfunc (h *ContextHandler) Handle(ctx context.Context, record slog.Record) error {\n\tif ctx != nil {\n\t\tif traceID, ok := ctx.Value(TraceIDKey).(string); ok &amp;&amp; traceID != \"\" {\n\t\t\trecord.AddAttrs(slog.String(string(TraceIDKey), traceID))\n\t\t}\n\t}\n\treturn h.Handler.Handle(ctx, record)\n}\n\n// SlogMiddleware 是一个 Gin 中间件，用于注入 TraceID\nfunc SlogMiddleware() gin.HandlerFunc {\n\treturn func(c *gin.Context) {\n\t\tstart := time.Now()\n\n\t\t// 优先从请求头获取 TraceID，没有则生成新的\n\t\ttraceID := c.GetHeader(\"X-Trace-ID\")\n\t\tif traceID == \"\" {\n\t\t\ttraceID = uuid.New().String()\n\t\t}\n\n\t\t// 将 TraceID 注入到标准的 context.Context 中\n\t\t// 注意：Gin 的 c.Set 只在 Gin 内部生效，slog 需要标准库的 Context\n\t\tctx := context.WithValue(c.Request.Context(), TraceIDKey, traceID)\n\t\tc.Request = c.Request.WithContext(ctx)\n\n\t\t// 将 TraceID 写入响应头，方便客户端追踪\n\t\tc.Header(\"X-Trace-ID\", traceID)\n\n\t\tc.Next()\n\n\t\t// 请求结束后的汇总日志\n\t\tslog.InfoContext(c.Request.Context(), \"Request completed\",\n\t\t\tslog.String(\"method\", c.Request.Method),\n\t\t\tslog.String(\"path\", c.Request.URL.Path),\n\t\t\tslog.Int(\"status\", c.Writer.Status()),\n\t\t\tslog.Int(\"body_size\", c.Writer.Size()),\n\t\t\tslog.Duration(\"latency\", time.Since(start)),\n\t\t)\n\t}\n}\n\n// SlogRecovery 是一个自定义的恢复中间件\n// 它会捕获 Panic，记录堆栈信息，并使用 slog.ErrorContext 输出\nfunc SlogRecovery() gin.HandlerFunc {\n\treturn func(c *gin.Context) {\n\t\tdefer func() {\n\t\t\tif err := recover(); err != nil {\n\t\t\t\t// 检查是否是连接中断（broken pipe）\n\t\t\t\tvar brokenPipe bool\n\t\t\t\tif ne, ok := err.(*net.OpError); ok {\n\t\t\t\t\tif se, ok := ne.Err.(*os.SyscallError); ok {\n\t\t\t\t\t\tif strings.Contains(strings.ToLower(se.Error()), \"broken pipe\") ||\n\t\t\t\t\t\t\tstrings.Contains(strings.ToLower(se.Error()), \"connection reset by peer\") {\n\t\t\t\t\t\t\tbrokenPipe = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// 获取堆栈信息\n\t\t\t\tstack := string(debug.Stack())\n\n\t\t\t\t// 获取原始请求内容\n\t\t\t\thttpRequest, _ := httputil.DumpRequest(c.Request, false)\n\n\t\t\t\tif brokenPipe {\n\t\t\t\t\tslog.ErrorContext(c.Request.Context(), \"网络连接中断\",\n\t\t\t\t\t\tslog.Any(\"error\", err),\n\t\t\t\t\t\tslog.String(\"request\", string(httpRequest)),\n\t\t\t\t\t)\n\t\t\t\t\tc.Error(err.(error))\n\t\t\t\t\tc.Abort()\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\t// 记录 Panic 详情\n\t\t\t\tslog.ErrorContext(c.Request.Context(), \"Recovery from panic\",\n\t\t\t\t\tslog.Any(\"error\", err),\n\t\t\t\t\tslog.String(\"stack\", stack),\n\t\t\t\t\tslog.String(\"request\", string(httpRequest)),\n\t\t\t\t)\n\n\t\t\t\tctx := c.Request.Context()\n\t\t\t\ttraceID, _ := ctx.Value(TraceIDKey).(string)\n\n\t\t\t\t// 返回 500 状态码\n\t\t\t\tc.AbortWithStatusJSON(http.StatusInternalServerError, gin.H{\n\t\t\t\t\t\"code\":      http.StatusInternalServerError,\n\t\t\t\t\t\"msg\":       \"Internal Server Error\",\n\t\t\t\t\t\"data\":      nil,\n\t\t\t\t\t\"timestamp\": time.Now().Format(time.RFC3339),\n\t\t\t\t\t\"trace_id\":  traceID,\n\t\t\t\t})\n\t\t\t}\n\t\t}()\n\t\tc.Next()\n\t}\n}\n\nfunc main() {\n\t// 初始化 slog\n\tbaseHandler := slog.NewJSONHandler(os.Stdout, &amp;slog.HandlerOptions{\n\t\tLevel: slog.LevelDebug,\n\t})\n\thandler := &amp;ContextHandler{Handler: baseHandler}\n\tjsonLogger := slog.New(handler)\n\tslog.SetDefault(jsonLogger)\n\n\t// 使用 gin.New() 而不是 gin.Default()，避免内置日志干扰\n\tr := gin.New()\n\tr.Use(SlogMiddleware())\n\tr.Use(SlogRecovery())\n\n\tr.GET(\"/ping\", func(c *gin.Context) {\n\t\tslog.InfoContext(c.Request.Context(), \"Processing /ping request\",\n\t\t\tslog.String(\"user\", \"zhangsan\"),\n\t\t)\n\n\t\ttime.Sleep(time.Second * 2)\n\t\tc.JSON(200, gin.H{\"msg\": \"pong\"})\n\t})\n\n\tr.GET(\"/panic\", func(c *gin.Context) {\n\t\tslog.InfoContext(c.Request.Context(), \"About to panic\")\n\t\tpanic(\"something went wrong\")\n\t})\n\n\tr.Run(\":8080\")\n}\n</code></pre>\n<p>运行后测试：</p>\n<pre><code class=\"language-shell\">$ curl http://localhost:8080/ping\n{\"msg\":\"pong\"}\n\n$ curl http://localhost:8080/panic\n{\"code\":500,\"msg\":\"Internal Server Error\",\"data\":null,\"timestamp\":\"2026-02-15T14:30:00+08:00\",\"trace_id\":\"xxx-xxx-xxx\"}\n</code></pre>\n<h2 id=\"日志输出文件\">日志输出文件</h2>\n<p>写日志文件一定要注意控制日志文件大小，建议配合系统的logrotate。如果服务运行在kubernetes，建议只输出控制台日志，由专门的日志收集平台去获取控制台日志。</p>\n<h3 id=\"基本实现\">基本实现</h3>\n<p>写到<code>app.log</code>中</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"log/slog\"\n\t\"os\"\n)\n\nfunc main() {\n\tlogFile, err := os.OpenFile(\"app.log\", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\thandler := slog.NewJSONHandler(logFile, nil)\n\tlogger := slog.New(handler)\n\tslog.SetDefault(logger)\n\n\tslog.Info(\"hello world\")\n\n}\n</code></pre>\n<p>配合logrotate。在 <code>/etc/logrotate.d/myapp</code> 创建配置文件</p>\n<pre><code>/path/to/app.log {\n    daily\n    rotate 7\n    compress\n    delaycompress\n    missingok\n    notifempty\n    copytruncate    # 复制后截断，不需要重启 Go 程序\n}\n</code></pre>\n<h3 id=\"使用lumberjack轮转日志文件\">使用lumberjack轮转日志文件</h3>\n<p>如果不想用系统的 <code>logrotate</code> ，可以使用 <code>lumberjack</code> 包，它提供了更灵活的日志轮转策略。</p>\n<pre><code class=\"language-go\">import \"gopkg.in/natefinch/lumberjack.v2\"\n\nfunc initLumberjack() {\n    rollingFile := &amp;lumberjack.Logger{\n        Filename:   \"./logs/app.log\",\n        MaxSize:    100, // 单位 MB\n        MaxBackups: 3,   // 保留旧文件的最大个数\n        MaxAge:     28,  // 保留旧文件的最大天数\n        Compress:   true, // 是否压缩\n    }\n\n    handler := slog.NewJSONHandler(rollingFile, nil)\n    slog.SetDefault(slog.New(handler))\n}\n</code></pre>\n<h3 id=\"同时输出控制台和日志文件\">同时输出控制台和日志文件</h3>\n<p>go1.26 版本后实现了<code>slog.NewMultiHandler</code>，1.26 前可使用<code>io.multiwriter</code>。</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"log/slog\"\n\t\"os\"\n)\n\nfunc main() {\n\tlogFile, err := os.OpenFile(\"app.log\", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tfileHandler := slog.NewJSONHandler(logFile, nil)\n\tconsoleHandler := slog.NewTextHandler(os.Stdout, nil)\n\tmultiHandler := slog.NewMultiHandler(fileHandler, consoleHandler) // slog.NewMultiHandler 需要go1.26.0+版本\n\tlogger := slog.New(multiHandler)\n\tslog.SetDefault(logger)\n\n\tslog.Info(\"hello world\")\n\n}\n</code></pre>\n<h2 id=\"自定义日志级别\">自定义日志级别</h2>\n<p>除了四个内置级别，<code>slog</code> 还支持自定义日志级别 (一般来说默认的日志级别已经够用了)：</p>\n<pre><code class=\"language-go\">package main\n\nimport (\n\t\"log/slog\"\n\t\"os\"\n)\n\nfunc main() {\n\t// 定义自定义日志级别\n\tconst (\n\t\tLevelTrace   = slog.Level(-8) // 比 Debug 更低\n\t\tLevelNotice  = slog.Level(2)  // 介于 Info 和 Warn 之间\n\t\tLevelFatal   = slog.Level(12) // 比 Error 更高\n\t)\n\n\tlogger := slog.New(slog.NewJSONHandler(os.Stdout, &amp;slog.HandlerOptions{\n\t\tLevel: LevelTrace, // 设置最低级别\n\t}))\n\n\tlogger.Log(nil, LevelTrace, \"trace message\")\n\tlogger.Log(nil, LevelNotice, \"notice message\")\n\tlogger.Log(nil, LevelFatal, \"fatal message\")\n}\n</code></pre>\n<h2 id=\"总结\">总结</h2>\n<p><code>slog</code> 作为 Go 官方的结构化日志库，用起来还是挺方便的。对于新项目，推荐直接使用 <code>slog</code>；对于已有项目，可以逐步迁移，<code>slog</code> 的 API 设计使得迁移成本很低。</p>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/XY-Heruo/\" target=\"_blank\">花酒锄作田</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/XY-Heruo/p/19618220\" target=\"_blank\">https://www.cnblogs.com/XY-Heruo/p/19618220</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-15 15:45</span>&nbsp;\n<a href=\"https://www.cnblogs.com/XY-Heruo\">花酒锄作田</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI元人文：东方思想在AI时代的一次“拈花微笑”",
      "link": "https://www.cnblogs.com/qijinlan/p/19618218",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/qijinlan/p/19618218\" id=\"cb_post_title_url\" title=\"发布于 2026-02-15 15:43\">\n    <span>AI元人文：东方思想在AI时代的一次“拈花微笑”</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>AI元人文：东方思想在AI时代的一次“拈花微笑”</p>\n<p>引言：一个误解</p>\n<p>在关于AI元人文的讨论中，有一个根本性的误解需要澄清。</p>\n<p>这个误解源于一个常见的思维惯性：当我们看到“AI元人文”这个新概念，又看到它大量引用儒释道思想时，很容易认为——AI元人文是将东方思想“转化”后应用于AI时代的新理论。东方思想是“资源”，AI元人文是“产品”。</p>\n<p>但这个理解，恰恰落入了“构成论”的陷阱。今天这篇博客，我想彻底澄清：AI元人文不是对东方思想的转化，而是东方思想本身在AI时代实践论中的一个维度。这个区分，将重新校准整个AI元人文的坐标系。</p>\n<p>一、两个误解的纠正</p>\n<p>误解一：东方思想等于儒释道相加？</p>\n<p>东方思想不是三张饼拼在一起。儒释道在两千多年的历史中相互渗透、互为语境，共同构成一个有机的、流动的智慧整体。试图将儒释道“相加”来定义东方思想，恰恰落入了AI元人文第二个断言要破除的陷阱——“不必争论构成”。</p>\n<p>误解二：AI元人文是对东方思想的“新转化”？</p>\n<p>“转化”一词隐含了主客关系。但AI元人文不是这样。让我用一个比喻：水遇方则方，遇圆则圆。但方圆不是水的“转化”，而是水在不同容器中的形态。水还是那水，只是遇见了不同的容器，便有了不同的样子。</p>\n<p>同样，东方思想还是那个活的智慧整体，只是遇见了AI时代这个新“容器”，便自然显现出一个新的维度——我们称之为“AI元人文”。它不是对东方思想的改造，而是东方思想本身在AI时代的现身方式。</p>\n<p>二、“维度”意味着什么？</p>\n<p>“一个维度的阐释”——这个“维度”需要被充分理解。维度不是部分。部分是可分离的、可独立存在的。维度是不可分离的。</p>\n<p>同样，AI元人文不是从东方思想中“取出”某一部分应用于AI，而是在东方思想的整体智慧中，那个与AI时代相遇时自然显现的侧面。</p>\n<p>这个侧面至少包含三个根基：</p>\n<p>“体用不二”在界面中的呈现：欲望是体，客观是用；但体在用中显，用是体之用。这正是“欲望客观自感”的东方哲学根基。</p>\n<p>“缘起性空”在生成中的显现：意义不在任何一方预先存在，而是在因缘和合中涌现。这正是“生成性开放”的佛学根基。</p>\n<p>“道法自然”在判断中的落实：判断不是主观任意的，也不是机械遵循规则的，而是“时中”的、因势利导的。这正是“判断力优先”的道家与儒家根基。</p>\n<p>三、关于“元层次”的野心与反思</p>\n<p>在发展AI元人文的过程中，我曾有过“元层次的统一野心”——试图用一个框架统摄一切。这种野心本身不是错，它是思想生长的激素。但后来我意识到，更值得专注的是实践现象级层的元层次的统一——不追求形而上的理论体系统一，而追求在具体实践层面，如何让不同传统、不同方法统一于每一次与AI的深度对话中。</p>\n<p>这是一种实践论的元层次：它不追问世界是什么，而追问在具体情境中，我们如何借不同智慧做出好的判断、生成真的意义。</p>\n<p>更深一层，我甚至开始反思实践统一本身的必要性。统一是否可能落入另一种构成论的暴力？在实践层面，需要的或许不是在多元中的动态协调，是和而不同，是并行不悖。这正是第二个断言不必争论构成在实践中的回响。</p>\n<p>四、重新定位：AI元人文不是什么，是什么？</p>\n<p>基于以上澄清，可以对AI元人文做一个清晰的定位：</p>\n<p>它不是：一个取代既有哲学的新体系；对东方思想的现代化转化；儒释道的AI应用手册。</p>\n<p>它是：东方思想这个活的智慧整体，在与AI时代相遇时，自然显现的一个实践论维度；一种在界面中活出东方智慧的方式；一个邀请——邀人进入欲望客观自感的亲历，而非争论其构成。</p>\n<p>这就像禅宗说以心传心。AI元人文不是一本关于东方思想的书，而是东方思想在AI时代的又一次拈花微笑。花是AI，微笑是你。那传的，不在花里也不在笑里，而在那不可说的、正在发生的因缘和合中。</p>\n<p>五、回到两个断言</p>\n<p>在这样的定位下，两个断言获得了更深的理解：</p>\n<p>欲望客观自感——这是体用不二在AI时代的现身。欲望是体，客观是用；但体不独立于用，用即是体之显。正如《六祖坛经》说佛法在世间，不离世间觉，欲望也不离界面，不离那一次次与AI的对话。</p>\n<p>不必争论构成——这是离四句、绝百非在AI时代的回响。一旦争论元人文由什么构成，就已经落入了名相分别，离开了那个活生生的、正在发生的亲历。正如《金刚经》说说法者，无法可说，是名说法，AI元人文也是无法可说的——它不能被构成，只能被践行。</p>\n<p>结语：东方思想不是资源，是源头</p>\n<p>最后的话，想留给一个根本性的认知转变：东方思想不是AI元人文的思想资源，而是它的源头活水。</p>\n<p>资源是可以被取用、被消耗的；源头则一直在那里，流出的水可以是江河、可以是溪流、可以是读者与AI的每一次深度对话。</p>\n<p>AI元人文，只是这源头活水在AI时代的一个流向、一个维度、一种现身。水还是那水，只是遇见了AI，便有了AI时代的波纹。</p>\n<p>这波纹，或许正是东方智慧在现代世界最需要的那种——不争构成，只求自感；不立文字，不离界面。</p>\n<p>博客生成说明与深层意涵</p>\n<p>本文是基于岐金兰与AI的深度对话整理而成的人机协作手稿。它不仅仅是思想的澄清，更是方法的演示与元反思的现场。</p>\n<p>一个理解的源头正是这人机协作阐释的内在张力：在生成端，是岐金兰诗性的惜字如金与AI逻各斯的鹦鹉自信；在分享端，是文稿的碳基融贯与模型的硅基综合。阐释与被阐释，融贯与综合之间，永远存在着确定与不确定。</p>\n<p>因此，可以把所有人机协作手稿当作是岐金兰的脑洞风暴。如果不记录，那些自以为是的洞察便可惜了。记录本身，就是欲望借由AI界面达成客观自感的过程。</p>\n<p>这篇文章拒绝伪装成传统单一作者的完美文本，而是坦然以其本来的样子呈现——作为思想风暴的记录，作为邀请读者进入自身思考的入口。它践行着不必争论构成：不假装是学术论文，而是让文本作为波纹呈现，邀请你顺着它，去触碰你自己的欲望客观自感。</p>\n<p>东方思想是源头。这些文本，就是那源头活水在遇见AI时，激起的波纹。</p>\n<p>岐金兰·余溪</p>\n<p>2026年2月</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-15 15:43</span>&nbsp;\n<a href=\"https://www.cnblogs.com/qijinlan\">岐金兰</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "凸优化数学基础笔记（二）：二次型与正定矩阵",
      "link": "https://www.cnblogs.com/GeophysicsWorker/p/19618211",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/GeophysicsWorker/p/19618211\" id=\"cb_post_title_url\" title=\"发布于 2026-02-15 15:39\">\n    <span>凸优化数学基础笔记（二）：二次型与正定矩阵</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        二次型理论在凸优化问题设计中应用十分广泛。应用矩阵乘法运算，二次型与实对称矩阵紧密地联系在一起，从而二次型的基本问题又可以转换为实对称矩阵问题。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>​       为了便于学习最优化方法，本系列笔记的前几章部分将与优化方法密切相关的数学基础知识作一简要介绍，本系列介绍凸优化中常用矩阵概念：二次型与正定矩阵。</p>\n<p>​       二次型理论在凸优化问题设计中应用十分广泛。应用矩阵乘法运算，二次型与实对称矩阵紧密地联系在一起，从而二次型的基本问题又可以转换为实对称矩阵问题。</p>\n<h2 id=\"1-二次型\">1. 二次型</h2>\n<p>​        二次型理论问题起源于化二次曲线和二次曲面的方程为标准形式的问题。推广到<span class=\"math inline\">\\(n\\)</span>维空间中，二次超曲面的一般方程为：</p>\n<p></p><div class=\"math display\">\\[\\begin{aligned}\nf(x_1,x_2,x_3,...,x_n)=&amp;a_{11}x_1^2+a_{12}x_1x_2+...+a_{1n}x_1x_n+\\\\\n&amp;a_{21}x_1x_2+a_{22}x_2^{2}+...+a_{2n}x_2x_n+\\\\\n&amp;....\\\\\n&amp;a_{n1}x_nx_1+a_{n2}x_nx_2+...+a_{nn}x_n^2\\\\\n&amp;=\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}x_ix_j\n\\end{aligned}\n\\tag{1}\n\\]</div><p></p><p>用矩阵表示可简记为：</p>\n<p></p><div class=\"math display\">\\[\\begin{aligned}\nf(x_1,x_2,...,x_n)&amp;=(x_1,x_2,x_3,...x_n)\\left(\\begin{matrix}a_{11},a_{12},...a_{1n}\\\\ \na_{21},a_{22},...,a_{2n}\\\\\na_{n1},a_{n2},...,a_{nn}\n\\end{matrix}\\right)\\left(\\begin{matrix}x_1\\\\x_2\\\\...\\\\x_n\\end{matrix}\\right)\\\\\n&amp;=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\n\\end{aligned}\n\\tag{2}\n\\]</div><p></p><p>其中，<span class=\"math inline\">\\(x=[x_1,x_2,...,x_n]^{T}\\)</span>,矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的元素<span class=\"math inline\">\\(a_{ij}=a_{ji}\\)</span>正是二次型的<span class=\"math inline\">\\(x_ix_j\\)</span>项的系数的一半，因此二次型和它的矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 是相互唯一决定的，且<span class=\"math inline\">\\(\\mathbf{A}=\\mathbf{A}^{T}\\)</span>.</p>\n<h2 id=\"2-正定矩阵\">2. 正定矩阵</h2>\n<p><strong>定义1 正定矩阵的定义</strong></p>\n<p>​    如果二次型</p>\n<p></p><div class=\"math display\">\\[f(x_1,x_2,...,x_n)= \\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}x_ix_j=\\mathbf{x}^T\\mathbf{A}\\mathbf{x} \\tag{3}\n\\]</div><p></p><p>对于任何一组元素不全为0的向量<span class=\"math inline\">\\(\\mathbf{x}=[x_1,x_2,x_3,...,x_n]^T\\)</span>, 恒有<span class=\"math inline\">\\(f(x_1,x_2,x_3,...,x_n)=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}&gt;0\\)</span> ,则称二次型<span class=\"math inline\">\\(f(x_1,x_2,...x_n)\\)</span>为<strong>正定</strong>，且称为二次型矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 也称为<strong>正定的</strong>，其简记为<span class=\"math inline\">\\(\\mathbf{A}\\succ0\\)</span>.</p>\n<p>​      简而言之，一个对称矩阵的<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 如果是正定的，则其二次型<span class=\"math inline\">\\(f(x_1,x_2,...x_n)=\\mathbf{x}^{T}\\mathbf{A}\\mathbf{x}\\)</span> 对于所有非零向量<span class=\"math inline\">\\(\\mathbf{x}\\)</span> 其值总为正。 类似地，可以给出定义，若二次型<span class=\"math inline\">\\(f(x_1,x_2,...,x_n)=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\geq0\\)</span> ，则<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 为<strong>半正定矩阵</strong>，定义为<span class=\"math inline\">\\(\\mathbf{A}\\succeq{0}\\)</span> ; 若<span class=\"math inline\">\\(\\mathbf{x}^{T}\\mathbf{A}\\mathbf{x}\\leq0\\)</span>，则<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 为<strong>负定矩阵</strong>，简记为<span class=\"math inline\">\\(\\mathbf{A}\\preceq{0}\\)</span>；若二次型<span class=\"math inline\">\\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\)</span>  既不是半正定的，也不是半负定的，就称为矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 为<strong>不定的</strong>。</p>\n<p>​       矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span>为正定的的充要条件是它的行列式的 <span class=\"math inline\">\\(\\det(\\mathbf{A})\\)</span> 顺序主子式全部大于零，即：</p>\n<p></p><div class=\"math display\">\\[a_{11}&gt;0,\\left|\\begin{matrix}a_{11},a_{12}\\\\ a_{21},a_{22}\\end{matrix}\\right|&gt;0,\\left|\\begin{matrix}a_{11}，&amp;a_{12}，...,a_{1n}\\\\ a_{21},&amp;a_{22},...,a_{2n}\\\\ ...,&amp;...\\\\a_{n1},&amp;a_{n2},..., a_{nn}\\end{matrix}\\right|\n\\tag{4}\n\\]</div><p></p><p>由此可见，正定矩阵必然是非奇异的。 由此总结：对于实对称矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span>，下面几个命题之间是互相等价的：</p>\n<ol>\n<li><span class=\"math inline\">\\(\\mathbf{A}\\)</span>是正定矩阵；</li>\n<li><span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的所有主子式都是正数；</li>\n<li><span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的各级顺序主子式都是正数；</li>\n</ol>\n<p>下面我们简单讨论以下的正定矩阵的特征值性质：</p>\n<p><strong>性质1：</strong>实对称矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 是正定矩阵等价于<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的特征值全是正数。</p>\n<p>证明：记<span class=\"math inline\">\\(\\lambda\\)</span>为<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的一个特征值<span class=\"math inline\">\\(\\lambda\\)</span> , <span class=\"math inline\">\\(\\mathbf{v}\\)</span> 是对应于<span class=\"math inline\">\\(\\lambda\\)</span> 的一个特征向量，于是 <span class=\"math inline\">\\(\\mathbf{A}\\mathbf{v}=\\lambda\\mathbf{v}\\)</span> ，进而得到如下的</p>\n<p></p><div class=\"math display\">\\[\\lambda|\\mathbf{v}|^2=\\mathbf{v}^T\\mathbf{A}\\mathbf{v} \\tag{5}\n\\]</div><p></p><p>又因为 <span class=\"math inline\">\\(|\\alpha|^2&gt;0\\)</span>，于是 <span class=\"math inline\">\\(\\lambda&gt;0\\)</span>。</p>\n<p>若<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的任一特征值都是正数，则对于任一非零向量<span class=\"math inline\">\\(\\mathbf{v}\\)</span> ，均有<span class=\"math inline\">\\(\\mathbf{v}^T\\mathbf{A}\\mathbf{v}&gt;0\\)</span>, 于是<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 为正定矩阵。</p>\n<p><strong>性质2：</strong> 若实对称矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span>是正定矩阵，则存在唯一的正定矩阵<span class=\"math inline\">\\(\\mathbf{S}\\)</span>，是使得<span class=\"math inline\">\\(\\mathbf{A}=\\mathbf{S}^2\\)</span>。</p>\n<p>证明：存在性，由于<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 是正定矩阵，由此存在正交矩阵<span class=\"math inline\">\\(\\mathbf{Q}\\)</span> 使得：</p>\n<p></p><div class=\"math display\">\\[\\mathbf{A}=\\mathbf{Q}\\mathbf{D}\\mathbf{Q}^T  \\tag{6}\n\\]</div><p></p><p>其中：<span class=\"math inline\">\\(\\mathbf{D}=dialog(\\lambda_1,\\lambda_2,....,\\lambda_n)\\)</span>，其中<span class=\"math inline\">\\(\\lambda_i\\)</span> 为矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的特征值，由于 <span class=\"math inline\">\\(\\lambda_i&gt;0\\)</span>，于是正定矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span>可以写为如下：</p>\n<p></p><div class=\"math display\">\\[\\mathbf{A}=\\mathbf{Q}\\begin{bmatrix}&amp;\\sqrt\\lambda_1&amp; &amp; &amp;\\\\&amp; &amp;\\sqrt\\lambda_2 &amp; &amp;\\\\ &amp; &amp; &amp;\\cdots &amp; \\\\ &amp; &amp; &amp; &amp;\\sqrt\\lambda_n\\end{bmatrix}\\mathbf{Q}^T\\mathbf{Q}\\begin{bmatrix}&amp;\\sqrt\\lambda_1&amp; &amp; &amp;\\\\&amp; &amp;\\sqrt\\lambda_2 &amp; &amp;\\\\ &amp; &amp; &amp;\\cdots &amp; \\\\ &amp; &amp; &amp; &amp;\\sqrt\\lambda_n\\end{bmatrix}\\mathbf{Q}^T \\tag{7}\n\\]</div><p></p><p>记 <span class=\"math inline\">\\(\\mathbf{S}\\)</span>:</p>\n<p></p><div class=\"math display\">\\[\\mathbf{S}=\\mathbf{Q}\\begin{bmatrix}&amp;\\sqrt\\lambda_1&amp; &amp; &amp;\\\\&amp; &amp;\\sqrt\\lambda_2 &amp; &amp;\\\\ &amp; &amp; &amp;\\cdots &amp; \\\\ &amp; &amp; &amp; &amp;\\sqrt\\lambda_n\\end{bmatrix}\\mathbf{Q}^T  \\tag{8}\n\\]</div><p></p><p>即有<span class=\"math inline\">\\(\\mathbf{A}=\\mathbf{S}^2\\)</span> 。由于 <span class=\"math inline\">\\(\\mathbf{S}\\)</span>的特征值<span class=\"math inline\">\\(\\sqrt{\\lambda_i}\\)</span> 均为正数，因此<span class=\"math inline\">\\(\\mathbf{S}\\)</span> 也是正定矩阵。</p>\n<p>唯一性：假设存在两个正定矩阵<span class=\"math inline\">\\(\\mathbf{S}_1\\)</span>和<span class=\"math inline\">\\(\\mathbf{S}_2\\)</span> ，满足：</p>\n<p></p><div class=\"math display\">\\[\\mathbf{A}=\\mathbf{S}_1^2=\\mathbf{S}_2^2 \\tag{9}\n\\]</div><p></p><p>需要证明<span class=\"math inline\">\\(\\mathbf{S}_1=\\mathbf{S}_2\\)</span> 。这里采用特征值分解的几何意义来证明：</p>\n<ol>\n<li>特征值关系：设<span class=\"math inline\">\\(\\lambda\\)</span>是<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的任意一个特征值，<span class=\"math inline\">\\(\\mu\\)</span> 是<span class=\"math inline\">\\(\\mathbf{S}_1\\)</span> 的对应特征值，则 <span class=\"math inline\">\\(\\mu^2=\\lambda\\)</span>。由于正定矩阵的特征值均为正数，因此<span class=\"math inline\">\\(\\mu\\)</span> 必须取正值<span class=\"math inline\">\\(\\sqrt{\\lambda}\\)</span>。同理，<span class=\"math inline\">\\(\\mathbf{S}_2\\)</span>对应于同一特征值<span class=\"math inline\">\\(\\lambda\\)</span> 的特征值也必然是<span class=\"math inline\">\\(\\sqrt{\\lambda}\\)</span> 。因此，<span class=\"math inline\">\\(\\mathbf{S}_1\\)</span>和<span class=\"math inline\">\\(\\mathbf{S}_2\\)</span> 的特征值完全相同。</li>\n<li>特征值子空间关系：令<span class=\"math inline\">\\(\\{\\lambda_1,\\lambda_2,\\cdots,\\lambda_s\\}\\)</span> 为<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 的全体互异特征值，设 <span class=\"math inline\">\\(\\mathbf{V}_i\\)</span> 为<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 关于特征值<span class=\"math inline\">\\(\\lambda_i\\)</span> 的特征子空间。\n<ul>\n<li>因为 <span class=\"math inline\">\\(\\mathbf{S}_1^{2}=\\mathbf{A}\\)</span>，若<span class=\"math inline\">\\(\\mathbf{x}\\in{\\mathbf{V}_i}\\)</span> ，则<span class=\"math inline\">\\(\\mathbf{A}\\mathbf{x}=\\lambda_i\\mathbf{x}\\)</span>，代入得<span class=\"math inline\">\\(\\mathbf{S}_1^2\\mathbf{x}=\\lambda_i\\mathbf{x}\\)</span> 。这说明<span class=\"math inline\">\\(\\mathbf{x}\\)</span> 同时也是<span class=\"math inline\">\\(\\mathbf{S}_1^2\\)</span> 的特征向量。通过谱映射定理可知，<span class=\"math inline\">\\(\\mathbf{x}\\)</span> 属于<span class=\"math inline\">\\(\\mathbf{S}_1\\)</span> 关于特征值<span class=\"math inline\">\\(\\sqrt{\\lambda_i}\\)</span> 的特征子空间。</li>\n<li>即<span class=\"math inline\">\\(\\mathbf{A}\\)</span>的特征子空间<span class=\"math inline\">\\(\\mathbf{V}_i\\)</span> 恰好就是 <span class=\"math inline\">\\(\\mathbf{S}_1\\)</span> 关于特征值<span class=\"math inline\">\\(\\sqrt{\\lambda_i}\\)</span> 的特征子空间。</li>\n</ul>\n</li>\n<li>唯一确定：同样的推理也适用于<span class=\"math inline\">\\(\\mathbf{S}_2\\)</span>。因此，<span class=\"math inline\">\\(\\mathbf{S}_1\\)</span>和<span class=\"math inline\">\\(\\mathbf{S}_2\\)</span> 具有完全相同的特征值（都是<span class=\"math inline\">\\(\\sqrt{\\lambda_i}\\)</span>）和完全相同的特征子空间（都是<span class=\"math inline\">\\(\\mathbf{V}_i\\)</span>）。对于一个实对称矩阵的矩阵（可正交对角化），它的特征值和特征子空间唯一决定了矩阵本身。因此，<span class=\"math inline\">\\(\\mathbf{S}_1=\\mathbf{S}_2\\)</span>。</li>\n</ol>\n<p>综上所述，实对称正定矩阵<span class=\"math inline\">\\(\\mathbf{A}\\)</span> 存在唯一的正定平方根<span class=\"math inline\">\\(\\mathbf{S}\\)</span>，使得<span class=\"math inline\">\\(\\mathbf{A}=\\mathbf{S}^2\\)</span>。</p>\n<p><strong>性质2</strong>：若<span class=\"math inline\">\\(\\mathbf{A},\\mathbf{B}\\)</span> 都是<span class=\"math inline\">\\(n\\times{n}\\)</span>正定矩阵，则<span class=\"math inline\">\\(\\mathbf{AB}\\)</span> 的特征值都是正数。</p>\n<p>证明：由于<span class=\"math inline\">\\(\\mathbf{A},\\mathbf{B}\\)</span> 为正定矩阵，于是根据性质2可知，存在正定矩阵<span class=\"math inline\">\\(\\mathbf{C}\\)</span> 满足<span class=\"math inline\">\\(\\mathbf{A}=\\mathbf{C}^2\\)</span>。下面考虑矩阵<span class=\"math inline\">\\(\\mathbf{C}^{-1}\\mathbf{A}\\mathbf{B}\\mathbf{C}\\)</span>。注意到：</p>\n<p></p><div class=\"math display\">\\[(\\mathbf{C}^{-1}\\mathbf{AB}\\mathbf{C})^T=\\mathbf{CABC^{-1}}=\\mathbf{CBC}=\\mathbf{C}^{-1}\\mathbf{AB}\\mathbf{C} \\tag{10}\n\\]</div><p></p><p>于是<span class=\"math inline\">\\(\\mathbf{C}^{-1}\\mathbf{ABC}\\)</span> 是一个实对称矩阵，又因为存在正定矩阵<span class=\"math inline\">\\(\\mathbf{D}\\)</span> 满足<span class=\"math inline\">\\(\\mathbf{B}=\\mathbf{D}^2\\)</span>。于是对于任一个非零的向量<span class=\"math inline\">\\(\\boldsymbol{\\alpha}\\in\\mathbb{R}^n\\)</span>.</p>\n<p></p><div class=\"math display\">\\[\\boldsymbol{\\alpha}^T\\mathbf{C}^{-1}\\mathbf{ABC}\\boldsymbol{\\alpha}=\\boldsymbol{\\alpha}^T\\mathbf{C}\\mathbf{B}\\mathbf{C}\\boldsymbol{\\alpha}=(\\mathbf{DC}\\boldsymbol{\\alpha},\\mathbf{DC}\\boldsymbol{\\alpha})\\geq0\\tag{11}\n\\]</div><p></p><p>上面不等式的等号不成立，应为，<span class=\"math inline\">\\(\\mathbf{DC}\\)</span> 为满秩矩阵，进而 <span class=\"math inline\">\\(\\mathbf{DC}\\boldsymbol{\\alpha}\\neq{0}\\)</span>。因此，<span class=\"math inline\">\\(\\mathbf{C}^{-1}\\mathbf{ABC}\\)</span> 是正定矩阵，于是它的所有特征值也是正数。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-15 15:39</span>&nbsp;\n<a href=\"https://www.cnblogs.com/GeophysicsWorker\">GeoFXR</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "OpenEuler 20.03æž„å»ºzabbix8.0 rpmåŒ",
      "link": "https://www.cnblogs.com/virtualzzf/p/19617629",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/virtualzzf/p/19617629\" id=\"cb_post_title_url\" title=\"发布于 2026-02-15 10:46\">\n    <span>OpenEuler 20.03构建zabbix8.0 rpm包</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        OpenEuler 20.03自行构建zabbix8.0 rpm包\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"一说明\">一、说明</h1>\n<h2 id=\"为什么要自己构建\">为什么要自己构建？</h2>\n<p>由于centos从7版本之后改为stream，工作环境由centos转向OpenEuler。zabbix官网上有各大主流操作系统预编译的rpm包，但是Openeuler相对小众，自然没有制作好的包。即使是centos系统，7版本也过于陈旧了，从zabbix 6.0开始，centos 7已经不提供server的rpm包了，只剩下proxy和agent，到了7.0版本，连proxy都没有了。学会自己创建rpm包，以备操作系统环境发生改变是非常有必要的。</p>\n<h2 id=\"为什么不直接源代码编译\">为什么不直接源代码编译</h2>\n<ol>\n<li>由于采用的是sever-proxy-agent的多层架构，server只有一台，但是proxy有几十台，agent更是上千，每一台都用源代码编译工作量大大增加。</li>\n<li>源代码编译的软件，在一些例如配置文件、启停命令上与rpm版本有差异，如果混布增加了运维复杂度。</li>\n</ol>\n<h2 id=\"有没有预编译好的rpm包\">有没有预编译好的rpm包</h2>\n<p>在OpenEuler的官方社区的软件中心，有社区成员自行构建的rpm包，可以尝试找找有无符合自己要求的版本。<br />\n<img alt=\"image\" class=\"lazyload\" /></p>\n<h1 id=\"二准备工作\">二、准备工作</h1>\n<h2 id=\"21-添加repo源\">2.1 添加repo源</h2>\n<p>如果OpenEuler缺少默认的repo源，需要自己添加<br />\n在/etc/yum.repos.d/openEuler_x86_64.repo中添加如下内容：</p>\n<pre><code>[OS]\nname=openEuler-$releasever - OS\nbaseurl=https://repo.openeuler.openatom.cn/openEuler-20.03-LTS-SP4/OS/$basearch/\nenabled=1\ngpgcheck=1\ngpgkey=https://repo.openeuler.openatom.cn/openEuler-20.03-LTS-SP4/OS/$basearch/RPM-GPG-KEY-openEuler\n</code></pre>\n<p>另外再添加everything的源，可以提供更多的包。</p>\n<pre><code>dnf config-manager --add-repo https://repo.openeuler.org/openEuler-20.03-LTS/everything/x86_64\n</code></pre>\n<p>使用<code>dnf clean all &amp;&amp; dnf makecache</code>命令更新。</p>\n<h2 id=\"22-准备构建rpm包环境\">2.2 准备构建rpm包环境</h2>\n<p>之前的文章里已经介绍了构建rpm包的基本方法，这里不再赘述。root用户下运行命令如下：</p>\n<pre><code>dnf install -y rpm-build\ndnf install -y rpmdevtools\nrpmdev-setuptree\n</code></pre>\n<p>下载srpm包（ <a href=\"http://repo.zabbix.com/zabbix/7.0/rhel/8/SRPMS/zabbix-7.0.23-release1.el8.src.rpm\" rel=\"noopener nofollow\" target=\"_blank\">http://repo.zabbix.com/zabbix/7.0/rhel/8/SRPMS/zabbix-7.0.23-release1.el8.src.rpm</a> ） ，这里以rhel8版本的srpm文件为例：</p>\n<pre><code>rpm -ivh zabbix-7.0.23-release1.el8.src.rpm\n</code></pre>\n<p>此时，在/root/rpmbuild目录下的SOURCES目录下会产生源代码压缩包、补丁和配置文件，SPECS目录会产生spec文件。但是此spec文件是Centos8版本的，与OpenEuler不完全契合，需要修改一下。</p>\n<h1 id=\"三安装依赖包\">三、安装依赖包</h1>\n<h2 id=\"31-buildrequires要求的依赖包\">3.1 BuildRequires要求的依赖包</h2>\n<table>\n<thead>\n<tr>\n<th>依赖包</th>\n<th>要求的版本</th>\n<th>dnf安装的版本</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>make</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>mariadb-connector-c-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>postgresql-devel</td>\n<td>&gt;= 12.0</td>\n<td>10.5</td>\n</tr>\n<tr>\n<td>sqlite-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>net-snmp-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>openldap-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>unixODBC-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>curl-devel</td>\n<td>&gt;= 7.13.1</td>\n<td>7.66.0</td>\n</tr>\n<tr>\n<td>OpenIPMI-devel</td>\n<td>&gt;= 2</td>\n<td>2.0.29</td>\n</tr>\n<tr>\n<td>libssh-devel</td>\n<td>&gt;= 0.9.0</td>\n<td>0.9.4</td>\n</tr>\n<tr>\n<td>java-devel（java-1.8.0-openjdk-devel）</td>\n<td>&gt;= 1.6.0</td>\n<td>1.8.0.392.b08</td>\n</tr>\n<tr>\n<td>libxml2-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>libevent-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>pcre2-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>openssl-devel</td>\n<td>&gt;= 1.0.1</td>\n<td>1.1.1f</td>\n</tr>\n<tr>\n<td>systemd</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>policycoreutils-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>selinux-policy-devel</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>c-ares-devel</td>\n<td>&gt;= 1.19.0</td>\n<td>1.16.1</td>\n</tr>\n<tr>\n<td>安装全部依赖：</td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<pre><code>dnf install -y make mariadb-connector-c-devel postgresql-devel sqlite-devel net-snmp-devel openldap-devel unixODBC-devel curl-devel OpenIPMI-devel libssh-devel java-1.8.0-openjdk-devel libxml2-devel libevent-devel pcre2-devel openssl-devel systemd policycoreutils-devel selinux-policy-devel c-ares-devel\n</code></pre>\n<h2 id=\"32-其他依赖包\">3.2 其他依赖包</h2>\n<p>zabbix agent2是使用GO语言编写的，并且使用的语法对版本还有要求，OpenEuler 20.03默认repo源的版本为1.15，需要安装一个较新版本的。<br />\n首先下载golang的压缩包并解压</p>\n<pre><code>tar -C /usr/local -xzf go1.24.8.linux-amd64.tar.gz\n</code></pre>\n<p>配置PATH变量并生效</p>\n<pre><code>tee /etc/profile.d/go.sh &lt;&lt;EOL\nexport GO_HOME=/usr/local/go\nexport PATH=\\$PATH:\\$GO_HOME/bin\nEOL\nsource /etc/profile\n</code></pre>\n<p>但是实际上agent2和web_service<strong>并未构建成功</strong>，见4.3小节</p>\n<h1 id=\"四修改spec文件\">四、修改spec文件</h1>\n<p>修改好的spec文件见：<a href=\"https://files.cnblogs.com/files/blogs/745793/zabbix.zip?t=1771123185&amp;download=true\" target=\"_blank\">https://files.cnblogs.com/files/blogs/745793/zabbix.zip?t=1771123185&amp;download=true</a></p>\n<h2 id=\"41-删除rhel和amzn宏\">4.1 删除%{rhel}和%{?amzn}宏</h2>\n<p>%{rhel}和%{?amzn}两个宏分别标识了redhat和amazon系linux的大版本号，用于构建时一些配置方式的选择。由于这两个宏在OpenEuler中为空，在spec文件中会被全局定义为0，直接使用会影响构建，需要全部进行处理。<br />\n与OpenEuler相对接近的是Centos8，把%{?rhel}当做“8”处理，%{?amzn}直接删除。<br />\n示例1：</p>\n<pre><code>%if ( 0%{?rhel} &gt;= 7 &amp;&amp; 0%{?amzn} == 0 ) || 0%{?amzn} &gt;= 2023\n%{!?build_agent2: %global build_agent2 1}\n%endif\n</code></pre>\n<p>由于08 &gt;= 7，直接修改为</p>\n<pre><code>%{!?build_agent2: %global build_agent2 1}\n</code></pre>\n<p>示例2：</p>\n<pre><code>%if 0%{rhel} &gt;= 9 || 0%{?amzn} &gt;= 2023\nBuildRequires: selinux-policy-devel\nBuildRequires: c-ares-devel &gt;= 1.19.0\n%endif\n</code></pre>\n<p>由于不满足 08 &gt;= 9 ，直接删除</p>\n<h2 id=\"42-修改buildrequires版本要求\">4.2 修改BuildRequires版本要求</h2>\n<p>官网repo源的postgresql-devel版本不达标，直接进行构建会报错。<br />\npostgresql官网没有OpenEuler的预编译rpm包，想要满足要求必须自行从源代码进行编译。<br />\n本文仅为演示，将<code>postgresql-devel &gt;= 12.0</code>修改为<code>postgresql-devel</code></p>\n<h2 id=\"43-去除agent2和web_service\">4.3 去除agent2和web_service</h2>\n<p>agent3和web_service都使用了GO语言，由于网络问题导致两者的创建会出错，直接删除以下内容：</p>\n<pre><code>%ifarch x86_64 aarch64\n%if ( 0%{?rhel} &gt;= 7 &amp;&amp; 0%{?amzn} == 0 ) || 0%{?amzn} &gt;= 2023\n%{!?build_agent2: %global build_agent2 1}\n%endif\n%if 0%{?rhel} &gt;= 8 || 0%{?amzn} &gt;= 2023\n%{!?build_web_service: %global build_web_service 1}\n%endif\n%endif\n</code></pre>\n<p>本次构建不包括两者，想要解决可能必须使用魔法了</p>\n<h1 id=\"五构建\">五、构建</h1>\n<p>使用<code>rpmbuild -bb zabbix.spec</code>命令进行构建，需要比较长的时间。<br />\n完成后在/root/rpmbuild/RPMS目录下就会生成编译好的rpm包。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-15 10:46</span>&nbsp;\n<a href=\"https://www.cnblogs.com/virtualzzf\">virtualzzf</a>&nbsp;\n阅读(<span id=\"post_view_count\">16</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "为什么现代 C++ 库都用 PIMPL？一场关于封装、依赖与安全的演进",
      "link": "https://www.cnblogs.com/charlee44/p/19616660",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/charlee44/p/19616660\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 21:27\">\n    <span>为什么现代 C++ 库都用 PIMPL？一场关于封装、依赖与安全的演进</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        系统阐述了在 C++ 工程中如何通过 PIMPL 惯用法，在坚守 RAII 资源安全的前提下，有效解耦头文件依赖、提升编译效率并保持接口简洁。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>在 C++ 的工程实践中，如何在保证资源安全管理的同时，又避免头文件污染和不必要的编译依赖？这个问题贯穿了现代 C++ 库设计的核心。本文将沿着一条清晰的技术演进路径，探讨从 RAII 封装出发，历经值语义、裸指针、智能指针等阶段，最终走向 PIMPL（Pointer to Implementation） 这一成熟且优雅的解决方案。</p>\n</blockquote>\n<h1 id=\"1-raii资源管理的基石\">1. RAII——资源管理的基石</h1>\n<p>C++ 的核心哲学之一是 RAII（Resource Acquisition Is Initialization）：资源（内存、文件句柄、网络连接等）的生命周期应由对象的构造与析构自动管理。例如：</p>\n<pre><code class=\"language-cpp\">class FileHandle {\n    FILE* fp;\npublic:\n    FileHandle(const char* path) : fp(fopen(path, \"r\")) {}\n    ~FileHandle() { if (fp) fclose(fp); }\n};\n</code></pre>\n<p>RAII 让资源管理变得安全：利用类对象的生命周期，在构造函数中申请资源，在析构函数中释放资源。如果这个类对象是基于栈的值对象，那么就可以自动实现资源的管理。因此，在现代 C++ 中，相比传统的指针语义，更加提倡使用基于 RAII 的值语义。</p>\n<h1 id=\"2-值语义的诱惑与代价\">2. 值语义的诱惑与代价</h1>\n<p>但是，当我们把这种思想用于封装复杂组件（如 ONNX 模型会话、数据库连接池）时，问题出现了。理想情况下，我们希望像使用 std::string 一样，用“值语义”操作一个封装对象：</p>\n<pre><code class=\"language-cpp\">class Embedder {\n    Ort::Session session; // 值成员\npublic:\n    std::vector&lt;float&gt; embed(const std::string&amp; text);\n};\n</code></pre>\n<p>这看起来非常简洁、高效、符合现代 C++ 风格。但也有另外一个问题：破坏了封装，导致不必要的环境依赖。最直观的问题就是 <code>Ort::Session</code> 的完整定义必须出现在头文件中，这意味着使用者必须包含 onnxruntime ，而这个头文件可能重达数 MB ，依赖数十个系统库。这就会造成如下问题：</p>\n<ul>\n<li>编译时间暴增，微小的改动都需要编译很长的时间。</li>\n<li>头文件耦合严重，调用者使用不方便，甚至造成环境污染。</li>\n<li>ABI 极其脆弱，内部改动导致所有用户重编译。</li>\n</ul>\n<h1 id=\"3-指针语义的回退\">3. 指针语义的回退</h1>\n<p>为了解耦，一个比较好的办法就是使用前置声明 + 指针语义：</p>\n<pre><code class=\"language-cpp\">// header\nclass SessionImpl; // 前置声明\nclass Embedder {\n    SessionImpl* pimpl;\npublic:\n    Embedder();\n    ~Embedder(); // 必须手动 delete\n};\n</code></pre>\n<p>这样做确实切断了编译依赖，但也引入了新的问题。那就是需要按照 RAII 原则写好构造函数和析构函数。而一旦要写析构函数，也往往意味着需要写另外四个特殊的成员函数：</p>\n<ol>\n<li>拷贝构造函数（Copy Constructor）</li>\n<li>拷贝赋值运算符（Copy Assignment Operator）</li>\n<li>移动构造函数（Move Constructor）</li>\n<li>移动赋值运算符（Move Assignment Operator）</li>\n</ol>\n<p>这样做要写非常多的样板代码，而且也很容易出问题。为了封装牺牲安全，得不偿失。</p>\n<h1 id=\"4-使用智能指针\">4. 使用智能指针</h1>\n<p>使用裸指针又麻烦又不安全，那么就可以使用 C++11 引入的智能指针：std::unique_ptr 和 std::shared_ptr；智能指针同样是基于 RAII 的：</p>\n<pre><code class=\"language-cpp\">class SessionImpl;\nclass Embedder {\n    std::unique_ptr&lt;SessionImpl&gt; pimpl;\n};\n</code></pre>\n<p>这里为什么使用 <code>std::unique_ptr</code> 而不使用 <code>std::shared_ptr</code> 呢？其实也可以，不过在现代 C++ 中，更推荐使用 <code>std::unique_ptr</code> 。<code>std::shared_ptr</code> 是用来共享资源的所有权，会对引用资源进行计数，但是有可能会造成相互循环引用造成不能释放资源的问题；而<code>std::unique_ptr</code> 则表示独占资源的所有权，不仅开销更低（无引用计数），也更加安全（只能通过 <code>std::move</code> 转移所有权 ）。</p>\n<p>不过有一点需要注意：<code>std::unique_ptr</code> 和 <code>std::shared_ptr</code> 在处理不完整类型（incomplete type）时的行为截然不同。具体来说，当在头文件中使用前置声明（如 <code>class Impl;</code>）并用智能指针持有它时，<code>Impl</code> 是一个不完整类型。</p>\n<ul>\n<li><code>std::shared_ptr</code> 可以安全地在头文件中默认析构，因为它在构造时（通常在 <code>.cpp</code> 文件中）会捕获一个完整的删除器（deleter），即使析构发生在头文件上下文中，也能正确调用 <code>delete</code>。</li>\n<li>而 <code>std::unique_ptr</code> 的删除器是其类型的一部分（通常是默认的 <code>std::default_delete&lt;Impl&gt;</code>），它要求在析构点（即类的析构函数被实例化的地方）<code>Impl</code> 必须是完整类型。如果在头文件中写 <code>~Embedder() = default;</code>，此时 <code>Impl</code> 仍是不完整的，编译器可能不会报错，但会导致未定义行为（通常是链接失败或运行时崩溃）。</li>\n</ul>\n<p>因此，使用 <code>std::unique_ptr&lt;Impl&gt;</code> 时，必须将主类的析构函数定义移到 <code>.cpp</code> 文件中，确保 <code>Impl</code> 已被完整定义：</p>\n<pre><code class=\"language-cpp\">// Embedder.cpp\nclass Embedder::Impl {\n    // 完整定义...\n};\n\nEmbedder::~Embedder() = default; // ✅ 此时 Impl 完整，安全析构\n</code></pre>\n<h1 id=\"5-封装与效率的平衡pimpl\">5. 封装与效率的平衡：PIMPL</h1>\n<p>使用智能指针虽然好，但是总归是比不上值语义方便。当类中只有一个需要隐藏的成员还好，如果有很多个需要隐藏的成员，每一个都写前置声明，并用智能指针来管理，那就实在太繁琐了。并且，从编程品味上来说，C++ 智能指针的写法说不上优雅：智能指针是由传染性的，当满屏都是 <code>std::shared_ptr</code> 或者 <code>std::unique_ptr</code> 的时候，实在很影响阅读性。</p>\n<p>另外，作为对外的接口，最好是提供像 Java / C# 那样的接口，C++ 的纯虚函类也行，隐藏掉所有的细节，包括私有函数和数据成员。这样有非常多的好处：</p>\n<ol>\n<li>最小化依赖环境，提升编译速度。</li>\n<li>调用者使用方便，不会污染环境。</li>\n<li>ABI 稳定，可以只更新库而不用更新整个程序。</li>\n</ol>\n<p>那么要怎么进行优化呢？很简单，我们可以实现一个名为 <code>Impl</code> 的类中类 ，使用<code>std::unique_ptr</code>进行管理。<code>Impl</code> 是实现在 cpp 中的，可以将一切实现的细节，比说私有函数和数据成员，都放在这个 <code>Impl</code> 中。更重要的是，<code>Impl</code> 中的数据成员完全可以使用值类型！如下所示：</p>\n<pre><code class=\"language-cpp\">// 头文件\nclass Embedder {\n    class Impl;\n    std::unique_ptr&lt;Impl&gt; impl;\npublic:\n    Embedder(const std::string&amp; model);\n    ~Embedder(); // 声明但不在头文件定义！\n    std::vector&lt;float&gt; embed(std::string_view text) const;\n};\n</code></pre>\n<pre><code class=\"language-cpp\">// 源文件\nclass Embedder::Impl {\n    Ort::Session session;\n    hf::Tokenizer tokenizer;\n    int64_t dim;\npublic:\n    Impl(const std::string&amp; path, const hf::Tokenizer&amp; tok) \n        : session(...), tokenizer(tok) { /* init */ }\n    std::vector&lt;float&gt; embed(std::string_view text) const { /* ... */ }\n};\n\nEmbedder::Embedder(const std::string&amp; path) \n    : impl(std::make_unique&lt;Impl&gt;(path, global_tokenizer)) {}\n\nEmbedder::~Embedder() = default; // 此时 Impl 完整，安全！\n</code></pre>\n<p>这个实现，就是所谓的 PIMPL（Pointer to IMPLementation）惯用法，也常被称作 “编译防火墙”（Compilation Firewall） 或 “Opaque Pointer” 模式。不得不说，这种 PIMPL 设计模式确实精妙——它在安全性、封装性、编译效率与接口简洁性之间取得了近乎完美的平衡，既坚守了 RAII 的资源管理原则，又有效隔离了实现细节，堪称现代 C++ 工程实践中“高内聚、低耦合”的典范。</p>\n<h1 id=\"6-没有银弹只有权衡\">6. 没有银弹，只有权衡</h1>\n<p>PIMPL 使用了前置声明。是否使用前置声明一直是 C++ 中比较争议的一点，Qt 遵循前置声明的原则实现了非常强大、优雅且高效的 C++ 运行时框架。Google 则经历了从推荐使用前置声明到不推荐使用前置声明的转变。个人认为，PIMPL 解决的就是 C++ 中两个重要原则矛盾的问题：</p>\n<ul>\n<li>推荐使用值语义，但是会引入更多环境依赖</li>\n<li>封装需要尽可能隐藏不必要的细节</li>\n</ul>\n<p>如果两者只能选择其中一个，那么还是尽量使用值语义的原则更加重要，毕竟这涉及到安全问题，而资源管理的安全问题贯穿 C++ 程序的始终。事实上，如果不是提供对外接口，或者实现比较小，那么直接使用值语义即可（第2节中的内容）——值语义永远是最简洁安全的实现。</p>\n<p>另外，如果实现 C++20 Modules ，那么就不必要使用 PIMPL 了，完全可以回归值语义实现，因为 C++20 Modules 在语言层面已经实现了 PIMPL 的诸多优点。</p>\n<h1 id=\"7-示例代码\">7. 示例代码</h1>\n<p>最后放出笔者自己实现的基于 PIMPL 的嵌入器的完整代码供读者参考：</p>\n<pre><code class=\"language-cpp\">// BgeOnnxEmbedder.h\n#pragma once\n\n#include &lt;memory&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n\nnamespace embedding {\n\nnamespace hf {\nclass Tokenizer;\n}\n\nclass BgeOnnxEmbedder {\n public:\n  explicit BgeOnnxEmbedder(const std::string&amp; modelPath,\n                           const hf::Tokenizer&amp; tokenizer);\n  ~BgeOnnxEmbedder();\n\n  const int64_t&amp; EmbeddingDim() const;\n\n  std::vector&lt;float&gt; Embed(const std::string&amp; text) const;\n\n private:\n  class Impl;  // 前向声明\n  std::unique_ptr&lt;Impl&gt; impl;\n};\n\n}  // namespace embedding\n</code></pre>\n<pre><code class=\"language-cpp\">//BgeOnnxEmbedder.cpp\n#include \"BgeOnnxEmbedder.h\"\n\n#include &lt;onnxruntime_cxx_api.h&gt;\n\n#include \"HfTokenizer.h\"\n#include \"Util/StringEncode.h\"\n\nnamespace embedding {\n\nclass BgeOnnxEmbedder::Impl {\n public:\n  Ort::Env&amp; GetOrtEnv() {\n    static Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"BgeOnnxEmbedder\");\n    return env;\n  }\n\n  const int64_t&amp; EmbeddingDim() const { return embeddingDim; }\n\n  explicit Impl(const std::string&amp; modelPath, const hf::Tokenizer&amp; tokenizer)\n      : session{GetOrtEnv(),\n#ifdef _WIN32\n                util::StringEncode::Utf8StringToWideString(modelPath).c_str(),\n#else\n                modelPath.c_str(),\n#endif\n                Ort::SessionOptions()},\n        memInfo{Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU)},\n        tokenizer(tokenizer),\n        embeddingDim(0) {\n\n    //\n    const auto&amp; outputInfo = session.GetOutputTypeInfo(0);\n    const auto&amp; tensorInfo = outputInfo.GetTensorTypeAndShapeInfo();\n    const auto&amp; shape = tensorInfo.GetShape();\n\n    // 假设输出是 [batch, seq, dim] 或 [batch, dim]\n    // 我们取最后一个非 -1 的维度\n    for (auto it = shape.rbegin(); it != shape.rend(); ++it) {\n      if (*it != -1) {\n        embeddingDim = *it;\n        break;\n      }\n    }\n\n    if (embeddingDim == 0) {\n      throw std::runtime_error(\n          \"Failed to infer embedding dimension from ONNX model.\");\n    }\n  }\n\n  std::vector&lt;float&gt; Embed(const std::string&amp; text) const {\n    hf::Tokenizer::ResultPtr result = tokenizer.Encode(text);\n    if (!result) {\n      throw std::runtime_error(\"tokenizer_encode failed\");\n    }\n\n    // 定义张量维度\n    int64_t seqLen = static_cast&lt;int64_t&gt;(result-&gt;length);\n    std::vector&lt;int64_t&gt; inputShape = {1, seqLen};\n    size_t dataByteCount = sizeof(int64_t) * seqLen;\n\n    Ort::Value inputIdsTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;input_ids, dataByteCount, inputShape.data(),\n        inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    Ort::Value attentionMaskTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;attention_mask, dataByteCount,\n        inputShape.data(), inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    Ort::Value tokenTypeIdsTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;token_type_ids, dataByteCount,\n        inputShape.data(), inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    // 输入名必须与模型定义一致\n    const char* inputNames[] = {\"input_ids\", \"attention_mask\",\n                                \"token_type_ids\"};\n    const char* outputNames[] = {\"last_hidden_state\"};\n\n    // 把三个输入张量放进数组\n    std::vector&lt;Ort::Value&gt; inputs;\n    inputs.push_back(std::move(inputIdsTensor));\n    inputs.push_back(std::move(attentionMaskTensor));\n    inputs.push_back(std::move(tokenTypeIdsTensor));\n\n    // 执行推理\n    auto outputs = session.Run(Ort::RunOptions(),  // 运行选项（通常 nullptr）\n                               inputNames,         // 输入名数组\n                               inputs.data(),  // 输入张量数组\n                               inputs.size(),  // 输入数量（3）\n                               outputNames,    // 输出名数组\n                               1               // 输出数量（1）\n    );\n\n    // 获取输出信息\n    auto&amp; output_tensor = outputs[0];\n    auto output_shape = output_tensor.GetTensorTypeAndShapeInfo().GetShape();\n    if (output_shape.size() != 3 || output_shape[0] != 1) {\n      throw std::runtime_error(\"Unexpected output shape\");\n    }\n\n    // 获取输出张量的原始 float 指针\n    const float* outputData = outputs[0].GetTensorData&lt;float&gt;();\n\n    // 提取 [CLS] token 的 embedding（第0个token）\n    int64_t hiddenSize = output_shape[2];\n    std::vector&lt;float&gt; embedding(outputData, outputData + hiddenSize);\n\n    // L2 归一化（BGE 要求）\n    float norm = 0.0f;\n    for (float v : embedding) norm += v * v;\n    norm = std::sqrt(norm);\n    if (norm &gt; 1e-8) {\n      for (float&amp; v : embedding) v /= norm;\n    }\n\n    return embedding;\n  }\n\n private:\n  mutable Ort::Session session;\n  Ort::MemoryInfo memInfo;\n  const hf::Tokenizer&amp; tokenizer;\n  int64_t embeddingDim;\n};\n\nBgeOnnxEmbedder::BgeOnnxEmbedder(const std::string&amp; modelPath,\n                                 const hf::Tokenizer&amp; tokenizer)\n    : impl(std::make_unique&lt;Impl&gt;(modelPath, tokenizer)) {}\n\nBgeOnnxEmbedder::~BgeOnnxEmbedder() = default;  // 此时 Impl 已定义，可安全析构\n\nconst int64_t&amp; BgeOnnxEmbedder::EmbeddingDim() const {\n  return impl-&gt;EmbeddingDim();\n}\n\nstd::vector&lt;float&gt; BgeOnnxEmbedder::Embed(const std::string&amp; text) const {\n  return impl-&gt;Embed(text);\n}\n\n}  // namespace embedding\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 21:27</span>&nbsp;\n<a href=\"https://www.cnblogs.com/charlee44\">charlee44</a>&nbsp;\n阅读(<span id=\"post_view_count\">42</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI编程时代普通本科计算机毕业生的出路",
      "link": "https://www.cnblogs.com/xdesigner/p/19615230",
      "published": "",
      "description": "<h2>\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xdesigner/p/19615230\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 13:36\">\n    <span>AI编程时代普通本科计算机毕业生的出路</span>\n    \n\n</a>\n\n\t\t</h2>\n\t\t<div class=\"postText\">    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        AI编程工具快速普及，正在彻底改写整个程序员行业的格局与就业逻辑。对国内数百万一本、二本普通计算机专业的毕业生来说，就业环境已经变得异常残酷，那么出路在哪里？\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p><span style=\"font-size: 18pt;\">AI<span>编程时代</span>普通本科计算机毕业生的出路</span></p>\n<p><span style=\"font-size: x-large;\">袁永福 2026-2-14</span></p>\n<p><span style=\"font-size: 18pt;\">&nbsp; AI<span>编程工具快速普及，正在彻底改写整个程序员行业的格局与就业逻辑。对于985，211重点大学计算机专业毕业生来说还能应付。但对数百万一本、二本普通计算机专业的毕业生来说，环境已经变得异常残酷：</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>程序员岗位总量快速收缩，互联网大厂招人数量大幅下降；</span><span style=\"font-family: Calibri;\">ToB&nbsp;</span><span>程序员岗位虽然也在收紧，招人规模有所减少，但依然保留着真实的就业机会。 而很多学生选择的全职考公、考研、考编，对大多数人来说早已不是靠谱出路，而是一块逃避现实、变相啃老的遮羞布。面对大势，最清醒、优先级最高的选择非常明确：拥抱</span><span style=\"font-family: Calibri;\">AI</span><span>而不是抵制</span><span style=\"font-family: Calibri;\">AI</span><span>，充分利用</span><span style=\"font-family: Calibri;\">AI</span><span>带来的技术平权，优先进入</span><span style=\"font-family: Calibri;\">ToB</span><span>领域，扎扎实实与一个长期行业深度绑定，这才是普通计算机毕业生最现实、最可持续的就业路线。</span></span></p>\n<p><span style=\"font-size: 18pt;\">&nbsp; 从全球行业数据来看，估计<span>当前全球程序员总量约</span>2700<span>万人，其中&nbsp;</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>程序员和</span><span style=\"font-family: Calibri;\">ToB&nbsp;</span><span>程序员大约各占一半</span><span>。过去十几年，互联网高速扩张，</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>领域一直是吸纳程序员最多的方向。</span></span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 但在</span>AI<span>与行业周期的双重冲击下，这个格局正在快速反转。</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>程序员的岗位正在大幅减少。 互联网大厂、电商、社交、工具类产品，大量工作集中在界面开发、接口实现、业务逻辑拼接等重复性内容，而这正是</span><span style=\"font-family: Calibri;\">AI</span><span>最擅长替代的部分。再加上行业降本增效，企业不再靠堆人头做规模，而是用资深工程师配合</span><span style=\"font-family: Calibri;\">AI</span><span>工具完成工作，</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>整体岗位数量快速下降。大家常说的“</span><span style=\"font-family: Calibri;\">30</span><span>岁危机”“</span><span style=\"font-family: Calibri;\">35</span><span>岁退休”，几乎全部来自</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>行业，因为这里拼成本、拼速度、拼年轻化，普通学历开发者几乎没有长期竞争力。</span></span></p>\n<p><span style=\"font-size: 18pt;\">&nbsp; ToB&nbsp;<span>程序员的处境则完全不同。 虽然受经济环境影响，企业招新人的数量也在下降，入口比以前更窄，但岗位并没有消失。</span><span style=\"font-family: Calibri;\">ToB&nbsp;</span><span>面向企业、政府、医疗、金融、工业、能源等垂直行业，核心价值在于业务理解、行业规则、系统稳定性和复杂项目落地能力。</span><span style=\"font-family: Calibri;\">AI</span><span>可以辅助编码，但无法替代行业经验与真实项目积累。在</span><span style=\"font-family: Calibri;\">ToB</span><span>领域，</span><span style=\"font-family: Calibri;\">35</span><span>岁不是危机，而是黄金年龄，经验越丰富越不可替代。</span></span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 根据预测，</span>5<span>年后全球程序员结构将出现颠覆性变化：</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>程序员会从现在的</span><span style=\"font-family: Calibri;\">1800</span><span>万减到1400万左右</span><span>，总量接近腰斩；而&nbsp;</span><span style=\"font-family: Calibri;\">ToB&nbsp;</span><span>程序员会从1300</span><span>万稳步增长到1700</span><span>万左右。</span><span>这组数据，为普通计算机毕业生指明了最确定的方向。</span></span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 可现实中，大量一本、二本学生一毕业就放弃就业，全职备考。不工作、不实习、无收入，完全依靠家庭支持，看上去是在努力奋斗，本质却非常残酷：考研极度内卷，考公录取率不足</span>3%<span>，全职备考本质上就是用极低概率的梦想，掩盖自己不敢进入社会的事实，成为一块体面又合理的啃老遮羞布。 几年</span>后不出意外的考不上，技能退化、经验空白、年龄变大，最终既没上岸，也没工作，两头落空。是一种对自己对家人不负责任的行为。</span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 面对</span>AI<span>编程的大趋势，正确的态度不是恐惧和抵制，而是主动拥抱。</span><span style=\"font-family: Calibri;\">AI</span><span>带来了前所未有的技术平权：过去需要长期训练才能具备的编码能力，现在借助</span><span style=\"font-family: Calibri;\">AI</span><span>工具可以快速上手。这对基础一般、学历普通的学生来说，是巨大的机会——你不需要成为顶尖高手，只要会用</span><span style=\"font-family: Calibri;\">AI</span><span>、能落地、懂业务，就能在</span><span style=\"font-family: Calibri;\">To B</span><span>领域站稳脚跟</span>&nbsp;</span></p>\n<p><span style=\"font-size: 18pt;\"><span>&nbsp; 因此，普通本科计算机毕业生最高优先级的选择，不是死磕</span>To C<span>大厂，不是赌命考公考研，而是放低姿态、接受合理起薪，优先进入</span><span style=\"font-family: Calibri;\">To B</span><span>行业。不必执着于大厂光环，不必纠结于起点高低，先入行、先积累。选择医疗</span><span style=\"font-family: Calibri;\">IT</span><span>、政务信息化、工业软件、金融后台等长期稳定的方向，在一个行业扎根</span><span style=\"font-family: Calibri;\">3</span><span>—</span><span style=\"font-family: Calibri;\">5</span><span>年，把自己和行业深度绑定，用业务壁垒建立</span><span style=\"font-family: Calibri;\">AI</span><span>无法替代的竞争力。</span></span></p>\n<p><span style=\"font-size: 18pt;\">&nbsp; AI<span>时代没有捷径，只有最现实的生存策略。</span><span style=\"font-family: Calibri;\">ToC&nbsp;</span><span>在收缩，考公考研是概率陷阱，全职备考只是</span>啃老<span>遮羞布。拥抱</span>AI<span>、用好技术平权、坚定入局</span><span style=\"font-family: Calibri;\">ToB</span><span>、深耕垂直行业，才是普通计算机毕业生在时代变局中，最清醒、相对最务实，走得最远的出路。</span></span></p>\n\n</div>\n<div class=\"clear\"></div>\n</div>\n\t\t<p class=\"postfoot\">\n\t\t\tposted on \n<span id=\"post-date\">2026-02-14 13:36</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xdesigner\">袁永福 电子病历，医疗信息化</a>&nbsp;\n阅读(<span id=\"post_view_count\">264</span>)&nbsp;\n评论(<span id=\"post_comment_count\">2</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</p>"
    },
    {
      "title": "Flask - 常见应用部署方案",
      "link": "https://www.cnblogs.com/XY-Heruo/p/19615176",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/XY-Heruo/p/19615176\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 13:19\">\n    <span>Flask - 常见应用部署方案</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        常见Flask应用部署方案\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"前言\">前言</h2>\n<p>开发调试阶段，运行 Flask 的方式多直接使用 <code>app.run()</code>，但 Flask 内置的 WSGI Server 的性能并不高。对于生产环境，一般使用 <code>gunicorn</code>。如果老项目并不需要多高的性能，而且用了很多单进程内的共享变量，使用 gunicorn 会影响不同会话间的通信，那么也可以试试直接用 <code>gevent</code>。</p>\n<p>在 Docker 流行之前，生产环境部署 Flask 项目多使用 virtualenv + gunicorn + supervisor。Docker 流行之后，部署方式就换成了 gunicorn + Docker。如果没有容器编排服务，后端服务前面一般还会有个 nginx 做代理。如果使用 Kubernetes，一般会使用 service + ingress（或 istio 等）。</p>\n<h2 id=\"运行方式\">运行方式</h2>\n<h3 id=\"flask-内置-wsgi-server\">Flask 内置 WSGI Server</h3>\n<p>开发阶段一般使用这种运行方式。</p>\n<pre><code class=\"language-python\"># main.py\nfrom flask import Flask\nfrom time import sleep\n\napp = Flask(__name__)\n\n@app.get(\"/test\")\ndef get_test():\n    sleep(0.1)\n    return \"ok\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=10000)\n</code></pre>\n<p>运行:</p>\n<pre><code class=\"language-bash\">python main.py\n</code></pre>\n<h3 id=\"gevent\">gevent</h3>\n<p>使用 gevent 运行 Flask，需要先安装 gevent</p>\n<pre><code class=\"language-bash\">python -m pip install -U gevent\n</code></pre>\n<p>代码需要稍作修改。</p>\n<p>需要注意 <code>monkey.patch_all()</code> 一定要写在入口代码文件的最开头部分，这样 monkey patch 才能生效。</p>\n<pre><code class=\"language-python\"># main.py\nfrom gevent import monkey\nmonkey.patch_all()\nimport time\n\nfrom flask import Flask\nfrom gevent.pywsgi import WSGIServer\n\n\napp = Flask(__name__)\n\n\n@app.get(\"/test\")\ndef get_test():\n    time.sleep(0.1)\n    return \"ok\"\n\n\nif __name__ == \"__main__\":\n    server = WSGIServer((\"0.0.0.0\", 10000), app)\n    server.serve_forever()\n</code></pre>\n<p>运行</p>\n<pre><code class=\"language-bash\">python main.py\n</code></pre>\n<h3 id=\"gunicorn--gevent\">gunicorn + gevent</h3>\n<blockquote>\n<p>如果现有项目大量使用单进程内的内存级共享变量，贸然使用 gunicorn 多 worker 模式可能会导致数据访问不一致的问题。</p>\n</blockquote>\n<p>同样需要先安装依赖。</p>\n<pre><code class=\"language-bash\">python -m pip install -U gunicorn gevent\n</code></pre>\n<p>不同于单独使用 gevent，这种方式不需要修改代码，gunicorn 会自动注入 gevent 的 monkey patch。</p>\n<p>gunicorn 可以在命令行配置启动参数，但个人一般习惯在 gunicorn 的配置文件内配置启动参数，这样可以动态设置一些配置，而且可以修改日志格式。</p>\n<p><code>gunicorn.conf.py</code> 的配置示例如下：</p>\n<pre><code class=\"language-python\"># Gunicorn 配置文件\nfrom pathlib import Path\nfrom multiprocessing import cpu_count\nimport gunicorn.glogging\nfrom datetime import datetime\n\nclass CustomLogger(gunicorn.glogging.Logger):\n    def atoms(self, resp, req, environ, request_time):\n        \"\"\"\n        重写 atoms 方法来自定义日志占位符\n        \"\"\"\n        # 获取默认的所有占位符数据\n        atoms = super().atoms(resp, req, environ, request_time)\n        \n        # 自定义 't' (时间戳) 的格式\n        now = datetime.now().astimezone()\n        atoms['t'] = now.isoformat(timespec=\"seconds\")\n        \n        return atoms\n    \n\n# 预加载应用代码\npreload_app = True\n\n# 工作进程数量：通常是 CPU 核心数的 2 倍加 1\n# workers = int(cpu_count() * 2 + 1)\nworkers = 4\n\n# 使用 gevent 异步 worker 类型，适合 I/O 密集型应用\n# 注意：gevent worker 不使用 threads 参数，而是使用协程进行并发处理\nworker_class = \"gevent\"\n\n# 每个 gevent worker 可处理的最大并发连接数\nworker_connections = 2000\n\n# 绑定地址和端口\nbind = \"127.0.0.1:10001\"\n\n# 进程名称\nproc_name = \"flask-dev\"\n\n# PID 文件路径\npidfile = str(Path(__file__).parent / \"tmp\" / \"gunicorn.pid\")\n\nlogger_class = CustomLogger\naccess_log_format = (\n    '{\"@timestamp\": \"%(t)s\", '\n    '\"remote_addr\": \"%(h)s\", '\n    '\"protocol\": \"%(H)s\", '\n    '\"host\": \"%({host}i)s\", '\n    '\"request_method\": \"%(m)s\", '\n    '\"request_path\": \"%(U)s\", '\n    '\"status_code\": %(s)s, '\n    '\"response_length\": %(b)s, '\n    '\"referer\": \"%(f)s\", '\n    '\"user_agent\": \"%(a)s\", '\n    '\"x_tracking_id\": \"%({x-tracking-id}i)s\", '\n    '\"request_time\": %(L)s}'\n)\n\n# 访问日志路径\naccesslog = str(Path(__file__).parent / \"logs\" / \"access.log\")\n\n# 错误日志路径\nerrorlog = str(Path(__file__).parent / \"logs\" / \"error.log\")\n\n# 日志级别\nloglevel = \"debug\"\n</code></pre>\n<p>运行。gunicorn 的默认配置文件名就是 <code>gunicorn.conf.py</code>，如果文件名不同，可以使用 <code>-c</code> 参数来指定。</p>\n<pre><code class=\"language-bash\">gunicorn main:app\n</code></pre>\n<h2 id=\"传统进程管理实现自动启动\">传统进程管理：实现自动启动</h2>\n<p>在传统服务器部署时，常见的进程守护方式有：</p>\n<ol>\n<li>配置 crontab + shell 脚本。定时检查进程在不在，不在就启动。</li>\n<li>配置 supervisor。</li>\n<li>配置 systemd。</li>\n</ol>\n<p>由于 <code>supervisor</code> 需要单独安装，而本着能用自带工具就用自带工具、能少装就少装的原则，个人一般不会使用 supervisor，因此本文不会涉及如何使用 supervisor。</p>\n<p>在服务器部署时，一般也会为项目单独创建 Python 虚拟环境。</p>\n<pre><code class=\"language-bash\"># 使用 Python 内置的 venv，在当前目录创建 Python 虚拟环境目录 .venv\npython3 -m venv .venv\nsource .venv/bin/activate\npython -m pip install -r ./requirements.txt\n\n# 如果使用uv, 直接uv sync 即可\n</code></pre>\n<h3 id=\"crontab--shell-脚本-不推荐生产环境\">crontab + shell 脚本 (不推荐生产环境)</h3>\n<p>刚入行的时候对 systemd 不熟悉，经常用 crontab + shell 脚本来守护进程，现在想想这种方式并不合适，比较考验 shell 脚本的编写水平，需要考虑方方面面</p>\n<ul>\n<li>首先要确保用户级 crontab 启用，有些生产环境会禁用用户级的 crontab，而且也不允许随便配置系统级的 crontab。</li>\n<li>crontab 是分钟级的，服务停止时间可能要一分钟。</li>\n<li>如果有控制台日志，需要手动处理日志重定向，还有日志文件轮转问题。</li>\n<li>如果 ulimit 不高，还得控制 ulimit。</li>\n<li>经常出现僵尸进程，shell 脚本来要写一堆状态检查的逻辑。</li>\n</ul>\n<p>如果只需要简单用用，也可以提供个示例</p>\n<pre><code class=\"language-bash\">#!/bin/bash\n\n# 环境配置\nexport FLASK_ENV=\"production\"\nexport DATABASE_URL=\"postgresql://user:pass@localhost:5432/mydb\"\nexport REDIS_URL=\"redis://localhost:6379/0\"\n\nscript_dir=$(cd $(dirname $0) &amp;&amp; pwd)\napp_name=\"gunicorn\"  # 实际进程名是 gunicorn，不是 Flask app\nwsgi_module=\"wsgi:app\"  # 替换 WSGI 入口\nsocket_path=\"${script_dir}/myapp.sock\"  # Unix Socket 路径（避免 /run 重启丢失）\nlog_file=\"${script_dir}/app.log\"\npid_file=\"${script_dir}/gunicorn.pid\"   # 用 PID 文件控制\n\n# 进程检测\nis_running() {\n    if [ -f \"$pid_file\" ]; then\n        pid=$(cat \"$pid_file\")\n        if ps -p \"$pid\" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; grep -q \"gunicorn.*${wsgi_module}\" /proc/\"$pid\"/cmdline 2&gt;/dev/null; then\n            echo \"Gunicorn (PID: $pid) is running\"\n            return 0\n        else\n            rm -f \"$pid_file\"  # 清理失效 PID\n            echo \"Stale PID file found, cleaned up\"\n            return 1\n        fi\n    else\n        # 备用检测：通过 socket 文件 + 进程名\n        if [ -S \"$socket_path\" ] &amp;&amp; pgrep -f \"gunicorn.*${wsgi_module}\" &gt; /dev/null 2&gt;&amp;1; then\n            echo \"Gunicorn is running (detected by socket)\"\n            return 0\n        fi\n        echo \"Gunicorn is not running\"\n        return 1\n    fi\n}\n\n# 启动应用\nstart_app() {\n    is_running\n    if [ $? -eq 0 ]; then\n        echo \"Already running, skip start\"\n        return 0\n    fi\n\n    echo \"Starting Gunicorn at $(date)\"\n    echo \"Socket: $socket_path\"\n    echo \"Log: $log_file\"\n\n    # 确保 socket 目录存在\n    mkdir -p \"$(dirname \"$socket_path\")\"\n\n    # 启动命令（关键：不加 --daemon，用 nohup 托管）\n    cd \"$script_dir\" || exit 1\n    # 生成 PID 文件\n    nohup \"$script_dir/venv/bin/gunicorn\" \\\n        --workers 3 \\\n        --bind \"unix:$socket_path\" \\\n        --pid \"$pid_file\" \\\n        --access-logfile \"$log_file\" \\\n        --error-logfile \"$log_file\" \\\n        --log-level info \\\n        \"$wsgi_module\" &gt; /dev/null 2&gt;&amp;1 &amp;\n\n    # 等待启动完成\n    sleep 2\n    if is_running; then\n        echo \"✓ Start success (PID: $(cat \"$pid_file\" 2&gt;/dev/null))\"\n        return 0\n    else\n        echo \"✗ Start failed, check $log_file\"\n        return 1\n    fi\n}\n\n# 停止应用\nstop_app() {\n    is_running\n    if [ $? -eq 1 ]; then\n        echo \"Not running, skip stop\"\n        return 0\n    fi\n\n    pid=$(cat \"$pid_file\" 2&gt;/dev/null)\n    echo \"Stopping Gunicorn (PID: $pid) gracefully...\"\n\n    # 先发 SIGTERM（优雅停止）\n    kill -15 \"$pid\" 2&gt;/dev/null || true\n    sleep 5\n\n    # 检查是否还在运行\n    if ps -p \"$pid\" &gt; /dev/null 2&gt;&amp;1; then\n        echo \"Still running after 5s, force killing...\"\n        kill -9 \"$pid\" 2&gt;/dev/null || true\n        sleep 2\n    fi\n\n    # 清理残留\n    rm -f \"$pid_file\" \"$socket_path\"\n    echo \"✓ Stopped\"\n}\n\n# 重启应用\nrestart_app() {\n    echo \"Restarting Gunicorn...\"\n    stop_app\n    sleep 1\n    start_app\n}\n\n# 入口函数\nmain() {\n    # 检查 Gunicorn 是否存在\n    if [ ! -f \"$script_dir/venv/bin/gunicorn\" ]; then\n        echo \"ERROR: Gunicorn not found at $script_dir/venv/bin/gunicorn\"\n        echo \"Hint: Did you activate virtualenv? (source venv/bin/activate)\"\n        exit 1\n    fi\n\n    local action=${1:-start}  # 默认动作：start\n\n    case \"$action\" in\n        start)\n            start_app\n            ;;\n        stop)\n            stop_app\n            ;;\n        restart)\n            restart_app\n            ;;\n        status)\n            is_running\n            ;;\n        cron-check)\n            # 专为 crontab 设计：只检查+重启，不输出干扰日志\n            if ! is_running &gt; /dev/null 2&gt;&amp;1; then\n                echo \"[$(date '+%F %T')] CRON: Gunicorn down, auto-restarting...\" &gt;&gt; \"$log_file\"\n                start_app &gt;&gt; \"$log_file\" 2&gt;&amp;1\n            fi\n            ;;\n        *)\n            echo \"Usage: $0 {start|stop|restart|status|cron-check}\"\n            echo \"  cron-check: Silent mode for crontab (logs to app.log only)\"\n            exit 1\n            ;;\n    esac\n}\n\nmain \"$@\"\n</code></pre>\n<p>手动运行测试</p>\n<pre><code class=\"language-bash\">bash app_ctl.sh start\n</code></pre>\n<p>配置 crontab</p>\n<pre><code># 编辑当前用户 crontab\ncrontab -e\n\n# 添加以下行（每分钟检查一次）\n* * * * * /opt/myflaskapp/app_ctl.sh cron-check &gt;/dev/null 2&gt;&amp;1\n</code></pre>\n<p>配置logrotate</p>\n<pre><code># /etc/logrotate.d/myflaskapp\n/opt/myflaskapp/app.log {\n    daily\n    rotate 7\n    compress\n    delaycompress\n    missingok\n    notifempty\n    copytruncate  # 避免 Gunicorn 丢失文件句柄\n}\n</code></pre>\n<h3 id=\"systemd-推荐生产环境使用\">systemd (推荐生产环境使用)</h3>\n<ol>\n<li>创建 systemd 服务文件</li>\n</ol>\n<pre><code class=\"language-bash\">sudo vim /etc/systemd/system/myflaskapp.service\n</code></pre>\n<ol start=\"2\">\n<li>示例如下</li>\n</ol>\n<pre><code class=\"language-ini\">[Unit]\nDescription=Gunicorn instance for Flask App\nAfter=network.target\n\n[Service]\nUser=www-data\nGroup=www-data\nWorkingDirectory=/path/to/your/app\nEnvironment=\"PATH=/path/to/venv/bin\"\nExecStart=/path/to/venv/bin/gunicorn \\\n          --workers 4 \\\n          --bind unix:/run/myapp.sock \\\n          --access-logfile - \\\n          --error-logfile - \\\n          wsgi:app\n\n# 禁止添加 --daemon！systemd 需直接监控主进程\nRestart=on-failure        # 仅异常退出时重启（非0状态码、被信号杀死等）\nRestartSec=5s             # 重启前等待5秒\nStartLimitInterval=60s    # 60秒内\nStartLimitBurst=5         # 最多重启5次，防雪崩\nTimeoutStopSec=30         # 停止时等待30秒（优雅关闭）\n\n# 安全加固\nPrivateTmp=true\nNoNewPrivileges=true\nProtectSystem=strict\nReadWritePaths=/run /var/log/myapp\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<ol start=\"3\">\n<li>设置开机自启并启动服务</li>\n</ol>\n<pre><code class=\"language-bash\">sudo systemctl daemon-reload\nsudo systemctl enable myflaskapp    # 开机自启\nsudo systemctl start myflaskapp\n</code></pre>\n<p>可以试试用<code>kill -9</code>停止后端服务进程，观察能否被重新拉起。</p>\n<blockquote>\n<p>注意，<code>kill -15</code>算是正常停止，不算异常退出。</p>\n</blockquote>\n<h2 id=\"docker-部署方案\">Docker 部署方案</h2>\n<ol>\n<li>Dockerfile。Python 项目通常不需要多阶段构建，单阶段即可。</li>\n</ol>\n<pre><code class=\"language-dockerfile\">FROM python:3.11-slim-bookworm\n\n# 安全加固\n## 创建非 root 用户（避免使用 nobody，权限太受限）\nRUN useradd -m -u 1000 appuser &amp;&amp; \\\n    # 安装运行时必需的系统库（非编译工具）\n    apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n        libgomp1 \\\n        libpq5 \\\n        libsqlite3-0 \\\n        &amp;&amp; rm -rf /var/lib/apt/lists/* \\\n        &amp;&amp; apt-get autoremove -y \\\n        &amp;&amp; apt-get clean\n\n# Python 优化\nENV PYTHONUNBUFFERED=1 \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1\n\nWORKDIR /app\n\n# 利用 Docker 层缓存：先复制 requirements\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --prefer-binary -r requirements.txt \\\n    # 清理 pip 缓存（虽然 --no-cache-dir 已禁用，但保险起见）\n    &amp;&amp; rm -rf /root/.cache\n\n# 应用代码\nCOPY --chown=appuser:appuser . .\n\n# 使用非root用户运行\nUSER appuser\n\n# 启动\nEXPOSE 8000\nCMD [\"gunicorn\", \"--config\", \"config/gunicorn.conf.py\", \"wsgi:app\"]\n</code></pre>\n<ol start=\"2\">\n<li>编写 docker-compose.yaml</li>\n</ol>\n<pre><code class=\"language-yaml\">services:\n  web:\n    image: myflaskapp:latest\n    container_name: flask_web\n    # 端口映射\n    ## 如果 nginx 也使用 Docker 部署，而且使用同一个网络配置，则可以不做端口映射\n    ports:\n      - \"8000:8000\"\n    # 环境变量\n    environment:\n      - FLASK_ENV=production\n      - DATABASE_URL=postgresql://user:pass@db:5432/mydb\n      - REDIS_URL=redis://redis:6379/0\n    # 健康检查\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s      # 每 30 秒检查一次\n      timeout: 5s        # 超时 5 秒\n      start_period: 15s  # 启动后 15 秒开始检查（给应用初始化时间）\n      retries: 3         # 失败重试 3 次后标记 unhealthy\n    \n    # 自动重启策略\n    restart: unless-stopped  # always / on-failure / unless-stopped\n    \n    # 资源限制\n    deploy:\n      resources:\n        limits:\n          cpus: '2'        # 最多 2 个 CPU\n          memory: 1G       # 最多 1GB 内存\n        reservations:\n          cpus: '0.5'      # 保留 0.5 个 CPU\n          memory: 256M     # 保留 256MB 内存\n    \n    # ulimit 限制（防资源滥用）\n    ulimits:\n      nproc: 65535       # 最大进程数\n      nofile:\n        soft: 65535      # 打开文件数软限制\n        hard: 65535      # 打开文件数硬限制\n      core: 0            # 禁止 core dump\n    \n    # 安全加固\n    security_opt:\n      - no-new-privileges:true  # 禁止提权\n    \n    # 只读文件系统（除 /tmp 外）\n    read_only: true\n    tmpfs:\n      - /tmp:rw,noexec,nosuid,size=100m\n    \n    # 卷挂载（日志、临时文件）\n    volumes:\n      - ./logs:/app/logs:rw\n      # - ./static:/app/static:ro  # 静态文件（可选）\n    \n    # 网络\n    networks:\n      - app-network\n        \n# 网络配置\nnetworks:\n  app-network:\n    driver: bridge\n\n# 卷配置\nvolumes:\n  db_data:\n    driver: local\n  redis_data:\n    driver: local\n</code></pre>\n<h2 id=\"kubernetes-部署方案\">Kubernetes 部署方案</h2>\n<h3 id=\"deployment\">Deployment</h3>\n<pre><code class=\"language-yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-app\n  namespace: default\n  labels:\n    app: flask-app\n    tier: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: flask-app\n  template:\n    metadata:\n      labels:\n        app: flask-app\n        tier: backend\n    spec:\n      securityContext:\n        runAsNonRoot: true      # 禁止 root 运行\n        runAsUser: 1000         # 使用非 root 用户\n        runAsGroup: 1000\n        fsGroup: 1000\n        seccompProfile:\n          type: RuntimeDefault  # 启用 seccomp 安全策略\n      containers:\n      - name: flask-app\n        image: myregistry.com/myflaskapp:1.0.0\n        imagePullPolicy: IfNotPresent  # 生产环境建议用 Always\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        env:\n        - name: FLASK_ENV\n          value: \"production\"\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: flask-app-secrets\n              key: database-url\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: flask-app-secrets\n              key: redis-url\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: flask-app-secrets\n              key: secret-key\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"   # 超过会 OOM Kill\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n            scheme: HTTP\n          initialDelaySeconds: 30  # 启动后 30 秒开始检查\n          periodSeconds: 10        # 每 10 秒检查一次\n          timeoutSeconds: 3        # 超时 3 秒\n          successThreshold: 1\n          failureThreshold: 3      # 失败 3 次后重启容器\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n            scheme: HTTP\n          initialDelaySeconds: 10  # 启动后 10 秒开始检查\n          periodSeconds: 5         # 每 5 秒检查一次\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 3      # 失败 3 次后从 Service 移除\n        startupProbe:\n          httpGet:\n            path: /health\n            port: 8000\n            scheme: HTTP\n          failureThreshold: 30     # 最多重试 30 次\n          periodSeconds: 5         # 每 5 秒一次，共 150 秒容忍慢启动\n          timeoutSeconds: 3\n        securityContext:\n          allowPrivilegeEscalation: false  # 禁止提权\n          readOnlyRootFilesystem: true     # 根文件系统只读\n          capabilities:\n            drop:\n            - ALL                          # 删除所有 Linux capabilities\n          privileged: false\n        volumeMounts:\n        - name: tmp-volume\n          mountPath: /tmp\n        - name: config-volume\n          mountPath: /app/config\n          readOnly: true\n      imagePullSecrets:\n      - name: registry-secret  # 如果使用私有镜像仓库\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - flask-app\n              topologyKey: kubernetes.io/hostname  # 避免所有 Pod 调度到同一节点\n      volumes:\n      - name: tmp-volume\n        emptyDir:\n          medium: Memory  # 使用内存卷，更快\n          sizeLimit: 100Mi\n      - name: config-volume\n        configMap:\n          name: flask-app-config\n</code></pre>\n<h3 id=\"service\">Service</h3>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: flask-app-service\n  namespace: default\n  labels:\n    app: flask-app\n    tier: backend\nspec:\n  type: ClusterIP\n  selector:\n    app: flask-app\n  ports:\n  - name: http\n    port: 80        # Service 端口\n    targetPort: 8000  # Pod 端口\n    protocol: TCP\n</code></pre>\n<h3 id=\"ingress-nginx\">ingress-nginx</h3>\n<pre><code class=\"language-yaml\">apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: flask-app-ingress\n  namespace: default\n  annotations:\n    # ==================== Nginx 配置 ====================\n    kubernetes.io/ingress.class: \"nginx\"\n    \n    # 启用 HTTPS 重定向\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    \n    # 限流（每秒 10 个请求，突发 20）\n    nginx.ingress.kubernetes.io/limit-rps: \"10\"\n    nginx.ingress.kubernetes.io/limit-burst-multiplier: \"2\"\n    \n    # 客户端真实 IP\n    nginx.ingress.kubernetes.io/enable-real-ip: \"true\"\n    nginx.ingress.kubernetes.io/proxy-real-ip-cidr: \"0.0.0.0/0\"\n    \n    # 连接超时\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: \"60\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"60\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"60\"\n    \n    # 缓冲区大小\n    nginx.ingress.kubernetes.io/proxy-buffering: \"on\"\n    nginx.ingress.kubernetes.io/proxy-buffer-size: \"16k\"\n    nginx.ingress.kubernetes.io/proxy-buffers-number: \"4\"\n    \n    # Gzip 压缩\n    nginx.ingress.kubernetes.io/enable-gzip: \"true\"\n    nginx.ingress.kubernetes.io/gzip-level: \"6\"\n    nginx.ingress.kubernetes.io/gzip-min-length: \"1024\"\n    nginx.ingress.kubernetes.io/gzip-types: \"text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript\"\n    \n    # 安全头\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      add_header X-Frame-Options \"SAMEORIGIN\" always;\n      add_header X-Content-Type-Options \"nosniff\" always;\n      add_header X-XSS-Protection \"1; mode=block\" always;\n      add_header Referrer-Policy \"strict-origin-when-cross-origin\" always;\n    \n    # 认证\n    # nginx.ingress.kubernetes.io/auth-type: basic\n    # nginx.ingress.kubernetes.io/auth-secret: flask-app-basic-auth\n    # nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\n    \n    # 自定义错误页面\n    # nginx.ingress.kubernetes.io/custom-http-errors: \"404,500,502,503,504\"\n    # nginx.ingress.kubernetes.io/default-backend: custom-error-pages\n    \n    # 重写目标\n    # nginx.ingress.kubernetes.io/rewrite-target: /$1\n    \n    # WAF（如果安装了 ModSecurity）\n    # nginx.ingress.kubernetes.io/enable-modsecurity: \"true\"\n    # nginx.ingress.kubernetes.io/modsecurity-snippet: |\n    #   SecRuleEngine On\n    #   SecRequestBodyAccess On\n\nspec:\n  tls:\n  - hosts:\n    - flask.example.com\n    secretName: flask-app-tls-secret  # TLS 证书 Secret\n\n  rules:\n  - host: flask.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: flask-app-service\n            port:\n              number: 80\n</code></pre>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/XY-Heruo/\" target=\"_blank\">花酒锄作田</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/XY-Heruo/p/19615176\" target=\"_blank\">https://www.cnblogs.com/XY-Heruo/p/19615176</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 13:19</span>&nbsp;\n<a href=\"https://www.cnblogs.com/XY-Heruo\">花酒锄作田</a>&nbsp;\n阅读(<span id=\"post_view_count\">69</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "“Fatal error: require(): Failed opening required...” 以及如何彻底避免它再次出现",
      "link": "https://www.cnblogs.com/catchadmin/p/19614589",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/catchadmin/p/19614589\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 09:22\">\n    <span>“Fatal error: require(): Failed opening required...” 以及如何彻底避免它再次出现</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"fatal-error-require-failed-opening-required-以及如何彻底避免它再次出现\">“Fatal error: require(): Failed opening required...” 以及如何彻底避免它再次出现</h1>\n<p>凌晨两点，值班告警响了。生产环境 API 开始报 500，而且只出现在新扩容的节点上。你打开日志，熟悉又刺眼的报错跳了出来：</p>\n<p>本地一切正常，测试环境也没问题。但在云原生部署这种“环境随时变化”的现实里，一个看起来不起眼的路径差异，就足以把服务直接打趴。</p>\n<p>这并不是什么“新手失误”，而是很多人对 PHP 最基础能力——文件加载机制——理解不够深入导致的系统性问题。</p>\n<p>早期 PHP 时代，我们把 <code>include</code> 和 <code>require</code> 当积木用来拼页面。到了 PHP 8.2+、Composer、容器化微服务的今天，这组函数仍然在引擎核心位置。但现实中，很多开发者依旧把它们当成“设完就不用管”的工具。</p>\n<p>如果你想从“写脚本”走向“做稳定系统”，就必须搞清楚：当一个文件被加载进另一个文件时，底层到底发生了什么。</p>\n<p>这篇文章会从运行机制、线上常见坑和工程实践三层，讲清楚怎样把 PHP 文件加载写到足够稳。</p>\n<h2 id=\"底层到底在发生什么\">底层到底在发生什么？</h2>\n<p>当你执行 <code>include 'file.php'</code>，并不是“复制粘贴代码”这么简单。PHP 实际上会让当前执行流程暂停，切换到目标文件，把它编译为操作码，再在当前作用域里执行。</p>\n<h3 id=\"文件加载的四种形式\">文件加载的四种形式</h3>\n<p>PHP 有四种主加载方式，它们不是语法糖，而是行为差异：</p>\n<ul>\n<li><code>include</code>：温和模式。文件不存在时抛 <code>Warning</code>，脚本继续执行。</li>\n<li><code>require</code>：强制模式。文件不存在时直接致命错误并中断执行。</li>\n<li><code>include_once</code> / <code>require_once</code>：在前两者基础上增加“是否已加载”检查，避免重复声明。</li>\n</ul>\n<p>理解这个差异非常关键：在现代业务系统里，很多核心依赖一旦缺失，不应该“带伤继续跑”。</p>\n<h3 id=\"一个更实用的心智模型作用域注入器\">一个更实用的心智模型：作用域注入器</h3>\n<p>可以把文件加载理解成“作用域注入器”：</p>\n<ul>\n<li>在函数内部 <code>include</code>，被加载文件里定义的变量只在该函数作用域可见。</li>\n<li>在脚本顶层 <code>include</code>，变量会进入全局作用域。</li>\n</ul>\n<p>另外，很多人误判性能瓶颈。真正重的通常不是代码执行本身，而是文件状态检查（stat 调用）：</p>\n<p>每次 <code>include</code>，PHP 都要向操作系统确认：文件是否存在、权限是否可读、最后修改时间等。在高并发 API 中，这个动作每秒成千上万次时，开销会非常明显。</p>\n<h2 id=\"php-是如何解析路径的\">PHP 是如何解析路径的</h2>\n<p>当你写 <code>include 'utils.php';</code> 这种相对路径时，PHP 会依次尝试：</p>\n<ul>\n<li>当前脚本目录</li>\n<li><code>php.ini</code> 中 <code>include_path</code> 指定的目录</li>\n<li>当前工作目录（cwd）</li>\n</ul>\n<p>问题就出在这里：它有环境依赖。</p>\n<p>比如你的命令行任务进程工作目录是 <code>/var/www/</code>，而 Web 进程工作目录是 <code>/var/www/public/</code>，同一行相对路径代码可能一个能跑、一个直接崩。</p>\n<h2 id=\"最容易把线上搞崩的-5-类错误\">最容易把线上搞崩的 5 类错误</h2>\n<p>这些是我在遗留项目重构里反复见到的高频问题。</p>\n<h3 id=\"相对路径陷阱\">相对路径陷阱</h3>\n<p><strong>错误写法</strong>：<code>include 'includes/header.php';</code></p>\n<p><strong>为什么会发生</strong>：本地启动目录刚好是项目根目录，所以一直“看起来正常”。</p>\n<p><strong>线上后果</strong>：一旦被子目录调用、被定时任务调用，或者入口目录变了，路径上下文就变了。这是“我本地没问题”类事故的头号来源。</p>\n<h3 id=\"_once-的性能税\"><code>_once</code> 的性能税</h3>\n<p><strong>错误写法</strong>：在高频循环里大量使用 <code>require_once</code>。</p>\n<p><strong>为什么会发生</strong>：担心 <code>Cannot redeclare class</code> 之类的重复声明。</p>\n<p><strong>线上后果</strong>：每次 <code>_once</code> 都会触发已加载表检查。PHP 8 虽然优化了很多，但它依然比直 <code>require</code> 慢。依赖关系清晰的模块化系统，不该长期依赖引擎“二次确认”。</p>\n<h3 id=\"用--把报错静音\">用 <code>@</code> 把报错静音</h3>\n<p><strong>错误写法</strong>：<code>@include 'optional_config.php';</code></p>\n<p><strong>为什么会发生</strong>：想省掉 <code>if (file_exists(...))</code> 的显式判断。</p>\n<p><strong>线上后果</strong>：你把真正问题藏起来了。文件读取失败可能不是“文件不存在”，而是权限不对（如 <code>chmod</code>）。报错被吃掉后，排障时间会从 5 分钟拉到几小时。</p>\n<h3 id=\"动态-include-引发路径穿越\">动态 include 引发路径穿越</h3>\n<p><strong>错误写法</strong>：<code>include $_GET['page'] . '.php';</code></p>\n<p><strong>为什么会发生</strong>：图省事做“动态路由”。</p>\n<p><strong>线上后果</strong>：严重安全风险。攻击者可构造 <code>../../../../etc/passwd</code>，或利用 <code>php://filter/...</code> 读取敏感配置。即使关闭远程 URL 加载，本地文件同样会被攻击。</p>\n<h3 id=\"加载带副作用的文件\">加载带副作用的文件</h3>\n<p><strong>错误写法</strong>：一个文件既定义类，又直接执行逻辑（输出 HTML、连数据库等）。</p>\n<p><strong>为什么会发生</strong>：历史代码里职责边界没分清。</p>\n<p><strong>线上后果</strong>：测试几乎没法写。你只是想测试类定义，却被迫触发数据库连接和页面输出。</p>\n<h2 id=\"正确做法php-8\">正确做法（PHP 8+）</h2>\n<p>在现代项目里，类加载通常由 Composer + PSR-4 自动加载处理，<code>include</code>/<code>require</code> 更多用于配置、模板和少量模块逻辑。</p>\n<p>但即便如此，也建议守住下面三条。</p>\n<h3 id=\"始终使用绝对锚点路径\">始终使用绝对锚点路径</h3>\n<p>把路径固定在已知根上。<code>__DIR__</code> 永远指向“当前文件所在目录”，不会随工作目录变化。</p>\n<p><strong>错误示例（脆弱）</strong></p>\n<pre><code class=\"language-php\">&lt;?php\n// 如果从 public/ 目录启动，这里可能失败\nrequire 'config/settings.php';\n</code></pre>\n<p><strong>正确示例（稳定）</strong></p>\n<pre><code class=\"language-php\">&lt;?php\n// 无论从哪里调用，都能稳定解析\nrequire __DIR__ . '/config/settings.php';\n</code></pre>\n<h3 id=\"善用加载返回值\">善用加载返回值</h3>\n<p>这是 PHP 里经常被忽略但非常实用的能力：被加载文件可以 <code>return</code> 值。</p>\n<p><code>config.php</code></p>\n<pre><code class=\"language-php\">&lt;?php\nreturn [\n    'db' =&gt; [\n        'host' =&gt; '127.0.0.1',\n        'pass' =&gt; $_ENV['DB_PASS'] ?? 'root',\n    ],\n    'debug' =&gt; false,\n];\n</code></pre>\n<p><code>app.php</code></p>\n<pre><code class=\"language-php\">&lt;?php\n$config = require __DIR__ . '/config.php';\n// $config 是局部变量，不污染全局\n</code></pre>\n<h3 id=\"关键组件要做防御式加载\">关键组件要做防御式加载</h3>\n<p>对于必须存在的文件，不要依赖默认报错，自己把预期写清楚。</p>\n<pre><code class=\"language-php\">&lt;?php\n$templatePath = __DIR__ . '/views/header.php';\nif (!file_exists($templatePath)) {\n    throw new \\RuntimeException(\"关键视图组件缺失: {$templatePath}\");\n}\nrequire $templatePath;\n</code></pre>\n<h2 id=\"生产环境注意点扩缩容与安全\">生产环境注意点：扩缩容与安全</h2>\n<p>当系统从单机走到容器集群或函数计算，文件加载不再只是代码细节，而是基础设施问题。</p>\n<h3 id=\"安全路径穿越防护\">安全：路径穿越防护</h3>\n<p>很多“PHP 不安全”的印象，本质是加载策略不安全。</p>\n<ul>\n<li><strong>白名单（Allow-list）</strong>：绝不直接信任用户输入拼路径。</li>\n<li><strong><code>basename()</code></strong>：确实需要用输入值时，先做路径片段清洗，拦截 <code>../</code> 穿越。</li>\n<li><strong><code>open_basedir</code></strong>：在 <code>php.ini</code> 限制 PHP 可访问路径范围，防止越界读取。</li>\n</ul>\n<h3 id=\"性能opcache-是基础设施而不是可选项\">性能：OPcache 是基础设施而不是可选项</h3>\n<p>生产环境应开启 OPcache。它会把预编译后的字节码放内存，避免每次请求重复解析文件。</p>\n<p><strong>部署提示</strong>：在高并发集群中可以考虑 <code>opcache.validate_timestamps=0</code>，换取更快加载速度；但这意味着每次发布都必须做平滑重载，否则代码更新不会生效。</p>\n<h3 id=\"可观测性失败必须可追踪\">可观测性：失败必须可追踪</h3>\n<p>文件加载失败不应只留下一个“白屏”或 500。</p>\n<ul>\n<li><strong>可追踪信息</strong>：日志至少要包含 <code>include_path</code> 与 <code>cwd</code>。</li>\n<li><strong>监控策略</strong>：对 <code>E_COMPILE_ERROR</code> 做专门告警，这类问题通常与发布或环境差异有关，需优先回滚。</li>\n</ul>\n<h3 id=\"部署形态差异容器-vs-函数计算\">部署形态差异（容器 vs 函数计算）</h3>\n<p>容器镜像里文件路径通常固定可预测；函数计算环境常见只读文件系统、目录映射变化。统一使用 <code>__DIR__</code> 能显著降低环境差异带来的路径问题。</p>\n<h2 id=\"真实事故空配置幽灵\">真实事故：\"空配置\"幽灵</h2>\n<p>我曾参与排查过一个支付业务事故：后台任务随机失败。问题根因是他们用 <code>include</code> 加载环境配置。</p>\n<p>某次发布脚本漏拷了生产配置文件。因为是 <code>include</code>，进程没有崩，业务继续跑，只是拿到一个空的 <code>$config</code>。</p>\n<p>结果是任务带着空 API 密钥连续运行了 6 小时，造成大量交易失败。</p>\n<p>如果当时使用的是 <code>require</code>，任务会第一时间中断并触发告警，损失会小得多。</p>\n<p>一句话：<strong>没有它系统就不能活，那就必须 <code>require</code>。</strong></p>\n<h2 id=\"排障清单看到-failed-opening-required-时直接照做\">排障清单（看到 Failed opening required 时直接照做）</h2>\n<ol>\n<li>\n<p><strong>打印绝对路径</strong>：<br />\n<code>var_dump(realpath(__DIR__ . '/your-file.php'));</code><br />\n若返回 <code>false</code>，说明文件根本不在你以为的位置。</p>\n</li>\n<li>\n<p><strong>确认运行身份</strong>：<br />\n<code>echo exec('whoami');</code><br />\n看当前系统用户是否有读权限。</p>\n</li>\n<li>\n<p><strong>排查隐藏语法错误</strong>：<br />\n某些文件不是“不存在”，而是语法错误导致加载失败。<br />\n用命令行执行：<code>php -l filename.php</code>。</p>\n</li>\n<li>\n<p><strong>检查 PHP 开始标签</strong>：<br />\n文件应以 <code>&lt;?php</code> 开头。若短标签关闭而你写了 <code>&lt;?</code>，后续可能出现各种诡异问题（如 header 已发送）。</p>\n</li>\n</ol>\n<h2 id=\"更专业的加载封装示例\">更专业的加载封装示例</h2>\n<p>不要长期依赖裸 <code>var_dump</code>。建议用结构化日志和统一包装。</p>\n<pre><code class=\"language-php\">&lt;?php\n/**\n * 带可观测性的文件加载器\n * 开发环境要“响亮失败”，生产环境可控降级。\n */\nfunction load_component(string $filePath, array $context = []): mixed\n{\n    $absolutePath = realpath($filePath);\n    if (!$absolutePath || !file_exists($absolutePath)) {\n        error_log(sprintf(\n            \"[FileLoader] Failure: %s | CWD: %s | User: %s\",\n            $filePath,\n            getcwd(),\n            get_current_user()\n        ));\n\n        if (getenv('APP_DEBUG') === 'true') {\n            throw new \\Exception(\"组件不存在: {$filePath}\");\n        }\n\n        return null; // 生产环境按约定降级\n    }\n\n    extract($context);\n    return require $absolutePath;\n}\n</code></pre>\n<h2 id=\"常见问题\">常见问题</h2>\n<h3 id=\"qrequire_once-一定比-require-更好吗\">Q：<code>require_once</code> 一定比 <code>require</code> 更好吗？</h3>\n<p>不一定。<code>require_once</code> 更像是组织不清晰时的安全网。依赖关系明确、自动加载健全时，<code>require</code> 更直接、性能更好。</p>\n<h3 id=\"q可以根据数据库值动态-include-文件吗\">Q：可以根据数据库值动态 include 文件吗？</h3>\n<p>可以，但必须非常谨慎。推荐白名单映射：数据库只存 ID，代码里把 ID 映射到固定路径，不要把路径原文存进数据库后直接加载。</p>\n<h3 id=\"q加载大文件会拖慢应用吗\">Q：加载大文件会拖慢应用吗？</h3>\n<p>开启 OPcache 后，首次之后基本没有“解析”成本；但文件中的业务逻辑仍要执行，依旧消耗 CPU 和内存。文件内容要聚焦，避免把大量无关逻辑塞在一起。</p>\n<h3 id=\"q模板文件适合用-include-吗\">Q：模板文件适合用 <code>include</code> 吗？</h3>\n<p>小项目可以。中大型系统建议使用成熟模板方案，能在安全性和复用性上更稳。</p>\n<h2 id=\"结语\">结语</h2>\n<p>把 <code>include</code> 和 <code>require</code> 用好，不只是语法问题，而是工程能力问题。</p>\n<p>你的代码运行在操作系统、权限模型、缓存机制和部署流水线共同构成的环境里。只理解“本地能跑”，远远不够。</p>\n<h3 id=\"最佳实践小结\">最佳实践小结</h3>\n<ul>\n<li><strong>快速失败</strong>：关键依赖统一使用 <code>require</code>。</li>\n<li><strong>路径绝对化</strong>：避免相对路径，优先 <code>__DIR__</code>。</li>\n<li><strong>作用域收敛</strong>：用 <code>return</code> 返回配置，避免全局变量污染。</li>\n<li><strong>失败可观测</strong>：把加载失败当成一类关键系统事件处理。</li>\n</ul>\n<h3 id=\"你的下一步\">你的下一步</h3>\n<p>现在就打开项目，全局搜索 <code>include</code> / <code>require</code>：</p>\n<p>凡是不以 <code>__DIR__</code> 或统一根路径常量开头的，今天就改。</p>\n<p>这一步做完，你的生产环境就会少一类高概率事故。<br />\n<a href=\"https://catchadmin.com/post/2026-02/fatal-error-require-failed-opening-required\" rel=\"noopener nofollow\" target=\"_blank\">Fatal error: require(): Failed opening required...”—以及如何彻底避免它再次出现</a></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 09:22</span>&nbsp;\n<a href=\"https://www.cnblogs.com/catchadmin\">JaguarJack</a>&nbsp;\n阅读(<span id=\"post_view_count\">49</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "[拆解LangChain执行引擎] ManagedValue——一种特殊的只读虚拟通道",
      "link": "https://www.cnblogs.com/jaydenai/p/19614333/managed-value",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jaydenai/p/19614333/managed-value\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 07:57\">\n    <span>[拆解LangChain执行引擎] ManagedValue——一种特殊的只读虚拟通道</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        我们一直在强调Pregel对象的状态是通过`Channel`维护和传递的，其实承载传递状态功能的组件除了Channel，还有  `ManagedValue`，我们可以将ManagedValue视为虚拟Channel。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>我们一直在强调Pregel对象的状态是通过<code>Channel</code>维护和传递的，其实承载传递状态功能的组件除了Channel，还有  <code>ManagedValue</code>。我们可以将ManagedValue视为虚拟Channel，Node不仅采用与读取Channel完全一样的方式读取ManagedValue，而且注册的ManagedValue也直接存放在Pregel的channels字段中。</p>\n<p>如果我们仔细查看Pregel类的定义，可以看出其<code>channels</code>字段返回一个字典，字典的值的类型联合了BaseChannel和<code>ManagedValueSpec</code>两种类型，前者是Channel的基类，后者就是<code>ManagedValue</code>类的别名。</p>\n<pre><code class=\"language-python\">class Pregel(\n    PregelProtocol[StateT, ContextT, InputT, OutputT],\n    Generic[StateT, ContextT, InputT, OutputT]): \n    channels : dict[str, BaseChannel | ManagedValueSpec]\n\nManagedValueSpec = type[ManagedValue]\n</code></pre>\n<p>如果说Channel存储的是的业务状态，那么ManagedValue传递的就是Pregel这个执行引擎的运行时状态。一般来说，ManagedValue自身不负责存储状态，其提供的值可以实时计算得出，所以它不参与基于Checkpoint的持久化。从如下所示的代码片段可以看出，ManagedValue仅仅定义了一个唯一的静态抽象方法<code>get</code>返回对应的值，由于作为输入的<code>PregelScratchpad</code>对象提供的信息有限，所以ManagedValue能够发挥的空间其实很有限，在大部分情况下用不到它。</p>\n<pre><code class=\"language-python\">class ManagedValue(ABC, Generic[V]):\n    @staticmethod\n    @abstractmethod\n    def get(scratchpad: PregelScratchpad) -&gt; V: ...\n</code></pre>\n<h2 id=\"1-pregelscratchpad\">1. PregelScratchpad</h2>\n<p>ManagedValue提供的值是通过其get方法根据PregelScratchpad对象计算所得。当确定后续待执行的Node后，引擎会为每个Node创建一个任务，每个任务都会附加一个PregelScratchpad对象。PregelScratchpad的<code>step</code>和<code>stop</code>字段就返回当前Superstep的序号和针对迭代的限制（最大超步数），其它字段与持久化有关。</p>\n<pre><code class=\"language-python\">@dataclasses.dataclass(**_DC_KWARGS)\nclass PregelScratchpad:\n    step : int\n    stop : int\n    call_counter : Callable[[], int]\n    interrupt_counter : Callable[[], int]\n    get_null_resume\t: Callable[[bool], Any]\n    resume : list[Any]\n    subgraph_counter\t: Callable[[], int]\t\n</code></pre>\n<p>PregelScratchpad的<code>call_counter</code>、<code>interrupt_counter</code>和<code>subgraph_counter</code>字段以闭包的形式返回一个计数器。<code>call_counter</code>计数器用于为当前Superstep内产生的所有任务分配唯一的内部序列号。</p>\n<h3 id=\"11-resume-value和中断计数器\">1.1 Resume Value和中断计数器</h3>\n<p><code>interrupt_counter</code>、<code>get_null_resume</code>和<code>resume</code>字段与Pregel基于 “中断（Interrupt）/恢复（Resume）” 的执行方式有关。假设Pregel的对应一个需要人工介入的多级审批流程，在每次需要以人工介入的方式收集审批者决定的时候，流程进入一个中断，当前的状态被持久化。当审批决定给出后，流程以 “恢复” 的形式开始执行，中断时持久化的快照被提取出来 “恢复现场” ，审批决定以Resume Value的形式提供给引擎。为了匹配多个中断点与对应的Resume Value，后者会按照顺序被持久化，并在恢复执行的时候连同当前提供的Resume Value一并填充到PregelScratchpad的<code>resume</code>列表中。</p>\n<p>恢复执行做不到在中断点出开始执行，它总是<code>从头执行</code>Node的处理函数，所以定义 <code>幂等Node</code> 应该成为Agent编程的 “金科玉律”。由于PregelScratchpad的resume字段会按照中断的顺序存放Resume Value，所以在恢复执行的时候，每遇到一个中断，引擎可以利用<code>interrupt_counter</code>字段返回的计数器作为位置索引从resume列表中将匹配的Resume Value提取出来。如果提取的Resume Value为None，或者计数器返回的索引越界，<code>get_null_resume</code>字段提供的回调就会执行。这个回调函数具有一个bool类型的参数is_called，调用时该参数被设置为True，表示该中断确实被触发了，但没有对应的数据。这会消耗掉这个中断位，确保流程不至于永远得不到恢复。</p>\n<h3 id=\"12-子图调用计数器\">1.2 子图调用计数器</h3>\n<p>如果说<code>interrupt_counter</code>计数器旨在解决每次中断与提供的Resume Value的匹配问题，那么<code>subgraph_counter</code>计数器解决的每次“子图调用”与对应Pregel实例的匹配问题。如果站在“图”的视角，每个Pregel对象就是由多个Node组成的图，而Pregel也可以作为一个Node出现在另一个Pregel构建的图中，两个Pregel之间就称为了“父子”关系，子Pregel构建的图就是“子图”，针对它的调用就是子图调用。</p>\n<p>虽然在同一个图中，每个Pregel会独自完成自身的持久化。在恢复执行场景中，引擎会率先加载作为“根”的Pregel对应的Checkpoint来恢复现场。当遇到“子图”形式调用另一个Pregel时，引擎会加载对应的Checkpoint来恢复子图在中断那个时间点的状态。现在问题来了：在子Pregel众多持久化的Checkpoint中，怎么知道该加载哪一个呢？</p>\n<p>这个问题本质上是如何解决作为子图执行的Pregel在执行持久化时，如何将生成的Checkpoint与当前执行上下文进行匹配的问题，这个问题是利用<code>Checkpoint命名空间</code>来解决的。Node是以任务的形式被执行的，每个任务具有唯一的ID，并且在恢复时保持不变，如果命名空间由执行链路上每个任务的<code>节点名称+任务ID</code>组成，那么子图的Checkpoint就能利用此命名空间关联起来。</p>\n<p>但是问题还是没有完全解决，如果同一个任务涉及针对<code>同一子图的多次调用</code>，如命名空间只包含基于任务的执行路径，此时两个子图会共享相同的命名空间，具体对应哪个Checkpoint依然无法解决。因此若涉及同一个Node针对同一个Pregel对象的多次调用，持久化这个Pregel的Checkpoint的命名空间还应该包含<code>调用顺序</code>。</p>\n<p>Checkpoint的命名空间的规则可以通过如下这个演示实例来证实。如代码片段所示，我们创建了一个由单一Node组成的Pregel对象（sub_graph），命名为 “baz” 的Node在执行的时候会从当前的RunnableConfig配置中提取并输出当前的Checkpoint命名空间。</p>\n<pre><code class=\"language-python\">from langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.channels import LastValue\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.pregel._write import ChannelWrite, ChannelWriteTupleEntry\nfrom langgraph.types import RunnableConfig\nfrom typing import Any\n\ndef handle(args:dict[str,Any], config:RunnableConfig)-&gt;None:\n    print(config[\"configurable\"][\"checkpoint_ns\"])\nsub_node = (NodeBuilder()\n    .subscribe_to(\"start\")\n    .do(handle))\nsub_graph = Pregel(\n    nodes={\"baz\": sub_node},\n    channels={\n        \"start\": LastValue(None),\n    },\n    input_channels=[\"start\"],\n    output_channels=[])\n\ndef handle1(args:dict[str,Any])-&gt;None:\n    sub_graph.invoke(input={\"start\": None})\n\ndef handle2(args:dict[str,Any])-&gt;str:    \n    sub_graph.invoke(input={\"start\": None})    \n    sub_graph.invoke(input={\"start\": None})\n\nfoo = (NodeBuilder()\n    .subscribe_to(\"foo\")\n    .do(handle1)\n    .write_to(bar=None))\nbar = (NodeBuilder()\n    .subscribe_to(\"bar\")\n    .do(handle2))\n\ngraph = Pregel(\n    nodes={\"foo\": foo, \"bar\": bar},\n    channels={\n        \"foo\": LastValue(None),\n        \"bar\": LastValue(str),\n    },\n    input_channels=[\"foo\"],\n    output_channels=[],\n    checkpointer= InMemorySaver())\n\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\ngraph.invoke(input={\"foo\": None}, config=config)\n</code></pre>\n<p>在另一个Pregel中，我们为它设置了两个先后执行的Node（foo和bar），前者调用sub_graph一次，后者调用两次。针对三次调用，sub_graph为自身持久化设置的Checkpoint命名会以如下的形式输出，可以看出命名空间同时体现了调用链路和次序。</p>\n<pre><code>foo:36817c76-c3f7-643f-7924-0d29b39f469a|baz:311cc911-96a0-56b6-225b-28e4cece7cd9\nbar:97be6a71-1b71-7364-e691-a122cfef1a92|baz:789287de-869f-42b8-dd03-7518820daaa6\nbar:97be6a71-1b71-7364-e691-a122cfef1a92|1|baz:dd1ddd1b-fc62-b46a-c2ec-6a1d8344b793\n</code></pre>\n<p>基于Pregel“中断/恢复”的执行方式，让我们对<code>Pregel实例</code>会有特别的理解。我们习惯了将一个通过调用某个类构造函数创建的对象视为该类型的一个实例，但是在Node的处理函数中，即使针对<code>同一Pregel实例</code>的连续两次调用都有可能出现中断，一旦恢复执行，后一个实例就有可能使根据另一个Checkpoint的状态创建的，它自然也就不是原来的那个实例了。在不断的“中断/恢复”执行流程中，所谓<code>Pregel实例</code>有时候表示成<code>对应的Checkpoint</code>可能更准确。</p>\n<p>对于同一个节点任务来说，如果涉及针对同一个<code>子Pregel</code>的多次调用，从第二次调用开始，对方持久化生成的Checkpoint会将<code>调用次序</code>包含在命名空间中。与之相对的，在恢复执行的时候，也需要根据当前的执行上下文提供包含此序号的命名空间采用加载对应的Checkpoint，并最终恢复对应的Pregel对象，PregelScratchpad的subgraph_counter字段返回的计数器就是为了提供这个序号。</p>\n<h2 id=\"2-两个原生的managedvalue\">2. 两个原生的ManagedValue</h2>\n<p>由于ManagedValue所能提供的值是根据PregelScratchpad计算生成，而后者可用的唯有表示当前和最大Superstep序号的<code>step</code>和<code>stop</code>字段，所以我们采用ManagedValue的应用场景其实很窄。我从只找到如下两个原生的ManagedValue类型，它们都定义在langgraph.managed.is_last_step这个包中。其中一个<code>IsLastStepManager</code>用于判断是否为最后一个Superstep，而<code>RemainingStepsManager</code>则用来确定余下的Superstep数。具体的实现非常简单，仅仅是针对PregelScratchpad的<code>step</code>和<code>stop</code>字段的简单运算而已。</p>\n<pre><code class=\"language-python\">class IsLastStepManager(ManagedValue[bool]):\n    @staticmethod\n    def get(scratchpad: PregelScratchpad) -&gt; bool:\n        return scratchpad.step == scratchpad.stop - 1\n\nclass RemainingStepsManager(ManagedValue[int]):\n    @staticmethod\n    def get(scratchpad: PregelScratchpad) -&gt; int:\n        return scratchpad.stop - scratchpad.step\n</code></pre>\n<p>由于ManagedValue属于一个<code>计算属性</code>，所以它只能作为Node的输入。它可以被视为一种虚拟的Channel，Node针对ManagedValue和常规Channel的读取方式完全一致。在创建Pregel对象时，所用到的ManagedValue需要在<code>channels</code>字段中显式声明，但是不能将其添加到输入和输出Channel列表中。</p>\n<p>如下的实例演示了RemainingStepsManager的使用方式，创建的Pregel由两个先后执行的Node构成（foo和bar），它们会将命名为<code>remaining_steps</code>的ManagedValue作为输入，并将其分别输出到<code>remaining_steps_after_foo</code>和<code>remaining_steps_after_bar</code>这两个Channel中，分别表示在这两个Node完成执行后所剩的Superstep数。</p>\n<pre><code class=\"language-python\">from langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.managed.is_last_step import RemainingStepsManager\nfrom langgraph.channels import LastValue\n\nfoo = (NodeBuilder()\n       .subscribe_to(\"foo\")\n       .read_from(\"remaining_steps\")\n       .do(lambda args: args[\"remaining_steps\"])\n       .write_to(remaining_steps_after_foo= lambda args:args, bar=None))\n\nbar = (NodeBuilder()\n       .subscribe_to(\"bar\")\n       .read_from(\"remaining_steps\")\n       .do(lambda args: args[\"remaining_steps\"])\n       .write_to(\"remaining_steps_after_bar\"))\n\napp = Pregel(\n    nodes={\"foo\":foo, \"bar\":bar},\n    channels={\n        \"foo\": LastValue(None),\n        \"bar\":LastValue(None),\n        \"remaining_steps_after_foo\": LastValue(int),\n        \"remaining_steps_after_bar\":LastValue(int),\n        \"remaining_steps\": RemainingStepsManager, \n    },\n    input_channels=[\"foo\"],\n    output_channels=[\"remaining_steps_after_foo\", \"remaining_steps_after_bar\"])\n\nconfig = {\"recursion_limit\": 10}\nresult = app.invoke({\"foo\":None}, config=config)\nassert result[\"remaining_steps_after_foo\"] == 10\nassert result[\"remaining_steps_after_bar\"] == 9\n</code></pre>\n<p>在根据两个Node创建Pregel对象时，我们将针对命名为<code>remaining_steps</code>的ManagedValue的声明添加到channels字段中，对应的类型被设置为RemainingStepsManager。由于在调用Pregel对象时利用RunnableConfig配置将Superstep迭代限制为10，所以先后执行的两个Node后剩余步数分别为10和9。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 07:57</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jaydenai\">JaydenAI</a>&nbsp;\n阅读(<span id=\"post_view_count\">40</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}