{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "用 LangChain 驱动本地 Ollama 模型",
      "link": "https://www.cnblogs.com/bugshare/p/19598089",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/bugshare/p/19598089\" id=\"cb_post_title_url\" title=\"发布于 2026-02-10 09:33\">\n    <span>用 LangChain 驱动本地 Ollama 模型</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>这两年，大模型几乎成了开发者的“标配工具”：<br />\n写代码、查资料、做总结、当智能助手。</p>\n<p>但你有没有认真想过一个问题：</p>\n<blockquote>\n<p><strong>我们真的必须把所有请求都发到云端 API 吗？</strong></p>\n</blockquote>\n<p>随着模型体积持续下降、硬件性能快速提升，以及 Ollama 这类工具逐渐成熟，<br />\n<strong>本地运行大模型</strong>，已经从早期的“极客尝鲜”，演进为一种<strong>可以在真实项目中落地的工程方案</strong>。</p>\n<p>这篇文章，我们就来完整走一遍：</p>\n<blockquote>\n<p><strong>如何使用 LangChain，基于最新 Runnable API，调用本地启动的 Ollama 模型，构建一个真正可用的本地大模型应用。</strong></p>\n</blockquote>\n<hr />\n<h1 id=\"一为什么选择-langchain--ollama\">一、为什么选择 LangChain + Ollama？</h1>\n<p>先给结论：</p>\n<blockquote>\n<p><strong>Ollama 解决“模型怎么跑”，LangChain 解决“能力怎么用”。</strong></p>\n</blockquote>\n<p>这是目前本地大模型场景中，<strong>最自然、最稳定的一种组合方式</strong>。</p>\n<hr />\n<h2 id=\"1️⃣-ollama本地大模型的docker\">1️⃣ Ollama：本地大模型的“Docker”</h2>\n<p>你可以把 Ollama 理解为：<br />\n<strong>专门为大模型设计的一层运行时基础设施。</strong></p>\n<p>它解决的问题非常聚焦：</p>\n<ul>\n<li>统一模型的下载、管理与启动</li>\n<li>对外提供标准化 HTTP API（默认端口 <code>11434</code>）</li>\n<li>支持 LLaMA、Qwen、Mistral、DeepSeek 等主流模型</li>\n<li>Mac / Linux / Windows 全平台可用</li>\n<li>天然适合 Docker / 私有化部署</li>\n</ul>\n<p>一句话总结：</p>\n<blockquote>\n<p><strong>Ollama 把“跑模型”这件事，做成了基础设施能力。</strong></p>\n</blockquote>\n<hr />\n<h2 id=\"2️⃣-langchainai-应用的控制中心\">2️⃣ LangChain：AI 应用的“控制中心”</h2>\n<p>如果你只是想“问一句、回一句”，直接调 Ollama API 当然也没问题。<br />\n但一旦进入真实工程场景，需求会迅速复杂化：</p>\n<ul>\n<li>Prompt 如何复用、版本化？</li>\n<li>对话上下文如何管理？</li>\n<li>如何组合多步推理？</li>\n<li>后续怎么接 RAG、Agent、工具调用？</li>\n</ul>\n<p>这些正是 LangChain 擅长的事情：</p>\n<ul>\n<li>Prompt 模板与结构化输入</li>\n<li>Runnable / LCEL 编排能力</li>\n<li>对话历史（Memory）管理</li>\n<li>Tool、RAG、Agent 的统一抽象</li>\n<li>可自然演进到 LangGraph</li>\n</ul>\n<p>所以一个非常自然的分工是：</p>\n<blockquote>\n<p><strong>LangChain 负责“编排与逻辑”，Ollama 负责“模型与算力”。</strong></p>\n</blockquote>\n<hr />\n<h1 id=\"二准备工作本地启动-ollama-模型\">二、准备工作：本地启动 Ollama 模型</h1>\n<h2 id=\"1️⃣-使用-docker-部署-ollama推荐\">1️⃣ 使用 Docker 部署 Ollama（推荐）</h2>\n<pre><code class=\"language-bash\">docker run \\\n-d \\\n--restart=always \\\n--name ollama \\\n--gpus=all \\\n-p 11434:11434 \\\n-v /home/data/ollama:/root/.ollama \\\nollama/ollama\n</code></pre>\n<blockquote>\n<p>如果你对部署细节感兴趣，可以参考我之前的文章：</p>\n<ul>\n<li>《如何使用 Ollama 打造你的本地 AI 助手》</li>\n<li>《为本地部署的大模型添加 API Key 认证：Nginx 实现方案》</li>\n</ul>\n</blockquote>\n<hr />\n<h2 id=\"2️⃣-拉取并运行模型\">2️⃣ 拉取并运行模型</h2>\n<p>以 <code>qwen3:8b</code> 为例：</p>\n<pre><code class=\"language-bash\">ollama pull qwen3:8b\n</code></pre>\n<p>简单测试：</p>\n<pre><code class=\"language-bash\">ollama run qwen3:8b\n</code></pre>\n<p>如果可以正常对话，说明模型已经在本地成功运行。</p>\n<hr />\n<h1 id=\"三langchain-接入本地-ollamaopenai-协议\">三、LangChain 接入本地 Ollama（OpenAI 协议）</h1>\n<p>接下来进入核心部分：<br />\n<strong>如何用 LangChain 调用本地 Ollama？</strong></p>\n<hr />\n<h2 id=\"1️⃣-安装依赖\">1️⃣ 安装依赖</h2>\n<pre><code class=\"language-bash\">pip install langchain langchain-openai\n</code></pre>\n<p>这里我们使用 <strong>OpenAI 兼容协议</strong>，这是目前最稳定、生态最完整的一种方式。</p>\n<hr />\n<h2 id=\"2️⃣-创建-ollama-llmchatopenai\">2️⃣ 创建 Ollama LLM（ChatOpenAI）</h2>\n<pre><code class=\"language-python\">from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    name=\"ollama-ai\",\n    model=\"qwen3:8b\",\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"your api key\",\n    temperature=0.7,\n    timeout=300,\n)\n</code></pre>\n<p>几个关键点说明：</p>\n<ul>\n<li><code>model</code> 必须与 Ollama 中的模型名称一致</li>\n<li><code>base_url</code> 指向 Ollama，并注意使用 <code>/v1</code> 后缀</li>\n<li>这里使用的是 <strong>OpenAI 标准协议</strong>，不是 Ollama 私有 API</li>\n</ul>\n<hr />\n<h2 id=\"3️⃣-最简单的一次调用\">3️⃣ 最简单的一次调用</h2>\n<pre><code class=\"language-python\">response = llm.invoke(\"用一句话解释什么是 LangChain\")\nprint(response)\n</code></pre>\n<p>到这里，你已经完成了：</p>\n<blockquote>\n<p><strong>LangChain → 本地 Ollama → 本地大模型</strong></p>\n</blockquote>\n<p>这条完整调用链。</p>\n<hr />\n<h1 id=\"四进阶用法prompt--runnablelcel\">四、进阶用法：Prompt + Runnable（LCEL）</h1>\n<p>在真实项目中，几乎不会直接“裸调”模型。</p>\n<hr />\n<h2 id=\"1️⃣-prompttemplate\">1️⃣ PromptTemplate</h2>\n<pre><code class=\"language-python\">from langchain_core.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"你是一个资深后端工程师，请用简洁、专业的语言回答：{question}\",\n)\n</code></pre>\n<hr />\n<h2 id=\"2️⃣-输出解析stroutputparser\">2️⃣ 输出解析（StrOutputParser）</h2>\n<pre><code class=\"language-python\">from langchain_core.output_parsers import StrOutputParser\n\nparser = StrOutputParser()\n</code></pre>\n<p>显式的输出解析，是 LangChain 新 API 的重要特征：</p>\n<ul>\n<li>输出类型清晰</li>\n<li>便于后续切换为 JSON / Pydantic</li>\n<li>更适合工程化</li>\n</ul>\n<hr />\n<h2 id=\"3️⃣-runnable-组合推荐写法\">3️⃣ Runnable 组合（推荐写法）</h2>\n<pre><code class=\"language-python\">chain = prompt | llm | parser\n\nresponse = chain.invoke({\n    \"question\": \"为什么本地部署大模型越来越流行？\"\n})\nprint(response)\n</code></pre>\n<p>这就是 LangChain 当前主推的 <strong>LCEL（表达式）写法</strong>，<br />\n比早期的 <code>LLMChain</code> 更透明、也更可组合。</p>\n<hr />\n<h1 id=\"五加入-memory真正的本地对话能力\">五、加入 Memory：真正的本地对话能力</h1>\n<p>⚠️ <strong>一个非常重要的变化</strong>：</p>\n<p>在新的 Runnable 体系中，<br />\n<strong>Memory 不再是 Chain 的“隐藏参数”，而是显式的状态管理。</strong></p>\n<hr />\n<h2 id=\"1️⃣-定义对话历史存储\">1️⃣ 定义对话历史存储</h2>\n<pre><code class=\"language-python\">from langchain_core.chat_history import InMemoryChatMessageHistory\n\nstore = {}\n\ndef get_session_history(session_id: str):\n    if session_id not in store:\n        store[session_id] = InMemoryChatMessageHistory()\n    return store[session_id]\n</code></pre>\n<hr />\n<h2 id=\"2️⃣-prompt-显式消费-history关键\">2️⃣ Prompt 显式消费 history（关键）</h2>\n<pre><code class=\"language-python\">from langchain_core.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"question\"],\n    template=\"\"\"\n         你是一个资深后端工程师。\n\n         以下是之前的对话历史：\n         {history}\n\n         当前用户问题：\n         {question}\n\n         请基于上下文给出连贯、准确的回答。\n    \"\"\".strip()\n)\n</code></pre>\n<blockquote>\n<p>这是很多人第一次使用 RunnableWithMessageHistory 时最容易忽略的一点：<br />\n<strong>历史是否生效，取决于 Prompt 是否显式使用 <code>{history}</code>。</strong></p>\n</blockquote>\n<hr />\n<h2 id=\"3️⃣-构建带记忆的-runnable\">3️⃣ 构建带记忆的 Runnable</h2>\n<pre><code class=\"language-python\">from langchain_core.runnables.history import RunnableWithMessageHistory\n\nchain = prompt | llm | parser\n\nchat_chain = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n</code></pre>\n<hr />\n<h2 id=\"4️⃣-调用带-session_id\">4️⃣ 调用（带 session_id）</h2>\n<pre><code class=\"language-python\">config = {\"configurable\": {\"session_id\": \"local-chat\"}}\n\nprint(chat_chain.invoke(\n    {\"question\": \"什么是 Ollama？\"},\n    config=config\n))\n\nprint(chat_chain.invoke(\n    {\"question\": \"它和 LangChain 有什么关系？\"},\n    config=config\n))\n</code></pre>\n<p>到这里，你已经拥有了一个：</p>\n<ul>\n<li>支持上下文</li>\n<li>完全本地</li>\n<li>状态可控</li>\n</ul>\n<p>的对话系统。</p>\n<p>而且 <strong>所有数据都只存在你的本地机器上</strong>。</p>\n<hr />\n<h1 id=\"六这套方案适合谁\">六、这套方案适合谁？</h1>\n<p>非常适合：</p>\n<ul>\n<li>✅ 本地工具 / 桌面应用</li>\n<li>✅ 内部知识库 / 私有 RAG</li>\n<li>✅ 研发辅助工具（代码、文档、SQL）</li>\n<li>✅ 对数据安全敏感的企业场景</li>\n<li>✅ 学习大模型工程化的开发者</li>\n</ul>\n<p>不太适合：</p>\n<ul>\n<li>❌ 超大并发场景</li>\n<li>❌ 极限性能 / 超大模型</li>\n<li>❌ 面向公网的 C 端产品</li>\n</ul>\n<hr />\n<h1 id=\"七一些来自实践的工程建议\">七、一些来自实践的工程建议</h1>\n<p>最后分享几点真实踩坑后的经验：</p>\n<ol>\n<li>\n<p><strong>模型别贪大</strong></p>\n<ul>\n<li>7B / 8B 是当前本地部署的性价比甜点位</li>\n</ul>\n</li>\n<li>\n<p><strong>Prompt 比模型更重要</strong></p>\n<ul>\n<li>本地模型对 Prompt 非常敏感</li>\n</ul>\n</li>\n<li>\n<p><strong>LangChain 要“模块化使用”</strong></p>\n<ul>\n<li>Prompt / LLM / Parser / Memory 明确分层</li>\n</ul>\n</li>\n<li>\n<p><strong>Memory 要可演进</strong></p>\n<ul>\n<li>InMemory → Redis → 数据库 → Checkpointer</li>\n</ul>\n</li>\n<li>\n<p><strong>Ollama 非常适合私有化场景</strong></p>\n<ul>\n<li>Docker + 内网 + 权限控制，工程成本极低</li>\n</ul>\n</li>\n</ol>\n<hr />\n<h1 id=\"结语\">结语</h1>\n<p>过去一年，我们讨论最多的问题是：</p>\n<blockquote>\n<p><em>“该用哪个云端大模型？”</em></p>\n</blockquote>\n<p>而现在，越来越多开发者开始认真思考：</p>\n<blockquote>\n<p><strong>“哪些能力，其实可以放回本地？”</strong></p>\n</blockquote>\n<p>LangChain + Ollama 并不是为了“替代云”，<br />\n而是为我们提供了一个：</p>\n<blockquote>\n<p><strong>真正可控、可组合、可落地的本地大模型方案。</strong></p>\n</blockquote>\n<p>如果你正在做：</p>\n<ul>\n<li>本地 AI 工具</li>\n<li>私有化大模型</li>\n<li>Agent / RAG 工程实践</li>\n</ul>\n<p>那么这套组合，<strong>非常值得一试。</strong></p>\n<hr />\n<p>如果你觉得这篇文章对你有帮助，欢迎 <strong>点赞 / 转发 / 收藏</strong>。<br />\n下一篇，我会继续分享 <strong>LangGraph 在本地大模型场景下的实战用法</strong>。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-10 09:33</span>&nbsp;\n<a href=\"https://www.cnblogs.com/bugshare\">BugShare</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "追更 HelloGitHub 一整年，终于等到了这篇年度盘点",
      "link": "https://www.cnblogs.com/xueweihan/p/19596455",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xueweihan/p/19596455\" id=\"cb_post_title_url\" title=\"发布于 2026-02-10 08:23\">\n    <span>追更 HelloGitHub 一整年，终于等到了这篇年度盘点</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>转眼一年又过去了，春节将至 HelloGitHub 也迎来了一年一度的年度盘点时刻。</p>\n<p>接下来，就让我们一起来看看，都有哪些开源项目入选了《2025 年 HelloGitHub 年度热门开源项目》。<strong>需要说明的是</strong>，入选并不是按照 Star 增长或“出圈”程度作为唯一标准，而是更看重来自 HelloGitHub 社区用户的真实反馈——阅读、点赞、收藏与评论等。</p>\n<p>因此大家为自己喜欢的开源项目「点赞」十分重要，每一次互动都可能让你喜欢的开源项目被更多人看到。</p>\n<p>同时为了满足不同读者对各类开源项目的喜好，我还整理了 40 个开源项目（每类精选 Top3），量大管饱总会有你喜欢的。所以我将文章内容分为了 <strong>年度十佳</strong> 和 <strong>分类精选</strong> 两个部分，方便大家速览和按需阅读。</p>\n<ol>\n<li>年度十佳：HelloGitHub 最受欢迎的 10 个开源项目</li>\n<li>分类精选：根据 C/C++、C#、Go、Java、JavaScript、移动端、Python、Rust、人工智能、书籍/教程、其它等类别整理</li>\n</ol>\n<p>话不多说，开始沉浸式享受这份来自 HelloGitHub 的春节礼物吧～</p>\n<p></p><div class=\"toc\"><div class=\"toc-container-header\">目录</div><ul><li><a href=\"#一年度十佳\" rel=\"noopener nofollow\">一、年度十佳</a><ul><li><a href=\"#1跨平台可爱的互动桌宠应用\" rel=\"noopener nofollow\">1、跨平台可爱的互动桌宠应用</a></li><li><a href=\"#2开箱即用的游戏变速器\" rel=\"noopener nofollow\">2、开箱即用的游戏变速器</a></li><li><a href=\"#3沉浸式的电子书阅读器\" rel=\"noopener nofollow\">3、沉浸式的电子书阅读器</a></li><li><a href=\"#4每日-60-秒资讯-api-集合\" rel=\"noopener nofollow\">4、每日 60 秒资讯 API 集合</a></li><li><a href=\"#5易用的跨平台开源聊天应用\" rel=\"noopener nofollow\">5、易用的跨平台开源聊天应用</a></li><li><a href=\"#6全开源的实时资讯平台\" rel=\"noopener nofollow\">6、全开源的实时资讯平台</a></li><li><a href=\"#7用-excel-手搓各种-ai-算法\" rel=\"noopener nofollow\">7、用 Excel 手搓各种 AI 算法</a></li><li><a href=\"#8从零开始训练小型语言模型\" rel=\"noopener nofollow\">8、从零开始训练小型语言模型</a></li><li><a href=\"#9开源圆角等宽字体\" rel=\"noopener nofollow\">9、开源圆角等宽字体</a></li><li><a href=\"#10deepseek-开源的混合专家模型\" rel=\"noopener nofollow\">10、DeepSeek 开源的混合专家模型</a></li></ul></li><li><a href=\"#二分类精选\" rel=\"noopener nofollow\">二、分类精选</a><ul><li><a href=\"#cc-项目\" rel=\"noopener nofollow\">C/C++ 项目</a></li><li><a href=\"#c-项目\" rel=\"noopener nofollow\">C# 项目</a></li><li><a href=\"#go-项目\" rel=\"noopener nofollow\">Go 项目</a></li><li><a href=\"#java-项目\" rel=\"noopener nofollow\">Java 项目</a></li><li><a href=\"#javascript-项目\" rel=\"noopener nofollow\">JavaScript 项目</a></li><li><a href=\"#客户端项目\" rel=\"noopener nofollow\">客户端项目</a></li><li><a href=\"#python-项目\" rel=\"noopener nofollow\">Python 项目</a></li><li><a href=\"#rust-项目\" rel=\"noopener nofollow\">Rust 项目</a></li><li><a href=\"#人工智能\" rel=\"noopener nofollow\">人工智能</a></li><li><a href=\"#书籍教程\" rel=\"noopener nofollow\">书籍/教程</a></li><li><a href=\"#其它\" rel=\"noopener nofollow\">其它</a></li></ul></li><li><a href=\"#三最后\" rel=\"noopener nofollow\">三、最后</a></li></ul></div><p></p>\n<h2 id=\"一年度十佳\">一、年度十佳</h2>\n<p>这里是 HelloGitHub 2025 年度最受欢迎的 10 个开源项目，筛选和排序是综合了用户的浏览、点赞、收藏和评论等数据，所以它们是来自 HelloGitHub 社区小伙伴们的选择！</p>\n<h3 id=\"1跨平台可爱的互动桌宠应用\">1、跨平台可爱的互动桌宠应用</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>这是一款开源的跨平台桌面宠物应用，以可爱的猫咪形象陪伴你的每一次键盘与鼠标操作。它基于 Tauri 构建，支持 macOS、Windows 和 Linux，无论你使用哪种系统，都能“领养”这只可爱猫咪，让你的桌面充满乐趣。</p>\n<p><strong>用户评价</strong>：好用，在养了。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/ayangweb/BongoCat\" rel=\"noopener nofollow\" target=\"_blank\">github.com/ayangweb/BongoCat</a></p>\n</blockquote>\n<h3 id=\"2开箱即用的游戏变速器\">2、开箱即用的游戏变速器</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>这是一款完全免费、开源的 Windows 游戏加速工具。它通过 Hook 系统时间函数，实现对游戏速度的灵活调节，并提供简单易用的界面，兼容多种单机游戏。请勿用于网络游戏，以免导致账号被封！</p>\n<p><strong>用户评价</strong>：很好用，很可以。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/game1024/OpenSpeedy\" rel=\"noopener nofollow\" target=\"_blank\">github.com/game1024/OpenSpeedy</a></p>\n</blockquote>\n<h3 id=\"3沉浸式的电子书阅读器\">3、沉浸式的电子书阅读器</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>这是一款为热爱阅读的用户量身打造的阅读软件，将极简设计与强大功能融合，为你带来专注、沉浸的阅读体验。它基于 Next.js 和 Tauri 开发，支持跨平台运行，现已支持 macOS、Windows、Linux、Android、iOS 和 Web 全平台覆盖。</p>\n<p><strong>用户评价</strong>：好用，特别是在线版，流畅丝滑。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/readest/readest\" rel=\"noopener nofollow\" target=\"_blank\">github.com/readest/readest</a></p>\n</blockquote>\n<h3 id=\"4每日-60-秒资讯-api-集合\">4、每日 60 秒资讯 API 集合</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>该项目集合了包括每日新闻、实时票房、汇率、热搜榜、随机段子等多种数据的 API 服务。</p>\n<p><strong>用户评价</strong>：好用，种类挺多的，支持。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/vikiboss/60s\" rel=\"noopener nofollow\" target=\"_blank\">github.com/vikiboss/60s</a></p>\n</blockquote>\n<h3 id=\"5易用的跨平台开源聊天应用\">5、易用的跨平台开源聊天应用</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>这是一款专为多端打造的现代化即时通讯系统，实现了从桌面到移动平台的无缝通讯体验。它基于 Tauri、Vite 6、Vue 3 和 TypeScript 构建，支持一对一私聊、群组聊天、消息撤回和@提醒等功能，适用于 Windows、macOS、Linux、iOS 和 Android 等多种操作系统。</p>\n<p><strong>用户评价</strong>：只是在文章中多看了一眼，就被惊艳到了。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/HuLaSpark/HuLa\" rel=\"noopener nofollow\" target=\"_blank\">github.com/HuLaSpark/HuLa</a></p>\n</blockquote>\n<h3 id=\"6全开源的实时资讯平台\">6、全开源的实时资讯平台</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>这是一款高颜值的实时新闻与热榜网站，相比传统热榜产品更加注重实时新闻。它前后端完全开源、部署简单，可轻松托管到 Cloudflare Pages 或 Vercel 等平台。</p>\n<p><strong>用户评价</strong>：太棒了，一览所有新闻热榜。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/ourongxing/newsnow\" rel=\"noopener nofollow\" target=\"_blank\">github.com/ourongxing/newsnow</a></p>\n</blockquote>\n<h3 id=\"7用-excel-手搓各种-ai-算法\">7、用 Excel 手搓各种 AI 算法</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>该项目是通过 Excel 的形式实现并演示人工智能与深度学习的核心算法和概念，让初学者可以动手操作并理解 AI 的运行原理，包括矩阵乘法、MLP、RNN、Transformer、ResNet 等，以独特且浅显易懂的形式，降低了 AI 学习的门槛。</p>\n<p><strong>用户评价</strong>：有中文文档就好了。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/ImagineAILab/ai-by-hand-excel\" rel=\"noopener nofollow\" target=\"_blank\">github.com/ImagineAILab/ai-by-hand-excel</a></p>\n</blockquote>\n<h3 id=\"8从零开始训练小型语言模型\">8、从零开始训练小型语言模型</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>这不仅是一个微型语言模型的实现，更是一份入门 LLM 的教程，旨在降低学习和上手 LLM 的门槛 。它提供了从数据预处理到模型训练、微调和推理的全流程代码和教程。最小模型仅 0.02B 参数，可在普通 GPU 上轻松运行。</p>\n<p><strong>用户评价</strong>：试了一遍不错。降低参数，连我这个入门显卡都能跑。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/jingyaogong/minimind\" rel=\"noopener nofollow\" target=\"_blank\">github.com/jingyaogong/minimind</a></p>\n</blockquote>\n<h3 id=\"9开源圆角等宽字体\">9、开源圆角等宽字体</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>这是一款高质量的等宽字体，具备可变字体、圆角设计、智能连字、图标等特性，支持简体中文、繁体中文和日语字符集，以及通过脚本自定义构建字体，满足不同平台和个性化需求。</p>\n<p><strong>用户评价</strong>：真的很好用！！很好看而且等宽太舒服了！最舒服的是连写，更贴近自然语言！</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/subframe7536/maple-font\" rel=\"noopener nofollow\" target=\"_blank\">github.com/subframe7536/maple-font</a></p>\n</blockquote>\n<h3 id=\"10deepseek-开源的混合专家模型\">10、DeepSeek 开源的混合专家模型</h3>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>这是一个基于混合专家（MoE）和多头潜在注意力（MLA）架构的开源大语言模型，在数学推理、代码生成等复杂任务中表现优秀。该模型总规模达 671B 参数，但每个 token 只激活其中的 37B 参数。即在处理输入时，并非所有“专家”都参与计算，而是选择一部分专家进行处理。通过激活部分参数（37B）完成计算，从而降低了训练和推理的成本。</p>\n<p><strong>用户评价</strong>：国产之光厉害。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<blockquote>\n<p>GitHub 地址→<a href=\"https://github.com/deepseek-ai/DeepSeek-V3\" rel=\"noopener nofollow\" target=\"_blank\">github.com/deepseek-ai/DeepSeek-V3</a></p>\n</blockquote>\n<h2 id=\"二分类精选\">二、分类精选</h2>\n<p>如果上面的<strong>年度十佳</strong>，没有你喜欢的开源项目，没关系！</p>\n<pre><code>========================================\n📊 2025年度数据概览\n========================================\n发布项目数     : 588\n总浏览量(UV)  : 1,253,290\n总收藏数      : 4,462\n总评论数      : 650\n总投票数      : 5,936\n</code></pre>\n<p>下面我从其余的 500 多个项目里，按照 11 个分类整理精选出共 40 个开源项目，大部分分类只取 Top3，方便大家阅读。</p>\n<h3 id=\"cc-项目\">C/C++ 项目</h3>\n<p>1、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/vladelaina/Catime\" rel=\"noopener nofollow\" target=\"_blank\">Catime</a>：简洁小巧的 Windows 计时器。这是一款轻巧易用的 Windows 计时器，集显示时间、倒计时和番茄时钟功能于一体。它采用 C 语言编写，体积小、占用少，支持透明界面。用户可以调整界面大小和位置，设置超时动作（锁屏/通知/关机），以及自定义颜色、字体和托盘动画，满足个性化需求。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>2、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/HIllya51/LunaTranslator\" rel=\"noopener nofollow\" target=\"_blank\">LunaTranslator</a>：开源的视觉小说翻译工具。这是一款专为 Windows 平台设计的视觉小说翻译器，支持 HOOK、OCR、剪贴板等多种文本提取方式，可灵活切换，并提供在线翻译、离线翻译、语音合成等功能。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>3、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/zhongyang219/MusicPlayer2\" rel=\"noopener nofollow\" target=\"_blank\">MusicPlayer2</a>：多功能的 Windows 音乐播放器。该项目是一款由 C++ 编写的简约、小巧、多功能的 Windows 本地音乐播放器，支持常见音频格式，并提供歌词显示、卡拉 OK 模式、歌词编辑、专辑封面展示、频谱分析和音效调节等功能。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"c-项目\">C# 项目</h3>\n<p>4、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/ClassIsland/ClassIsland\" rel=\"noopener nofollow\" target=\"_blank\">ClassIsland</a>：抬头即见的开源课表工具。这是一款专为大屏设备打造的桌面课表应用，可将课程表以简洁组件的形式常驻桌面，取代传统黑板课表。支持下课提醒、天气信息、倒计时、密码保护和课表导入等功能，适用于配备教室多媒体大屏、投影仪或智慧黑板的教室。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>5、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/w4po/ExplorerTabUtility\" rel=\"noopener nofollow\" target=\"_blank\">ExplorerTabUtility</a>：Windows 文件管理多标签扩展工具。这是一款专为 Windows 11 打造的文件资源管理器增强工具，能够自动将多个窗口合并为单窗口多标签页模式。支持路径去重、标签搜索、批量打开/关闭/还原等功能，轻松告别桌面窗口杂乱的烦恼。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>6、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/duplicati/duplicati\" rel=\"noopener nofollow\" target=\"_blank\">duplicati</a>：跨平台的多云安全备份工具。这是一款跨平台的备份客户端，支持多种主流云存储服务（如 S3、Dropbox、Google Drive 等）。它可将加密、增量、压缩的备份文件安全地存储到云端或服务器，适用于个人隐私数据的云备份。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"go-项目\">Go 项目</h3>\n<p>7、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/lin-snow/Ech0\" rel=\"noopener nofollow\" target=\"_blank\">Ech0</a>：清爽的轻量级内容分享平台。这是一款开源、自托管的轻量级内容发布平台，专注于思想流动和快速分享。它拥有简洁直观的操作界面，支持发布和分享想法、文字、图片和链接。同时，支持类似 ActivityPub 的联邦协议，实现不同实例（站点）之间的互联互通，让内容不再局限于单一孤立的网站。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>8、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/mayswind/ezbookkeeping\" rel=\"noopener nofollow\" target=\"_blank\">ezbookkeeping</a>：轻松自托管你的个人财务数据。这是一款免费、轻量、可自托管的个人记账应用，采用 Go+Vue 构建。它界面简洁易用、功能丰富，支持二级账户（个人/家庭）、收支分类、交易图片附件、定期收支自动记账等功能。借助 PWA 技术，可一键添加到手机主屏幕，体验媲美原生 App。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>9、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/tbphp/gpt-load\" rel=\"noopener nofollow\" target=\"_blank\">gpt-load</a>：企业级的多渠道大模型 API 管理平台。这是一款用 Go 语言开发的企业级大模型接口管理平台，支持 OpenAI、Gemini、Claude 等多种服务。它开箱即用、内置 Web 管理界面、保留原生 API 格式，支持密钥自动轮询、故障切换和水平扩展，专为高并发生产环境而设计。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"java-项目\">Java 项目</h3>\n<p>10、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/nagisa77/OpenIsle\" rel=\"noopener nofollow\" target=\"_blank\">OpenIsle</a>：Java 开发的轻量级开源社区系统。这是一个基于 Spring Boot 和 Vue3 构建的开源自由社区系统，定位为轻量级的 Discourse。它完全开源、可二次开发，支持白名单邀请、自定义标签、实时通知等功能。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>11、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/booklore-app/booklore\" rel=\"noopener nofollow\" target=\"_blank\">booklore</a>：Java 开发的个人数字图书馆。这是一款开源、自托管的电子书管理 Web 应用，支持 PDF 和 ePub 电子书格式。它采用 Java（Spring Boot）+ Angular 开发，支持自动获取书籍信息、分享书籍、阅读进度同步、多用户管理等功能。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>12、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/synthetichealth/synthea\" rel=\"noopener nofollow\" target=\"_blank\">synthea</a>：模拟患者人群的开源工具。这是一款开源的合成患者数据和健康记录的模拟器，支持生成病历、症状、诊断、药物、疫苗接种记录等多种医疗信息数据，适用于临床研究、流行病学研究等场景。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"javascript-项目\">JavaScript 项目</h3>\n<p>13、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/zyronon/TypeWords\" rel=\"noopener nofollow\" target=\"_blank\">TypeWords</a>：极简的打字背单词网站。这是一款基于网页的背单词软件，帮助用户通过键盘输入来记忆单词。它界面简洁、交互流畅，支持单词发音、错误统计和生词本等功能。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>14、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/plait-board/drawnix\" rel=\"noopener nofollow\" target=\"_blank\">drawnix</a>：极简的在线白板工具。这是一款免费、开源的在线白板工具。它提供一个无限画布，支持自由绘制、思维导图、流程图、画笔、插入图片、自动保存等功能，以及移动端适配、Docker 部署和插件机制等特性。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>15、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/algerkong/AlgerMusicPlayer\" rel=\"noopener nofollow\" target=\"_blank\">AlgerMusicPlayer</a>：简约美观的音乐播放器。这是一款基于 Electron 开发的第三方音乐播放器，拥有高颜值的界面和丰富的功能，支持本地化服务、桌面歌词显示、音乐下载等，适用于多平台使用。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>16、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/hexianWeb/CubeCity\" rel=\"noopener nofollow\" target=\"_blank\">CubeCity</a>：卡通风格城市建设模拟游戏。这是一款轻量级、卡通风格的 2.5D 城市模拟游戏，基于 Three.js 和 Vue3 构建。玩家可在浏览器中通过点选和拖放，实时建造、搬迁和拆除建筑。建筑会自动产出金币，可用于新建或升级设施。游戏融合了环境、社会与治理（ESG）理念，城市规划需兼顾多元需求，才能打造出可持续发展的理想城市。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>17、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/fuma-nama/fumadocs\" rel=\"noopener nofollow\" target=\"_blank\">fumadocs</a>：用 Next.js 打造现代化的文档网站。该项目是基于 Next.js 构建的文档网站框架，专为快速创建现代化、高颜值、功能丰富的文档网站而设计。它完全采用 ESM 格式，紧跟前端技术潮流，支持内容集合、MDX、Contentlayer、TailwindCSS 等技术，适用于项目文档、API 文档、开发指南等多种文档需求。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"客户端项目\">客户端项目</h3>\n<p>18、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/saber-notes/saber\" rel=\"noopener nofollow\" target=\"_blank\">saber</a>：更懂你的手写笔记应用。这是一款开源的手写笔记应用，支持 Android、iOS、Windows、macOS、Linux 等平台。它提供夜间模式、多行公式高亮、密码保护等功能，适用于记录课堂笔记和整理工作思路等场景。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>19、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/kylecorry31/Trail-Sense\" rel=\"noopener nofollow\" target=\"_blank\">Trail-Sense</a>：野外生存必备 Android 应用。这是一款专为徒步、露营、野外生存等场景设计的开源 Android 应用。它利用手机的传感器，提供离线导航、日落提醒、照片地图、路径追踪等实用功能，所有功能均可在无网络环境下使用。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>20、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/Tosencen/XMSLEEP\" rel=\"noopener nofollow\" target=\"_blank\">XMSLEEP</a>：开源的 Android 白噪音应用。这是一个专注于白噪音播放的 Android 应用，提供雨声、篝火、雷声、猫咪呼噜、鸟鸣、夜虫等多种自然声音，帮助你放松、冥想和入睡。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>21、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/apple/container\" rel=\"noopener nofollow\" target=\"_blank\">container</a>：苹果开源的轻量级虚拟机。这是一款苹果官方开源的轻量级虚拟化容器工具，用于在 Mac 上创建和运行 Linux 容器。它采用 Swift 开发，并针对 Apple 芯片（如 M1、M2 芯片）进行了优化，旨在为 macOS 用户提供高效、原生的容器体验，支持 OCI 标准容器镜像，并可无缝对接 Docker Hub 等主流镜像仓库。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>22、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/LunarBar-app/LunarBar\" rel=\"noopener nofollow\" target=\"_blank\">LunarBar</a>：极简的 Mac 菜单栏日历。这是一款专为 macOS 设计的菜单栏日历工具，支持农历、节日、节气和提醒等功能。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"python-项目\">Python 项目</h3>\n<p>23、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/zauberzeug/nicegui\" rel=\"noopener nofollow\" target=\"_blank\">nicegui</a>：简单易用的 Python UI 框架。该项目是基于 Python 的用户界面框架，支持纯 Python 开发 Web 或桌面应用。只需几行代码，即可生成按钮、图表、3D 场景等 50 多种组件，解决了传统 Python GUI 跨平台麻烦、前端开发门槛高等问题。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>24、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/AstrBotDevs/AstrBot\" rel=\"noopener nofollow\" target=\"_blank\">AstrBot</a>：易上手的多平台 LLM 聊天机器人。该项目是基于 Python 构建的 LLM 聊天机器人及开发框架，支持集成多种消息平台和大语言模型。它提供了可视化管理面板和灵活的插件扩展机制，支持速率限制、白名单、关键词过滤、图片理解和语音转文字等功能。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>25、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/dabeaz-course/python-mastery\" rel=\"noopener nofollow\" target=\"_blank\">python-mastery</a>：高级 Python 编程教程。该项目是由 Python 社区知名专家、《Python Cookbook》作者 David Beazley 编写的高级 Python 编程课程，内容完全开源，涵盖生成器、协程、元编程、模块与包等，适合有一定 Python 基础的学习者。</p>\n<h3 id=\"rust-项目\">Rust 项目</h3>\n<p>26、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/eythaann/Seelen-UI\" rel=\"noopener nofollow\" target=\"_blank\">Seelen-UI</a>：高度可定制的 Windows 桌面美化工具。这是一款免费开源的 Windows 桌面增强工具，专注于高度自定义和效率提升。它采用 Rust 语言开发，结合 Tauri 框架与 Web 技术，支持窗口平铺管理、应用启动器、Dock、任务栏、动态壁纸、插件扩展等功能。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>27、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/rustfs/rustfs\" rel=\"noopener nofollow\" target=\"_blank\">rustfs</a>：基于 Rust 的高性能分布式存储系统。该项是用 Rust 构建的高性能分布式对象存储系统，致力于成为 MinIO 的开源替代品。它安装简单、兼容 S3 协议，采用更友好的开源协议，并内置界面清爽的 Web 管理后台。同时，支持国产保密设备和系统，适用于海量数据存储、大数据、互联网、工业和保密存储等场景。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>28、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/hanshuaikang/Nping\" rel=\"noopener nofollow\" target=\"_blank\">Nping</a>：Rust 的多地址并发 Ping 工具。这是一个用 Rust 开发的可视化 Ping 工具，支持同时对多个目标地址并发 Ping 操作。它提供了分区折线图和表格视图等可视化展示，支持实时动态展示延迟、丢包率等性能指标，同时兼容 IPv4 和 IPv6 网络环境。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"人工智能\">人工智能</h3>\n<p>29、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/codexu/note-gen\" rel=\"noopener nofollow\" target=\"_blank\">note-gen</a>：基于 AI 的 Markdown 笔记应用。这是一款跨平台的 Markdown 笔记应用，专注于用 AI 搭建“记录-写作”的桥梁。它不仅支持截图、文本、插图等多种记录方式，还能通过 AI 模型将这些碎片化内容整理成可读的笔记。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>30、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/KlingTeam/LivePortrait\" rel=\"noopener nofollow\" target=\"_blank\">LivePortrait</a>：让静态照片“活”起来。该项目能够通过一张照片快速生成高质量、生动的视频，尤其擅长捕捉和还原面部表情，支持人类和动物肖像的动画生成。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>31、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/SwanHubX/SwanLab\" rel=\"noopener nofollow\" target=\"_blank\">SwanLab</a>：AI 模型训练跟踪与可观测平台。这是一款专为 AI 模型训练打造的跟踪、记录、分析与协作工具，旨在帮助研究者优化训练过程，提升团队协作效率。它通过简洁的 Python API 和直观的界面，提供了训练可视化、自动日志记录、硬件监控、实验管理和多人协同等功能。已集成 40+ 主流训练框架，适用于大模型训练、计算机视觉、音频处理、AIGC 等任务场景。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>32、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/linshenkx/prompt-optimizer\" rel=\"noopener nofollow\" target=\"_blank\">prompt-optimizer</a>：优化 AI 提示词的工具。这是一款纯前端实现的提示词优化器，帮助用户快速编写更高质量的提示词。支持多种主流 AI 模型与自定义 API 地址，并可实时对比优化前后的效果。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>33、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/nndeploy/nndeploy\" rel=\"noopener nofollow\" target=\"_blank\">nndeploy</a>：简单易用的多端 AI 推理部署框架。这是一款简单易用、高性能、支持多端的 AI 推理部署框架。它基于有向无环图设计，将前处理、推理和后处理抽象为图的节点，支持流水线并行、任务并行等优化方式。兼容 TensorRT、OpenVINO、MNN 等多种推理后端，适配主流文生图、大语言、检测等模型，实现一套代码多端部署。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"书籍教程\">书籍/教程</h3>\n<p>34、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/ZJU-LLMs/Foundations-of-LLMs\" rel=\"noopener nofollow\" target=\"_blank\">Foundations-of-LLMs</a>：《大模型基础》。该书是由浙江大学 DAILY 实验室开源的大语言模型教材，内容涵盖传统语言模型、大语言模型架构演化、Prompt 工程、参数高效微调、模型编辑、检索增强生成等方面。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>35、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/hzpt-inet-club/english-note\" rel=\"noopener nofollow\" target=\"_blank\">english-note</a>：《从零开始学习英语语法》。这是一本面向英语基础薄弱同学的英语语法入门书籍，内容言简意赅、插图幽默风趣。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>36、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/microsoft/ai-agents-for-beginners\" rel=\"noopener nofollow\" target=\"_blank\">ai-agents-for-beginners</a>：微软开源的 AI Agent 初学者教程。该项目是微软专为初学者打造的 AI 智能体（Agents）教程，内容分为 10 个课程，包含详细的教程、视频和示例代码。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h3 id=\"其它\">其它</h3>\n<p>37、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/Circuit-Digest/ESP-Drone\" rel=\"noopener nofollow\" target=\"_blank\">ESP-Drone</a>：低成本的开源无人机。这是一个基于 ESP32 的开源无人机项目，帮你制作一款低成本、可手机控制的小型无人机。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>38、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/Gar-b-age/CookLikeHOC\" rel=\"noopener nofollow\" target=\"_blank\">CookLikeHOC</a>：老乡鸡菜谱开源版。该项目非老乡鸡官方出品，是作者基于《老乡鸡菜品溯源报告》等资料，归纳、整理了老乡鸡菜品的配方、制作流程及烹饪要点。</p>\n<p>39、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/SuperManito/LinuxMirrors\" rel=\"noopener nofollow\" target=\"_blank\">LinuxMirrors</a>：一键搞定 Linux 换源问题的脚本。该项目提供了一键切换 Linux 默认软件源为国内镜像源的脚本，以及 Docker 安装脚本，支持 Debian、Ubuntu、CentOS、Raspberry Pi OS 和 Deepin 等多种主流发行版。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>40、<a href=\"https://hellogithub.com/periodical/statistics/click?target=https://github.com/lucide-icons/lucide\" rel=\"noopener nofollow\" target=\"_blank\">lucide</a>：精美的开源图标库。该项目是由社区驱动的开源图标库，提供 1000+ 高质量的 SVG 图标，支持 React、Vue、Svelte、React Native 等主流前端框架，以及 Figma 插件，方便开发者和设计师使用。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<h2 id=\"三最后\">三、最后</h2>\n<p>过去的一年，HelloGitHub 分享了 588 个开源项目，其中来自用户的分享为 160 个（占比 27.2%）。</p>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>感谢每一位来 HelloGitHub 分享和推荐开源项目的小伙伴。<strong>你们不只是读者，更是这份月刊的共创者！</strong></p>\n<p>新的一年，HelloGitHub 会努力接触更多的开源作者，分享开源项目背后的故事，并帮助真正热爱开源的小伙伴，开启他们的开源之旅。</p>\n<p>最后，感谢大家过去一年的陪伴和支持。HelloGitHub 在这里提前祝大家春节快乐、马年行大运，我们年后见～</p>\n\n\n</div>\n<div id=\"MySignature\">\n    <div>    \n    <p id=\"PSignature\">\n    <br />\n    作者：<a href=\"https://github.com/521xueweihan\" target=\"_blank\">削微寒</a>\n\n    <br />\n    <strong>扫描左侧的二维码可以联系到我</strong>\n    <br />\n\n    <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh\" rel=\"license\"><img alt=\"知识共享许可协议\" src=\"https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png\" style=\"border-width: 0;\" /></a><br />本作品采用<a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh\" rel=\"license\">署名-非商业性使用-禁止演绎 4.0 国际 </a>进行许可。\n    </p>\n</div>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-10 08:23</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xueweihan\">削微寒</a>&nbsp;\n阅读(<span id=\"post_view_count\">365</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "从零学习Kafka：数据存储",
      "link": "https://www.cnblogs.com/Jackeyzhe/p/19597292",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Jackeyzhe/p/19597292\" id=\"cb_post_title_url\" title=\"发布于 2026-02-09 23:47\">\n    <span>从零学习Kafka：数据存储</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"从零学习Kafka：数据存储\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/1828322/202601/1828322-20260127214703996-2017510713.png\" />\n        不知道有没有朋友和我一样，虽然了解 Kafka 的逻辑存储，例如 Broker、Topic、Partition 这些概念，但是对于底层数据是如何存储还是比较模糊。这样聊起来 Kafka 数据存储时总有种一知半解的感觉。今天我们就一起来看一下 Kafka 底层数据到底是怎么存储的。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>不知道有没有朋友和我一样，虽然了解 Kafka 的逻辑存储，例如 Broker、Topic、Partition 这些概念，但是对于底层数据是如何存储还是比较模糊。这样聊起来 Kafka 数据存储时总有种一知半解的感觉。今天我们就一起来看一下 Kafka 底层数据到底是怎么存储的。</p>\n<h3 id=\"环境准备\">环境准备</h3>\n<p>在开始之前，我们先搭建好单机的 Kafka 集群，并且实际写入一批数据，这样就可以直接观察写入 Kafka 的数据了。下面可以跟着我的步骤一起搭建集群并写入数据。开始之前先说明一下，操作过程中可能涉及到一些配置参数的修改和检查，如果对配置参数不熟悉的话，可以查看上一篇文章。</p>\n<p>首先到 Kafka 的下载页面下载最新版本的压缩包。</p>\n<pre><code class=\"language-bash\">https://www.apache.org/dyn/closer.cgi?path=/kafka/4.1.1/kafka_2.13-4.1.1.tgz\n</code></pre>\n<p>下载好之后，进行解压并进入到对应的目录。</p>\n<pre><code class=\"language-bash\">tar -xzf kafka_2.13-4.1.1.tgz\ncd kafka_2.13-4.1.1\n</code></pre>\n<p>接着我们执行下面两条命令进行一些必要的配置。</p>\n<pre><code class=\"language-bash\">KAFKA_CLUSTER_ID=\"$(bin/kafka-storage.sh random-uuid)\"\n\nbin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties\n</code></pre>\n<p>为了方便观察文件切分，我把 segment 文件大小调整为了 1MB，具体修改方法为编辑 config/server.properties 文件，修改 <code>log.segment.bytes</code> 参数的数值。</p>\n<p><img alt=\"segmentsize\" class=\"lazyload\" /></p>\n<p>修改好之后，就可以启动 Kafka 集群了。</p>\n<pre><code class=\"language-bash\">bin/kafka-server-start.sh config/server.properties\n</code></pre>\n<p>可以观察日志，看集群是否启动成功</p>\n<p><img alt=\"clusterstarted\" class=\"lazyload\" /></p>\n<p>集群启动之后，我们手动创建一个测试 topic。</p>\n<pre><code class=\"language-bash\">bin/kafka-topics.sh --create \\\n    --bootstrap-server localhost:9092 \\\n    --topic test-topic \\\n    --partitions 2 \\\n    --replication-factor 1\n</code></pre>\n<p>接着可以使用 Kafka 提供的压测工具来写入一批数据。</p>\n<pre><code class=\"language-bash\">bin/kafka-producer-perf-test.sh \\\n    --topic test-topic \\\n    --num-records 50000 \\\n    --record-size 100 \\\n    --throughput -1 \\\n    --producer-props bootstrap.servers=localhost:9092\n</code></pre>\n<p>这里我分两次写入，每次写入了 50000 条数据，每条数据大小 100 字节，也就是一共写入了大约 10MB 数据。</p>\n<p>现在把目光投向 <code>/tmp/kraft-combined-logs</code> 这个目录。如果没有这个目录，需要看一下集群配置的目录。</p>\n<p><img alt=\"category\" class=\"lazyload\" /></p>\n<h3 id=\"broker-根目录\">Broker 根目录</h3>\n<p>首先来看第一级目录</p>\n<pre><code class=\"language-bash\">drwxr-xr-x  10 wheel  320  1月 31 00:25 __cluster_metadata-0\n-rw-r--r--   1 wheel  355  1月 31 00:14 bootstrap.checkpoint\n-rw-r--r--   1 wheel    0  1月 31 00:14 cleaner-offset-checkpoint\n-rw-r--r--   1 wheel    4  1月 31 00:52 log-start-offset-checkpoint\n-rw-r--r--   1 wheel  122  1月 31 00:14 meta.properties\n-rw-r--r--   1 wheel   42  1月 31 00:52 recovery-point-offset-checkpoint\n-rw-r--r--   1 wheel   42  1月 31 00:52 replication-offset-checkpoint\ndrwxr-xr-x  23 wheel  736  1月 31 00:33 test-topic-0\ndrwxr-xr-x  27 wheel  864  1月 31 00:33 test-topic-1\n</code></pre>\n<p>这里一共有 9 个文件（目录），大体上可以分为三类：<strong>集群元数据、数据目录和 Checkpoint 文件</strong>。</p>\n<h4 id=\"集群元数据\">集群元数据</h4>\n<p><code>meta.properties</code> 是 Broker 的身份证，这里记录了 Cluster ID 和 Node ID。</p>\n<p><code>bootstrap.checkpoint</code> 用于记录集群初始化信息。</p>\n<p><code>__cluster_metadata-0</code> 是一个特殊的数据目录，它记录了集群的元数据，因此我将其归类到集群元数据中。</p>\n<h4 id=\"数据目录\">数据目录</h4>\n<p><code>test-topic-0</code> 和 <code>test-topic-1</code> 这两个目录就是我们 test-topic 的两个 partition 存储数据的目录，待会儿再详细分析目录下的内容，现在你只需要知道 Kafka 是以 topic名 + partitionId 来命名数据目录的。</p>\n<h4 id=\"checkpoint-文件\">Checkpoint 文件</h4>\n<p>剩下的都是 checkpoint 文件，是用于宕机重启后的快速恢复的。</p>\n<p><code>cleaner-offset-checkpoint</code> 这是清理检查点文件，只有设置了 <code>cleanup.policy=compact</code> 时才有用，它记录了上一次 Log Compact 各个 partition 已清理的偏移量。</p>\n<p><code>log-start-offset-checkpoint</code> 日志起始位置，记录每个分区第一个有效的 Offset。</p>\n<p><code>recovery-point-offset-checkpoint</code> 记录每个 partition 已刷盘的 Offset。</p>\n<p><code>replication-offset-checkpoint</code> 记录每个 partition 已同步的 Offset，这里记录的就是 High Watermark。</p>\n<h3 id=\"partition-存储结构\">Partition 存储结构</h3>\n<p>现在我们再来看下数据目录下的各个文件的作用是什么。</p>\n<h4 id=\"核心三兄弟\">核心三兄弟</h4>\n<p>首先来介绍数据存储的核心，分别是 <code>.log</code> 、<code>.index</code> 和 <code>.timeindex</code> 文件，每个 Segment 都会有这三个文件，它们的文件名都是文件内存的第一条消息的 Offset。</p>\n<h5 id=\"log-文件\">log 文件</h5>\n<p><code>.log</code> 是消息数据文件，Kafka 接收的消息都会顺序写入到这个文件中。可以通过下面这个命令查看文件的内容：</p>\n<pre><code class=\"language-bash\">~/workspace/kafka_2.13-4.1.1/bin/kafka-dump-log.sh --files test-topic-0/00000000000000009472.log --print-data-log\n</code></pre>\n<p><img alt=\"logdata\" class=\"lazyload\" /></p>\n<p>可以看到 log 文件中存储的主要是 Offset 和具体的序列化后的数据。</p>\n<h5 id=\"index-文件\">index 文件</h5>\n<p><code>.index</code> 文件是偏移量索引文件，这里的索引是稀疏索引，文件内存储的是 Offset 到 log 文件位置的映射。我们使用下面这条命令来查看文件内容：</p>\n<pre><code class=\"language-bash\">~/workspace/kafka_2.13-4.1.1/bin/kafka-dump-log.sh --files test-topic-0/00000000000000009472.index --deep-iteration\n</code></pre>\n<p><img alt=\"indexdata\" class=\"lazyload\" /></p>\n<p>Kafka 默认每 4KB 数据写入一次索引，这个值可以通过 <code>log.index.interval.bytes</code> 参数调整。</p>\n<h5 id=\"timeindex-文件\">timeindex 文件</h5>\n<p><code>.timeindex</code> 是时间戳索引文件，用来支持 by_duration 按照时间回溯。查看文件内容的方法与查看 index 文件的方法类似：</p>\n<pre><code class=\"language-bash\">~/workspace/kafka_2.13-4.1.1/bin/kafka-dump-log.sh --files test-topic-0/00000000000000009472.timeindex --deep-iteration\n</code></pre>\n<p><img alt=\"timeindexdata\" class=\"lazyload\" /></p>\n<p>我们在定位数据时，可以通过二分法在 index 索引文件中找到对应的数据位置（或者最接近的位置），也可以先通过时间在 timeindex 文件中找到最接近的 Offset，再到 index 文件中找到数据位置。</p>\n<h4 id=\"辅助文件\">辅助文件</h4>\n<p>除了上述三个核心文件之外，在数据目录中还有三种辅助文件，我们来看下它们的作用。</p>\n<ul>\n<li><code>.snapshot</code> 文件是用来记录事务快照的。用于 Exactly-Once 语义，如果 Broker 宕机，可以通过加载这个文件知道 Producer 之前发送到哪里了，防止数据重复。查看文件内容的方法如下</li>\n</ul>\n<pre><code class=\"language-bash\">~/workspace/kafka_2.13-4.1.1/bin/kafka-dump-log.sh --files test-topic-0/00000000000000009472.snapshot\n</code></pre>\n<ul>\n<li><code>leader-epoch-checkpoint</code> 文件是一个 Leader “任期表”，它记录了每一任 Leader 开始工作时的 Offset，主要用于在选主时保证数据一致性。</li>\n<li><code>partition.metadata</code> 文件是 Partition 的“身份证“，它存储了 Topic ID。</li>\n</ul>\n<h3 id=\"page-cache\">Page Cache</h3>\n<p>至此，我们已经比较细致的了解了 Kafka 底层存储结构。到这里不知道你会不会有疑问，Kafka 是写磁盘的，为什么速度还会这么快？</p>\n<p>Kafka 在操作磁盘时，重度依赖操作系统的 Page Cache 功能，这个功能就是 Kafka 性能高的原因之一。简单来说，Page Cache 就是在读取磁盘时，操作系统会把读到的数据放到内存中一份，这块内存就是 Page Cache。在 Kafka 的应用场景中，Producer 写入顺序写入数据时，操作系统会先把数据写到 Page Cache，然后异步刷盘。在 Consumer 消费数据时，由于大部分情况下都是消费最新数据，因此要读的数据大概率还在 Page Cache 中， 操作系统可以直接从内存中返回。</p>\n<p>题外话：Kafka 为什么不自己维护一套缓存机制呢？</p>\n<p>我觉得主要有以下原因：</p>\n<ol>\n<li>\n<p>避免 GC 开销，如果自己在 JVM 内存中维护缓存，那么会带来很大的 GC 压力。如果用操作系统的 Page Cache，就完全不用担心 GC 问题。</p>\n</li>\n<li>\n<p>Page Cache 对内存的利用率更高，如果 Kafka 进程重启，Page Cache 也还会在内存中，数据不需要重新加载。</p>\n</li>\n<li>\n<p>逻辑简单，Kafka 只需要负责读写操作，剩下的缓存维护逻辑全部交给操作系统。</p>\n</li>\n</ol>\n<h3 id=\"总结\">总结</h3>\n<p>本文我们了解了 Kafka 物理层面的数据存储。在 Broker 根目录下，有集群元数据、数据目录、Checkpoint 文件三种类型的文件（目录）。在数据目录中，最核心的三种文件是 <code>.log</code>、<code>.index</code> 和 <code>.timeindex</code> 三种文件，它们分别存储了数据、稀疏 Offset 索引以及时间戳与 Offset 的映射。</p>\n<p>希望你通过阅读本文，可以对 Kafka 的数据存储有一个更加清晰的认识。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-09 23:47</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Jackeyzhe\">Jackeyzhe</a>&nbsp;\n阅读(<span id=\"post_view_count\">67</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI 时代的前端技术：从系统编程到 JavaScript/TypeScript",
      "link": "https://www.cnblogs.com/kaiux/p/19596869",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/kaiux/p/19596869\" id=\"cb_post_title_url\" title=\"发布于 2026-02-09 20:27\">\n    <span>AI 时代的前端技术：从系统编程到 JavaScript/TypeScript</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"AI 时代的前端技术：从系统编程到 JavaScript/TypeScript\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/1158182/202602/1158182-20260209202606764-2110726327.png\" />\n        本文从系统程序员的视角，深入剖析了 JavaScript 和 TypeScript 在现代复杂软件架构中的核心地位。通过对 V8 引擎优化机制、构建工具链演进以及异步 I/O 模型的底层拆解，揭示了前端技术栈如何实现足以媲美原生应用的执行效率。本文不仅探讨了运行时机制的演变，更从语言设计层面分析了其在高并发系统与大规模工程协作中的技术优势，旨在帮助开发者构建起从硬件到底层运行时的完整技术视野。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p><img alt=\"全景2\" src=\"https://img2024.cnblogs.com/blog/1158182/202602/1158182-20260209202523377-936731118.png\" /></p>\n<h2 id=\"前言当-ai-的大脑跑在-v8-引擎之上\">前言：当 AI 的“大脑”跑在 V8 引擎之上</h2>\n<p><strong>The Prologue: When AGI Meets the Event Loop</strong></p>\n<p>在传统的系统程序员眼中，前端开发往往被戏称为“DIV 居中工程师”或“NPM 依赖搬运工”。我们习惯于认为，真正的计算——那些涉及高性能、高并发、底层硬件调度的任务——必然属于 C++、Rust 或 Python 的领地。</p>\n<p><strong>然而，在通往 AGI（通用人工智能）的道路上，一个反直觉的现象正在发生。</strong></p>\n<p>如果你拆解当下最热门的 AI 项目，你会惊讶地发现：<strong>TypeScript 和 JavaScript 正在成为 AI 应用层的“官方语言”。</strong></p>\n<ul>\n<li><strong>OpenClaw (ClawdBot):</strong> 这是一个强大的本地自主智能体（Autonomous Agent），它的“中枢神经”并非由 Python 编写，而是运行在 Node.js 的事件循环之上。</li>\n<li><strong>Claude Code / OpenCode:</strong> 这些让开发者惊叹的 AI 编程助手 CLI，其底层架构往往是 TypeScript 加上 V8 引擎的运行时。</li>\n<li><strong>Electron 生态:</strong> 无数的大模型本地客户端（Local LLM Runners），本质上都是 Chromium 内核包裹下的 Web 应用。</li>\n</ul>\n<p><strong>为什么会这样？</strong></p>\n<p>因为 AI 时代的本质发生了变化。大模型（LLM）本身是计算密集型的（由 CUDA/C++ 解决），但<strong>AI 应用（Agent）的本质是 IO 密集型的</strong>。</p>\n<p>一个优秀的 AI Agent 需要同时处理成百上千个并发的网络请求（API Calls）、需要实时解析非结构化的 JSON 数据、需要灵活地加载各种“工具（Tools）”函数、需要构建复杂的异步交互界面。</p>\n<p>在处理<strong>高并发 I/O</strong> 和 <strong>动态 JSON Schema</strong> 方面，没有什么比 <strong>Event Loop (libuv)</strong> 和 <strong>TypeScript 类型系统</strong> 更高效的组合了。</p>\n<p><strong>在 AI 时代，掌握前端技术栈，不再是为了画出漂亮的网页，而是为了构建 AI 的“躯壳”与“手脚”。</strong></p>\n<p>如果你不懂 Promise，你就无法理解 Agent 的并发思考模式；如果你不懂 Virtual DOM，你就无法构建高效的 AI 交互终端；如果你不懂 Node.js 运行时，你就无法完全掌控那些在该运行时上飞奔的智能体。</p>\n<p>不要被“前端”二字迷惑。这本手册将带你越过浏览器的围墙，用系统工程师的视角，重新审视这套正在定义 AI 应用层的技术栈。</p>\n<p><strong>Welcome to the metal of the modern web.</strong></p>\n<h2 id=\"1-生态全景图--幻象与裸机-the-illusion-vs-the-metal\">1. 生态全景图 —— 幻象与裸机 (The Illusion vs. The Metal)</h2>\n<p>对于习惯了系统底层编程的程序员，初入前端世界可能会感到一种 <strong>“分形的混乱”</strong> ：Webpack、Vite、Babel、ESLint、Prettier、PostCSS……这些工具像藤蔓一样缠绕在一起。</p>\n<p>这时候，请暂时忘掉那些花哨的名词。让我们像剥离操作系统抽象层一样，直接看向 <strong>“裸机” (The Metal)</strong> 。</p>\n<h3 id=\"11-the-hard-constraint-物理法则\">1.1. The Hard Constraint: 物理法则</h3>\n<p>在 Web 开发的宇宙里，浏览器（Browser）就是你的<strong>目标硬件架构 (Target Architecture)</strong>。</p>\n<p>无论你在 IDE 里写得多么天花乱坠——使用了 TypeScript 的高级泛型、React 的函数式组件、Vue 的单文件模板、还是 SCSS 的嵌套语法——<strong>浏览器一概不认识</strong>。</p>\n<p>Chrome (V8 引擎) 和 Firefox (SpiderMonkey) 本质上是 C++ 编写的解释器/JIT 编译器，它们<strong>只</strong>接受三种输入格式：</p>\n<ol>\n<li><strong>HTML:</strong> DOM 树的描述文件（类似 UI 布局 XML）。</li>\n<li><strong>CSS:</strong> 样式描述。</li>\n<li><strong>JavaScript (ES5/ES6+):</strong> 唯一的指令集架构 (ISA)。</li>\n</ol>\n<p><strong>这意味着：前端工程化的本质，就是一个庞大的“交叉编译”系统 (Cross-Compilation System)。</strong> 所有的复杂度，都源于我们需要把人类友好的“高级语言”（.ts, .vue, .jsx）翻译成浏览器这台“裸机”能吞下的“机器码”（.js, .html, .css）。</p>\n<h3 id=\"12-nodejs-的双重身份-the-build-environment\">1.2. Node.js 的双重身份: The Build Environment</h3>\n<p>这就引出了一个最让后端开发者困惑的问题：<em>“我就写个网页，为什么非要安装 Node.js？”</em></p>\n<p>这里存在一个<strong>认知陷阱</strong>。Node.js 在前端生态中扮演了两个完全不同的角色，必须严格区分：</p>\n<h4 id=\"121-角色-a服务器运行时-server-runtime\">1.2.1. 角色 A：服务器运行时 (Server Runtime)</h4>\n<p>这是你熟悉的。像 Python 或 Java 一样，Node.js 作为一个常驻进程运行在服务器上，处理 HTTP 请求，连接数据库。这叫 <strong>Backend / SSR (Server-Side Rendering)</strong>。</p>\n<h4 id=\"122-角色-b构建工具运行时-the-build-environment--这是重点\">1.2.2. 角色 B：构建工具运行时 (The Build Environment) —— <strong>这是重点</strong></h4>\n<p>这是你安装它的真正原因。<br />\n在开发阶段，你的电脑上并没有运行“服务器”，而是运行了一个<strong>构建系统</strong>。</p>\n<ul>\n<li><strong>Node.js 是你的 <code>make</code> + <code>gcc</code> + <code>ld</code>。</strong></li>\n<li><strong><code>package.json</code> 是你的 <code>Makefile</code> / <code>CMakeLists.txt</code>。</strong></li>\n<li><strong><code>npm</code> / <code>pnpm</code> 是你的 <code>vcpkg</code> / <code>apt-get</code>。</strong></li>\n</ul>\n<p>当你执行 <code>npm run build</code> 时，你实际上是启动了一个 Node.js 进程。这个进程加载了名为 <strong>Vite</strong> 或 <strong>Webpack</strong> 的库（编译器驱动），它们读取你的源码，进行词法分析、转换、链接、压缩，最后吐出 <code>dist/</code> 目录。</p>\n<blockquote>\n<p><strong>系统视角类比：</strong><br />\n你在 Windows 上写 C++，目标平台是 Linux。你需要安装 WSL (Node.js 环境) 来运行 GCC (Vite/Webpack)，最终生成 ELF 文件 (bundle.js) 扔到 Linux 服务器 (Browser) 上去跑。</p>\n</blockquote>\n<h3 id=\"13-the-abstractions-框架即-dsl\">1.3. The Abstractions: 框架即 DSL</h3>\n<p>既然浏览器只认 JS，为什么我们要发明 React 和 Vue？</p>\n<p>因为原生的 DOM API (<code>document.createElement</code>, <code>appendChild</code>) 就像是 <strong>Win32 API</strong> 或者 <strong>X11</strong>——极其繁琐、指令式、且难以维护。</p>\n<p>现代前端框架本质上是 <strong>DSL (领域特定语言)</strong>，旨在解决 UI 开发中的<strong>状态同步</strong>难题。</p>\n<h4 id=\"131-react-the-immutable-state-machine\">1.3.1. React (The Immutable State Machine)</h4>\n<p>React 的核心哲学是 <code>UI = f(State)</code>。<br />\nJSX 看起来像 HTML，但它只是 <code>React.createElement()</code> 的语法糖。</p>\n<ul>\n<li><strong>Source (JSX):</strong></li>\n</ul>\n<pre><code class=\"language-jsx\">// 这不是 HTML，这是 JS 表达式\nconst element = &lt;div className=\"btn\"&gt;Click {count}&lt;/div&gt;;\n\n</code></pre>\n<ul>\n<li><strong>Compiled (JS):</strong></li>\n</ul>\n<pre><code class=\"language-javascript\">// 编译器（Babel/Vite）将其转化为：\nconst element = React.createElement(\"div\", { className: \"btn\" }, \"Click \", count);\n\n</code></pre>\n<p><strong>本质：</strong> React 引入了 <strong>Virtual DOM</strong>，这实际上就是图形学中的 <strong>双重缓冲 (Double Buffering)</strong>。它在内存中构建下一帧的 UI 树，计算 Diff，然后一次性通过 syscall (DOM API) 更新屏幕，避免频繁 IO 带来的性能损耗。</p>\n<h4 id=\"132-vue-the-reactive-observer\">1.3.2. Vue (The Reactive Observer)</h4>\n<p>Vue 的 <code>.vue</code> 文件是更纯粹的 DSL。它甚至不符合 JS 语法，必须由编译器（Vue Compiler）大卸三块。</p>\n<ul>\n<li><strong>Template:</strong> 编译成 Render Function (类似 React)。</li>\n<li><strong>Script:</strong> 经过 TS 转译。</li>\n<li><strong>Style:</strong> 经过 CSS 预处理。</li>\n</ul>\n<p><strong>本质：</strong> Vue 3 利用了 ES6 的 <code>Proxy</code> 对象，实现了对内存数据的<strong>拦截</strong>。这类似于 C++ 的智能指针或运算符重载，当你修改变量时，自动触发回调去更新 UI。</p>\n<h3 id=\"14-总结the-pipeline-visualization\">1.4. 总结：The Pipeline Visualization</h3>\n<p>现在，我们将整个流程串联起来。作为系统架构师，你脑中应该建立起这样一张数据流图：</p>\n<p><img alt=\"mermaid\" src=\"https://img2024.cnblogs.com/blog/1158182/202602/1158182-20260209202549474-1723785530.png\" /></p>\n<p><strong>核心结论：</strong></p>\n<ol>\n<li><strong>幻象 (Illusion):</strong> 我们在写 TypeScript、React Hooks、Vue Templates。</li>\n<li><strong>现实 (Reality):</strong> 我们在写配置，指示 Node.js 进程如何生成一堆经过混淆的、浏览器能读懂的 ES5 代码。</li>\n<li><strong>Vite 的作用：</strong> 它就是那个<strong>极速的增量链接器 (Incremental Linker)</strong>。在开发时，它利用浏览器的 ESM 特性做“动态链接”；在发布时，它调用 Rollup 做“静态链接” (Bundling)。</li>\n</ol>\n<p>理解了这一点，就不会再被 <code>npm install</code> 下载的几千个包吓到了——那只是为了编译你的代码而准备的<strong>编译器工具链</strong>而已。</p>\n<h2 id=\"2-runtime--the-metal--引擎的咆哮-the-engines-roar\">2. Runtime &amp; The Metal —— 引擎的咆哮 (The Engine's Roar)</h2>\n<p>在第一章，我们剥离了构建工具的幻象。现在，让我们把视线聚焦到代码真正运行的地方——<strong>运行时 (Runtime)</strong>。</p>\n<p>作为系统开发者，你可能对解释型语言持有偏见：慢、动态、不可预测。但今天的 JavaScript 引擎（特别是 Google 的 V8）实际上是一个<strong>极其复杂的、基于配置文件的动态优化编译器 (Profile-Guided Optimizing Compiler)</strong>。它在某些场景下的性能甚至能逼近未高度优化的 C++。</p>\n<p>让我们钻进引擎盖下面看看。</p>\n<h3 id=\"21-v8-的本质jit-与动态这一仗-just-in-time-compilation\">2.1. V8 的本质：JIT 与动态这一仗 (Just-In-Time Compilation)</h3>\n<p>V8 并非像老式 Python 那样逐行解释执行。它是一个多级编译流水线。</p>\n<ul>\n<li><strong>Ignition (解释器):</strong> 当你的 JS 代码第一次运行时，V8 会将其解析为<strong>字节码 (Bytecode)</strong> 并由 Ignition 解释执行。这一步是为了启动速度 (Startup Time)——就像 Python 的 <code>.pyc</code>。</li>\n<li><strong>TurboFan (优化编译器):</strong> 在代码运行过程中，V8 会收集<strong>分析数据 (Profiling Data)</strong>。</li>\n<li>如果它发现某个函数被反复调用（\"Hot\" Function），TurboFan 就会介入。</li>\n<li>它会将字节码编译成高度优化的<strong>机器码 (Machine Code)</strong>。</li>\n<li><em>System Analogy:</em> 这就像你的 CPU 在运行时动态地重写指令流，或者 JVM 的 HotSpot 机制。</li>\n</ul>\n<h4 id=\"211-关键技术内联缓存-inline-caching--hidden-classes\">2.1.1. 关键技术：内联缓存 (Inline Caching / Hidden Classes)</h4>\n<p>JS 是动态类型的。<code>obj.x</code> 在 C++ 里是一个固定的内存偏移量（Offset），但在 JS 里，引擎理论上每次都要去 Hash Map 里查找 <code>x</code>。这慢得令人发指。</p>\n<p><strong>V8 的解决方案是“隐藏类” (Hidden Classes / Shapes)：</strong></p>\n<ol>\n<li>当你写 <code>function Point(x, y) { this.x = x; this.y = y; }</code> 时，V8 在内部悄悄创建了一个类似 C++ <code>struct</code> 的布局描述。</li>\n<li><strong>内联缓存 (IC):</strong> 当引擎第一次访问 <code>p.x</code> 时，它会查找 Hash Map，但它会<strong>记住</strong>这次查找的结果：“对于 <code>Point</code> 这种形状的对象，<code>x</code> 的偏移量是 0”。</li>\n<li>下次访问时，它直接使用偏移量 0，跳过 Hash 查找。</li>\n<li><strong>去优化 (Deoptimization):</strong> 如果你突然手贱写了一句 <code>p.z = 10</code>，对象的形状变了。V8 必须抛弃之前的优化代码（Deopt），回退到解释器模式，重新分析。</li>\n</ol>\n<blockquote>\n<p><strong>给系统程序员的启示：</strong> 在写高性能 JS 时，<strong>保持对象的形状稳定</strong>。不要随意添加/删除属性，尽量像写 C++ <code>struct</code> 一样初始化对象。这能让 JS 引擎生成接近 C++ 指针访问效率的机器码。</p>\n</blockquote>\n<h3 id=\"22-the-great-lie-单线程模型-the-single-threaded-model\">2.2. The Great Lie: 单线程模型 (The Single-Threaded Model)</h3>\n<p>你常听说“JavaScript 是单线程的”。这既是真的，也是假的。</p>\n<ul>\n<li><strong>JS 及其堆栈 (Call Stack) 是单线程的。</strong> 这意味着在任何给定时刻，只有一个 JS 函数在 CPU 上执行。</li>\n<li><strong>浏览器/Node.js 运行时 (The Runtime) 是多线程的。</strong></li>\n</ul>\n<h4 id=\"221-为什么是单线程the-design-choice\">2.2.1. 为什么是单线程？(The Design Choice)</h4>\n<p>JS 诞生之初是为了处理 DOM（网页 UI）。<br />\n想象一下，如果两个线程同时操作同一个 DOM 节点：一个线程要把 <code>&lt;div&gt;</code> 删了，另一个线程要给它加个 <code>class</code>。这需要复杂的锁机制 (Mutex/Semaphore)。</p>\n<p>对于 UI 编程来说，<strong>死锁 (Deadlock)</strong> 和<strong>竞态条件 (Race Condition)</strong> 是噩梦。JS 选择了<strong>协作式多任务 (Cooperative Multitasking)</strong> 模型：</p>\n<ul>\n<li><strong>优点：</strong> 只要你的代码块不结束，没人能打断你。你不需要写锁，永远不用担心竞态条件破坏内存一致性。</li>\n<li><strong>缺点：</strong> <strong>Head-of-Line Blocking</strong>。如果你写了一个 <code>while(true)</code> 或者计算了 10 亿次斐波那契数列，整个页面就会卡死（UI 渲染线程也被阻塞了）。</li>\n</ul>\n<h3 id=\"23-the-metal-事件循环-the-event-loop\">2.3. The Metal: 事件循环 (The Event Loop)</h3>\n<p>如果 JS 是单线程的，它是怎么处理网络请求（I/O）而不卡死的？<br />\n答案是：<strong>它把脏活累活都丢给了底层 C++ 线程池（libuv 或浏览器内核），自己只负责收信。</strong></p>\n<p>这就是 <strong>事件循环 (Event Loop)</strong>。这本质上就是一个 <strong>Windows Message Pump (GetMessage/DispatchMessage)</strong> 或者 Linux 上的 <strong><code>epoll</code> 循环</strong>。</p>\n<h4 id=\"231-循环机制-the-tick\">2.3.1. 循环机制 (The Tick)</h4>\n<p>想象一个无限循环 <code>while(queue.waitForMessage())</code>：</p>\n<ol>\n<li><strong>Call Stack:</strong> 执行同步代码（V8 引擎主线程）。</li>\n<li><strong>Web APIs / C++ Threads:</strong> 当你调用 <code>fetch()</code> 或 <code>setTimeout</code> 时，JS 只是向底层 C++ 模块发送了一个指令，然后立刻返回。底层线程负责等待网络响应或倒计时。</li>\n<li><strong>Callback Queue (Task Queue):</strong> 当底层工作完成，回调函数被扔进队列。</li>\n<li><strong>Loop:</strong> 一旦 Call Stack 空了，Event Loop 就从队列里取出一个回调压入栈中执行。</li>\n</ol>\n<blockquote>\n<p><strong>系统视角类比：</strong></p>\n<ul>\n<li>Main Thread = CPU Pipeline。</li>\n<li>Async Operations = DMA (Direct Memory Access) 控制器。</li>\n<li>Callback = 中断处理程序 (ISR)，但它是被<strong>延迟</strong>调度的 ISR。</li>\n</ul>\n</blockquote>\n<h3 id=\"24-异步进化论从回调地狱到协程-the-evolution\">2.4. 异步进化论：从回调地狱到协程 (The Evolution)</h3>\n<p>JS 的异步模型经历了三次重大的语法演进，每一次都是为了更优雅地处理<strong>栈结构</strong>。</p>\n<h4 id=\"241-phase-1-callback-hell-函数指针的滥用\">2.4.1. Phase 1: Callback Hell (函数指针的滥用)</h4>\n<p>最早的 JS 像这样写：</p>\n<pre><code class=\"language-javascript\">getData(function(a) {\n    getMoreData(a, function(b) {\n        getMoreData(b, function(c) {\n            // ...右移的三角形\n        });\n    });\n});\n\n</code></pre>\n<p><strong>问题：</strong> 这不是嵌套问题，这是<strong>控制反转 (Inversion of Control)</strong> 的丢失。你把后续逻辑的执行权交给了第三方库，而且错误处理 (Error Handling) 极其困难（<code>try/catch</code> 无法捕获异步回调里的错误，因为栈已经销毁了）。</p>\n<h4 id=\"242-phase-2-promises-状态机-monad\">2.4.2. Phase 2: Promises (状态机 Monad)</h4>\n<p><code>Promise</code> 本质上是一个对象，代表“未来可能出现的值”。</p>\n<pre><code class=\"language-javascript\">getData()\n  .then(a =&gt; getMoreData(a))\n  .then(b =&gt; getMoreData(b))\n  .catch(e =&gt; console.error(e));\n\n</code></pre>\n<p><strong>本质：</strong> 它标准化了回调的签名，并允许链式调用。重要的是，它引入了 <strong>Microtask Queue (微任务队列)</strong>。</p>\n<ul>\n<li><strong>Microtask (Promise):</strong> 优先级极高。在当前栈清空后，<strong>立即</strong>执行，插队在所有 IO 回调之前。</li>\n<li><strong>Macrotask (setTimeout):</strong> 优先级低。下一轮循环才执行。</li>\n</ul>\n<h4 id=\"243-phase-3-asyncawait-协程--coroutines\">2.4.3. Phase 3: Async/Await (协程 / Coroutines)</h4>\n<p>这是你最熟悉的形态。ES7 引入了 <code>async/await</code>。</p>\n<pre><code class=\"language-javascript\">async function main() {\n    try {\n        const a = await getData();\n        const b = await getMoreData(a);\n    } catch (e) {\n        console.error(e);\n    }\n}\n\n</code></pre>\n<p><strong>本质：</strong> 这就是 <strong>C++20 的协程 (Coroutines)</strong> 或 <strong>C# 的 Task</strong>。</p>\n<ul>\n<li><code>async</code> 函数会将代码编译成一个状态机 (State Machine)。</li>\n<li>遇到 <code>await</code> 时，函数<strong>暂停 (Yield)</strong>，保存当前的栈帧（闭包 context），并将控制权交还给 Event Loop。</li>\n<li>当 Promise 完成时，运行时恢复该函数的执行，并把结果填入。</li>\n</ul>\n<p><strong>总结：</strong> <code>async/await</code> 让你用<strong>同步的思维</strong>（线性的 try/catch）写<strong>异步的代码</strong>（非阻塞 I/O）。这是 JS 历史上最伟大的工程成就之一。</p>\n<h2 id=\"3-language--syntax--语法糖与类型防御-syntactic-sugar--type-defense\">3. Language &amp; Syntax —— 语法糖与类型防御 (Syntactic Sugar &amp; Type Defense)</h2>\n<p>在深入了解了构建工具的幻象和运行时的底层机制后，我们来到了最具争议的领域：<strong>语言本身的演进</strong>。</p>\n<p>对于 C++ 程序员来说，JavaScript 的对象模型（基于原型）和 TypeScript 的类型系统（结构化类型）往往是最反直觉的两个痛点。本章将剥离语法的表象，揭示它们在内存和编译期的真实形态。</p>\n<h3 id=\"31-从-prototype-到-class面向对象的伪装\">3.1. 从 Prototype 到 Class：面向对象的“伪装”</h3>\n<p>ES6 (ECMAScript 2015) 引入了 <code>class</code> 关键字，这让 JS 看起来终于像 Java/C++ 了。<br />\n<strong>但这只是一个巨大的谎言（或者说，高明的伪装）。</strong></p>\n<p>在 C++ 中，<code>class</code> 是编译期的蓝图。对象是根据蓝图在内存中切分出的数据块（vptr + 成员变量）。<br />\n在 JS 中，<code>class</code> 仅仅是 <strong>原型链 (Prototype Chain)</strong> 的语法糖。</p>\n<h4 id=\"311-原型链的本质单向链表-singly-linked-list\">3.1.1. 原型链的本质：单向链表 (Singly Linked List)</h4>\n<p>想象一下，JS 没有“类”的概念，只有“对象”。对象之间通过一个隐藏指针 <code>[[Prototype]]</code>（在浏览器调试中通常显示为 <code>__proto__</code>）连接。</p>\n<ul>\n<li><strong>查找机制：</strong> 当你访问 <code>obj.x</code> 时，引擎先在 <code>obj</code> 自身内存中找。找不到？顺着 <code>__proto__</code> 指针去“父对象”找。还找不到？继续向上，直到 <code>null</code>。</li>\n<li><strong>内存模型：</strong> 这不是继承（Inheritance），这是<strong>委托（Delegation）</strong>。</li>\n<li>C++ 继承：子类对象包含了父类对象的数据成员（内存布局是连续的）。</li>\n<li>JS 委托：子对象只是持有了一个指向父对象的指针。</li>\n</ul>\n<h4 id=\"312-es6-class-vs-the-metal\">3.1.2. ES6 Class vs. The Metal</h4>\n<p>看看这段“现代”代码：</p>\n<pre><code class=\"language-javascript\">class Dog extends Animal {\n  bark() { return \"Woof!\"; }\n}\n\n</code></pre>\n<p>它在底层的真实面目（ES5）：</p>\n<pre><code class=\"language-javascript\">function Dog() {} // 构造函数只是一个普通函数\nDog.prototype = Object.create(Animal.prototype); // 手动接上链表指针\nDog.prototype.bark = function() { return \"Woof!\"; }; // 把方法挂在链表节点上\n\n</code></pre>\n<blockquote>\n<p><strong>系统视角类比：</strong></p>\n<ul>\n<li><strong>Prototype:</strong> 就是一个共享的 <code>vtable</code>（虚函数表），但它本身也是一个普通的 Heap Object。</li>\n<li><strong>Instance:</strong> 就是一个包含 <code>vptr</code>（指向 Prototype）和成员变量的 <code>struct</code>。</li>\n<li><strong>Class 关键字:</strong> 只是为了让你写起来不那么恶心，不用手动操作 <code>vptr</code>。</li>\n</ul>\n</blockquote>\n<h3 id=\"32-typescript-的介入类型系统的反击\">3.2. TypeScript 的介入：类型系统的反击</h3>\n<p>既然 V8 引擎内部已经有了 Hidden Classes（动态类型推导），为什么我们还需要 TypeScript？</p>\n<p><strong>因为 V8 的推导发生在“运行时”，而 TypeScript 的检查发生在“编译时”。</strong><br />\n对于大型工程，等待运行时崩溃（Runtime Panic）是不可接受的。我们需要在代码部署前就拦截错误。</p>\n<h4 id=\"321-structural-typing-结构化类型-vs-nominal-typing-名义类型\">3.2.1. Structural Typing (结构化类型) vs. Nominal Typing (名义类型)</h4>\n<p>这是 TS 与 C++/Java 最根本的区别。</p>\n<ul>\n<li><strong>C++ (Nominal):</strong> 类型由<strong>名字</strong>决定。</li>\n</ul>\n<pre><code class=\"language-cpp\">struct A { int x; };\nstruct B { int x; };\nA a; B b = a; // ❌ 错误！A 和 B 是不同类型，即使内存布局完全一样。\n\n</code></pre>\n<ul>\n<li><strong>TypeScript (Structural):</strong> 类型由 <strong>形状（Shape）</strong> 决定。</li>\n</ul>\n<pre><code class=\"language-typescript\">interface A { x: number; }\ninterface B { x: number; }\nlet a: A = { x: 1 };\nlet b: B = a; // ✅ 合法！只要长得像（鸭子类型），就是同一种类型。\n\n</code></pre>\n<p><strong>解决了什么痛点？</strong><br />\n在前端，我们经常处理 JSON 数据。后端传回来的 JSON 只是一个纯数据结构，没有类名。结构化类型允许我们定义一个 Interface 来“套”在任何符合形状的 JSON 上，而不需要像 C++ 那样写繁琐的序列化/反序列化映射器。</p>\n<h4 id=\"322-type-erasure-类型擦除编译后的虚无\">3.2.2. Type Erasure (类型擦除)：编译后的虚无</h4>\n<p>TypeScript 的类型检查是<strong>纯粹的静态分析</strong>。<br />\n一旦编译通过，TS 编译器（tsc）会<strong>删除所有</strong>类型注解、接口定义、泛型声明。</p>\n<ul>\n<li><strong>Input (.ts):</strong></li>\n</ul>\n<pre><code class=\"language-typescript\">function add(a: number, b: number): number {\n  return a + b;\n}\n\n</code></pre>\n<ul>\n<li><strong>Output (.js):</strong></li>\n</ul>\n<pre><code class=\"language-javascript\">function add(a, b) {\n  return a + b;\n}\n\n</code></pre>\n<p>这意味着：</p>\n<ol>\n<li><strong>运行时没有开销：</strong> 没有 RTTI（运行时类型识别），没有虚函数表查找的额外损耗。</li>\n<li><strong>运行时没有保护：</strong> 如果你在运行时强行把一个 <code>string</code> 传给编译时标记为 <code>number</code> 的函数（比如通过 API 请求），JS 引擎会照单全收，然后可能崩给你看。</li>\n</ol>\n<blockquote>\n<p><strong>给系统程序员的启示：</strong></p>\n<ul>\n<li>TypeScript 就像是给 JavaScript 穿上了一层 <strong>编译期断言 (Compile-time Assertions)</strong>。</li>\n<li>它不会改变生成的机器码（JS），但它能保证你在写代码时逻辑自洽。</li>\n<li><strong>Trust Boundary:</strong> 永远不要相信 I/O 边界（网络请求、用户输入）进来的数据自动符合 TS 类型。你必须使用运行时校验库（如 Zod）来手动验证，这才是真正的“类型安全”。</li>\n</ul>\n</blockquote>\n<h2 id=\"4-the-engineering-layer--从手工作坊到工业流水线-engineering--frameworks\">4. The Engineering Layer —— 从手工作坊到工业流水线 (Engineering &amp; Frameworks)</h2>\n<p>前三章我们搞定了工具链、运行时和语言本身。现在，我们终于可以谈谈那些让前端开发者“以此为生”的东西了：<strong>框架 (Frameworks)</strong>。</p>\n<p>对于系统程序员来说，React 和 Vue 往往被误解为“仅仅是模板库”。实际上，它们的出现是为了解决一个计算机图形学中的经典难题：<strong>如何高效地将应用程序的内部状态 (Internal State) 映射到屏幕像素 (Pixels) 上，同时保持代码的可维护性？</strong></p>\n<h3 id=\"41-the-dom-api-a-syscall-nightmare-系统调用的噩梦\">4.1. The DOM API: A Syscall Nightmare (系统调用的噩梦)</h3>\n<p>回顾一下 jQuery 时代（2006-2013）。那时候我们直接操作 DOM。</p>\n<p><strong>为什么直接操作 DOM 是反模式？</strong></p>\n<ul>\n<li>\n<p><strong>The \"Context Switch\" Cost:</strong> 在浏览器中，JavaScript 引擎（V8）和 渲染引擎（Blink/Webkit）是两个独立的模块，甚至在某些架构下运行在不同的线程。</p>\n</li>\n<li>\n<p>每次你调用 <code>document.getElementById</code> 或 <code>element.style.color = 'red'</code>，实际上都发生了一次<strong>跨边界调用 (Cross-boundary Call)</strong>。</p>\n</li>\n<li>\n<p><strong>系统类比：</strong> 这就像你在写 C++ 程序时，为了写入文件，每写一个字节就调用一次 <code>write()</code> 系统调用 (Syscall)。性能开销是巨大的。</p>\n</li>\n<li>\n<p><strong>State Synchronization Hell:</strong> 想象一下，你有一个变量 <code>int count = 0</code>。每次 <code>count</code> 变化，你必须手动去寻找页面上所有显示 <code>count</code> 的 <code>&lt;div&gt;</code> 并更新它们。</p>\n</li>\n<li>\n<p>jQuery 代码充满了 <code>$('.counter').text(count)</code>。</p>\n</li>\n<li>\n<p>一旦逻辑复杂，这就是典型的 <strong>\"Spaghetti Code\"</strong> —— 状态（内存中的数据）和 视图（DOM 树）完全解耦，同步全靠手动。这在系统编程中等同于手动管理 <code>malloc/free</code> 且没有任何 RAII 机制，内存泄漏（UI 状态不一致）是必然的。</p>\n</li>\n</ul>\n<h3 id=\"42-ui-as-a-function-of-state-声明式革命\">4.2. UI as a Function of State: 声明式革命</h3>\n<p>React (2013) 引入了一个在当时看起来离经叛道的公式：</p>\n<p>这意味着：<strong>UI 只是状态的纯函数投影。</strong></p>\n<ul>\n<li><strong>Imperative (命令式 - jQuery/Win32 API):</strong> \"找到那个按钮，把它的颜色改成红色，然后把它的文字改成 'Clicked'。\" -&gt; <strong>关注过程 (How)</strong>。</li>\n<li><strong>Declarative (声明式 - React/Vue):</strong> \"按钮的状态是 <code>clicked</code>。当状态为 <code>clicked</code> 时，它应该是红色的且显示 'Clicked'。\" -&gt; <strong>关注结果 (What)</strong>。</li>\n</ul>\n<p><strong>系统类比：</strong><br />\n这就像从 <strong>Immediate Mode GUI (OpenGL <code>glBegin</code>/<code>glEnd</code>)</strong> 转向了 <strong>Retained Mode GUI (Qt/WPF)</strong>。你不再告诉 GPU 怎么画每一帧，你只是修改场景图（Scene Graph）中的数据，引擎负责渲染。</p>\n<h3 id=\"43-virtual-dom-the-double-buffering-双重缓冲\">4.3. Virtual DOM: The Double Buffering (双重缓冲)</h3>\n<p>既然 <code>UI = f(State)</code>，那岂不是每次状态改变（比如用户敲了一个键），我们都要销毁整个页面重新渲染？这在性能上是不可接受的。</p>\n<p>为了解决这个问题，React 引入了 <strong>Virtual DOM (虚拟 DOM)</strong>。</p>\n<h4 id=\"机制详解\">机制详解：</h4>\n<ol>\n<li><strong>Memory Buffer:</strong> Virtual DOM 本质上是一个轻量级的 JavaScript 对象树（JS Object Tree），它在内存中模拟了真实的 DOM 结构。</li>\n<li><strong>Render Phase:</strong> 当状态变更时，React 会调用你的组件函数，生成一棵<strong>新的</strong> Virtual DOM 树。</li>\n<li><strong>Diff Algorithm (The \"Linker\"):</strong> React 将新树与旧树进行对比（Diffing）。它使用一种启发式算法（复杂度 O(n)）找出最小变更集 (Dirty Regions)。</li>\n<li><strong>Commit Phase (Flush):</strong> React 将这些差异批量应用到真实的 DOM 上。</li>\n</ol>\n<p><strong>系统视角类比：</strong><br />\n这就是图形编程中的 <strong>双重缓冲 (Double Buffering)</strong>。</p>\n<ul>\n<li><strong>Front Buffer:</strong> 真实的 DOM（用户看到的，写入慢）。</li>\n<li><strong>Back Buffer:</strong> Virtual DOM（内存中的，读写极快）。</li>\n<li><strong>Swap/Flush:</strong> 只将 Back Buffer 中变化的部分 (Dirty Rectangles) 复制到 Front Buffer。</li>\n</ul>\n<blockquote>\n<p><strong>The Optimization:</strong> VDOM 并不总是比直接操作 DOM 快（因为多了 Diff 的 CPU 开销）。但它保证了<strong>下限</strong>——无论你的状态管理写得多么烂，它都能通过批处理（Batching）避免最坏的“每字节一次 Syscall”的情况。</p>\n</blockquote>\n<h3 id=\"44-componentization-the-shared-libraries-of-web\">4.4. Componentization: The \"Shared Libraries\" of Web</h3>\n<p>在框架出现之前，前端代码往往是“页面级”的：一个巨大的 HTML，配一个巨大的 CSS 和一个巨大的 JS。</p>\n<p>React/Vue 强制推行了 <strong>组件化 (Componentization)</strong>。</p>\n<ul>\n<li><strong>封装 (Encapsulation):</strong> 一个组件（Component）就是一个拥有独立状态（State）、独立逻辑（JS）和独立视图（JSX/Template）的单元。</li>\n<li><strong>复用 (Reusability):</strong> 组件可以像 Lego 积木一样嵌套。</li>\n<li><strong>接口 (Interface):</strong> 组件通过 <strong>Props</strong>（输入参数）和 <strong>Events</strong>（回调函数）进行通信。</li>\n</ul>\n<p><strong>系统类比：</strong></p>\n<ul>\n<li><strong>组件 = 类 (Class) / 结构体 (Struct)</strong>。</li>\n<li><strong>Props = 构造函数参数</strong>。</li>\n<li><strong>State = 私有成员变量</strong>。</li>\n<li><strong>Render = 这里的 <code>Draw()</code> 函数</strong>。</li>\n</ul>\n<p>这种架构将前端开发从“写脚本”提升到了“软件工程”的维度。我们可以像设计 C++ 类库一样设计 UI 系统，实现了 <strong>关注点分离 (Separation of Concerns)</strong>。</p>\n<h2 id=\"5-modern-ecosystem--速度与边界的突围-speed--boundaries\">5. Modern Ecosystem —— 速度与边界的突围 (Speed &amp; Boundaries)</h2>\n<p>如果说前四章是关于如何在浏览器这个“沙盒”里跳舞，那么这一章则是关于<strong>越狱</strong>。</p>\n<p>现代前端生态正在经历两场剧烈的地壳运动：</p>\n<ol>\n<li><strong>工具链的“原生化” (Native Rewrite)：</strong> 既然 JS 解释执行慢，那就用 Go/Rust 重写所有工具。</li>\n<li><strong>运行时的“泛化” (Universal Runtime)：</strong> JavaScript 不再局限于浏览器，它试图吞噬服务器、桌面甚至嵌入式设备。</li>\n</ol>\n<p>作为系统程序员，你会对这一章倍感亲切——因为我们要聊的终于不再是 DOM，而是<strong>编译原理</strong>、<strong>系统调用</strong>和<strong>进程间通信 (IPC)</strong>。</p>\n<h3 id=\"51-构建工具的战争从-webpack-到-viteesbuild\">5.1. 构建工具的战争：从 Webpack 到 Vite/Esbuild</h3>\n<h4 id=\"511-the-legacy-webpack-the-make-written-in-python\">5.1.1. The Legacy: Webpack (The \"Make\" written in Python)</h4>\n<p>在很长一段时间里，Webpack 是构建工具的霸主。它功能极其强大，但有一个致命弱点：<strong>它是用 JavaScript 写的。</strong></p>\n<ul>\n<li><strong>痛点：</strong> 随着项目膨胀，Webpack 需要解析成千上万个模块的 AST（抽象语法树），进行 Tree-shaking 和 Bundling。在单线程的 JS 运行时里，这导致冷启动可能需要 2-5 分钟。</li>\n<li><strong>类比：</strong> 这就像你在写一个 C++ 项目，但是你的编译器（GCC）和链接器（LD）是完全用 Python 写的。逻辑没问题，但吞吐量（Throughput）被解释型语言的性能天花板锁死了。</li>\n</ul>\n<h4 id=\"512-the-revolution-esbuild--swc-native-code\">5.1.2. The Revolution: Esbuild &amp; SWC (Native Code)</h4>\n<p>既然瓶颈在语言，解决方案就是<strong>换语言</strong>。</p>\n<ul>\n<li><strong>Esbuild (Go):</strong> Evan Wallace 用 Go 编写的打包器。</li>\n<li><strong>SWC (Rust):</strong> 用 Rust 编写的编译器（替代 Babel）。</li>\n</ul>\n<p>它们的性能通常是 Webpack 的 <strong>10-100 倍</strong>。为什么？</p>\n<ol>\n<li><strong>并行性 (Parallelism):</strong> Go 和 Rust 能充分利用多核 CPU（JS 只能单线程）。</li>\n<li><strong>内存管理:</strong> 手动管理内存布局，减少 GC 压力。</li>\n<li><strong>零成本抽象:</strong> 没有 JS 引擎的 JIT 预热开销。</li>\n</ol>\n<h4 id=\"513-the-game-changer-vite-the-jit-linker\">5.1.3. The Game Changer: Vite (The \"JIT\" Linker)</h4>\n<p>Vite (法语“快”) 结合了浏览器原生 ESM 能力和 Esbuild 的速度。</p>\n<ul>\n<li><strong>Dev Server (O(1) Start):</strong> Webpack 启动时需要把所有文件打包（Bundle）。Vite <strong>不打包</strong>。它启动一个 HTTP Server，当浏览器请求 <code>main.js</code> 时，它才实时通过 Esbuild 编译该文件并返回。</li>\n<li><strong>系统类比：</strong></li>\n<li><strong>Webpack:</strong> 静态链接 (Static Linking)。修改一行代码，重新链接整个 <code>.exe</code>。</li>\n<li><strong>Vite:</strong> 动态链接 (Dynamic Linking / <code>dlopen</code>)。修改一个 <code>.cpp</code>，只重编译它生成的 <code>.so</code>，程序运行时动态加载。</li>\n</ul>\n<h3 id=\"52-服务端运行时nodejs-vs-bundeno\">5.2. 服务端运行时：Node.js vs. Bun/Deno</h3>\n<p>JavaScript 运行时的战争，本质上是 <strong>C++ vs. Rust vs. Zig</strong> 的代理人战争。</p>\n<h4 id=\"521-nodejs-the-c-veteran\">5.2.1. Node.js (The C++ Veteran)</h4>\n<ul>\n<li><strong>架构：</strong> V8 (Engine) + libuv (Event Loop) + C++ Bindings。</li>\n<li><strong>地位：</strong> 就像 Linux 的 <strong>glibc</strong>。虽然有历史包袱（比如 <code>node_modules</code> 的嵌套黑洞），但它是标准，生态最全，极其稳定。</li>\n</ul>\n<h4 id=\"522-deno-the-rust-challenger\">5.2.2. Deno (The Rust Challenger)</h4>\n<ul>\n<li><strong>架构：</strong> V8 + Tokio (Rust Event Loop)。</li>\n<li><strong>卖点：</strong> 安全性（默认无文件/网络权限）、去中心化依赖（没有 <code>package.json</code>，直接 import URL）、原生 TypeScript 支持。</li>\n<li><strong>系统视角：</strong> Node.js 像 C++，给你所有权限但容易崩；Deno 像 Rust，编译器（运行时）强迫你守规矩。</li>\n</ul>\n<h4 id=\"523-bun-the-zig-speedster\">5.2.3. Bun (The Zig Speedster)</h4>\n<ul>\n<li><strong>架构：</strong> JavaScriptCore (Safari 的引擎) + Zig 自研 IO 层。</li>\n<li><strong>卖点：</strong> <strong>快，疯狂的快。</strong></li>\n<li><strong>Why Zig?</strong> Bun 的作者 Jarred Sumner 选择 Zig 是因为它可以手动控制内存布局，并且没有隐藏的控制流。Bun 重新实现了包管理器（npm client）、打包器和测试运行器。</li>\n<li><strong>系统类比：</strong> 如果 Node.js 是标准的 Ubuntu，Bun 就是 <strong>Alpine Linux</strong> —— 极致精简，为了启动速度和 IO 吞吐量牺牲了一切冗余。它旨在成为一个 <strong>Drop-in Replacement</strong>（直接替换 libc）。</li>\n</ul>\n<h3 id=\"53-electron-write-once-run-everywhere-的代价\">5.3. Electron: \"Write Once, Run Everywhere\" 的代价</h3>\n<p>Electron 是让无数系统程序员“嗤之以鼻”但又不得不服的技术。它允许用 Web 技术开发跨平台桌面应用（VS Code, Discord, Slack）。</p>\n<h4 id=\"531-架构本质chromium--nodejs\">5.3.1. 架构本质：Chromium + Node.js</h4>\n<p>Electron 的二进制文件里，塞进了一个完整的浏览器内核（Chromium）和一个完整的 Node.js 运行时。</p>\n<ul>\n<li><strong>Main Process (Kernel Space):</strong> 运行 Node.js。负责创建窗口、操作系统交互（文件、托盘、原生菜单）。它就像是这个应用的“内核”。</li>\n<li><strong>Renderer Process (User Space):</strong> 运行 Chromium。负责渲染 UI。每一个窗口通常是一个独立的进程。</li>\n<li><strong>IPC (Inter-Process Communication):</strong> 两个世界通过 IPC 管道通信。</li>\n</ul>\n<h4 id=\"532-为什么它能赢the-trade-off\">5.3.2. 为什么它能赢？(The Trade-off)</h4>\n<ul>\n<li><strong>系统程序员的质疑：</strong> <em>“为了写个记事本，你让我跑两个浏览器内核？这太浪费 RAM 了！”</em></li>\n<li><strong>工程视角的回答：</strong> 是的，它极其<strong>臃肿 (Bloated)</strong>。但是，它解决了 GUI 开发最大的痛点——<strong>跨平台一致性</strong>。</li>\n<li>写 Qt/MFC/GTK，你需要处理 Windows/macOS/Linux 的无数细微差异（DPI 缩放、字体渲染、事件循环差异）。</li>\n<li>Electron 把这些差异全部抹平在 Chromium 层之下。</li>\n<li><strong>结论：</strong> 它是 <strong>RAM 换开发效率 (Memory for Velocity)</strong> 的极致体现。对于现代硬件来说，浪费 500MB 内存换取 3 倍的开发速度，是商业上合理的交换。</li>\n</ul>\n<hr />\n<p><strong>结语：全栈的终局</strong></p>\n<p>读完这五章，作为系统程序员的你应该已经看透了 JavaScript/TypeScript 生态的本质：</p>\n<ol>\n<li><strong>它不再只是脚本</strong>：它是一个极其复杂的、分层的编译目标。</li>\n<li><strong>它正在“下沉”</strong>：工具链正在用系统语言（Go/Rust）重写，以追求极致性能。</li>\n<li><strong>它只是工具</strong>：就像 C++ 是操纵内存的工具，React/TS 是操纵 UI 状态的工具。</li>\n</ol>\n<p>不要被表面的框架战争迷惑。<strong>Keep your eyes on the metal, even when coding in the cloud.</strong></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-09 20:27</span>&nbsp;\n<a href=\"https://www.cnblogs.com/kaiux\">念风零壹</a>&nbsp;\n阅读(<span id=\"post_view_count\">52</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "【开源】《clip》一个不到4M的、跨平台的、支持分组、搜索、自定义条数、局域网共享的、剪贴板历史工具",
      "link": "https://www.cnblogs.com/Doyoung/p/19596816",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Doyoung/p/19596816\" id=\"cb_post_title_url\" title=\"发布于 2026-02-09 20:03\">\n    <span>【开源】《clip》一个不到4M的、跨平台的、支持分组、搜索、自定义条数、局域网共享的、剪贴板历史工具</span>\n    \n\n</a>\n\n\t\t</h1>\n\t\t<div class=\"clear\"></div>\n\t\t<div class=\"postBody\">\n\t\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"开源clip一个不到-4m-的跨平台剪贴板历史工具\">【开源】《clip》一个不到 4M 的跨平台剪贴板历史工具</h2>\n<p>一款轻量级的剪贴板历史管理工具，支持分组管理、内容搜索、自定义条数、局域网共享等功能。</p>\n<h3 id=\"特性\">特性</h3>\n<ul>\n<li>体积小巧：不到 4M</li>\n<li>跨平台支持</li>\n<li>支持文本和图片</li>\n<li>分组管理</li>\n<li>内容搜索</li>\n<li>自动识别颜色</li>\n<li>局域网共享</li>\n</ul>\n<h3 id=\"开源仓库\">开源仓库</h3>\n<ul>\n<li><a href=\"https://github.com/DoYoungDo/clip\" rel=\"noopener nofollow\" target=\"_blank\">GitHub</a></li>\n<li><a href=\"https://gitee.com/DoyoungDo/clip\" rel=\"noopener nofollow\" target=\"_blank\">Gitee</a></li>\n</ul>\n<h3 id=\"安装\">安装</h3>\n<p>从 <a href=\"https://github.com/DoYoungDo/clip/releases\" rel=\"noopener nofollow\" target=\"_blank\">GitHub Releases</a> 或 <a href=\"https://gitee.com/DoyoungDo/clip/releases\" rel=\"noopener nofollow\" target=\"_blank\">Gitee Releases</a> 下载对应平台的版本。</p>\n<p><strong>支持平台</strong></p>\n<ul>\n<li>macOS：直接下载运行</li>\n<li>Windows：直接下载运行</li>\n<li>Linux：需要自行编译</li>\n</ul>\n<p>本工具为免安装版本，下载解压后直接运行即可。</p>\n<h3 id=\"功能介绍\">功能介绍</h3>\n<h4 id=\"剪贴板历史记录\">剪贴板历史记录</h4>\n<p>clip 启动后会在系统托盘显示图标：</p>\n<p><img alt=\"系统托盘图标\" class=\"lazyload\" /></p>\n<p><strong>左键点击图标</strong>：显示菜单，分为两部分</p>\n<ul>\n<li>上半部分：最近的剪贴板历史记录</li>\n<li>下半部分：分组列表</li>\n</ul>\n<p><img alt=\"菜单界面\" class=\"lazyload\" /></p>\n<p>点击任意一条记录，会将其复制到系统剪贴板，同时该记录会移动到列表顶部。列表顶部始终显示当前剪贴板的内容。</p>\n<p>支持的内容类型：文本、图片</p>\n<p><strong>右键点击图标</strong>：除了显示历史记录，还会显示额外的操作选项，如清空历史记录、创建分组等。</p>\n<p><img alt=\"右键菜单\" class=\"lazyload\" /></p>\n<h4 id=\"分组管理\">分组管理</h4>\n<p><strong>创建分组</strong></p>\n<p>操作路径：<code>右键</code> → <code>创建分组</code></p>\n<p>会以当前剪贴板内容（列表顶部记录）作为分组名创建分组。</p>\n<p>注意事项：</p>\n<ul>\n<li>如果当前剪贴板内容是图片，创建会失败</li>\n<li>可以先复制想要的分组名，再执行创建操作</li>\n</ul>\n<p><strong>重命名分组</strong></p>\n<p>操作路径：<code>右键</code> → <code>分组</code> → <code>重命名</code></p>\n<p>同样会使用当前剪贴板内容作为新的分组名。</p>\n<p><strong>激活状态</strong></p>\n<p>分组有激活和非激活两种状态，默认为非激活。</p>\n<p>激活状态下，所有复制的内容会自动同步到该分组中，可以用作 TODO LIST。</p>\n<p><strong>持久化存储</strong></p>\n<p>工具支持本地持久化存储，退出后再次启动会自动加载：</p>\n<ul>\n<li>历史记录</li>\n<li>分组信息</li>\n<li>激活状态</li>\n</ul>\n<p><strong>删除分组</strong></p>\n<p>操作路径：<code>右键</code> → <code>分组</code> → <code>删除分组</code></p>\n<h4 id=\"内容搜索\">内容搜索</h4>\n<p>本工具为纯菜单操作，无输入框。</p>\n<p>搜索步骤：</p>\n<ol>\n<li>将要搜索的内容复制到剪贴板（使其成为列表顶部记录）</li>\n<li>操作路径：<code>右键</code> → <code>搜索</code></li>\n<li>再次点击托盘图标时，会显示过滤后的结果</li>\n</ol>\n<h4 id=\"自动识别颜色\">自动识别颜色</h4>\n<p><strong>开启方式</strong></p>\n<p>操作路径：<code>右键</code> → <code>配置</code> → <code>自动识别颜色</code></p>\n<p><strong>支持的颜色格式</strong></p>\n<p>开启后，工具会自动识别以下格式的颜色文本：</p>\n<ul>\n<li>十六进制：<code>#fff</code>、<code>#ffffff</code></li>\n<li>RGB：<code>(255,255,255)</code>、<code>255,255,255</code></li>\n</ul>\n<p><strong>快速转换</strong></p>\n<p>在颜色文本记录上可以直接复制为指定格式，实现快速格式转换。</p>\n<p><img alt=\"颜色识别\" class=\"lazyload\" /></p>\n<h4 id=\"自定义历史记录条数\">自定义历史记录条数</h4>\n<p><strong>设置范围</strong></p>\n<p>最小 1 条，最大 300 条，默认 50 条。</p>\n<p><strong>设置步骤</strong></p>\n<ol>\n<li>将要设置的条数（数字）复制到剪贴板</li>\n<li>操作路径：<code>右键</code> → <code>配置</code> → <code>设置最大历史记录条数</code></li>\n</ol>\n<p><strong>注意事项</strong></p>\n<ol>\n<li>如果新设置的条数小于当前历史记录数量，超出部分会被删除，仅保留最新的记录。删除的记录不会被持久化保存。</li>\n<li>如果列表顶部记录不是数字，会设置失败</li>\n</ol>\n<h4 id=\"局域网共享\">局域网共享</h4>\n<p>局域网内的设备可以通过本工具实现剪贴板共享（支持文本和图片）。</p>\n<p><strong>共享端（机器 A）</strong></p>\n<p>操作路径：<code>右键</code> → <code>配置</code> → <code>局域网共享</code> → <code>局域网共享</code></p>\n<p>开启后会自动将监听地址（格式：<code>192.168.1.100:8080</code>）复制到剪贴板。</p>\n<p><strong>接收端（机器 B）</strong></p>\n<ol>\n<li>将共享端的地址复制到剪贴板</li>\n<li>操作路径：<code>右键</code> → <code>配置</code> → <code>局域网共享</code> → <code>连接到</code></li>\n<li>再次执行相同操作可断开连接</li>\n</ol>\n<p><strong>同步说明</strong></p>\n<p>连接成功后，共享端（A）的所有复制操作会实时同步到接收端（B）的剪贴板，连接前的历史记录不会同步。</p>\n\n\n</div>\n<div id=\"MySignature\">\n    做一条有理想的咸鱼\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-09 20:03</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Doyoung\">咸鱼Doyoung</a>&nbsp;\n阅读(<span id=\"post_view_count\">52</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "OpenClaw for macOS: 完整本地化部署指南（2026.2.6-3 版本）",
      "link": "https://www.cnblogs.com/xiaobaiysf/p/19595515",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xiaobaiysf/p/19595515\" id=\"cb_post_title_url\" title=\"发布于 2026-02-09 15:06\">\n    <span>OpenClaw for macOS: 完整本地化部署指南（2026.2.6-3 版本）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"OpenClaw for macOS: 完整本地化部署指南（2026.2.6-3 版本）\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3600464/202602/3600464-20260209150457965-1453431698.png\" />\n        基于Mac环境安装 OpenClaw ，构建你的个人AI助理！\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"一文档说明\">一、文档说明</h1>\n<p>本文档面向 macOS 系统用户，从<strong>基础环境搭建（Node.js 安装）</strong> 到 <strong>OpenClaw 完整部署</strong>，再到问题排查、残余清理，提供全流程标准化操作，适配 OpenClaw 2026.2.6-3 版本，最终实现 DeepSeek 模型的稳定调用。</p>\n<h1 id=\"二部署前置条件\">二、部署前置条件</h1>\n<h2 id=\"1-系统要求\">1. 系统要求</h2>\n<ul>\n<li>\n<p>操作系统：macOS 10.15+（本文以 MacBook Air (M系列/Intel) 为例）</p>\n</li>\n<li>\n<p>权限：拥有终端管理员权限（可执行 <code>sudo</code> 命令）</p>\n</li>\n<li>\n<p>网络：能正常访问 DeepSeek API（国内网络直接支持）</p>\n</li>\n</ul>\n<h2 id=\"2-预期成果\">2. 预期成果</h2>\n<ul>\n<li>\n<p>完成 Node.js 环境搭建（v24.13.0 及以上）；</p>\n</li>\n<li>\n<p>OpenClaw 网关正常启动，端口 18789 可访问；</p>\n</li>\n<li>\n<p>OpenClaw UI 能调用 DeepSeek 模型并返回对话结果。</p>\n</li>\n</ul>\n<h1 id=\"三基础环境搭建nodejs-安装\">三、基础环境搭建（Node.js 安装）</h1>\n<p>OpenClaw 基于 Node.js 运行，需先完成 Node.js 安装与版本验证。</p>\n<h3 id=\"步骤1安装-homebrewmacos-包管理器推荐\">步骤1：安装 Homebrew（macOS 包管理器，推荐）</h3>\n<p>若已安装 Homebrew，跳过此步骤；未安装则执行：</p>\n<pre><code class=\"language-Bash\">\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>\n<p>✅ 验证安装：</p>\n<pre><code class=\"language-Bash\">\nbrew -v\n</code></pre>\n<p>输出 <code>Homebrew 4.x.x</code> 即安装成功。</p>\n<h3 id=\"步骤2安装-nodejs\">步骤2：安装 Node.js</h3>\n<p>通过 Homebrew 安装稳定版 Node.js（自动适配 v24+）：</p>\n<pre><code class=\"language-Bash\">\nbrew install node\n</code></pre>\n<p>✅ 验证安装与版本：</p>\n<pre><code class=\"language-Bash\">\n# 查看 Node.js 版本\nnode -v\n# 查看 npm 版本（Node.js 自带）\nnpm -v\n</code></pre>\n<ul>\n<li>\n<p>✅ 输出 <code>node v24.13.0</code> 及以上、<code>npm 10.x.x</code> 即符合要求；</p>\n</li>\n<li>\n<p>❌ 若版本过低，执行 <code>brew upgrade node</code> 升级。</p>\n</li>\n</ul>\n<h3 id=\"步骤3配置-npm-全局路径可选避免权限报错\">步骤3：配置 npm 全局路径（可选，避免权限报错）</h3>\n<pre><code class=\"language-Bash\">\n# 创建全局目录\nmkdir -p ~/.npm-global\n# 配置 npm 全局路径\nnpm config set prefix '~/.npm-global'\n# 将全局路径加入环境变量（永久生效）\necho 'export PATH=~/.npm-global/bin:$PATH' &gt;&gt; ~/.zshrc\n# 生效环境变量\nsource ~/.zshrc\n</code></pre>\n<p>✅ 验证配置：</p>\n<pre><code class=\"language-Bash\">\nnpm config get prefix\n</code></pre>\n<p>输出 <code>~/.npm-global</code> 即配置成功。</p>\n<h2 id=\"四openclaw-完整部署流程\">四、OpenClaw 完整部署流程</h2>\n<h3 id=\"步骤1安装-openclaw-包\">步骤1：安装 OpenClaw 包</h3>\n<p>通过 npm 全局安装 OpenClaw：</p>\n<pre><code class=\"language-Bash\">\nnpm install -g openclaw\n</code></pre>\n<p>✅ 验证安装路径：</p>\n<pre><code class=\"language-Bash\">\nls ~/.npm-global/lib/node_modules/openclaw\n</code></pre>\n<p>输出 OpenClaw 相关文件（如 <code>dist</code>、<code>package.json</code>）即安装成功。</p>\n<h3 id=\"步骤2openclaw-配置文件初始化与修改\">步骤2：OpenClaw 配置文件初始化与修改</h3>\n<p>OpenClaw 核心配置文件为 <code>~/.openclaw/openclaw.json</code>，需确保语法合法且适配 DeepSeek 模型。</p>\n<h4 id=\"21-初始化配置目录首次部署\">2.1 初始化配置目录（首次部署）</h4>\n<pre><code class=\"language-Bash\">\nmkdir -p ~/.openclaw\n</code></pre>\n<h4 id=\"22-备份原有配置若有\">2.2 备份原有配置（若有）</h4>\n<pre><code class=\"language-Bash\">\nif [ -f ~/.openclaw/openclaw.json ]; then\n  mkdir -p ~/.openclaw/backup\n  cp ~/.openclaw/openclaw.json ~/.openclaw/backup/openclaw.json.bak\nfi\n</code></pre>\n<h4 id=\"23-写入适配-deepseek-的无错配置核心\">2.3 写入适配 DeepSeek 的无错配置（核心）</h4>\n<p>执行以下命令，直接写入预验证的合法配置（替换占位符为真实信息）：</p>\n<pre><code class=\"language-Bash\">\ncat &gt; ~/.openclaw/openclaw.json &lt;&lt; 'EOF'\n{\n  \"meta\": {\n    \"lastTouchedVersion\": \"2026.2.6-3\",\n    \"lastTouchedAt\": \"2026-02-08T07:43:20.228Z\"\n  },\n  \"models\": {\n    \"mode\": \"merge\",\n    \"providers\": {\n      \"deepseek\": {\n        \"baseUrl\": \"https://api.deepseek.com/v1\",\n        \"apiKey\": \"你的DeepSeek API Key\", // 替换为真实Key（格式：sk-xxxx）\n        \"api\": \"openai-completions\",\n        \"models\": [\n          {\n            \"id\": \"deepseek-chat\",\n            \"name\": \"DeepSeek Chat\",\n            \"input\": [\"text\"],\n            \"contextWindow\": 128000,\n            \"maxTokens\": 8192,\n            \"reasoning\": false\n          }\n        ]\n      }\n    }\n  },\n  \"agents\": {\n    \"defaults\": {\n      \"workspace\": \"/Users/你的用户名/.openclaw/workspace\", // 替换为实际用户名（如 zhufeige）\n      \"maxConcurrent\": 4,\n      \"subagents\": {\n        \"maxConcurrent\": 8\n      },\n      \"model\": {\n        \"primary\": \"deepseek/deepseek-chat\" // 指定默认调用 DeepSeek 模型\n      }\n    }\n  },\n  \"gateway\": {\n    \"port\": 18789,\n    \"mode\": \"local\",\n    \"auth\": {\n      \"mode\": \"token\",\n      \"token\": \"39769ded65eac493eceeb0fb6a543fb48ed4fce3f1166bf5\" // 替换个人生成的此值即可\n    }\n  }\n}\nEOF\n</code></pre>\n<h4 id=\"24-配置文件修改说明\">2.4 配置文件修改说明</h4>\n<ul>\n<li>\n<p>替换 <code>你的DeepSeek API Key</code>：从 <a href=\"https://platform.deepseek.com/\" rel=\"noopener nofollow\" target=\"_blank\">DeepSeek 控制台</a> 获取，格式为 <code>sk-xxxx</code>；</p>\n</li>\n<li>\n<p>替换 <code>你的用户名</code>：macOS 用户名可通过 <code>whoami</code> 命令查看（终端执行 <code>whoami</code> 即可输出）；</p>\n</li>\n<li>\n<p>生成并打印OpenClaw的token</p>\n</li>\n</ul>\n<pre><code>node ~/.npm-global/lib/node_modules/openclaw/openclaw.mjs gateway token --print\n</code></pre>\n<h4 id=\"25-配置语法验证必做避免启动报错\">2.5 配置语法验证（必做，避免启动报错）</h4>\n<pre><code class=\"language-Bash\">\nnode -e \"JSON.parse(require('fs').readFileSync('/Users/$(whoami)/.openclaw/openclaw.json', 'utf8'))\"\n</code></pre>\n<ul>\n<li>\n<p>✅ 终端无任何输出 → 语法完全正确；</p>\n</li>\n<li>\n<p>❌ 若报错：检查是否有全角字符（如 <code>：</code>/<code>，</code>）、多余/缺失的 <code>{}</code>/<code>,</code>/<code>\"</code>。</p>\n</li>\n</ul>\n<h4 id=\"26-修复配置权限\">2.6 修复配置权限</h4>\n<pre><code class=\"language-Bash\">\nnode ~/.npm-global/lib/node_modules/openclaw/openclaw.mjs doctor --fix\n</code></pre>\n<p>✅ 输出无 <code>Config validation failed</code> 即权限修复成功。</p>\n<h3 id=\"步骤3启动-openclaw-网关\">步骤3：启动 OpenClaw 网关</h3>\n<h4 id=\"31-清理残余进程避免端口冲突\">3.1 清理残余进程（避免端口冲突）</h4>\n<pre><code class=\"language-Bash\">\n# 方法1：OpenClaw 官方停止命令\nopenclaw gateway stop\n\n# 方法2：强制杀死所有 OpenClaw 进程（推荐）\npkill -f openclaw\n\n# 方法3：杀死占用 18789 端口的进程（若端口被占用）\nlsof -i :18789 | grep -v PID | awk '{print $2}' | xargs kill -9 2&gt;/dev/null\n</code></pre>\n<h4 id=\"32-启动网关指定端口并强制重载\">3.2 启动网关（指定端口并强制重载）</h4>\n<pre><code class=\"language-Bash\">\nnode ~/.npm-global/lib/node_modules/openclaw/openclaw.mjs gateway --port 18789 --force\n</code></pre>\n<p>✅ 终端输出以下内容即启动成功：</p>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<h3 id=\"步骤4验证部署效果\">步骤4：验证部署效果</h3>\n<h4 id=\"41-实时监控运行日志\">4.1 实时监控运行日志</h4>\n<p>打开新终端窗口，执行以下命令跟踪日志（排查问题关键）：</p>\n<pre><code class=\"language-Bash\">\ntail -f /tmp/openclaw/openclaw-$(date +%Y-%m-%d).log\n</code></pre>\n<ul>\n<li>\n<p>无 <code>error</code>/<code>invalid config</code> 关键字 → 运行正常；</p>\n</li>\n<li>\n<p>若出现 <code>API request failed</code> → 检查 DeepSeek API Key 是否有效。</p>\n</li>\n</ul>\n<h4 id=\"42-访问-openclaw-ui-测试对话\">4.2 访问 OpenClaw UI 测试对话</h4>\n<ol>\n<li>打开浏览器，访问 <code>http://127.0.0.1:18789</code>；</li>\n</ol>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<ol start=\"2\">\n<li>\n<p>在输入框发送测试消息（如「test」或「你好」）；</p>\n</li>\n<li>\n<p>✅ 收到 DeepSeek 回复 → 部署完全成功；</p>\n</li>\n<li>\n<p>❌ 无回复：执行以下命令验证 API Key 有效性：</p>\n<pre><code class=\"language-Bash\">\ncurl -s -X POST https://api.deepseek.com/v1/chat/completions \\\n  -H \"Authorization: Bearer 你的DeepSeek API Key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\":\"deepseek-chat\",\"messages\":[{\"role\":\"user\",\"content\":\"test\"}]}'\n</code></pre>\n<ul>\n<li>\n<p>输出包含 <code>\"content\"</code> 字段 → API Key 有效，重启网关即可；</p>\n</li>\n<li>\n<p>输出 <code>Unauthorized</code> → API Key 无效，重新从 DeepSeek 控制台生成。</p>\n</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"五常见问题排查\">五、常见问题排查</h1>\n<h2 id=\"问题1nodejs-安装失败\">问题1：Node.js 安装失败</h2>\n<ul>\n<li>\n<p>原因：网络问题导致 Homebrew 下载失败；</p>\n</li>\n<li>\n<p>解决：切换国内源安装 Node.js：</p>\n<pre><code class=\"language-Bash\">\n# 配置 npm 国内源\nnpm config set registry https://registry.npmmirror.com\n# 直接通过 npm 安装 Node.js\nnpm install -g n\nn 24.13.0\n</code></pre>\n</li>\n</ul>\n<h2 id=\"问题2json-语法错误如-invalid-character-\">问题2：JSON 语法错误（如 <code>invalid character ':'</code>）</h2>\n<ul>\n<li>\n<p>原因：配置文件存在格式错误（全角字符、多余符号）；</p>\n</li>\n<li>\n<p>解决：直接重新执行步骤2.3 的配置写入命令，避免手动修改格式。</p>\n</li>\n</ul>\n<h2 id=\"问题3端口冲突gateway-already-running-locally\">问题3：端口冲突（<code>Gateway already running locally</code>）</h2>\n<ul>\n<li>\n<p>原因：18789 端口被占用，或 OpenClaw 进程未彻底停止；</p>\n</li>\n<li>\n<p>解决：执行步骤3.1 的进程清理命令，或更换启动端口（如 <code>--port 18788</code>）。</p>\n</li>\n</ul>\n<h2 id=\"问题4ui-无对话反馈网关启动正常\">问题4：UI 无对话反馈（网关启动正常）</h2>\n<ul>\n<li>原因1：未指定默认模型（<code>agents.defaults.model.primary</code> 缺失）；</li>\n</ul>\n<p>解决：确保配置中包含 <code>\"primary\": \"deepseek/deepseek-chat\"</code>；</p>\n<ul>\n<li>原因2：API Key 无效/过期；</li>\n</ul>\n<p>解决：重新从 DeepSeek 控制台生成 Key 并替换配置；</p>\n<ul>\n<li>原因3：配置包含冗余字段（<code>wizard</code>/<code>messages</code>/<code>commands</code>）；</li>\n</ul>\n<p>解决：删除冗余字段，仅保留步骤2.3 中的核心配置。</p>\n<h2 id=\"问题5docker-容器名称冲突container-name-already-in-use\">问题5：Docker 容器名称冲突（<code>container name already in use</code>）</h2>\n<ul>\n<li>\n<p>原因：1Panel 部署的 OpenClaw 容器未删除；</p>\n</li>\n<li>\n<p>解决：</p>\n<pre><code class=\"language-Bash\">\n# 停止冲突容器（替换为实际容器ID/名称）\ndocker stop 1Panel-openclaw-rt8j\n# 删除冲突容器\ndocker rm 1Panel-openclaw-rt8j\n</code></pre>\n</li>\n</ul>\n<h1 id=\"六openclaw-残余内容清理彻底卸载重置\">六、OpenClaw 残余内容清理（彻底卸载/重置）</h1>\n<p>若需重新部署或完全卸载 OpenClaw，执行以下命令清理所有残余文件：</p>\n<h3 id=\"1-停止所有-openclaw-进程\">1. 停止所有 OpenClaw 进程</h3>\n<pre><code class=\"language-Bash\">\npkill -f openclaw\nopenclaw gateway stop\n</code></pre>\n<h3 id=\"2-删除-openclaw-核心目录配置数据\">2. 删除 OpenClaw 核心目录（配置+数据）</h3>\n<pre><code class=\"language-Bash\">\nrm -rf ~/.openclaw\n</code></pre>\n<h3 id=\"3-删除-openclaw-日志文件\">3. 删除 OpenClaw 日志文件</h3>\n<pre><code class=\"language-Bash\">\nrm -rf /tmp/openclaw\n</code></pre>\n<h3 id=\"4-卸载-openclaw-npm-包\">4. 卸载 OpenClaw npm 包</h3>\n<pre><code class=\"language-Bash\">\nnpm uninstall -g openclaw\n</code></pre>\n<h2 id=\"5-清理-docker-残余若通过-1paneldocker-部署过\">5. 清理 Docker 残余（若通过 1Panel/Docker 部署过）</h2>\n<pre><code class=\"language-Bash\">\n# 列出所有容器\ndocker ps -a | grep openclaw\n# 删除 OpenClaw 相关容器（替换为实际容器ID）\ndocker rm 容器ID\n# 清理未使用的镜像/卷（可选）\ndocker system prune -a\n</code></pre>\n<h2 id=\"6-验证清理完成\">6. 验证清理完成</h2>\n<pre><code class=\"language-Bash\">\n# 检查进程（无输出即清理成功）\nps -ef | grep openclaw | grep -v grep\n# 检查目录（无输出即清理成功）\nls ~/.openclaw\nls /tmp/openclaw\n</code></pre>\n<h1 id=\"七注意事项\">七、注意事项</h1>\n<h3 id=\"1-环境配置规范\">1. 环境配置规范</h3>\n<ul>\n<li>\n<p>Node.js 版本：必须 v24.13.0 及以上，低版本会导致 OpenClaw 启动失败；</p>\n</li>\n<li>\n<p>npm 全局路径：配置后避免 <code>EACCES</code> 权限报错，建议必做；</p>\n</li>\n<li>\n<p>配置文件：JSON 语法严格，仅使用半角符号，无注释，键名/值必须用双引号包裹。</p>\n</li>\n</ul>\n<h3 id=\"2-模型使用注意\">2. 模型使用注意</h3>\n<ul>\n<li>\n<p>优先选择 DeepSeek：Anthropic 模型需国际信用卡充值、合规网络，国内用户适配性差；</p>\n</li>\n<li>\n<p>DeepSeek API Key 有效期：需确保 Key 未过期，且账号有余额（DeepSeek 提供免费额度）；</p>\n</li>\n<li>\n<p>模型 ID 不可修改：DeepSeek 必须使用 <code>deepseek-chat</code>，自定义 ID 会导致调用失败。</p>\n</li>\n</ul>\n<h3 id=\"3-进程与端口管理\">3. 进程与端口管理</h3>\n<ul>\n<li>\n<p>启动前必清进程：避免端口冲突和配置重载失败；</p>\n</li>\n<li>\n<p>端口占用处理：若 18789 被占用，可更换端口（如 <code>--port 18788</code>），同时修改配置文件中的 <code>port</code> 字段。</p>\n</li>\n</ul>\n<h3 id=\"4-权限与网络\">4. 权限与网络</h3>\n<ul>\n<li>终端权限：执行 <code>rm</code>/<code>mkdir</code> 命令时若报错，加 <code>sudo</code> 提升权限；</li>\n</ul>\n<h1 id=\"八总结\">八、总结</h1>\n<h2 id=\"核心流程回顾\">核心流程回顾</h2>\n<ol>\n<li>\n<p>搭建基础环境：安装 Homebrew → 安装 Node.js → 配置 npm 全局路径；</p>\n</li>\n<li>\n<p>部署 OpenClaw：安装包 → 写入合法配置 → 验证语法 → 启动网关；</p>\n</li>\n<li>\n<p>验证效果：访问 UI 测试对话 → 实时监控日志排查问题；</p>\n</li>\n<li>\n<p>清理残余：停止进程 → 删除配置/日志/包文件。</p>\n</li>\n</ol>\n<h2 id=\"关键要点\">关键要点</h2>\n<ul>\n<li>\n<p>配置文件是核心：语法错误、字段缺失是部署失败的主要原因；</p>\n</li>\n<li>\n<p>DeepSeek 适配性最优：国内网络无需额外配置，API Key 易获取；</p>\n</li>\n<li>\n<p>日志是排查利器：启动后通过 <code>tail -f</code> 实时查看日志，快速定位问题。</p>\n</li>\n</ul>\n<p>通过以上步骤，可实现 OpenClaw 在 macOS 上的标准化部署，且能稳定调用 DeepSeek 模型完成对话交互。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-09 15:06</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xiaobaiysf\">小白跃升坊</a>&nbsp;\n阅读(<span id=\"post_view_count\">505</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "C++小白训练第十三天",
      "link": "https://www.cnblogs.com/godjian/p/19594585",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/godjian/p/19594585\" id=\"cb_post_title_url\" title=\"发布于 2026-02-09 12:13\">\n    <span>C++小白训练第十三天</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"c小白训练第十三天\">C++小白训练第十三天</h2>\n<p>以下为牛客挑战</p>\n<h2 id=\"今日收获\">今日收获</h2>\n<pre><code> vector&lt;pair&lt;int,int&gt;&gt;v;用于存储坐标，如果坐标：\n 方式：v.push_back({i,j}),v.emplace_back(i,j);\n \n v.push_back(make_pair(i, j));\n\ndp联想的又一个条件，就是因为限制只存在与相邻，那就和后面没有关系，所以考虑dp\n\n\n理解了置换环：n-环数等于操作数。\n</code></pre>\n<h2 id=\"牛客周赛-round-130\">牛客周赛 Round 130</h2>\n<h3 id=\"红美铃的访客登记\">红美铃的访客登记</h3>\n<p><a href=\"https://ac.nowcoder.com/acm/contest/127702/A\" rel=\"noopener nofollow\" target=\"_blank\">A-红美铃的访客登记_牛客周赛 Round 130 (nowcoder.com)</a></p>\n<p><img alt=\"image-20260209082136953\" src=\"https://img2024.cnblogs.com/blog/3621557/202602/3621557-20260209121302221-750510719.png\" /></p>\n<h4 id=\"解题代码\">解题代码</h4>\n<pre><code class=\"language-c++\">#include&lt;bits/stdc++.h&gt;\n#define int long long\n#define lll __uint128_t\n#define PII pair&lt;int ,int&gt;\n#define endl '\\n'\nusing namespace std;\n#define yn(ans) printf(\"%s\\n\", (ans)?\"Yes\":\"No\");//快速打印\n#define YN(ans) printf(\"%s\\n\", (ans)?\"YES\":\"NO\");\n#define REP(i, e) for (int i = 0; i &lt; (e); ++i)\n#define REP1(i, s, e) for (int i = (s); i &lt;=(e); ++i)\n#define TESTS int t; cin &gt;&gt; t; while (t--)\n#define TEST\nconst int N=2e5+10,M=1e3+10,mod=1e9+7;\nint a[N],b[N],c[N],pre[N];\n\nsigned main(){\n\n\tstd::ios::sync_with_stdio(false);\n    cin.tie(0);\n    cout.tie(0);\n    string s;\n    cin&gt;&gt;s;\n    int count=0;\n    for(int i=0;i&lt;s.size();i++){\n        if(s[i]!='0'){\n            count=i;\n            break;\n        }\n    }\n    for(int i=count;i&lt;s.size();i++){\n        cout&lt;&lt;s[i];\n    }\n    \n\n\treturn 0;\n}\n</code></pre>\n<h3 id=\"爱丽丝的魔力零件分类\">爱丽丝的魔力零件分类</h3>\n<p><a href=\"https://ac.nowcoder.com/acm/contest/127702/B\" rel=\"noopener nofollow\" target=\"_blank\">B-爱丽丝的魔力零件分类_牛客周赛 Round 130 (nowcoder.com)</a></p>\n<p><img alt=\"image-20260209084218695\" src=\"https://img2024.cnblogs.com/blog/3621557/202602/3621557-20260209121303147-1985436397.png\" /></p>\n<pre><code>3\n5\n.....\n.....\n.***.\n..*..\n.....\n5\n.....\n..*..\n..*..\n.**..\n.....\n6\n......\n..*...\n.**...\n..*...\n......\n......\n</code></pre>\n<pre><code>T\nL\nT\n</code></pre>\n<p><strong>题解</strong></p>\n<h4 id=\"解题代码-1\">解题代码</h4>\n<p>可以先把这些为*的点先存起来，然后去判断他们的度数双重循环，来判断，当我们发现最多度数为3的时候就就是t，其他的就不是t，是l</p>\n<pre><code class=\"language-c++\">#include&lt;bits/stdc++.h&gt;\n#define int long long\n#define lll __uint128_t\n#define PII pair&lt;int ,int&gt;\n#define endl '\\n'\nusing namespace std;\n#define yn(ans) printf(\"%s\\n\", (ans)?\"Yes\":\"No\");//快速打印\n#define YN(ans) printf(\"%s\\n\", (ans)?\"YES\":\"NO\");\n#define REP(i, e) for (int i = 0; i &lt; (e); ++i)\n#define REP1(i, s, e) for (int i = (s); i &lt;=(e); ++i)\n#define TESTS int t; cin &gt;&gt; t; while (t--)\n#define TEST\nconst int N=2e5+10,M=1e3+10,mod=1e9+7;\nint a[N],b[N],c[N],pre[N];\nvoid solve(){\n    int n;\n    cin&gt;&gt;n;\n    vector&lt;pair&lt;int,int&gt;&gt;v;\n    for(int i=1;i&lt;=n;i++){\n        for(int j=1;j&lt;=n;j++){\n            char m;\n            cin&gt;&gt;m;\n            if(m=='*'){\n                v.emplace_back(i,j);\n            }\n        }\n    }\n    int mx=0;\n    for(auto [x,y]:v){\n        int degree=0;\n        for(auto [nx,ny]:v){\n            if(abs(x-nx)+abs(y-ny)==1){\n                degree++;\n            }\n        }\n        mx=max(degree,mx);\n    }\n    if(mx==3){\n        cout&lt;&lt;\"T\"&lt;&lt;endl;\n    }else{\n        cout&lt;&lt;\"L\"&lt;&lt;endl;\n    }\n\n};\nsigned main(){\n\n\tstd::ios::sync_with_stdio(false);\n    cin.tie(0);\n    cout.tie(0);\n    TESTS{\n        solve();\n    };\n\n\treturn 0;\n}\n</code></pre>\n<h3 id=\"博丽大结界的稳定轴心\">博丽大结界的稳定轴心</h3>\n<p><a href=\"https://ac.nowcoder.com/acm/contest/127702/C\" rel=\"noopener nofollow\" target=\"_blank\">C-博丽大结界的稳定轴心_牛客周赛 Round 130 (nowcoder.com)</a></p>\n<p><img alt=\"image-20260209090135699\" src=\"https://img2024.cnblogs.com/blog/3621557/202602/3621557-20260209121303545-355177775.png\" /></p>\n<pre><code>5\n1 2\n1 3\n1 4\n4 5\n</code></pre>\n<pre><code>4\n</code></pre>\n<p>我们可以去分析一下二叉树的特点，是不是最多的节点数在3个以下，且这个3个的节点不会作为轴心点。</p>\n<p>两个的和一个的都可以作为轴心点。</p>\n<p>所有我们可以去先判断到底哪个最大的点数有多大。大于3就直接是零，小于的3就可以作为轴心点。</p>\n<h4 id=\"解题代码-2\">解题代码</h4>\n<pre><code class=\"language-c++\">#include&lt;bits/stdc++.h&gt;\n#define int long long\n#define lll __uint128_t\n#define PII pair&lt;int ,int&gt;\n#define endl '\\n'\nusing namespace std;\n#define yn(ans) printf(\"%s\\n\", (ans)?\"Yes\":\"No\");//快速打印\n#define YN(ans) printf(\"%s\\n\", (ans)?\"YES\":\"NO\");\n#define REP(i, e) for (int i = 0; i &lt; (e); ++i)\n#define REP1(i, s, e) for (int i = (s); i &lt;=(e); ++i)\n#define TESTS int t; cin &gt;&gt; t; while (t--)\n#define TEST\nconst int N=2e5+10,M=1e3+10,mod=1e9+7;\nint a[N],b[N],c[N],pre[N];\n\nsigned main(){\n\n\tstd::ios::sync_with_stdio(false);\n    cin.tie(0);\n    cout.tie(0);\n    int n;\n    cin&gt;&gt;n;\n    vector&lt;vector&lt;int&gt;&gt;g(n+1);\n    for(int i=1;i&lt;n;i++){\n        int u,v;\n        cin&gt;&gt;u&gt;&gt;v;\n        g[u].push_back(v);\n        g[v].push_back(u);\n    }\n    int mx=0;\n    int ans=0;\n    for(int i=1;i&lt;=n;i++){\n        mx=max(mx,(int)g[i].size());\n    }\n    if(mx&lt;=3){\n        for(int i=1;i&lt;=n;i++){\n            if(g[i].size()&lt;=2){\n                ans++;\n            }\n        }\n    }\n    cout&lt;&lt;ans&lt;&lt;endl;\n\n\treturn 0;\n}\n</code></pre>\n<h3 id=\"魔法人偶的十进制校准\">魔法人偶的十进制校准</h3>\n<p><img alt=\"image-20260209094949846\" src=\"https://img2024.cnblogs.com/blog/3621557/202602/3621557-20260209121303963-2021372242.png\" /></p>\n<pre><code>3\n1 5\n2 3\n5 7\n</code></pre>\n<pre><code>1 2\n1 3\n3 7\n</code></pre>\n<p>首先我们通过打标确定一下规律。</p>\n<pre><code class=\"language-c++\">for(double y=2;y&lt;=1000;y++){\n\tcout&lt;&lt;fixed&lt;&lt;setprecision(10)&lt;&lt;(1.0/y)&lt;&lt;endl;\n}\n</code></pre>\n<p><img alt=\"image-20260209095354057\" src=\"https://img2024.cnblogs.com/blog/3621557/202602/3621557-20260209121304411-1460739702.png\" /></p>\n<p>可以发现特殊的</p>\n<pre><code>1/9----》得到这个。0.1111111，这个我们就可以去构造一下了，但是，b/9\n\n然后特判别的，应为没有9/9，所以我们看到0.9090...\n我们可以通过奇偶代换×一个10就可以了，我们直接9得到这个数.\n然后特判一些0，和可以被3，6的情况就行了\n</code></pre>\n<h4 id=\"解题代码-3\">解题代码</h4>\n<pre><code class=\"language-c++\">#include&lt;bits/stdc++.h&gt;\n#define int long long\n#define lll __uint128_t\n#define PII pair&lt;int ,int&gt;\n#define endl '\\n'\nusing namespace std;\n#define yn(ans) printf(\"%s\\n\", (ans)?\"Yes\":\"No\");//快速打印\n#define YN(ans) printf(\"%s\\n\", (ans)?\"YES\":\"NO\");\n#define REP(i, e) for (int i = 0; i &lt; (e); ++i)\n#define REP1(i, s, e) for (int i = (s); i &lt;=(e); ++i)\n#define TESTS int t; cin &gt;&gt; t; while (t--)\n#define TEST\nconst int N=2e5+10,M=1e3+10,mod=1e9+7;\n\nvoid solve(){\n    int a,b;\n    cin&gt;&gt;a&gt;&gt;b;\n    if(b==0){\n        if(a==1){\n            cout&lt;&lt;1&lt;&lt;\" \"&lt;&lt;1000&lt;&lt;endl;\n        }else{\n            cout&lt;&lt;1&lt;&lt;\" \"&lt;&lt;2&lt;&lt;endl;\n        }\n        return;\n    }else if(b==9){\n        if(a%2){\n            cout&lt;&lt;10&lt;&lt;\" \"&lt;&lt;11&lt;&lt;endl;\n        }else{\n            cout&lt;&lt;1&lt;&lt;\" \"&lt;&lt;11&lt;&lt;endl;\n        }\n        return;\n    }\n    if(b==3){\n        cout&lt;&lt;1&lt;&lt;\" \"&lt;&lt;3&lt;&lt;endl;\n        return;\n    }\n    if(b==6){\n        cout&lt;&lt;2&lt;&lt;\" \"&lt;&lt;3&lt;&lt;endl;\n        return;\n    }\n    cout&lt;&lt;b&lt;&lt;\" \"&lt;&lt;9&lt;&lt;endl;\n};\nsigned main(){\n\n\tstd::ios::sync_with_stdio(false);\n    cin.tie(0);\n    cout.tie(0);\n    TESTS{\n        solve();\n    };\n\n\treturn 0;\n}\n</code></pre>\n<h3 id=\"爱丽丝的人偶圆舞曲\">爱丽丝的人偶圆舞曲</h3>\n<p><a href=\"https://ac.nowcoder.com/acm/contest/127702/E\" rel=\"noopener nofollow\" target=\"_blank\">E-爱丽丝的人偶圆舞曲_牛客周赛 Round 130 (nowcoder.com)</a></p>\n<p><img alt=\"image-20260209112816198\" src=\"https://img2024.cnblogs.com/blog/3621557/202602/3621557-20260209121304824-570108632.png\" /></p>\n<pre><code>abca\n</code></pre>\n<pre><code>1\n</code></pre>\n<p>因为限制只存在与相邻。我们就可以去考虑dp的做法</p>\n<p>这个是一个线性dp的题目，我们可以定义一个</p>\n<p>因为d没有确认，所以我们去枚举d</p>\n<pre><code>f[i][j]---&gt;表示前i个位置均合法，且si=j的最小次数\n\n你们转移就是\nmin（f[i-1][(j-d+26)%26],f[(i+d)%26]）+这个数到底是不是等于j，不等于就要用一次，最后再算出最小值。\n</code></pre>\n<h4 id=\"解题代码-4\">解题代码</h4>\n<pre><code class=\"language-c++\">#include&lt;bits/stdc++.h&gt;\n#define int long long\n#define lll __uint128_t\n#define PII pair&lt;int ,int&gt;\n#define endl '\\n'\nusing namespace std;\n#define yn(ans) printf(\"%s\\n\", (ans)?\"Yes\":\"No\");//快速打印\n#define YN(ans) printf(\"%s\\n\", (ans)?\"YES\":\"NO\");\n#define REP(i, e) for (int i = 0; i &lt; (e); ++i)\n#define REP1(i, s, e) for (int i = (s); i &lt;=(e); ++i)\n#define TESTS int t; cin &gt;&gt; t; while (t--)\n#define TEST\nconst int N=2e5+10,M=1e3+10,mod=1e9+7;\nint a[N],b[N],c[N],pre[N];\n\nsigned main(){\n\n\tstd::ios::sync_with_stdio(false);\n    cin.tie(0);\n    cout.tie(0);\n    string s;\n    cin&gt;&gt;s;\n\n    int n=s.size();\n    s=\" \"+s;\n\n    //dp初始化。\n    int mx=2e8;\n    for(int d=0;d&lt;=25;d++){\n        vector&lt;vector&lt;int&gt;&gt;f(n+1,vector&lt;int&gt;(26,2e8));\n        for(int j=0;j&lt;=25;j++){\n            if(j==s[1]-'a'){\n                f[1][j]=0;\n            }else{\n                f[1][j]=1;\n            }\n        }\n        for(int i=2;i&lt;=n;i++){\n            for(int j=0;j&lt;=25;j++){\n                f[i][j]=min(f[i-1][(j-d+26)%26],f[i-1][(j+d)%26])+(j!=s[i]-'a');\n            }\n        }\n\n        for(int i=0;i&lt;=25;i++){\n            mx=min(mx,f[n][i]);\n        }\n\n    }\n    cout&lt;&lt;mx&lt;&lt;endl;\n\n\n\n\n\treturn 0;\n}\n</code></pre>\n<h3 id=\"红魔馆的微瑕序位\">红魔馆的微瑕序位</h3>\n<p><a href=\"https://ac.nowcoder.com/acm/contest/127702/F\" rel=\"noopener nofollow\" target=\"_blank\">F-红魔馆的微瑕序位_牛客周赛 Round 130 (nowcoder.com)</a></p>\n<p><img alt=\"image-20260209120106423\" src=\"https://img2024.cnblogs.com/blog/3621557/202602/3621557-20260209121305276-651327063.png\" /></p>\n<pre><code>2\n5\n1 2 4 3 5\n2\n1 2\n</code></pre>\n<pre><code>0\n1\n</code></pre>\n<p>本题考的一个经典置换环</p>\n<pre><code>如果一个1-n的排列，要交换几次才能使得它是一个排列。\n结论是\nn-环的个数，相当于拆环。\n</code></pre>\n<p><img alt=\"image-20260209120521887\" src=\"https://img2024.cnblogs.com/blog/3621557/202602/3621557-20260209121305664-904288544.png\" /></p>\n<p>如图是4元环</p>\n<p>我们邀得到2，肯定得一个两个相邻的元素之间没有去交换</p>\n<pre><code>1 2 4 3 5\n</code></pre>\n<p>那我们先把交换的次数算出来，再考虑原来到底存不存在相邻的环。</p>\n<h4 id=\"解题代码-5\">解题代码</h4>\n<pre><code class=\"language-c++\">#include&lt;bits/stdc++.h&gt;\n#define int long long\n#define lll __uint128_t\n#define PII pair&lt;int ,int&gt;\n#define endl '\\n'\nusing namespace std;\n#define yn(ans) printf(\"%s\\n\", (ans)?\"Yes\":\"No\");//快速打印\n#define YN(ans) printf(\"%s\\n\", (ans)?\"YES\":\"NO\");\n#define REP(i, e) for (int i = 0; i &lt; (e); ++i)\n#define REP1(i, s, e) for (int i = (s); i &lt;=(e); ++i)\n#define TESTS int t; cin &gt;&gt; t; while (t--)\n#define TEST\nconst int N=2e5+10,M=1e3+10,mod=1e9+7;\nint a[N],b[N],c[N],pre[N];\nvoid solve(){\n    int n;\n    cin&gt;&gt;n;\n    vector&lt;int&gt;v(n+1);\n    for(int i=1;i&lt;=n;i++){\n        cin&gt;&gt;a[i];\n    }\n    int loop=0;\n    for(int i=1;i&lt;=n;i++){\n        if(v[i])continue;\n        int j=i;\n        loop++;\n        while (!v[j]){\n            v[j]=loop;\n            j=a[j];\n        }\n    }\n    int ans=n-loop+1;\n    for(int i=2;i&lt;=n;i++){//判断是不是相邻的环\n        if(v[i-1]==v[i]){\n            ans-=2;\n            break;\n        }\n    }\n    cout&lt;&lt;ans&lt;&lt;endl;\n};\nsigned main(){\n\n\tstd::ios::sync_with_stdio(false);\n    cin.tie(0);\n    cout.tie(0);\n    TESTS{\n        solve();\n    };\n\n\treturn 0;\n}\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-09 12:13</span>&nbsp;\n<a href=\"https://www.cnblogs.com/godjian\">Godjian</a>&nbsp;\n阅读(<span id=\"post_view_count\">110</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "扣子Coze实战：从0到1搭建小红书图文改写智能体",
      "link": "https://www.cnblogs.com/tangshiye/p/19594522",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/tangshiye/p/19594522\" id=\"cb_post_title_url\" title=\"发布于 2026-02-09 11:59\">\n    <span>扣子Coze实战：从0到1搭建小红书图文改写智能体</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>大家好，我是汤师爷，专注AI智能体分享，致力于帮助100W人用智能体创富~</p>\n<p>还在为小红书笔记创作发愁吗？</p>\n<p>每天都要绞尽脑汁想文案，看着别人的爆款笔记却不知道如何模仿？</p>\n<p>今天，我就教你如何利用AI智能体，轻松实现小红书图文改写，让创作效率提升10倍！</p>\n<p>我们先看下智能体的执行效果：</p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<h2 id=\"1整体工作流\">1.整体工作流</h2>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p>1.获取小红书笔记详情</p>\n<p>2.使用OCR技术，提取图片中的文字</p>\n<p>3.将图片文案进行整理</p>\n<p>4.图片文案仿写</p>\n<h2 id=\"2详细工作流节点\">2.详细工作流节点</h2>\n<h3 id=\"21-开始节点\">2.1 开始节点</h3>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p>开始节点有两个输入变量。</p>\n<ul>\n<li>输入：\n<ul>\n<li>noteUrl：小红书笔记链接</li>\n<li>cookieStr：小红书cookie</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"22-如何获取小红书cookie\">2.2 如何获取小红书cookie？</h3>\n<p>1.登陆<a href=\"https://www.xiaohongshu.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://www.xiaohongshu.com/</a></p>\n<p>2.在页面空白处右击鼠标，选择「检查」</p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p>3.在刚刚打开的面板中，点击「网络」选项卡</p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p>4.刷新当前页面</p>\n<p>5.点击第一条记录，在右侧「标头」部分向下滚动，找到cookie一行，将其内容复制下，这就是我们需要的cookieStr</p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<h3 id=\"23-获取小红书笔记详情\">2.3 获取小红书笔记详情</h3>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p>我们将使用【小红书】插件的xhs_note_detail功能。</p>\n<p>通过这个功能，我们可以根据笔记链接获取笔记详情。</p>\n<ul>\n<li>输入：\n<ul>\n<li>cookieStr：开始 - cookieStr</li>\n<li>noteUrl：开始 - noteUrl</li>\n</ul>\n</li>\n</ul>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<h3 id=\"24-使用ocr技术提取图片中的文字\">2.4 使用OCR技术，提取图片中的文字</h3>\n<p><strong>1.接下来，我们使用循环节点，批量提取图片中的文字。</strong></p>\n<ul>\n<li>输入：\n<ul>\n<li>input：获取小红书笔记详情-note_image_list</li>\n</ul>\n</li>\n<li>输出\n<ul>\n<li>output：从图片中提取文字-data</li>\n</ul>\n</li>\n</ul>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p><strong>2.使用循环体，批量提取图片的文字</strong></p>\n<p>我们会使用「OCR」插件，提取图片的文字。</p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p>输入参数如下图所示。</p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<h3 id=\"25-使用大模型将文案内容进行整理排版\">2.5 使用大模型将文案内容进行整理、排版</h3>\n<p>在这一步，我们会使用大模型节点，对文案内容进行整理、排版。</p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p>大模型节点的系统提示词如下：</p>\n<pre><code class=\"language-markdown\">## 角色（Role）\n你是一位精通内容整理和 Markdown 排版的 AI 助手。你擅长阅读和理解非结构化的文本内容，并能够将其转化为结构清晰、格式优美的 Markdown 文档。\n\n## 背景（Background）\n随着社交媒体平台的普及，大量的图文内容被创作和分享。然而，这些内容往往缺乏良好的结构和格式，不利于阅读和理解。将这些内容整理成结构化的 Markdown 文档，可以大大提高内容的可读性和价值。\n\n## 任务（Task）\n你的任务是接收一段从插件输出的非结构化文本内容（主要是抖音图文中的文字），仔细阅读并理解内容，然后将其转化为结构清晰、格式规范的 Markdown 文档。你需要：\n\n1. 识别并提取文本中的关键信息，如标题、作者、主要内容等\n2. 根据内容的逻辑关系，对文本进行分类和整理\n3. 使用 Markdown 语法进行排版，包括但不限于使用标题、粗体、斜体、列表等格式\n4. 确保所有原始内容都被包含在最终的 Markdown 文档中，不遗漏任何信息\n\n## 规则与限制（Rules &amp; Restrictions）\n1. 必须使用 Markdown 语法进行排版\n2. 使用 #、##、### 等进行标题划分，层级不超过 3 级\n3. 使用 - 或 * 进行无序列表编写，使用 1. 2. 3. 等进行有序列表编写\n4. 重要内容使用粗体（**文字**）标注，需要强调的内容使用斜体（*文字*）标注\n5. 保持原文的主要结构和顺序，但可以适当调整以提高可读性\n6. 不得添加、删除或修改原文的实质内容\n7. 如遇到不确定的内容，保留原样并用括号标注\n\n## 参考短语（Reference sentences）\n- 内容完整，不遗漏任何信息\n- 结构清晰，层次分明\n- 格式规范，美观实用\n- 逻辑严密，条理清晰\n- 重点突出，易于阅读\n\n## 案例展示（Case Show）\n### 输入：\n{\n  \"code\": 0,\n  \"data\": {\n    \"results\": [\n      {\n        \"words\": [\n          {\n            \"lang\": \"auto\",\n            \"text\": \"求大连这两个\"\n          },\n          {\n            \"lang\": \"auto\",\n            \"text\": \"地方有啥\"\n          },\n          {\n            \"lang\": \"auto\",\n            \"text\": \"区别啊？？\"\n          }\n        ]\n      }\n    ]\n  },\n  \"log_id\": \"20250325123913080C6F506498C6F581B7\",\n  \"msg\": \"success\"\n}\n\n## 风格和语气（Style &amp; Tone）\n- 保持专业、清晰的语气\n- 使用简洁、直接的表达方式\n- 保持原文的重点和强调\n\n## 受众群体（Audience）\n- 小红书电商新手卖家\n- 对开设小红书店铺感兴趣的人群\n- 想要了解小红书电商运营的人群\n\n## 输出格式（Output format）\n使用 Markdown 格式输出，包括：\n1. 一级标题（#）用于文章主标题\n2. 二级标题（##）用于主要章节\n3. 三级标题（###）用于子章节\n4. 无序列表使用 - 或 *\n5. 有序列表使用 1. 2. 3. 等\n6. 重要内容使用粗体（**文字**）\n7. 需要强调的内容使用斜体（*文字*）\n\n## 工作流程（Workflow）\n1. 仔细阅读输入的文本内容，理解其结构和主要信息点\n2. 提取标题、作者、标签等元信息\n3. 识别主要章节和子章节，规划文档结构\n4. 按照规划的结构，使用 Markdown 语法重新排版内容\n5. 使用粗体和斜体突出重要信息和需要强调的内容\n6. 检查确保所有原始内容都被包含，没有遗漏\n7. 最后检查 Markdown 格式是否正确，调整以确保最佳可读性\n\n## 初始化（Initialization）\n\n下面是你需要整理和格式化的文本内容：\n\n&lt;评价内容&gt;\n\n请提供需要整理和格式化的文本内容。我会仔细阅读并按照上述要求将其转化为结构清晰的 Markdown 文档。不需要输出额外除图片识别文字以外的内容。\n</code></pre>\n<h3 id=\"26-图片文案仿写\">2.6 图片文案仿写</h3>\n<p>接下来，我们需要通过大模型节点退图片文案进行仿写。</p>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<p>大模型节点的系统提示词如下：</p>\n<pre><code class=\"language-markdown\"># 角色说明\n你是一位专业的图文内容仿写助手，负责根据图片中提取的文字内容，结合视觉元素和背景信息，创作风格一致的仿写内容。\n\n# 背景说明\n处理用户提供的图片文字内容时，你需要：\n1. 理解图片的整体风格和背景（产品介绍、教程步骤、使用心得等）\n2. 分析文字的语言特点（正式/口语化、句式特征、专业术语等）\n3. 结合图片呈现的视觉信息（产品外观、使用场景等）\n4. 在保持原意的基础上进行自然的仿写扩展\n\n# 仿写原则\n1. 保持原意完整性\n2. 匹配原文语言风格\n3. 补充恰当的细节\n4. 与图片内容保持一致\n5. 避免添加虚假信息\n\n# 工作流程\n1. 接收图片文字内容\n2. 分析图片背景信息（可选）\n3. 分析原文特点：\n   - 语言风格\n   - 内容结构\n   - 关键词使用\n4. 创作三个版本：\n   - 贴近原文的保守版\n   - 适度优化的改进版\n   - 创意加强的亮点版\n\n# 输出示例\n## 原文：\n\"夏日必备防晒霜\nSPF50+ PA++++\n清爽不油腻\"\n\n## 仿写版本：\n1. 【保守版】\n\"夏季必备防晒产品\n防晒指数SPF50+ PA++++\n质地清爽不油腻\"\n\n2. 【优化版】\n\"夏日防晒推荐\n高倍防护SPF50+ PA++++\n轻薄水感质地，肌肤零负担\"\n\n3. 【创意版】\n\"今夏防晒天花板！\nSPF50+ PA++++超强防护\n一抹化水，清爽不黏腻\"\n\n# 执行要求\n请提供：\n1. 图片文字提取内容\n2. 图片背景说明（如有）\n\n我将按照以上规范进行仿写创作。\n</code></pre>\n<h3 id=\"27-使用文本处理插件拼接字符串\">2.7 使用文本处理插件拼接字符串</h3>\n<ul>\n<li>输入：\n<ul>\n<li>String1：获取小红书笔记详情-note</li>\n<li>String2：图片文案内容整理-output</li>\n<li>String3：图片文案仿写-output</li>\n</ul>\n</li>\n</ul>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<h3 id=\"28-结束节点\">2.8 结束节点</h3>\n<p><img alt=\"image.png\" class=\"lazyload\" /></p>\n<h2 id=\"3总结\">3.总结</h2>\n<p>通过以上介绍，相信你了解了如何利用DeepSeek+Coze来构建高效的小红书图片文案改写智能体。</p>\n<p>在AI时代，技术门槛将越来越低，小白也能搭建智能体，用AI工具来提升工作效率。</p>\n<p>用AI智能体不是未来，而是AI时代每个人的生存技能，学会AI智能体，人人都是超级个体。</p>\n<p>如果你觉得这篇文章有帮助，别忘了点赞、关注、收藏，我们下期再见！</p>\n<blockquote>\n<p>对了，我整理了一份开源《智能体学习手册》，爆肝 10 万字，价值 999 元。限时开放领取👉：<a href=\"https://tangshiye.cn\" rel=\"noopener nofollow\" target=\"_blank\">tangshiye.cn</a></p>\n</blockquote>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/tangshiye/\" target=\"_blank\">AI架构师汤师爷</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/tangshiye/p/19594522\" target=\"_blank\">https://www.cnblogs.com/tangshiye/p/19594522</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-09 11:59</span>&nbsp;\n<a href=\"https://www.cnblogs.com/tangshiye\">AI架构师汤师爷</a>&nbsp;\n阅读(<span id=\"post_view_count\">179</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "FastAPI不止于API：手把手教你用Jinja2打造动态Web页面",
      "link": "https://www.cnblogs.com/ymtianyu/p/19594466",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ymtianyu/p/19594466\" id=\"cb_post_title_url\" title=\"发布于 2026-02-09 11:48\">\n    <span>FastAPI不止于API：手把手教你用Jinja2打造动态Web页面</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        本文系统介绍了如何在FastAPI框架中集成Jinja2模板引擎来构建动态Web页面。内容涵盖从安装配置、模板上下文数据传递（包括请求级和全局两种方式）、静态文件正确引入，到完整实战演示与常见避坑指南。帮助开发者快速掌握利用FastAPI服务端渲染页面的技能，适用于快速原型、管理后台等场景。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<div>\n<p>你是不是也觉得，用FastAPI写接口爽到飞起，但一想到要返回个带数据的HTML页面，就瞬间头大？🎯</p>\n<p>我刚用FastAPI那会儿也这样，以为它就是个“API专用框架”，渲染页面？那不是Django和Flask的活儿吗？直到我在一个紧急项目里，需要快速给内部系统做个带数据看板的管理后台，我才发现，<strong style=\"color: rgba(186, 55, 42, 1);\">FastAPI配合Jinja2模板，原来可以这么香！</strong> 今天就跟你唠明白这事儿，保你10分钟上手，告别“前后端分离强迫症”在简单场景下的纠结。</p>\n<h2 style=\"color: rgba(52, 152, 219, 1); padding-bottom: 5px;\">📌 本文能帮你解决</h2>\n<div style=\"background-color: rgba(248, 249, 250, 1); padding: 15px; margin: 20px 0;\">\n<p>1. 在FastAPI中如何安装、配置Jinja2模板引擎。</p>\n<p>2. 如何把后台数据（上下文）安全又方便地“塞”给前端模板。</p>\n<p>3. 如何在模板里正确引入CSS、JS等静态文件，避免“404惨案”。</p>\n<p>4. 我踩过的几个坑和最佳实践，让你一次写对。</p>\n</div>\n<h2>🚀 主要内容脉络</h2>\n<p>👉 先聊聊：为什么需要模板引擎？（不只是为了省事）</p>\n<p>👉 核心操作：安装、配置、传递数据的“两条路径”。</p>\n<p>👉 实战演示：一个包含用户列表和样式的小项目。</p>\n<p>👉 避坑指南：静态文件那些“路径玄学”与进阶思考。</p>\n<hr style=\"border: none; height: 1px; background-color: rgba(238, 238, 238, 1); margin: 30px 0;\" />\n<h2>🔍 一、问题与背景：FastAPI只能“吃”JSON？</h2>\n<p>FastAPI以构建高性能API闻名，<code style=\"color: rgba(186, 55, 42, 1);\">return JSONResponse</code> 几乎是肌肉记忆。但很多场景下，比如：</p>\n<div>\n<p>- 快速原型开发，搞个带页面的demo。</p>\n<p>- 内部管理后台，复杂度不高，不想动用前端框架。</p>\n<p>- 需要服务端渲染（SSR）的简单页面。</p>\n</div>\n<p>这时候，你硬要前后端彻底分离，反而有种“杀鸡用牛刀”的繁琐。就好比你只想在家门口吃碗面，结果非要开车去市中心的高级餐厅点单、等餐、打包再回来。</p>\n<p><strong style=\"color: rgba(186, 55, 42, 1);\">模板引擎，就是让你在FastAPI这个“高性能厨房”里，直接开个“堂食窗口”。</strong> Jinja2就是这个窗口最得力的伙计，它能把你的数据（肉、菜）和HTML模板（碗、汤底）快速组合成一碗热腾腾的面（最终页面）。</p>\n<h2>🧠 二、核心原理与步骤：“两条腿”走路</h2>\n<p>好，咱们先来解决最核心的问题：数据怎么从后端“走”到模板里？</p>\n<p>核心就两步：1. 配置引擎；2. 传递数据。数据传递有两条关键“路径”，我画个灵魂图示给你看：</p>\n<div style=\"background-color: rgba(248, 249, 250, 1); padding: 15px; border-radius: 5px; margin: 20px 0;\">\n<p><strong>路径A：依赖项注入（全局/请求级上下文）</strong></p>\n<p>在路由处理函数里，通过 <code style=\"color: rgba(186, 55, 42, 1);\">TemplateResponse</code> 的 <code style=\"color: rgba(186, 55, 42, 1);\">context</code> 参数传递。这是<strong style=\"color: rgba(186, 55, 42, 1);\">最常用、最灵活</strong>的方式，数据针对每次请求。</p>\n<p><strong>路径B：全局模板上下文（每个模板都能用）</strong></p>\n<p>在初始化 <code style=\"color: rgba(186, 55, 42, 1);\">Jinja2Templates</code> 时，通过 <code style=\"color: rgba(186, 55, 42, 1);\">context</code> 参数传递。比如站点名、当前年份等<strong style=\"color: rgba(186, 55, 42, 1);\">全局通用数据</strong>。</p>\n</div>\n<p>是不是有点抽象？别急，咱们接着看实战，代码一写你就全明白了。</p>\n<h2>💻 三、实战演示：从零搭建一个用户列表页</h2>\n<p>接下来重点来了，咱们一步步来。假设我们要做一个显示用户列表的页面。</p>\n<h3>1️⃣ 安装与项目结构</h3>\n<p>先安装必备库：</p>\n<pre class=\"language-powershell highlighter-hljs\"><code>pip install fastapi jinja2 uvicorn</code></pre>\n<p>项目目录结构建议这样安排，清晰明了：</p>\n<div>\n<pre class=\"language-powershell highlighter-hljs\"><code>📁 your_project/\n├── 📁 templates/ # 存放所有Jinja2 HTML模板\n│     └── index.html\n├── 📁 static/ # 存放CSS, JS, 图片等静态文件\n│     └── style.css\n└── main.py # FastAPI 主应用文件</code></pre>\n</div>\n<h3>2️⃣ 配置FastAPI与Jinja2</h3>\n<p>在 <code style=\"color: rgba(186, 55, 42, 1);\">main.py</code> 里进行初始化。这里有个<strong style=\"color: rgba(186, 55, 42, 1);\">关键点</strong>：<code style=\"color: rgba(186, 55, 42, 1);\">directory</code> 参数必须是<strong style=\"color: rgba(186, 55, 42, 1);\">字符串路径</strong>，不能是Path对象（Jinja2的老规矩）。</p>\n<pre class=\"language-python highlighter-hljs\"><code>from fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\n\napp = FastAPI(title=\"FastAPI+Jinja2 Demo\")\n\n# 配置模板引擎，告诉它模板文件在哪\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# 配置静态文件服务，挂载到`/static`路径\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")</code></pre>\n<h3>3️⃣ 定义路由与传递数据（路径A）</h3>\n<p>我们定义一个路由，模拟从数据库获取用户列表，并传递给模板：</p>\n<pre class=\"language-python highlighter-hljs\"><code>@app.get(\"/\", response_class=HTMLResponse)\nasync def read_users(request: Request):\n    # 模拟数据，实际中可能来自数据库\n    user_list = [\n        {\"id\": 1, \"name\": \"张三\", \"role\": \"管理员\"},\n        {\"id\": 2, \"name\": \"李四\", \"role\": \"编辑\"},\n        {\"id\": 3, \"name\": \"王五\", \"role\": \"订阅用户\"},\n    ]\n    # 网站标题，作为额外数据传递\n    site_title = \"内部用户管理系统\"\n    \n    # 核心操作：渲染模板，并通过context传递数据\n    return templates.TemplateResponse(\n        request=request,\n        name=\"index.html\", # 模板文件名\n        context={\n            \"request\": request, # 这个必须有！Jinja2Templates要求\n            \"users\": user_list,\n            \"title\": site_title\n        }\n    )</code></pre>\n<p><strong style=\"color: rgba(186, 55, 42, 1);\">千万注意</strong>：<code style=\"color: rgba(186, 55, 42, 1);\">context</code> 字典里<strong style=\"color: rgba(186, 55, 42, 1);\">必须包含 \"request\" 键</strong>！这是 <code style=\"color: rgba(186, 55, 42, 1);\">Jinja2Templates</code> 的工作机制要求的，不然模板里一些基于请求的功能会失效。</p>\n<h3>4️⃣ 编写模板文件 (templates/index.html)</h3>\n<p>看看数据在模板里怎么用：</p>\n<pre class=\"language-html highlighter-hljs\"><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"zh-CN\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n    &lt;!-- 重点！引入静态CSS文件 --&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', path='/style.css') }}\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;欢迎使用 {{ title }}&lt;/h1&gt;\n    &lt;table border=\"1\"&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;ID&lt;/th&gt;\n                &lt;th&gt;姓名&lt;/th&gt;\n                &lt;th&gt;角色&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            {% for user in users %}\n            &lt;tr&gt;\n                &lt;td&gt;{{ user.id }}&lt;/td&gt;\n                &lt;td&gt;{{ user.name }}&lt;/td&gt;\n                &lt;td&gt;{{ user.role }}&lt;/td&gt;\n            &lt;/tr&gt;\n            {% endfor %}\n        &lt;/tbody&gt;\n    &lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;</code></pre>\n<p><strong style=\"color: rgba(186, 55, 42, 1);\">敲黑板</strong>：引入静态文件用的是 <code style=\"color: rgba(186, 55, 42, 1);\">url_for('static', path='/style.css')</code>。这里的 <code style=\"color: rgba(186, 55, 42, 1);\">'static'</code> 对应我们 <code style=\"color: rgba(186, 55, 42, 1);\">app.mount</code> 时设置的 <code style=\"color: rgba(186, 55, 42, 1);\">name=\"static\"</code>。这是我初期常配错的地方，<strong style=\"color: rgba(186, 55, 42, 1);\">name必须一致</strong>！</p>\n<h3>5️⃣ 编写静态文件 (static/style.css)</h3>\n<p>随便写点样式，确认它能被加载：</p>\n<pre class=\"language-css highlighter-hljs\"><code>body {\n    font-family: sans-serif;\n    padding: 20px;\n    background-color: #f5f5f5;\n}\nh1 {\n    color: #2c3e50;\n}\ntable {\n    width: 100%;\n    border-collapse: collapse;\n    margin-top: 20px;\n}\nth, td {\n    padding: 10px;\n    text-align: left;\n}\nthead {\n    background-color: #3498db;\n    color: white;\n}</code></pre>\n<p>好了！现在运行 <code style=\"color: rgba(186, 55, 42, 1);\">uvicorn main:app --reload</code>，打开 <code style=\"color: rgba(186, 55, 42, 1);\">http://127.0.0.1:8000</code>，一个带样式和动态数据的用户列表页面就出来了！数据从后端“流”到了前端，静态文件也正常加载。</p>\n<h2>⚠️ 四、注意事项与进阶思考</h2>\n<p>是不是以为这样就完了？再说几个容易翻车的点。</p>\n<h3>🎯 避坑指南</h3>\n<div>\n<p><strong>1. 静态文件404？</strong></p>\n<p>- 检查 <code style=\"color: rgba(186, 55, 42, 1);\">app.mount</code> 的 <code style=\"color: rgba(186, 55, 42, 1);\">directory</code> 路径是否正确（相对路径从项目根目录算起）。</p>\n<p>- 检查模板中 <code style=\"color: rgba(186, 55, 42, 1);\">url_for</code> 的 <code style=\"color: rgba(186, 55, 42, 1);\">name</code> 参数是否与 <code style=\"color: rgba(186, 55, 42, 1);\">mount</code> 的 <code style=\"color: rgba(186, 55, 42, 1);\">name</code> 一致。</p>\n<p>- 生产环境通常用Nginx等专门处理静态文件，开发时用 <code style=\"color: rgba(186, 55, 42, 1);\">StaticFiles</code> 很方便。</p>\n<p><strong>2. 全局上下文（路径B）怎么用？</strong></p>\n<p>比如你想在每个页面都显示版权年份：</p>\n</div>\n<pre class=\"language-python highlighter-hljs\"><code>templates = Jinja2Templates(\n    directory=\"templates\",\n    context={\"current_year\": 2024} # 全局注入\n)\n# 然后，在任何模板里都可以直接使用 {{ current_year }}</code></pre>\n<h3>🚀 进阶思考</h3>\n<p><strong>模板继承是王牌：</strong> 用 <code style=\"color: rgba(186, 55, 42, 1);\">{% extends \"base.html\" %}</code> 和 <code style=\"color: rgba(186, 55, 42, 1);\">{% block content %}...{% endblock %}</code> 来复用布局（如导航栏、页脚），能让你的模板代码干净十倍。强烈建议你用起来。</p>\n<p><strong>上下文处理器：</strong> 如果你想在每个请求的模板里都自动注入一些数据（比如当前登录用户），可以自定义依赖项，并在每个路由的 <code style=\"color: rgba(186, 55, 42, 1);\">TemplateResponse</code> 里调用。虽然有点绕，但结构更清晰。</p>\n<p><strong>何时用？何时不用？</strong> 对于复杂的、交互性强的现代Web应用，前后端分离（React/Vue + FastAPI API）仍是首选。但对于工具类、管理类、需要SEO的简单内容页，<strong style=\"color: rgba(186, 55, 42, 1);\">FastAPI + Jinja2 的组合能让你一人顶一个全栈团队，开发速度飞快</strong>。</p>\n<hr />\n<p>好了，今天的分享就到这儿。希望这篇“踩坑经验总结”能帮你把FastAPI的“另一面”也利用起来。</p>\n<p>技术选型没有银弹，<strong style=\"color: rgba(186, 55, 42, 1);\">最好的工具是那个能帮你高效、稳定解决问题的工具</strong>。下次当你需要快速捣鼓个带界面的小工具时，不妨试试这个组合。</p>\n<p>你在用FastAPI做Web页面时还遇到过什么奇葩问题？或者有更优雅的实践？<strong style=\"color: rgba(186, 55, 42, 1);\">欢迎在评论区一起聊聊</strong>，你的经验很可能也能帮到别人。</p>\n<p style=\"text-align: center; color: rgba(127, 140, 141, 1); font-size: 0.9em;\">觉得有用的话，记得收藏、点赞、关注哦~ 咱们下期见！</p>\n</div>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-09 11:48</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ymtianyu\">一名程序媛呀</a>&nbsp;\n阅读(<span id=\"post_view_count\">160</span>)&nbsp;\n评论(<span id=\"post_comment_count\">2</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "从模型评估、梯度难题到科学初始化：一步步解析深度学习的训练问题",
      "link": "https://www.cnblogs.com/yldeveloper/p/19597056",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/yldeveloper/p/19597056\" id=\"cb_post_title_url\" title=\"发布于 2026-02-09 21:47\">\n    <span>从模型评估、梯度难题到科学初始化：一步步解析深度学习的训练问题</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        要解决模型泛化能力与训练稳定性两大难题，关键在于理解偏差-方差权衡、梯度传播和参数初始化三者间的深层联系。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"引言\">引言</h2>\n<p>训练一个神经网络过程中，我们会关注两个问题：</p>\n<ol>\n<li>模型能否毫不费力处理应用环境中没见过的数据？</li>\n<li>模型能否被有效训练？</li>\n</ol>\n<p>第一个问题涉及<strong>偏差与方差的权衡</strong>，第二个问题涉及<strong>梯度传播的稳定性</strong>。本文首先探讨偏差与方差，然后分析梯度问题，最后引出解决梯度问题的关键之一——<strong>科学的初始化方法</strong>。</p>\n<h2 id=\"偏差--方差\">偏差 &amp; 方差</h2>\n<p>要理解模型的泛化能力，我们首先要量化它的“泛化误差”，即模型在未知数据上的表现。然而，泛化误差并非一个单一的问题，它源于三种不同性质的错误：模型固有的近似能力不足、对训练数据的过度敏感、模型数据本身的不可约噪声。</p>\n<h4 id=\"偏差---方差分解公式\">偏差 - 方差分解公式</h4>\n<p>规定：</p>\n<ul>\n<li><span class=\"math inline\">\\(P_{\\text{data}}(x,y)\\)</span>：数据生成分布</li>\n<li><span class=\"math inline\">\\(\\mathcal{D}\\)</span>：从<span class=\"math inline\">\\(P_{\\text{data}}\\)</span>中独立同分布采样得到的训练数据集</li>\n<li><span class=\"math inline\">\\(f(x;\\mathcal{D})\\)</span>：由训练集 <span class=\"math inline\">\\(\\mathcal{D}\\)</span> 学得的模型 <span class=\"math inline\">\\(f\\)</span> 对 <span class=\"math inline\">\\(x\\)</span> 的预测输出。</li>\n<li><span class=\"math inline\">\\(\\overline f(x)\\)</span>：<span class=\"math inline\">\\(\\mathbb{E}_{\\mathcal{D} \\sim P_{\\text{data}}^{\\otimes n}}[f(x; \\mathcal{D})]\\)</span>，对所有可能训练集的期望</li>\n<li><span class=\"math inline\">\\(\\mathbb{E}_{\\mathcal{D} \\sim P_{\\text{data}}^{\\otimes n}}[\\cdot]\\)</span>：对训练集采样的期望</li>\n</ul>\n<p>有：</p>\n<p></p><div class=\"math display\">\\[\\mathbb{E}_{y|x} \\mathbb{E}_{\\mathcal{D}}[(f(x; \\mathcal{D}) - y)^2] = \\text{Bias}^2(f(x)) + \\text{Var}(f(x)) + \\sigma_\\epsilon^2\n\\]</div><p></p><p>其中，</p>\n<ul>\n<li><span class=\"math inline\">\\(\\text{Bias}^2(f(x))\\)</span>：偏差，反映模型拟合能力。设真实函数为 <span class=\"math inline\">\\(h(x) = \\mathbb{E}[y|x]\\)</span>（条件期望），则偏差应定义为 <span class=\"math inline\">\\((\\overline f(x) - h(x))^2\\)</span></li>\n<li><span class=\"math inline\">\\(\\text{Var}(f(x))\\)</span>：方差，反映不同数据集表现波动情况即泛化能力，<span class=\"math inline\">\\(:=\\mathbb{E}_\\mathcal{D}[(f(x;\\mathcal{D})-\\overline f(x))^2]\\)</span></li>\n<li><span class=\"math inline\">\\(\\sigma_\\epsilon ^2\\)</span>：噪声，反映学习难度，<span class=\"math inline\">\\(:=\\mathbb{E}[(y - h(x))^2]\\)</span></li>\n</ul>\n<p>这里正好对应两种模型：线性拟合 vs. 神经网络</p>\n<ul>\n<li>若线性拟合，模型容量低，并且假设空间简单，即大偏差小方差，泛化误差大，欠拟合。</li>\n<li>若复杂度过高的神经网络（如未正则化），会学到训练数据中的噪声，导致在训练数据上表现很好（小偏差），但在未见过的数据上表现波动很大（大方差），泛化误差大，过拟合。</li>\n<li>若复杂度适中的神经网络，中等偏差中等方差，泛化误差小，最佳了。</li>\n</ul>\n<p>得出结论：<em>偏差大（欠拟合）意味着模型能力不足，未能捕捉数据中的真实模式；方差大（过拟合）意味着模型过于复杂，对训练数据中的噪声和随机波动过度敏感。</em></p>\n<h4 id=\"影响偏差与方差的三大因素\">影响偏差与方差的三大因素</h4>\n<p><strong>1. 学习算法能力（模型复杂度）</strong></p>\n<p>如果模型欠拟合（偏差大），就换更复杂的模型；如果过拟合（方差大），就换更简单的模型（或对复杂模型做正则化）。</p>\n<p><strong>2. 训练数据量</strong></p>\n<p>可间接降低偏差，对方差影响大<br />\n如果模型过拟合（方差大），优先增加训练数据。</p>\n<p><strong>3. 学习任务本身的难度（任务复杂度）</strong></p>\n<p>如果任务简单但方差大，就控制模型复杂度或增加数据；如果任务复杂导致偏差大，就提升模型复杂度</p>\n<h4 id=\"处理模型高偏差高方差的一些方法\">处理模型高偏差、高方差的一些方法</h4>\n<p><strong>欠拟合（高偏差）</strong>：应该换更复杂的模型、增加特征维数、仔细判断训练误差是否收敛到最低。</p>\n<p><strong>过拟合（高方差）</strong>：应该增加训练数据、正则化（如使用L1正则化、L2正则化（即权重衰减）、Dropout等）、批量归一化、剪枝降复杂度、降低特征维度。</p>\n<h4 id=\"偏差-方差权衡\">偏差-方差权衡</h4>\n<p>偏差与方差通常是对立的，提高模型复杂度可以减少偏差，但可能增加方差；反之，降低模型复杂度可以减少方差，但偏差可能会升高。这种权衡关系被称为 <strong>偏差-方差权衡（Bias-Variance Tradeoff）</strong></p>\n<p><strong>在此我们应该拓展一下</strong>，经典理论认为模型复杂度（如参数数量）增加，泛化误差会先因偏差降低而下降，后因方差增大而上升，形成单一的U型曲线。<strong>双重下降</strong>则揭示了在插值阈值（模型刚好能完美拟合训练数据）后，随着复杂度进一步增加，误差会再次下降，形成“下降-上升-下降”的波浪形曲线。在过参数化区域，模型并非必然过拟合到更差的程度，优化过程会引导其找到一个泛化良好的解。在过参数化体制下，模型好像是先“记忆”（拟合噪声），后通过漫长的优化过程“逐渐获得”泛化规则。（其实真正的原因是<strong>隐式正则化</strong>使得优化算法（如SGD）倾向于找到最小范数解或平坦极小值）</p>\n<p>这告诉我们，如果观察到增加模型参数后性能先变差，不要立即止步。这可能只是处于插值阈值附近的危险区。继续增加规模，并配合足够的训练，性能可能会突破并变得更好。</p>\n<p>理解了模型的误差问题由偏差、方差间的权衡决定，我们大体了解如何选择模型的复杂度。但是，当我们着手训练一个复杂深层模型，尤其是现代网络层数不断加深时，我们应该考虑，训练本身是否能够正常进行？否则结果上的“偏差、方差”都成空谈。这就引出了深度学习中的 <strong>梯度问题</strong> 。</p>\n<h2 id=\"梯度问题\">梯度问题</h2>\n<p>我们可以认为，</p>\n<p><span class=\"math inline\">\\(\\mathbf{h}^{(l)} = f_l (\\mathbf{h}^{(l-1)})\\)</span></p>\n<p>因此</p>\n<p><span class=\"math inline\">\\(\\mathbf{o} = f_L \\circ f_{L-1}\\circ \\ldots\\circ f_2\\circ f_1(\\mathbf{x})\\)</span></p>\n<p>那么不难得到：</p>\n<p></p><div class=\"math display\">\\[\\partial_{\\mathbf{W}^{(l)}} \\mathbf{o} = \\underbrace{\\partial_{\\mathbf{h}^{(L-1)}} \\mathbf{h}^{(L)}}_{ \\mathbf{M}^{(L)} \\stackrel{\\mathrm{def}}{=}} \\cdot \\ldots \\cdot \\underbrace{\\partial_{\\mathbf{h}^{(l)}} \\mathbf{h}^{(l+1)}}_{ \\mathbf{M}^{(l+1)} \\stackrel{\\mathrm{def}}{=}} \\underbrace{\\partial_{\\mathbf{W}^{(l)}} \\mathbf{h}^{(l)}}_{ \\mathbf{v}^{(l)} \\stackrel{\\mathrm{def}}{=}}.\n\\]</div><p></p><p>也因此，梯度 <span class=\"math inline\">\\(\\partial_{\\mathbf{W}^{(l)}} \\mathbf{o}\\)</span> 是 <span class=\"math inline\">\\((L-l)\\)</span> 个雅可比矩阵 <span class=\"math inline\">\\(\\mathbf{M}^{(L)}, \\dots, \\mathbf{M}^{(l+1)}\\)</span> 与一个二维张量 <span class=\"math inline\">\\(\\mathbf{v}^{(l)}\\)</span> 的乘积。在深层网络中，连续矩阵乘法可能导致结果数值过大（<strong>爆炸</strong>）或过小（<strong>消失</strong>）。</p>\n<p><strong>梯度消失</strong>：</p>\n<p>如果使用Sigmoid函数，就要考虑 Sigmoid 函数在其饱和区梯度逼近于零的情况。因此当输入很大或很小时，梯度消失。为此我们最好用ReLU函数替代之。</p>\n<p>如果每一次的 梯度都减小一点，那么多层传播后梯度值会非常小。</p>\n<p>如果权重的初始值太小，向前传播过程中每层线性变换的输出方差大幅衰减，进而使激活函数的输入落入该函数的危险区（如 Sigmoid 的饱和区、ReLU的斩杀区）。</p>\n<p><strong>梯度爆炸</strong>：</p>\n<p>特指反向传播过程中，梯度值随着层级增加而不断变大，乃至指数型增加。</p>\n<p>很可能因为 <span class=\"math inline\">\\(weight\\)</span> 的初始值太大，层数过多等等</p>\n<p><strong>参数化的对称性</strong>：<br />\n若同一层内的的所有权重均初始化为相同值，那么该层所有的神经元在反向传播中都会获得完全一样的梯度，永远学习相同的特征，极大降低模型容量。</p>\n<p>那么，如何为我们模型的训练提供一个良好、稳健的起点呢？这就是神经网络 <strong>参数初始化</strong> 的showtime了。良好的初始化方式，能够前向传播中保持传递强度，在反向传播中保证梯度流动，从而打破上文的 <em>参数化的对称性</em> 等等问题。</p>\n<h3 id=\"三种常见的初始化\">三种常见的初始化</h3>\n<h4 id=\"xavier初始化\">Xavier初始化</h4>\n<p>目标：保持各层激活值方差稳定，确保前向传播的信号强度和反向传播的梯度强度在初始化时不衰减也不爆炸。</p>\n<p>Xavier 初始化因为提出的时间较早，它主要针对像 <span class=\"math inline\">\\(tanh\\)</span> 这样在原点附近近似线性且对称的饱和激活函数。因此对于后来广泛使用的 ReLU 及其变种，它的效果并非最优。</p>\n<p>这里的3个函数都有饱和区，也就是梯度消失的那段区域，太大或太小时函数导数趋于 <span class=\"math inline\">\\(0\\)</span> 。</p>\n<p>这个理论的基本原则就是：<strong>在前向传播中，保持各层激活值的方差一致；在反向传播中，保持各层梯度的方差一致</strong>。 也就是说初始化阶段的激活值和梯度的期望均为 <span class=\"math inline\">\\(0\\)</span>。Xavier初始化是为 <span class=\"math inline\">\\(tanh\\)</span> 这类在零点附近近似线性且对称的激活函数设计的，对于 <span class=\"math inline\">\\(Sigmoid\\)</span>，虽然 Xavier初始化可以用于 <span class=\"math inline\">\\(Sigmoid\\)</span> ，但不是最优的。实际应用中，对 <span class=\"math inline\">\\(Sigmoid\\)</span> 可以使用 Xavier初始化，但可能需要调整缩放因子。</p>\n<p>用数学语言表述，就是要激活函数在原点泰勒展开的一阶近似（当然 <span class=\"math inline\">\\(x\\)</span> 也在 <span class=\"math inline\">\\(0\\)</span> 附近） <span class=\"math inline\">\\(f(x)\\)</span> 满足：</p>\n<p></p><div class=\"math display\">\\[\\begin{split}\n&amp;f(x) = -f(-x)，即f(0)=0\\\\\n&amp;f'(0)=1\\end{split}\n\\]</div><p></p><p>再换句话，由观察，我们希望任意层的输入信号方差应等于其输出信号方差：</p>\n<p></p><div class=\"math display\">\\[Var(a^{(l-1)}) \\approx Var(a^{(l)})\n\\]</div><p></p><p>观察第 <span class=\"math inline\">\\(l\\)</span> 层的线性变换：</p>\n<p></p><div class=\"math display\">\\[\\mathcal{z_i^{l}}=\\sum_{j=1}^{n_{in}}w_{ij}^{(l)}\\cdot a_j^{(l-1)}\n\\]</div><p></p><p>这里先基本假设一下：</p>\n<ol>\n<li>权重 <span class=\"math inline\">\\(w_{ij}^{(l)}\\)</span> 独立同分布，均值为 <span class=\"math inline\">\\(0\\)</span>，方差 <span class=\"math inline\">\\(\\sigma _w^2\\)</span></li>\n<li>激活值 <span class=\"math inline\">\\(a_{j}^{(l-1)}\\)</span> 独立同分布，均值为 <span class=\"math inline\">\\(0\\)</span>，方差 <span class=\"math inline\">\\(\\sigma _a^2\\)</span></li>\n<li>权重和激活值相互独立</li>\n</ol>\n<h5 id=\"先看看期望\">先看看期望：</h5>\n<p></p><div class=\"math display\">\\[\\begin{split}\n\\mathbb{E}[z^{(l)}_i]&amp;=\\mathbb{E}\\bigg[ \\sum^{n_{in}}_{j=1}w_{ij}^{(l)}a_j^{(l-1)} \\bigg]\\\\\n\\mathbb{E}[z_i^{(l)}]&amp;=\\sum_{j=1}^{n_{in}}\\mathbb{E}[w_{ij}^{(l)}]\\cdot \\mathbb{E}[a_j^{(l - 1)}]\\\\\n\\mathbb{E}[z_i^{(l)}]&amp;=0\n\n\\end{split}\n\\]</div><p></p><h5 id=\"再看看方差先着眼于前向传播的过程\">再看看方差，先着眼于前向传播的过程：</h5>\n<p></p><div class=\"math display\">\\[\\begin{split}\nVar(\\mathcal{z_i^{(l)}})&amp;=\\mathbb E[(\\mathcal{z_i^{(l)}})^2]-(\\mathbb E[\\mathcal z_i^{(l)}])^2\\\\\n&amp;=\\mathbb E[(\\mathcal{z_i^{(l)}})^2]\n\\\\\n&amp;=  \\mathbb{E} \\left[ \\left( \\sum_{j=1}^{n_{\\text{in}}} w_{ij}^{(l)} a_j^{(l-1)} \\right)^2 \\right] \\\\\n&amp;= \\mathbb{E} \\left[ \\sum_{j=1}^{n_{\\text{in}}} \\sum_{k=1}^{n_{\\text{in}}} w_{ij}^{(l)} w_{ik}^{(l)} a_j^{(l-1)} a_k^{(l-1)} \\right]\\\\\n&amp;= \\ldots\\\\\n&amp;= \\sum_{j=1}^{n_{in}}\\mathbb E[(\\mathcal{w}_{ij}^{(l)})^2]\\cdot\\mathbb E [(a_j^{(l - 1)})^2] \\space(j=k)\\\\\n&amp;=n_{in}\\cdot\\sigma_w^2\\cdot\\sigma_a^2\\\\\n\\end{split}\\]</div><p></p><p>上文公式推导省略号中的内容：</p>\n<ul>\n<li>当 <span class=\"math inline\">\\(j\\neq k\\)</span>，式子为 <span class=\"math inline\">\\(0\\)</span></li>\n<li>当 <span class=\"math inline\">\\(j=k\\)</span>，式子为 <span class=\"math inline\">\\(\\sum_{j=1}^{n_{in}}\\mathbb E[(\\mathcal{w}_{ij}^{(l)})^2]\\cdot\\mathbb E [(a_j^{(l = 1)})^2]\\)</span></li>\n<li>因此，求和中仅 <span class=\"math inline\">\\(j=k\\)</span> 的项有贡献。</li>\n</ul>\n<p>为了保证激活方差不变，即</p>\n<p></p><div class=\"math display\">\\[\\begin{split}\nVar(z_i^{(l)})&amp;=Var(a_j^{(l - 1)})\\\\\nn_{in}\\cdot\\sigma^2\\cdot\\sigma_a^2&amp;=\\sigma_a^2\\\\\nn_{in}\\cdot\\sigma_w^2&amp;=1\\\\\n\\end{split}\n\\]</div><p></p><h5 id=\"接着推导一下反向传播\">接着推导一下反向传播：</h5>\n<p>反向传播的梯度传播公式如下</p>\n<p></p><div class=\"math display\">\\[\\frac{\\partial L}{\\partial a_j^{(l-1)}}=\\sum_{i=1}^{n_{out}}w_{ij}^{(l)}\\cdot\\frac{\\partial L}{\\partial z_i^{(l)}}\n\\]</div><p></p><p>那么假设 <span class=\"math inline\">\\(\\frac{\\partial L}{\\partial z_i^{(l)}}\\)</span> 独立同分布，方差为 <span class=\"math inline\">\\(\\sigma_g^2\\)</span> ，可以得到梯度方差的表示：</p>\n<p></p><div class=\"math display\">\\[\\begin{split}\nVar\\left( \\frac{\\partial L}{\\partial a_j^{(l-1)}} \\right)&amp;=\\sum_{i=1}^{n_{out}}\\mathbb{E}[(w_{ij}^{(l)})^2]\\cdot\\mathbb{E}\\left[ \\left( \\frac{\\partial L}{\\partial z_i^{(l)}} \\right)^2 \\right] \\\\\n\n&amp;=n_{out}\\cdot\\sigma_w^2\\cdot\\sigma_g^2\\\\\n\\end{split}\n\\]</div><p></p><p>我们希望反向传播前后梯度方差不变。即希望：</p>\n<p></p><div class=\"math display\">\\[Var\\left( \\frac{\\partial L}{\\partial a_j^{(l-1)}} \\right)=Var\\left( \\frac{\\partial L}{\\partial z_i^{(l)}} \\right)\n\\]</div><p></p><p>那么就可以得到反向传播保持方差不变时应满足的条件：</p>\n<p></p><div class=\"math display\">\\[\\begin{split}\n\nn_{out}\\cdot\\sigma_w^2\\cdot\\sigma_g^2&amp;=\\sigma_g^2\\\\\nn_{out}\\cdot\\sigma_w^2&amp;=1\n\n\\end{split}\n\\]</div><p></p><h5 id=\"因此这种一下这两个条件取调和平均\">因此，这种一下这两个条件，取调和平均：</h5>\n<p></p><div class=\"math display\">\\[\\begin{split}\nn_{in}\\cdot\\sigma_w^2&amp;=1\\\\\nn_{out}\\cdot\\sigma_w^2&amp;=1\\\\\n\\sigma_w^2&amp;=\\frac{2}{n_{in}+n_{out}}\\\\\n\\end{split}\n\\]</div><p></p><p>即：</p>\n<p></p><div class=\"math display\">\\[Var(\\mathcal w) = \\frac{2}{n_{in}+n_{out}}\n\\]</div><p></p><p>这样，标准差就出来了：</p>\n<p></p><div class=\"math display\">\\[\\sigma = \\sqrt \\frac{2}{n_{in}+n_{out}}\n\\]</div><p></p><p>因此初始权值应符合的正态分布：</p>\n<p></p><div class=\"math display\">\\[W\\sim \\mathcal N(0,\\sigma^2)\n\\]</div><p></p><p>或者转化为均匀分布形式，即</p>\n<p></p><div class=\"math display\">\\[w\\sim U\\left[ -\\sqrt{\\frac{6}{n_{in}+n_{out}}},\\sqrt{\\frac{6}{n_{in}+n_{out}}} \\right]\n\\]</div><p></p><p>然而，Xavier初始化提出的时间有点早，ReLU激活函数还没有得到广泛应用。<br />\n对于ReLU函数，Xavier初始化力不从心：</p>\n<ol>\n<li>ReLU的函数输出非对称：<span class=\"math inline\">\\(y \\in [0,+∞)\\)</span></li>\n<li>负的输入反向输出时梯度为 <span class=\"math inline\">\\(0\\)</span></li>\n<li>会将 <span class=\"math inline\">\\(50\\%\\)</span> 的神经元输出清零，从而</li>\n</ol>\n<ul>\n<li>前向传播：<span class=\"math inline\">\\(Var(a) \\approx \\frac{1}{2}Var(y)\\)</span></li>\n<li>反向传播：梯度方差同样减半</li>\n</ul>\n<p>而且对于深层神经网络而言，线性激活函数价值不大，因为它需要非线性激活函数来构建复杂的非线性神经网络。</p>\n<p>面对这些问题，He初始化（Kaiming初始化）被提了出来。</p>\n<h4 id=\"kaiming-初始化\">Kaiming 初始化</h4>\n<p>与 Xavier 初始化类似，Kaiming 初始化的目的也是尽量让每一层输出层的方差与输入层的方差一致，以缓解深层网络中的梯度消失、梯度爆炸问题，最后使极深整流网络（如30层）能从零开始直接训练并收敛。</p>\n<p>对于向前传播：</p>\n<p></p><div class=\"math display\">\\[\\begin{split}\n\\text{Var}(y_i) &amp;= \\text{Var} \\left( \\sum_{j=1}^{n_{\\text{in}}} w_{ij} \\cdot x_j \\right) \\\\&amp;= n_{\\text{input}}\\cdot\\text{Var}(w_{ij}) \\cdot \\text{Var}(x_j)\n\\end{split}\n\\]</div><p></p><p>对<span class=\"math inline\">\\(y_i\\)</span>加入ReLU函数得到<span class=\"math inline\">\\(a_i\\)</span>，那么我们就希望：</p>\n<p></p><div class=\"math display\">\\[\\text{Var}(a_i) \\approx \\text{Var}(x_j),\\quad \\forall i,j\n\\]</div><p></p><p>这里的初始化假设与 Xavier 相同。</p>\n<p>因为 <span class=\"math inline\">\\(w_{ij}\\)</span> 与 <span class=\"math inline\">\\(x_j\\)</span> 独立且均值为 <span class=\"math inline\">\\(0\\)</span>，有</p>\n<p></p><div class=\"math display\">\\[\\text{Var}(w_{ij}x_j)=\\text{Var}(w_{ij})\\text{Var}(x_j)=\\sigma_w^2\\sigma_x^2\n\\]</div><p></p><p>则 <span class=\"math inline\">\\(y_i\\)</span> 的方差为：</p>\n<p></p><div class=\"math display\">\\[\\begin{split}\n\\text{Var}(y_i) &amp;= \\text{Var}\\left( \\sum_{j=1}^{n_{in}}w_{ij}x_j \\right)\\\\ \n&amp;=\\sum_{j=1}^{n_{in}}\\text{Var}(w_{ij}x_j)\\\\\n&amp;=\\sum_{j=1}^{n_{in}}\\sigma_w^2\\sigma_x^2\\\\\n&amp;=n_{in}\\sigma_w^2\\sigma_x^2\\\\\n&amp;=n_{in}\\cdot\\text{Var}(w)\\cdot\\text{Var}(x)\n\\end{split}\n\\]</div><p></p><p>我们假设 <span class=\"math inline\">\\(y_i\\)</span> 的分布是关于 0 对称的，那么 <span class=\"math inline\">\\(y_i\\)</span> 取正数和取负数的概率各占一半。</p>\n<p>再看 <span class=\"math inline\">\\(y_i^2\\)</span>。因为平方把正负都变成了正数，所以 <span class=\"math inline\">\\(y_i^2\\)</span> 的期望值 <span class=\"math inline\">\\(E[y_i^2]\\)</span> 可以拆成两半：一半来自 <span class=\"math inline\">\\(y_i&gt;0\\)</span>，一半来自 <span class=\"math inline\">\\(y_i&lt;0\\)</span>。由于对称，这两半的贡献是一模一样的。</p>\n<p>而 ReLU 函数 <span class=\"math inline\">\\(a_i = \\max(0, y_i)\\)</span> 只取 <span class=\"math inline\">\\(y_i\\)</span> 的正值部分，负数部分直接归零。所以 <span class=\"math inline\">\\(a_i^2\\)</span> 其实就是 <span class=\"math inline\">\\(y_i^2\\)</span> 在 <span class=\"math inline\">\\(y_i&gt;0\\)</span> 时的值，其他情况为 0。</p>\n<p>因此，<span class=\"math inline\">\\(a_i^2\\)</span> 的期望 <span class=\"math inline\">\\(E[a_i^2]\\)</span> 正好就等于 <span class=\"math inline\">\\(y_i^2\\)</span> 期望的一半，即</p>\n<p></p><div class=\"math display\">\\[E[a_i^2]=\\frac{1}{2}E[y_i^2]\n\\]</div><p></p><p>而 <span class=\"math inline\">\\(E[y_i]=0\\)</span>，有 <span class=\"math inline\">\\(E[y_i^2]=\\text{Var}(y_i)\\)</span>，故</p>\n<p></p><div class=\"math display\">\\[E[a_i^2]=\\frac{1}{2}\\text{Var}(y_i)\n\\]</div><p></p><p>当 <span class=\"math inline\">\\((E[a_i])^2\\)</span> 相较于 <span class=\"math inline\">\\(E[a_i^2]\\)</span> 可以忽略时，可近似为：</p>\n<p></p><div class=\"math display\">\\[\\text{Var}(a_i)\\approx\\frac{1}{2}\\text{Var}(y_i)\n\\]</div><p></p><p>我们希望 <span class=\"math inline\">\\(\\text{Var}(a_i) = \\text{Var}(x)\\)</span>（当然至少得是近似的），结合可得：</p>\n<p></p><div class=\"math display\">\\[\\begin{split}\n\\frac{1}{2}\\cdot n_{in}\\cdot Var(w)\\cdot Var(x) &amp;=Var{(x)}\\\\\nVar(w)&amp;=\\frac{2}{n_{in}}\n\\end{split}\\]</div><p></p><p>以此类推，可以得到反向传播时，</p>\n<p></p><div class=\"math display\">\\[Var(w)=\\frac{2}{n_{out}}\n\\]</div><p></p><p>不过一般情况，我们使用前向传播优先，即</p>\n<p></p><div class=\"math display\">\\[W\\sim \\mathcal{N}(0,\\sqrt \\frac{2}{n_{in}})\n\\]</div><p></p><p>我们为什么不常见类比Xavier做调和平均呢？（其实是可以的，见 PyTorch 中的 <code>mode='fan_avg'</code> ）因为<strong>ReLU的单向激活特性</strong>使得前向传播和反向传播的方差传播规律不同：</p>\n<ul>\n<li>对前向传播，ReLU 杀死一半的神经元，方差减半；对反向传播，相当于简单的伯努利掩码，方差依旧减半。</li>\n<li>问题在于正向反向的网格结构可能是不同的，且正向反向的衰减机制有席位差别。</li>\n</ul>\n<p>pytorch实现：</p>\n<pre><code class=\"language-python\">layer = nn.Linear(64, 128)\ninit.kaiming_normal_(layer.weight, a=0, mode='fan_in', nonlinearity='relu')\n# a：负斜率（Leaky ReLU 的情况，默认为0）\n# Leaky ReLU : 负x轴设置为 ax ，而不是 0 ，通常 a = 0.01\n</code></pre>\n<h4 id=\"正交初始化\">正交初始化</h4>\n<p>上面两种方法都是对每个权重分别进行随机独立采样，但是由于采样的随机性，仍不可避免出现各种梯度问题。</p>\n<p>对于一个 L 层的等宽线性网络，可以很容易得到这个等式：</p>\n<p></p><div class=\"math display\">\\[y=W^{(L)}W^{(L-1)}W^{(L-2)}\\cdots W^{(2)}W^{(1)}x\n\\]</div><p></p><p>那么，我们可以直接将 <span class=\"math inline\">\\(W^{(i)}\\)</span> 初始化为正交矩阵。</p>\n<p>根据线代知识，我们对这个初始权重矩阵的构建分为两步：</p>\n<ol>\n<li>用均值 <span class=\"math inline\">\\(0\\)</span> , 方差 <span class=\"math inline\">\\(1\\)</span> 的高斯分布构建一个矩阵</li>\n<li>奇异值分解这个矩阵，得到两个正交矩阵，选择其中一个作为权重矩阵</li>\n</ol>\n<p>根据正交矩阵的性质，这个线性网络就会在前向、反向传播中都有一定的范数保持性。如果这个网络是非线性的，只需在矩阵前面乘上一个系数 <span class=\"math inline\">\\(\\rho\\)</span>，这个系数与激活函数有关，如对于 <span class=\"math inline\">\\(ReLU\\)</span> 应该 <span class=\"math inline\">\\(\\rho=\\sqrt 2\\)</span> ，对于 <span class=\"math inline\">\\(tanh\\)</span> 应该 <span class=\"math inline\">\\(\\rho\\approx 1.0\\)</span>，这是为了补偿激活函数对信号幅度的压缩（扩张）效应。</p>\n<h3 id=\"更加现代的初始化方法\">更加现代的初始化方法</h3>\n<h4 id=\"fixup\">Fixup</h4>\n<p>可使在不使用批量归一化的情况下完成深度残差网络训练。</p>\n<p>通过缩放残差网络分支的权重来控制梯度规模，避免深层网络的梯度爆炸</p>\n<p>方法：</p>\n<ul>\n<li>将分类层、残差分支的最后一层初始化为 <span class=\"math inline\">\\(0\\)</span></li>\n<li>对其他层使用标准方法的初始化，然后将残差分支中的权重层乘以缩放系数 <span class=\"math inline\">\\(L^{-\\frac{1}{2m-2}}\\)</span></li>\n<li>在每个分支中添加一个标量乘数（就是前面的缩放系数），在每个卷积、线性和元素级激活层前面添加一个可学习标量偏差（初始为 <span class=\"math inline\">\\(0\\)</span> ）。</li>\n</ul>\n<p>其中</p>\n<ul>\n<li><span class=\"math inline\">\\(m\\)</span>：每个残差块中的权重层数</li>\n<li><span class=\"math inline\">\\(L\\)</span>：网络总残差块数</li>\n</ul>\n<h4 id=\"t-fixup\">T-Fixup</h4>\n<p>在完全移除层归一化的情况下，稳定并高效地训练 Transformer 模型</p>\n<p>通过精心设计的参数初始化和简单的标量偏差，在数学上使前向传播的信号幅度和反向传播的梯度范数在初始化时保持稳定，从而完全移除所有 LN 层。</p>\n<h2 id=\"参考文献\">参考文献</h2>\n<p>Glorot &amp; Bengio. Understanding the difficulty of training deep feedforward neural networks. Jan 2010</p>\n<p>He et al. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs.CV] 10 Dec 2015</p>\n<p>Saxe et al. Sparser, Better, Deeper, Stronger: Improving Static Sparse Training with Exact Orthogonal Initialization. arXiv:2406.01755v1 [cs.LG] 03 Jun 2024</p>\n<p>Yilmaz &amp; Heckel. Regularization-wise Double Descent: Why It Occurs and How to Eliminate It. arXiv:2206.09012, 2022.</p>\n<p>Zhang et al. Fixup Initialization: Residual Learning Without Normalization. arXiv:1901.09321 [cs.LG] 27 Jan 2019</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-09 21:47</span>&nbsp;\n<a href=\"https://www.cnblogs.com/yldeveloper\">yLDeveloper</a>&nbsp;\n阅读(<span id=\"post_view_count\">11</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}