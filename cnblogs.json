{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "又一款国产开源企业级文件管理系统诞生了！基于 Spring Boot 3.5.x + Sa-Token + MyBatis Flex",
      "link": "https://www.cnblogs.com/javaguide/p/19616059",
      "published": "",
      "description": "<h2>\n            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/javaguide/p/19616059\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 15:52\">\n    <span>又一款国产开源企业级文件管理系统诞生了！基于 Spring Boot 3.5.x + Sa-Token + MyBatis Flex</span>\n    \n\n</a>\n\n        </h2>\n        <div class=\"postbody\">\n            <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>前几天在技术群里看到大家讨论企业网盘选型。付费的太贵，开源的功能不够完整，部署维护又麻烦——这些问题很多团队都遇到过。想要一个能处理大文件、支持在线预览、还能多云存储的方案，确实不容易找。</p>\n<p>Dromara 开源社区前段时间新进了一个基于 Spring Boot 的文件管理系统 —— Free-FS，我研究了一下，功能比较完整，架构也清晰，分享给大家（JavaGuide 所有开源项目分享都无商务性质，纯分享，欢迎自荐，地址： <a href=\"https://github.com/CodingDocs/awesome-java%EF%BC%89%E3%80%82\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/CodingDocs/awesome-java）。</a></p>\n<p><img alt=\"图片\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200920-1716388728.webp\" /></p>\n<h2 id=\"free-fs-是什么\"><strong>Free-FS 是什么？</strong></h2>\n<p>Free-FS 是一个企业级文件管理系统后端，基于 Spring Boot 3.5 + MyBatis Flex + Sa-Token + React/Vue 构建。</p>\n<p>主要功能包括大文件分片上传/断点续传/秒传、多格式在线预览、多云存储插件化扩展、权限控制、回收站等。有配套的 Vue 3 前端。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200958-1718842586.webp\" /></p>\n<p>这里给不懂的朋友简单解释下上面提到的几个关键术语：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">术语</th>\n<th style=\"text-align: left;\">含义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>分片上传</strong></td>\n<td style=\"text-align: left;\">把大文件切成多个小块分别上传，网络中断后只需上传未完成的分片</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>断点续传</strong></td>\n<td style=\"text-align: left;\">记录上传进度，中断后从断点继续，不用重新上传整个文件</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>秒传</strong></td>\n<td style=\"text-align: left;\">通过计算文件哈希值（如 MD5、SHA-256），如果服务器已有相同哈希的文件，直接建立引用而不上传实际数据</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"它解决了什么问题\"><strong>它解决了什么问题？</strong></h2>\n<p>企业文件管理常见的几个麻烦：</p>\n<ul>\n<li><strong>大文件上传困难</strong>：没有分片上传、断点续传，网络一中断就得重新开始</li>\n<li><strong>存储平台绑定</strong>：绑死单一云存储，切换成本高，迁移困难</li>\n<li><strong>预览能力弱</strong>：只支持少数格式，Office/图片/PDF 预览要额外配置</li>\n<li><strong>权限管理粗糙</strong>：缺少细粒度权限控制，无法满足企业安全要求</li>\n<li><strong>部署复杂</strong>：依赖多、配置繁琐，开箱即用困难</li>\n</ul>\n<p>Free-FS 用主流技术栈做了一个功能完整、架构清晰、可扩展的文件管理后端。</p>\n<h2 id=\"核心功能\"><strong>核心功能</strong></h2>\n<h3 id=\"文件管理\"><strong>文件管理</strong></h3>\n<p>支持列表和网格两种视图。</p>\n<p><img alt=\"图片\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200928-973813390.webp\" /></p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200936-1846826600.webp\" /></p>\n<h3 id=\"大文件上传与秒传\"><strong>大文件上传与秒传</strong></h3>\n<p>前端把大文件切片并行上传，后端通过 SSE 实时推进度，精确到每个分片的状态。网络中断后，系统记录断点位置，续传时从断点继续。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200939-1166332110.webp\" /></p>\n<p>秒传的原理是先算文件 MD5 发给服务端，服务器有相同哈希的文件就直接建立引用，跳过数据传输。所以上传几个 G 的文件有时能\"秒完\"。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200907-1394867558.webp\" /></p>\n<h3 id=\"多云存储插件化\"><strong>多云存储插件化</strong></h3>\n<p>存储层用 SPI 插件化设计，把存储能力抽象成统一接口。</p>\n<p>内置的存储实现有本地存储、阿里云 OSS、七牛云 Kodo、AWS S3 兼容存储、RustFS。</p>\n<p>一套系统可以管理多个存储平台，按需分配或一键切换。新增存储平台只要实现核心接口并注册，不用改主业务代码。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200904-1719820535.webp\" /></p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200945-674949107.webp\" /></p>\n<p>添加存储配置</p>\n<h3 id=\"在线预览\"><strong>在线预览</strong></h3>\n<p>支持五大类文件预览：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">类型</th>\n<th style=\"text-align: left;\">支持格式</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">图片</td>\n<td style=\"text-align: left;\">jpg/png/gif/webp/svg 等</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Office 文档</td>\n<td style=\"text-align: left;\">doc/docx/xls/xlsx/ppt/pptx（需要 LibreOffice）</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">文本代码</td>\n<td style=\"text-align: left;\">30+ 种编程语言语法高亮</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">音视频</td>\n<td style=\"text-align: left;\">流式播放</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">压缩包</td>\n<td style=\"text-align: left;\">查看目录结构</td>\n</tr>\n</tbody>\n</table>\n<p>Office 文档通过 LibreOffice 转成 PDF 后交给前端，转换用进程池模式，可以配置并发度和超时。</p>\n<h3 id=\"权限与安全\"><strong>权限与安全</strong></h3>\n<p>用 Sa-Token 做权限认证，JWT 无状态会话，支持分布式部署。</p>\n<p>权限可以控制到文件的查看、下载、编辑、删除等操作。</p>\n<h3 id=\"文件分享和回收站\"><strong>文件分享和回收站</strong></h3>\n<p>文件分享有公开链接和授权码两种模式，授权码分享能设置有效期和访问次数。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200953-1235074064.webp\" /></p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200890-1175658548.webp\" /></p>\n<p>回收站给删除操作一个缓冲区，支持批量还原、永久删除和自动清理。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200924-1191218721.webp\" /></p>\n<p>回收站</p>\n<h2 id=\"安装与快速上手\"><strong>安装与快速上手</strong></h2>\n<h3 id=\"环境要求\"><strong>环境要求</strong></h3>\n<ul>\n<li>JDK 17+</li>\n<li>Maven 3.8+</li>\n<li>MySQL 8.0+ 或 PostgreSQL 14+（二选一）</li>\n<li>Redis</li>\n<li>LibreOffice (可选，用于 Office 文档预览功能)</li>\n</ul>\n<h3 id=\"安装步骤\"><strong>安装步骤</strong></h3>\n<pre><code>git clone https://gitee.com/dromara/free-fs.git\ncd free-fs\nmvn clean install -DskipTests\n</code></pre>\n<h3 id=\"数据库初始化\"><strong>数据库初始化</strong></h3>\n<p>MySQL：</p>\n<pre><code>CREATE DATABASE `free-fs` CHARACTER SET 'utf8mb4' COLLATE 'utf8mb4_general_ci';\n</code></pre>\n<p>PostgreSQL：</p>\n<pre><code>CREATE DATABASE free-fs ENCODING 'UTF8' LC_COLLATE='zh_CN.UTF-8' LC_CTYPE='zh_CN.UTF-8';\n</code></pre>\n<p>数据库创建完成之后，导入项目根目录下对应的 SQL 文件到刚创建的数据库中：</p>\n<ul>\n<li>MySQL: <code>_sql/mysql/free-fs.sql</code></li>\n<li>PostgreSQL: <code>_sql/postgresql/free-fs_pg.sql</code></li>\n</ul>\n<h3 id=\"配置与运行\"><strong>配置与运行</strong></h3>\n<p>修改 <code>fs-admin/src/main/resources/application-dev.yml</code> 中的数据库和 Redis 配置：</p>\n<pre><code>cd fs-admin\nmvn spring-boot:run\n</code></pre>\n<p>访问地址：</p>\n<ul>\n<li>服务地址：<a href=\"http://localhost:8080\" rel=\"noopener nofollow\" target=\"_blank\">http://localhost:8080</a></li>\n<li>API 文档：<a href=\"http://localhost:8080/swagger-ui.html\" rel=\"noopener nofollow\" target=\"_blank\">http://localhost:8080/swagger-ui.html</a></li>\n<li>默认账号：admin / admin</li>\n</ul>\n<p><a href=\"https://javaguide.cn/zhuanlan/interview-guide.html\" rel=\"noopener nofollow\" target=\"_blank\">《SpringAI 智能面试平台+RAG 知识库》</a>配套实战项目教程正在更新，涉及到 Prompt Engineering、大模型集成、RAG（检索增强生成）、高性能对象存储与向量数据库。后续的话，还会同步上Agent 项目。</p>\n<p>内容非常全面，非常适合想要实战 AI 项目或者准备 AI 大模型应用开发岗位面试的朋友，来一张刚写完的<strong>3.4w 字+35 道题目</strong>的 RAG 面试题总结，大家感受一下（点此链接了解： <a href=\"https://javaguide.cn/about-the-author/zhishixingqiu-two-years.html\" rel=\"noopener nofollow\" target=\"_blank\">星球</a>）：</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200946-1262024442.webp\" /></p>\n<h2 id=\"技术架构\"><strong>技术架构</strong></h2>\n<h3 id=\"技术栈选型\"><strong>技术栈选型</strong></h3>\n<p>后端用 Spring Boot 3.5.4，搭配 MyBatis Flex 做 ORM 层。</p>\n<p>MyBatis Flex 是 MyBatis 的增强版，和 MyBatis-Plus 有些差异：</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200941-432078960.webp\" /></p>\n<p>MyBatis Flex 的详细介绍可以参考这篇文章：<a href=\"https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&amp;mid=2247549675&amp;idx=1&amp;sn=d74db8ecc003815d890de5aef0baef86&amp;scene=21#wechat_redirect\" rel=\"noopener nofollow\" target=\"_blank\">再见 MyBatis，这款 ORM 框架确实太优雅！！</a></p>\n<p>认证授权用 Sa-Token，比 Spring Security 简洁。数据库支持 MySQL 8.0+ 和 PostgreSQL 14+，HikariCP 连接池，配合 Caffeine 本地缓存和 Redis 分布式缓存。</p>\n<h3 id=\"项目结构\"><strong>项目结构</strong></h3>\n<p>多模块 Maven 结构：</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/1843652/202602/1843652-20260214155200922-414133016.webp\" /></p>\n<p>存储插件用 SPI 设计，<code>storage-plugin-core</code> 定义接口，各存储实现添加新插件模块就行。</p>\n<h2 id=\"与其他方案对比\"><strong>与其他方案对比</strong></h2>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">对比维度</th>\n<th style=\"text-align: left;\">Free-FS</th>\n<th style=\"text-align: left;\">MinIO</th>\n<th style=\"text-align: left;\">Nextcloud</th>\n<th style=\"text-align: left;\">Seafile</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">技术栈</td>\n<td style=\"text-align: left;\">Spring Boot 3.x</td>\n<td style=\"text-align: left;\">Go</td>\n<td style=\"text-align: left;\">PHP</td>\n<td style=\"text-align: left;\">Go/C</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">定位</td>\n<td style=\"text-align: left;\">后端服务</td>\n<td style=\"text-align: left;\">对象存储</td>\n<td style=\"text-align: left;\">完整网盘</td>\n<td style=\"text-align: left;\">完整网盘</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">大文件上传</td>\n<td style=\"text-align: left;\">分片/断点续传/秒传</td>\n<td style=\"text-align: left;\">需自行实现</td>\n<td style=\"text-align: left;\">支持</td>\n<td style=\"text-align: left;\">支持</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">在线预览</td>\n<td style=\"text-align: left;\">多格式支持</td>\n<td style=\"text-align: left;\">需自行实现</td>\n<td style=\"text-align: left;\">支持</td>\n<td style=\"text-align: left;\">支持</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">多云存储</td>\n<td style=\"text-align: left;\">插件化切换</td>\n<td style=\"text-align: left;\">单一</td>\n<td style=\"text-align: left;\">支持</td>\n<td style=\"text-align: left;\">支持</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">权限管理</td>\n<td style=\"text-align: left;\">Sa-Token 细粒度</td>\n<td style=\"text-align: left;\">简单</td>\n<td style=\"text-align: left;\">细粒度</td>\n<td style=\"text-align: left;\">细粒度</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">部署难度</td>\n<td style=\"text-align: left;\">中（需数据库）</td>\n<td style=\"text-align: left;\">低</td>\n<td style=\"text-align: left;\">中</td>\n<td style=\"text-align: left;\">中</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">开源协议</td>\n<td style=\"text-align: left;\">Apache 2.0</td>\n<td style=\"text-align: left;\">AGPL v3</td>\n<td style=\"text-align: left;\">AGPL v3</td>\n<td style=\"text-align: left;\">GPL-3.0</td>\n</tr>\n</tbody>\n</table>\n<p>只做对象存储的话 MinIO 更合适；需要完整网盘 UI 和协作功能的话 Nextcloud 或 Seafile 更好；需要一个可定制、易集成的文件管理后端，Free-FS 可以考虑。</p>\n<h2 id=\"总结\"><strong>总结</strong></h2>\n<p>Free-FS 用 Spring Boot 3.x + MyBatis Flex + Sa-Token 做了一个功能完整的企业级文件管理后端。</p>\n<ul>\n<li><strong>优势</strong>：功能完整，开箱即用；大文件上传（分片/断点续传/秒传）；插件化存储扩展；多格式在线预览；Sa-Token 权限管理；Apache 2.0 协议友好。</li>\n<li><strong>局限</strong>：需要部署数据库（MySQL/PostgreSQL + Redis）；Office 预览要配置 LibreOffice。</li>\n<li><strong>适用场景</strong>：搭建企业级文件管理系统、需要多云存储支持、需要大文件上传能力。</li>\n</ul>\n<p>建议先部署基础版本跑通上传下载流程，再按需要接入云存储和配置预览。</p>\n<ul>\n<li>后端地址：<strong><a href=\"https://gitee.com/dromara/free-fs\" rel=\"noopener nofollow\" target=\"_blank\">https://gitee.com/dromara/free-fs</a></strong></li>\n<li>前端地址：<strong><a href=\"https://gitee.com/xddcode/free-fs-frontend\" rel=\"noopener nofollow\" target=\"_blank\">https://gitee.com/xddcode/free-fs-frontend</a></strong></li>\n</ul>\n<p><strong>⭐️推荐</strong>:</p>\n<ul>\n<li><a href=\"https://javaguide.cn/home.html\" rel=\"noopener nofollow\" target=\"_blank\">Java 面试 &amp; 后端通用面试指南</a></li>\n<li><a href=\"https://javaguide.cn/zhuanlan/interview-guide.html\" rel=\"noopener nofollow\" target=\"_blank\">《SpringAI 智能面试平台+RAG 知识库》</a></li>\n<li><a href=\"https://javaguide.cn/interview-preparation/java-roadmap.html\" rel=\"noopener nofollow\" target=\"_blank\">Java 学习路线(最新版，4w+字)</a></li>\n<li><a href=\"https://javaguide.cn/interview-preparation/pdf-interview-javaguide.html\" rel=\"noopener nofollow\" target=\"_blank\">后端面试 PDF 最新版</a></li>\n</ul>\n\n\n</div>\n<div class=\"clear\"></div>\n\n        </div>\n        <p class=\"postfoot\">\n            posted on \n<span id=\"post-date\">2026-02-14 15:52</span>&nbsp;\n<a href=\"https://www.cnblogs.com/javaguide\">JavaGuide</a>&nbsp;\n阅读(<span id=\"post_view_count\">148</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n        </p>"
    },
    {
      "title": "OneTrans：在工业级推荐系统中以单一 Transformer 实现特征交互与序列建模的统一框架",
      "link": "https://www.cnblogs.com/GlenTt/p/19614879",
      "published": "",
      "description": "<h2>\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/GlenTt/p/19614879\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 11:31\">\n    <span>OneTrans：在工业级推荐系统中以单一 Transformer 实现特征交互与序列建模的统一框架</span>\n    \n\n</a>\n\n\t\t</h2>\n\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"onetrans在工业级推荐系统中以单一-transformer-实现特征交互与序列建模的统一框架\">OneTrans：在工业级推荐系统中以单一 Transformer 实现特征交互与序列建模的统一框架</h1>\n<h2 id=\"摘要\">摘要</h2>\n<p>在推荐系统中，扩展特征交互模块（例如 Wukong、RankMixer）或用户行为序列模块（例如 LONGER）已经取得了显著成果。然而，这两类工作通常沿着彼此独立的路径推进，这不仅阻碍了双向信息交互，也限制了统一优化与统一扩展能力。</p>\n<p>本文提出 OneTrans，一种统一的 Transformer 主干网络，可同时完成用户行为序列建模与特征交互。OneTrans 采用统一的 tokenizer，将序列属性与非序列属性共同转换为单一的 token 序列。堆叠的 OneTrans 模块对相似的序列 token 共享参数，而对非序列 token 分配特定参数。通过因果注意力机制与跨请求的 KV 缓存机制，OneTrans 支持中间表示的预计算与缓存，在训练与推理阶段均显著降低计算成本。</p>\n<p>在工业级规模数据集上的实验结果表明，OneTrans 随参数规模扩展具有良好的可扩展性，稳定优于多个强基线模型，并在真实在线 A/B 实验中实现了用户级 GMV 提升 5.68%。</p>\n<h2 id=\"1-引言\">1 引言</h2>\n<p>推荐系统在各类信息服务中发挥着基础性作用，例如电商、流媒体和社交网络。工业级推荐系统通常采用级联式排序架构。首先，在召回阶段从十亿规模的候选库中选出数百个候选项；随后，在排序阶段对每个候选进行打分并返回 Top-k 结果。深度学习推荐模型（DLRM）已广泛应用于工业推荐系统的排序阶段。</p>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<p>本文聚焦排序阶段，遵循 DLRM 风格的排序范式。当前主流方法通常围绕两个相互独立的模块进行迭代优化：（a）序列建模模块，将用户多行为序列编码为与候选相关的表示，通常采用局部注意力或 Transformer 编码器；（b）特征交互模块，通过分解方法、显式交叉网络或基于特征组的注意力机制，学习非序列特征（如用户画像、物品画像和上下文特征）之间的高阶交叉关系。如图 1(a) 所示，这类方法通常先将用户行为编码为压缩的序列表示，然后与非序列特征拼接，再通过特征交互模块学习高阶交互。本文将这一设计称为“先编码后交互”（encode-then-interaction）流程。</p>\n<p>大语言模型（LLM）的成功表明，扩大模型规模（如参数规模和训练数据规模）可以带来可预测的性能提升。这一现象也启发了推荐系统中的相关研究。在特征交互方面，Wukong 通过堆叠因子分解机模块并结合线性压缩来建模高阶特征交互，并建立了扩展规律；RankMixer 通过硬件友好的 token 混合机制以及 token 特定的前馈网络（FFN）实现了良好的规模扩展能力。在序列建模方面，LONGER 采用因果 Transformer 建模长用户行为历史，并证明随着深度和宽度的增加，性能呈单调提升趋势。</p>\n<p>尽管上述方法在实践中有效，但将序列建模与特征交互作为独立模块会带来两个主要局限。第一，“先编码后交互”的流程限制了双向信息流，使静态或上下文特征难以充分影响序列表示的建模。第二，模块分离导致执行过程割裂并增加系统延迟；相比之下，统一的 Transformer 主干结构可以复用 LLM 的成熟优化技术，如 KV 缓存、内存高效注意力机制和混合精度训练，从而实现更高效的规模扩展。</p>\n<p>为此，本文提出 OneTrans，一种创新性的架构范式，通过统一的 Transformer 主干网络同时完成用户行为序列建模与特征交互。如图 1(b) 所示，OneTrans 在统一主干内部实现了双向信息交互。其核心在于一个统一的 tokenizer，将序列特征（多样化行为序列）和非序列特征（用户、物品及上下文静态特征）共同转换为单一的 token 序列，然后送入由多层堆叠的 OneTrans 模块组成的金字塔结构中。该结构是针对工业推荐系统定制的 Transformer 变体。</p>\n<p>考虑到推荐系统中 token 来源多样、语义异构（不同于 LLM 中单一文本 token），每个 OneTrans 模块采用类似 HiFormer 的混合参数化策略。具体而言，所有来自序列特征的 token 共享同一组 Q/K/V 和 FFN 参数，而每个非序列 token 则分配独立的特定参数，以保留其独特语义。</p>\n<p>不同于传统的“先编码后交互”框架，OneTrans 通过统一的因果 Transformer 主干消除了序列与非序列特征之间的结构性隔离，使推荐系统的扩展方式与 LLM 实践保持一致：通过调整主干网络的深度和宽度即可扩展整体模型规模，同时可无缝继承成熟的 LLM 优化技术，如 FlashAttention 和混合精度训练。尤其是跨候选与跨请求的 KV 缓存机制，可将包含 C 个候选的会话时间复杂度从 𝑂(C) 降至 𝑂(1)，从而使大规模 OneTrans 部署成为可能。</p>\n<h3 id=\"主要贡献\">主要贡献</h3>\n<p>本文的贡献可概括为四点：</p>\n<ol>\n<li>\n<p><strong>统一框架</strong>：提出 OneTrans 单一 Transformer 主干排序模型，配备统一 tokenizer，将序列与非序列特征编码为统一 token 序列，并通过统一 Transformer 模块同时完成序列建模与特征交互。</p>\n</li>\n<li>\n<p><strong>面向推荐系统的定制设计</strong>：为弥合 LLM 与推荐任务之间的差距，提出混合参数化机制，对多样化的非序列 token 分配特定参数，同时对所有序列 token 共享参数。</p>\n</li>\n<li>\n<p><strong>高效训练与推理</strong>：通过金字塔策略逐层裁剪序列 token，并引入跨请求 KV 缓存复用用户侧计算结果。同时采用 FlashAttention、混合精度训练和半精度推理等优化手段，降低内存与计算开销。</p>\n</li>\n<li>\n<p><strong>规模扩展与在线部署验证</strong>：OneTrans 随模型规模增加呈现近似对数线性性能增长趋势，在真实生产数据上验证了规模规律；在线部署后，在保持工业级延迟的前提下，显著提升关键业务指标。</p>\n</li>\n</ol>\n<h2 id=\"2-相关工作\">2 相关工作</h2>\n<p>早期推荐系统模型，如 DIN 及其面向会话的变体 DSIN，采用局部注意力机制学习与候选物品条件相关的用户历史行为摘要，但通常将行为压缩为针对每个候选的固定长度向量，从而限制了长程依赖关系的建模能力。自注意力方法，如 SASRec、BERT4Rec 和 BST，通过允许序列中每个位置关注完整历史，消除了这一瓶颈，并通过双向掩码机制提升了样本利用效率。</p>\n<p>近年来，随着推荐系统中规模规律（scaling law）研究的推进，LONGER 将序列建模扩展至工业级规模，通过高效注意力机制和面向服务的架构设计，支持超长用户行为序列建模。然而，在主流工业流程中，这些序列编码器通常仍与特征交互模块相互独立，导致特征融合发生在后期阶段，而非与静态上下文特征进行联合优化。</p>\n<p>在特征交互方向，早期推荐系统依赖人工构造的交叉特征或自动乘性交互层。经典模型如 Wide&amp;Deep、FM/DeepFM 和 DCN/DCNv2 提供了高效的低阶或有界阶交互能力。然而，近期的规模研究发现，一旦交叉层堆叠到一定程度，继续增加层数往往无法带来性能提升，模型效果趋于平台期。</p>\n<p>为突破预设交叉形式的刚性限制，基于注意力的模型能够自动学习高阶交互关系。AutoInt 学习任意阶关系，HiFormer 通过组特定投影更好地刻画异构且非对称的特征交互。随着规模化方法逐渐应用于特征交互模块，大规模系统如 Wukong 通过堆叠 FM 风格交互模块并结合线性压缩，实现了可预测的性能增长；RankMixer 则在严格延迟约束下，通过并行 token 混合和稀疏 MoE 实现了良好的规模扩展能力。然而，这类交互模块通常仍遵循“交互后置”的范式，将交互限制在独立阶段，阻碍了与用户序列建模的统一优化。</p>\n<p>总体而言，推荐系统的发展长期沿着两条相对独立的路径推进：序列建模与特征交互。InterFormer 通过基于摘要的双向交叉架构尝试弥合两者差距，实现信号互通，但仍保留为两个独立模块，其交叉结构增加了架构复杂度和执行割裂性。在缺乏统一主干进行联合建模与整体优化的情况下，实现系统级规模扩展仍面临挑战。</p>\n<p>近期生成式推荐（Generative Recommenders）将推荐任务建模为序列转导问题，并提出如 HSTU 等高效长上下文主干结构。这一路线与依赖丰富非序列特征的 DLRM 系统形成互补。</p>\n<h2 id=\"3-方法\">3 方法</h2>\n<p>在详细介绍方法之前，首先说明任务设定。</p>\n<p>在级联式工业推荐系统中，每当召回阶段为用户 𝑢 返回一个候选集合（通常为数百个候选物品）后，排序模型会为每个候选物品 𝑖 预测一个得分：</p>\n<p></p><div class=\"math display\">\\[\\hat{y}_{u,i} = f(i, NS, S; \\Theta)\n\\]</div><p></p><p>其中，NS 表示来自用户、候选物品及上下文的非序列特征集合；S 表示用户的历史行为序列集合；Θ 为可训练参数。常见的预测任务包括点击率（CTR）与点击后转化率（CVR）：</p>\n<p></p><div class=\"math display\">\\[CTR_{u,i} = P(click=1 \\mid NS, S; \\Theta)\n\\]</div><p></p><p></p><div class=\"math display\">\\[CVR_{u,i} = P(conv=1 \\mid click=1, NS, S; \\Theta)\n\\]</div><p></p><h3 id=\"31-onetrans-框架概述\">3.1 OneTrans 框架概述</h3>\n<p><img alt=\"image-1\" class=\"lazyload\" /></p>\n<p>如图 2(a) 所示，OneTrans 采用统一 tokenizer，将序列特征 S 映射为 S-tokens，将非序列特征 NS 映射为 NS-tokens。随后，一个金字塔式堆叠的 Transformer 在单一计算图中联合处理这一统一 token 序列。初始 token 序列定义为：</p>\n<p></p><div class=\"math display\">\\[X^{(0)} = [S\\text{-tokens}; NS\\text{-tokens}] \\in \\mathbb{R}^{(L_S + L_{NS}) \\times d}\n\\]</div><p></p><p>该序列由 (L_S) 个 S-tokens 与 (L_{NS}) 个 NS-tokens 拼接而成，所有 token 的维度均为 (d)。需要注意的是，在 S-tokens 中插入了可学习的 [SEP] token，用于区分不同类型的用户行为序列边界。</p>\n<p>如图 2(b) 所示，每个 OneTrans Block 通过如下方式逐层更新 token 表示：</p>\n<p></p><div class=\"math display\">\\[Z^{(n)} = \\text{MixedMHA}(\\text{Norm}(X^{(n-1)})) + X^{(n-1)}\n\\]</div><p></p><p></p><div class=\"math display\">\\[X^{(n)} = \\text{MixedFFN}(\\text{Norm}(Z^{(n)})) + Z^{(n)}\n\\]</div><p></p><p>其中，MixedMHA（混合多头注意力）与 MixedFFN（混合前馈网络）采用混合参数化策略（见图 2(c)）：所有序列 token 共享同一组 Q/K/V 与 FFN 权重，而每个非序列 token 在注意力层与前馈层中均分配独立的专属参数。</p>\n<p>模型采用统一的因果掩码（causal mask），施加自回归约束，使每个位置只能关注其之前的 token。具体而言，NS-tokens 允许访问全部 S-tokens 的历史信息，从而实现充分的跨 token 交互。</p>\n<p>通过堆叠多个此类 Block，并对 S-tokens 施加金字塔式尾部裁剪（tail truncation），模型逐层将高阶信息压缩并蒸馏至 NS-tokens 中。最终的 token 表示输入至任务特定的预测头完成输出。</p>\n<h3 id=\"统一建模带来的结构优势\">统一建模带来的结构优势</h3>\n<p>通过将序列与非序列特征统一为单一 token 序列，并使用因果 Transformer 进行建模，OneTrans 摒弃了传统“先编码后交互”流程。这种统一设计在单一 Transformer 堆叠中自然实现了：</p>\n<ol>\n<li>单序列内部的交互建模</li>\n<li>多序列之间的交互建模</li>\n<li>用户、物品与上下文等多源特征之间的交互</li>\n<li>序列特征与非序列特征之间的交互</li>\n</ol>\n<p>统一建模形式使得模型可以无缝继承成熟的大模型工程优化技术，包括 KV 缓存与内存高效注意力机制，从而显著降低推理延迟。该统一框架在单一可扩展架构下处理多序列与跨域推荐任务，具备良好的扩展潜力。</p>\n<h3 id=\"32-特征与-token-化\">3.2 特征与 Token 化</h3>\n<p>为构建初始 token 序列 (X^{(0)})，OneTrans 首先通过特征预处理流程，将所有原始特征映射为 embedding 向量。这些 embedding 随后被划分为两部分：（i）多行为序列子集；（ii）表示用户、物品或上下文的非序列特征子集。随后分别对两类特征应用不同的 tokenizer。</p>\n<h4 id=\"321-非序列特征的-token-化\">3.2.1 非序列特征的 Token 化</h4>\n<p>非序列特征 NS 包括数值特征（如价格、CTR）和类别特征（如用户 ID、物品类目）。所有特征先进行分桶或 one-hot 编码，再映射为 embedding。</p>\n<p>由于工业系统通常包含数百个特征，且重要性差异显著，因此控制非序列 token 数量 (L_{NS}) 有两种方式：</p>\n<p><strong>（1）分组式 Tokenizer（Group-wise Tokenizer）</strong><br />\n与 RankMixer 对齐，将特征手动划分为语义组 ({g_1, ..., g_{L_{NS}}})。每个组内特征拼接后通过独立的 MLP 投影：</p>\n<p></p><div class=\"math display\">\\[NS\\text{-tokens} =\nMLP_1(concat(g_1)), ..., MLP_{L_{NS}}(concat(g_{L_{NS}}))\n\\]</div><p></p><p><strong>（2）自动切分 Tokenizer（Auto-Split Tokenizer）</strong><br />\n将所有特征拼接后，通过一个统一 MLP 投影，再进行切分：</p>\n<p></p><div class=\"math display\">\\[NS\\text{-tokens} = split(MLP(concat(NS)), L_{NS})\n\\]</div><p></p><p>Auto-Split 方式通过一次稠密投影减少 kernel 启动开销，相较分组式方法更高效。论文将在实验中对两种方式进行对比。</p>\n<p>最终，非序列 token 化得到 (L_{NS}) 个非序列 token，每个维度为 (d)。</p>\n<h4 id=\"322-序列特征的-token-化\">3.2.2 序列特征的 Token 化</h4>\n<p>OneTrans 支持多行为序列输入：</p>\n<p></p><div class=\"math display\">\\[S = {S_1, ..., S_n}, \\quad S_i = {e_{i1}, ..., e_{iL_i}}\n\\]</div><p></p><p>每个序列 (S_i) 包含 (L_i) 个事件 embedding。每个事件 embedding 由物品 ID 及其附加信息（如类目、价格等）拼接构成。</p>\n<p>由于不同序列的原始维度可能不同，对每个序列 (S_i) 使用共享投影网络 (MLP_i)，将所有事件映射至统一维度 (d)：</p>\n<p></p><div class=\"math display\">\\[\\tilde{S}*i =\nMLP_i(e*{i1}), ..., MLP_i(e_{iL_i})\n\\in \\mathbb{R}^{L_i \\times d}\n\\]</div><p></p><p>对齐后的序列 (\\tilde{S}_i) 可通过两种规则合并为统一 token 序列：</p>\n<ol>\n<li><strong>时间感知（Timestamp-aware）</strong>：按时间顺序交错所有事件，并添加序列类型标识。</li>\n<li><strong>时间无关（Timestamp-agnostic）</strong>：按行为影响力排序拼接，例如“购买 → 加购 → 点击”，在序列之间插入可学习的 [SEP] token。高意图行为排在前面。</li>\n</ol>\n<p>消融实验表明，当时间戳可用时，时间感知方式优于按行为影响排序的方式。</p>\n<p>形式化表示为：</p>\n<p></p><div class=\"math display\">\\[S\\text{-tokens} = Merge(\\tilde{S}_1, ..., \\tilde{S}_n)\n\\in \\mathbb{R}^{L_S \\times d}\n\\]</div><p></p><p>其中</p>\n<p></p><div class=\"math display\">\\[L_S = \\sum_{i=1}^{n} L_i + L_{SEP}\n\\]</div><p></p><h3 id=\"33-onetrans-block\">3.3 OneTrans Block</h3>\n<p>如图 2(b) 所示，每个 OneTrans Block 是一个 pre-norm 因果 Transformer，输入为规范化后的 token 序列：前 (L_S) 个为序列 token，后 (L_{NS}) 个为非序列 token。</p>\n<p>受异构特征组研究的启发，OneTrans 对标准 Transformer 进行了轻量级改造，引入混合参数机制：同质的 S-tokens 共享参数；而来源与语义异构的 NS-tokens 使用各自的专属参数。</p>\n<p>由于推荐系统的 token 序列混合了统计分布差异较大的 S-tokens 与 NS-tokens，若采用 post-norm 结构容易导致注意力塌缩和训练不稳定。因此，OneTrans 采用 RMSNorm 作为 pre-norm，对所有 token 进行归一化，从而对齐尺度并稳定优化过程。</p>\n<h4 id=\"331-混合共享专属因果注意力\">3.3.1 混合（共享/专属）因果注意力</h4>\n<p>OneTrans 使用标准多头注意力（MHA）和因果掩码，唯一的变化在于 Q/K/V 的参数化方式。</p>\n<p>设第 (i) 个 token 为 (x_i \\in \\mathbb{R}^d)，则：</p>\n<p></p><div class=\"math display\">\\[q_i, k_i, v_i =\nW_i^Q x_i,\\quad\nW_i^K x_i,\\quad\nW_i^V x_i\n\\]</div><p></p><p>其中参数矩阵采用混合参数化：</p>\n<p></p><div class=\"math display\">\\[W_i^\\Psi =\n\\begin{cases}\nW_S^\\Psi, &amp; i \\le L_S \\quad （S\\text{-tokens 共享参数）} \\\nW_{NS,i}^\\Psi, &amp; i &gt; L_S \\quad （NS\\text{-tokens 专属参数）}\n\\end{cases}\n\\]</div><p></p><p>因果掩码的设计为 NS-tokens 排在 S-tokens 之后，从而形成如下结构：</p>\n<ol>\n<li><strong>S 侧行为</strong><br />\n每个 S-token 仅关注其之前的 S-token。</li>\n</ol>\n<ul>\n<li>若采用时间感知顺序，每个行为基于历史建模；</li>\n<li>若采用按意图排序方式，因果掩码使高意图行为影响后续低意图行为。</li>\n</ul>\n<ol start=\"2\">\n<li>\n<p><strong>NS 侧行为</strong><br />\n每个 NS-token 可访问完整 S 历史，相当于对行为证据进行目标注意力聚合；同时可访问前序 NS-token，增强非序列 token 之间的交互。</p>\n</li>\n<li>\n<p><strong>支持金字塔结构</strong><br />\n因果结构使信息逐步向后集中，为逐层裁剪 token 的金字塔机制提供自然支持。</p>\n</li>\n</ol>\n<h4 id=\"332-混合共享专属前馈网络\">3.3.2 混合（共享/专属）前馈网络</h4>\n<p>FFN 采用相同的混合参数策略：</p>\n<p></p><div class=\"math display\">\\[MixedFFN(x_i) = W_{2,i} , \\phi(W_{1,i} x_i)\n\\]</div><p></p><p>其中 (W_{1,i}) 与 (W_{2,i}) 遵循上述混合参数规则：<br />\nS-tokens 共享参数，NS-tokens 使用专属参数。</p>\n<h4 id=\"小结\">小结</h4>\n<p>与标准因果 Transformer 相比，OneTrans 仅在参数化层面进行修改：</p>\n<ul>\n<li>NS-tokens 使用专属 QKV 与 FFN；</li>\n<li>S-tokens 共享一套参数；</li>\n<li>单一因果掩码统一连接整个序列。</li>\n</ul>\n<p>这种设计使 NS-tokens 能够聚合完整行为历史，同时保持 Transformer 风格的高效计算结构。</p>\n<h2 id=\"34-金字塔堆叠pyramid-stack\">3.4 金字塔堆叠（Pyramid Stack）</h2>\n<p>如第 3.3 节所述，因果掩码会将信息逐步集中到序列的后部位置。利用这一“信息向尾部聚集”的结构特性，OneTrans 采用金字塔式调度策略：在每一层 OneTrans Block 中，仅最近的一部分 S-tokens 参与 Query 计算，而 Key 与 Value 仍在完整序列上计算；随着网络深度增加，参与 Query 的 token 数量逐层缩减。</p>\n<p>设输入 token 序列为</p>\n<p></p><div class=\"math display\">\\[X = {x_i}_{i=1}^{L}\n\\]</div><p></p><p>定义尾部索引集合</p>\n<p></p><div class=\"math display\">\\[Q = {L - L' + 1, ..., L}, \\quad L' \\le L\n\\]</div><p></p><p>在该层中，仅对 (i \\in Q) 的 token 计算 Query：</p>\n<p></p><div class=\"math display\">\\[q_i = W_i^Q x_i, \\quad i \\in Q\n\\]</div><p></p><p>而 Key 与 Value 仍在完整索引集合 ({1, ..., L}) 上计算。注意力计算完成后，仅保留 (i \\in Q) 的输出表示，从而将 token 长度缩减为 (L')，形成跨层逐步收缩的金字塔结构。</p>\n<p>该设计带来两个核心优势：</p>\n<ol>\n<li>\n<p><strong>渐进式信息蒸馏</strong><br />\n长行为序列的信息被逐层压缩至尾部 token，模型容量集中于最具信息量的近期行为，并逐步将高阶信息汇聚至 NS-tokens。</p>\n</li>\n<li>\n<p><strong>计算效率提升</strong><br />\n注意力复杂度由 (O(L^2 d)) 降为 (O(L L' d))，FFN 复杂度与 (L') 线性相关。随着 Query 集合缩小，FLOPs 与激活内存显著降低。</p>\n</li>\n</ol>\n<h2 id=\"35-训练与部署优化\">3.5 训练与部署优化</h2>\n<h3 id=\"351-跨请求-kv-缓存cross-request-kv-caching\">3.5.1 跨请求 KV 缓存（Cross-Request KV Caching）</h3>\n<p>在工业推荐系统中，同一请求下的多个候选样本在训练与推理阶段通常连续处理：它们的 S-tokens 完全相同，而 NS-tokens 随候选物品变化。基于这一结构，OneTrans 引入 KV Caching 机制，形成统一的两阶段计算范式。</p>\n<p><strong>阶段 I（S 侧，每请求一次）</strong><br />\n对全部 S-tokens 进行因果注意力计算，并缓存其 Key/Value 及注意力输出。该阶段在每个请求中仅执行一次。</p>\n<p><strong>阶段 II（NS 侧，每候选一次）</strong><br />\n对每个候选物品计算其 NS-tokens，并与缓存的 S 侧 Key/Value 进行跨注意力计算，随后进入 token-specific FFN 层。<br />\n对于候选相关的序列特征（如 SIM），由于无法复用 S 侧缓存，需先通过池化方式预聚合为 NS-tokens。</p>\n<p>KV 缓存机制将 S 侧计算在候选间进行摊销，使单候选计算量大幅下降，避免重复计算，从而显著提升吞吐率。</p>\n<p>此外，由于用户行为序列是追加式增长的，可将 KV 缓存扩展至跨请求复用：<br />\n每次新请求复用前一缓存，仅对新增行为计算增量 Key/Value。<br />\n因此单次请求的序列计算复杂度由 (O(L)) 降至 (O(\\Delta L))，其中 (\\Delta L) 为自上次请求以来新增行为数量。</p>\n<h3 id=\"352-统一-llm-优化策略\">3.5.2 统一 LLM 优化策略</h3>\n<p>为进一步提升效率，OneTrans 引入成熟的大模型工程优化方法：</p>\n<ol>\n<li>\n<p><strong>FlashAttention-2</strong><br />\n通过分块计算与 kernel 融合，减少注意力计算中的 I/O 开销与二次激活存储需求，从而在训练与推理阶段降低内存占用并提升吞吐率。</p>\n</li>\n<li>\n<p><strong>混合精度训练（BF16/FP16）与激活重计算</strong><br />\n采用低精度计算以降低显存占用，并结合激活重计算策略，在反向传播阶段重新计算部分前向激活值。<br />\n该策略以少量额外计算换取显著内存节省，使得更大 batch 与更深模型成为可能，而无需修改模型结构。</p>\n</li>\n</ol>\n<p>总体而言，金字塔结构负责控制序列长度与计算规模，KV 缓存负责跨候选与跨请求的计算复用，而 LLM 优化技术则进一步提升底层算子效率。三者结合，使 OneTrans 在工业规模下具备可扩展性与可部署性。</p>\n<h2 id=\"4-实验\">4 实验</h2>\n<p>通过离线评估与在线实验，本文围绕以下研究问题展开分析：</p>\n<p><strong>RQ1：统一堆叠结构 vs. “先编码后交互”范式</strong><br />\n在计算资源相当的条件下，单一 Transformer 主干是否能够稳定优于传统的 encode–then–interaction 架构？</p>\n<p><strong>RQ2：关键设计因素分析</strong><br />\n通过消融实验评估不同设计选择对性能与效率的影响，包括：</p>\n<ul>\n<li>输入层设计（如 tokenizer 方式、序列融合策略）；</li>\n<li>OneTrans Block 结构设计（如参数共享策略、注意力形式、金字塔堆叠机制）。</li>\n</ul>\n<p><strong>RQ3：系统效率评估</strong><br />\n在相同 OneTrans 计算图下，金字塔堆叠、跨请求 KV 缓存、FlashAttention-2，以及混合精度训练与激活重计算，是否能够有效降低 FLOPs、显存占用与推理延迟？</p>\n<p><strong>RQ4：规模规律（Scaling Law）</strong><br />\n随着模型长度（token 序列长度）、宽度（(d_{model})）、深度（层数）的扩展，损失或性能是否呈现预期的近似对数线性增长趋势？</p>\n<p><strong>RQ5：在线 A/B 测试效果</strong><br />\n在满足生产环境延迟约束的前提下，OneTrans 在线部署是否能够在关键业务指标（如人均订单数、用户级 GMV）上实现统计显著的提升？</p>\n<p>该实验设计系统性地从架构有效性、设计合理性、系统效率、规模扩展能力以及真实业务效果五个维度进行验证。</p>\n<h3 id=\"41-实验设置\">4.1 实验设置</h3>\n<h4 id=\"411-数据集\">4.1.1 数据集</h4>\n<p>在离线评估中，我们在一个大规模工业排序场景下评估 OneTrans，使用严格隐私合规条件下的生产日志数据（所有个人可识别信息均已匿名化并哈希处理）。</p>\n<p>数据按时间顺序划分，所有特征均在曝光时刻进行快照，以防止时间信息泄露并保证线上线下一致性。标签（如点击与下单）在与生产环境对齐的固定时间窗口内聚合。数据集统计信息见表 1。</p>\n<p><img alt=\"image-2\" class=\"lazyload\" /></p>\n<h4 id=\"412-任务与指标\">4.1.2 任务与指标</h4>\n<p>我们评估两个二分类排序任务（见公式 (2)）：CTR 与 CVR。性能指标包括：</p>\n<ul>\n<li><strong>AUC</strong></li>\n<li><strong>UAUC（基于曝光加权的用户级 AUC）</strong></li>\n</ul>\n<h5 id=\"next-batch-评估方式\">Next-batch 评估方式</h5>\n<p>数据按时间顺序处理。对于每个 mini-batch：</p>\n<ol>\n<li>在评估模式下记录预测结果；</li>\n<li>随后在同一 batch 上进行训练。</li>\n</ol>\n<p>AUC 与 UAUC 按天计算，然后对各天结果进行宏平均。</p>\n<h5 id=\"效率指标\">效率指标</h5>\n<ul>\n<li><strong>Params</strong>：模型参数量（不包含稀疏 embedding）</li>\n<li><strong>TFLOPs</strong>：训练计算量（batch size = 2048 时的 TFLOPs）</li>\n</ul>\n<h4 id=\"413-基线模型\">4.1.3 基线模型</h4>\n<p>我们构建了工业标准模型组合作为对比基线，使用相同特征并匹配计算预算。</p>\n<p>在 encode–then–interaction 范式下：</p>\n<ul>\n<li>从生产中常用的 DCNv2 + DIN 作为基础模型；</li>\n<li>逐步增强特征交互模块：<br />\nDCNv2 → Wukong → HiFormer → RankMixer。</li>\n</ul>\n<p>在固定 RankMixer 的前提下，进一步替换序列建模模块：</p>\n<ul>\n<li>StackDIN → Transformer → LONGER。</li>\n</ul>\n<h4 id=\"414-超参数设置\">4.1.4 超参数设置</h4>\n<p>我们报告两种模型规模：</p>\n<ul>\n<li>\n<p><strong>OneTrans-S</strong></p>\n<ul>\n<li>6 层 OneTrans Block</li>\n<li>宽度 (d = 256)</li>\n<li>4 个注意力头</li>\n<li>约 1 亿参数</li>\n</ul>\n</li>\n<li>\n<p><strong>OneTrans-L</strong></p>\n<ul>\n<li>8 层</li>\n<li>宽度 (d = 384)</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"输入与金字塔设置\">输入与金字塔设置</h4>\n<ul>\n<li>\n<p>序列特征采用时间感知融合方式；</p>\n</li>\n<li>\n<p>非序列特征使用 Auto-Split tokenizer；</p>\n</li>\n<li>\n<p>使用启发式金字塔调度：</p>\n<ul>\n<li>OneTrans-S：序列 Query token 从 1190 线性缩减至 12</li>\n<li>OneTrans-L：从 1500 缩减至 16</li>\n</ul>\n</li>\n</ul>\n<p>具体实现为：跨层线性减少序列 Query token 数量，并在每层将 token 数量四舍五入至 32 的倍数，最顶层 token 数量与非序列 token 数量一致。</p>\n<h4 id=\"优化与基础设施\">优化与基础设施</h4>\n<p>采用双优化器策略（不使用 weight decay）：</p>\n<ul>\n<li>稀疏 embedding：Adagrad（β₁=0.1，β₂=1.0）</li>\n<li>稠密参数：RMSProp（lr=0.005，alpha=0.99999，momentum=0）</li>\n</ul>\n<p>训练稳定性策略包括：</p>\n<ul>\n<li>Pre-Norm 结构</li>\n<li>全局梯度范数裁剪</li>\n</ul>\n<p>训练阶段：</p>\n<ul>\n<li>每 GPU batch size = 2048</li>\n<li>稠密层梯度裁剪阈值 = 90</li>\n<li>稀疏层梯度裁剪阈值 = 120</li>\n<li>使用 16 张 H100 GPU，数据并行 All-Reduce</li>\n</ul>\n<p>在线推理阶段：</p>\n<ul>\n<li>每 GPU batch size = 100</li>\n<li>在吞吐率与延迟之间进行权衡优化</li>\n</ul>\n<p>该实验设置确保了模型规模、训练稳定性与工业部署约束之间的平衡，使离线评估与线上效果具有高度一致性。</p>\n<h3 id=\"42-rq1性能评估\">4.2 RQ1：性能评估</h3>\n<p><img alt=\"image-3\" class=\"lazyload\" /></p>\n<p>我们以 DCNv2 + DIN 作为对比基准模型，该模型为本场景中规模扩展前的生产基线（见表 2）。</p>\n<p>在 encode–then–interaction 范式下，分别对两个组件进行独立扩展均能带来收益：</p>\n<ul>\n<li>升级特征交互模块（DCNv2 → Wukong → HiFormer → RankMixer）；</li>\n<li>升级序列建模模块（StackDIN → Transformer → LONGER）；</li>\n</ul>\n<p>上述改进在 CTR AUC/UAUC 与 CVR AUC 上均表现出稳定提升。</p>\n<p>在我们的系统中，AUC 或 UAUC 提升超过 +0.1% 即被视为具有实际意义；超过 +0.3% 通常对应在线 A/B 测试中的统计显著提升。由于用户级样本数量较少且波动较大，CVR UAUC 指标需谨慎解读。</p>\n<p>在统一架构下，OneTrans-S 相比基线取得：</p>\n<ul>\n<li>CTR：+1.13% / +1.77%（AUC / UAUC）</li>\n<li>CVR：+0.90% / +1.66%（AUC / UAUC）</li>\n</ul>\n<p>在相近参数规模与训练算力条件下（2.64T vs. 2.51T），OneTrans-S 也显著优于 RankMixer + Transformer，验证了统一建模的优势。</p>\n<p>进一步扩展模型规模，OneTrans-L 取得最佳整体表现：</p>\n<ul>\n<li>CTR：+1.53% / +2.79%</li>\n<li>CVR：+1.14% / +3.23%</li>\n</ul>\n<p>随着模型容量增加，性能呈现稳定、可预测的增长趋势。</p>\n<p>总结而言，在单一 Transformer 主干中统一序列建模与特征交互，相比独立扩展其中任一模块，能够带来更可靠且更具计算效率的性能提升。这一结果直接回答了 RQ1，并为统一架构提供了实证支持。</p>\n<h3 id=\"43-rq2设计选择的消融研究\">4.3 RQ2：设计选择的消融研究</h3>\n<p>我们对 OneTrans 的关键设计进行系统性消融实验，以量化各组件的贡献。完整结果见表 3。实验变体包括：</p>\n<p><img alt=\"image-4\" class=\"lazyload\" /></p>\n<p><img alt=\"image-5\" class=\"lazyload\" /></p>\n<h4 id=\"输入层变体\">输入层变体</h4>\n<ol>\n<li>将 Auto-Split Tokenizer 替换为 Group-wise Tokenizer（表 3 第 1 行）；</li>\n<li>使用时间无关的序列融合方式替代时间感知融合（第 2 行）；</li>\n<li>在时间感知融合中移除可学习的 [SEP] token（第 3 行）。</li>\n</ol>\n<h4 id=\"onetrans-block-结构变体\">OneTrans Block 结构变体</h4>\n<ol start=\"4\">\n<li>所有 token 共享同一组 Q/K/V 与 FFN 参数，而不对 NS-tokens 使用专属参数（第 4 行）；</li>\n<li>使用全注意力（full attention）替代因果注意力（第 5 行）；</li>\n<li>取消金字塔堆叠，在所有层保留完整 token 序列（第 6 行）。</li>\n</ol>\n<h4 id=\"消融结论\">消融结论</h4>\n<ol>\n<li>\n<p><strong>Auto-Split Tokenizer 优于人工分组方式</strong><br />\n相比将非序列特征人工划分为语义组，自动构建 NS-token 的方式效果更优。这表明由模型自动学习特征组织结构优于人为先验分组。</p>\n</li>\n<li>\n<p><strong>时间感知融合优于按行为意图排序</strong><br />\n在存在时间戳信息时，按时间顺序融合优于按行为影响力排序，说明时间信息比主观设定的行为强弱排序更具表达力。</p>\n</li>\n<li>\n<p><strong>[SEP] token 有助于序列区分</strong><br />\n在时间无关融合下，可学习的 [SEP] token 有助于模型区分不同序列，提高表示能力。</p>\n</li>\n<li>\n<p><strong>NS-token 专属参数优于共享参数</strong><br />\n为非序列 token 分配专属 Q/K/V 与 FFN 参数优于完全共享投影，有助于增强不同特征来源之间的表达区分能力。</p>\n</li>\n<li>\n<p><strong>因果注意力与全注意力效果接近，但因果注意力更具工程优势</strong><br />\n性能上二者相近，但全注意力会破坏 KV 缓存等标准优化手段，因此不具备系统层面的可扩展性。</p>\n</li>\n<li>\n<p><strong>金字塔设计不会损失效果，却显著节省计算</strong><br />\n在所有层保留完整 token 序列并未带来性能提升。OneTrans 能有效将信息压缩至尾部 token，因此金字塔结构可以安全裁剪 Query token，从而降低计算量。</p>\n</li>\n</ol>\n<p>此外，在固定 TFLOPs 预算下，金字塔设计支持接近 1.75 倍更长的输入序列，相比全长度设计更能有效利用序列扩展带来的收益。</p>\n<p>总体而言，消融结果表明 OneTrans 的关键设计并非经验性堆叠，而是具有明确结构必要性：自动 token 构建、时间感知融合、NS 专属参数以及金字塔裁剪共同构成了性能与效率兼顾的核心机制。</p>\n<h2 id=\"44-rq3系统效率\">4.4 RQ3：系统效率</h2>\n<p>为量化第 3.5 节提出的系统优化策略，我们在一个未做优化的 OneTransS 基线模型上逐项进行消融，并在表 4 中报告训练与推理阶段的效率指标。</p>\n<p><img alt=\"image-6\" class=\"lazyload\" /></p>\n<p>实验结果表明：</p>\n<ol>\n<li>\n<p><strong>金字塔堆叠（Pyramid Stack）</strong><br />\n通过裁剪序列 Query token，显著降低训练阶段的运行时间与显存占用，同时减少线上服务阶段的 p99 延迟与内存消耗。</p>\n</li>\n<li>\n<p><strong>跨请求 KV 缓存（Cross-request KV Caching）</strong><br />\n消除重复的序列计算，在训练与推理阶段均稳定提升运行效率与内存利用率。</p>\n</li>\n<li>\n<p><strong>FlashAttention</strong><br />\n在训练阶段带来显著加速效果，而在推理阶段的改进相对有限但仍有收益。</p>\n</li>\n<li>\n<p><strong>混合精度与重计算（Mixed Precision + Re-computation）</strong><br />\n对线上服务收益最大，显著降低 p99 延迟与推理显存开销，同时提升训练效率。</p>\n</li>\n</ol>\n<h3 id=\"综合结论\">综合结论</h3>\n<p><img alt=\"image-7\" class=\"lazyload\" /></p>\n<p>上述结果验证了将大模型（LLM）系统优化技术迁移到大规模推荐场景的有效性。在此基础上，模型进一步扩展至 OneTransL，并在表 5 中展示其线上效率可与规模远小得多的 DCNv2+DIN 基线模型相当。</p>\n<p>这说明统一的 Transformer 主干结构具有重要工程优势：它允许直接复用成熟的大模型优化技术，从而在模型规模显著提升的同时，维持可控的在线计算开销。</p>\n<p>总体来看，本节证明了 OneTrans 在扩大模型容量的同时，并未牺牲系统效率，而是通过架构统一化实现了性能与工程可扩展性的兼顾。</p>\n<h2 id=\"45-rq4scaling-law-验证\">4.5 RQ4：Scaling-Law 验证</h2>\n<p>我们从三个维度系统研究 OneTrans 的扩展规律（scaling laws）：</p>\n<ol>\n<li><strong>长度（Length）</strong>：输入 token 序列长度</li>\n<li><strong>深度（Depth）</strong>：堆叠 Transformer block 的层数</li>\n<li><strong>宽度（Width）</strong>：隐藏层维度大小</li>\n</ol>\n<h3 id=\"单轴扩展结果\">单轴扩展结果</h3>\n<p>如图 3(a) 所示：</p>\n<ul>\n<li>\n<p><strong>增加序列长度带来最大收益</strong>，因为更长的行为序列提供了更丰富的用户行为证据。</p>\n</li>\n<li>\n<p>在深度与宽度之间存在明显权衡：</p>\n<ul>\n<li>增加深度通常比单纯增加宽度带来更大的性能提升，因为更深的网络可以学习更高阶交互与更丰富的抽象表示。</li>\n<li>但更深的模型增加串行计算路径，影响延迟。</li>\n<li>扩宽维度则更易并行化，在硬件利用率上更友好。</li>\n</ul>\n</li>\n</ul>\n<p>因此，在实际部署时，深度与宽度的选择应综合考虑性能收益与目标硬件预算下的系统效率。</p>\n<h3 id=\"联合扩展与对比分析\">联合扩展与对比分析</h3>\n<p>我们进一步联合增加 OneTrans 的深度与宽度，同时对比扩展 RankMixer+Transformer 基线（主要在 RankMixer 侧扩展至 1B 参数规模），并绘制 ΔUAUC 相对于训练 FLOPs（对数尺度）的关系曲线。</p>\n<p>图 3(b) 表明：</p>\n<ul>\n<li>OneTrans 与 RankMixer 均呈现清晰的对数线性趋势（log-linear scaling）。</li>\n<li>但 <strong>OneTrans 的斜率更陡峭</strong>，即在相同计算增量下带来更大的性能提升。</li>\n</ul>\n<p>其原因可能在于：</p>\n<ul>\n<li>RankMixer 的扩展缺乏统一主干结构；</li>\n<li>基于 MoE 的扩展主要体现在 FFN 隐藏层维度的“宽度扩展”；</li>\n<li>而 OneTrans 通过统一 Transformer 主干实现结构层面的整体扩展，提升了参数与计算利用效率。</li>\n</ul>\n<h3 id=\"结论与部署边界\">结论与部署边界</h3>\n<p>这些结果表明：</p>\n<ul>\n<li>OneTrans 在参数效率与计算效率方面更具优势；</li>\n<li>在工业部署场景中，其性能–计算权衡更具吸引力。</li>\n</ul>\n<p>目前，OneTransL 已可在严格的线上 p99 延迟约束下稳定部署。但进一步超出当前规模的扩展仍受到在线效率瓶颈限制。未来工作将聚焦于系统与模型的联合优化，以突破这一扩展上限。</p>\n<p>从本节可以提炼出一个核心结论：<br />\n<strong>在推荐系统场景中，统一 Transformer 主干不仅带来结构表达优势，也带来更优的 scaling-law 斜率，即单位算力对应更高的性能增益。这一点是其相较于传统“组件拼接式扩展”方法的根本优势。</strong></p>\n<h2 id=\"46-rq5在线-ab-实验\">4.6 RQ5：在线 A/B 实验</h2>\n<p>我们在两个大规模工业场景中评估 OneTrans 的业务影响：</p>\n<ol>\n<li><strong>Feeds</strong>（首页信息流）</li>\n<li><strong>Mall</strong>（包含 Feeds 及其他子场景的整体业务环境）</li>\n</ol>\n<p>流量在用户/账号层级进行哈希切分与随机分配。控制组与实验组均基于过去 1.5 年的生产数据进行训练与部署，以确保对比公平。</p>\n<h3 id=\"实验设置\">实验设置</h3>\n<ul>\n<li>\n<p><strong>控制组（Control）</strong>：<br />\nRankMixer + Transformer（约 1 亿神经网络参数），不使用序列 KV 缓存。</p>\n</li>\n<li>\n<p><strong>实验组（Treatment）</strong>：<br />\nOneTransL，并采用第 3.5 节所述的服务端优化策略。</p>\n</li>\n</ul>\n<p>评估指标包括：</p>\n<ul>\n<li>用户级 click/u、order/u、gmv/u（相对 RankMixer+Transformer 的 Δ%）；</li>\n<li>双侧 95% 置信区间（基于用户分层 bootstrap）；</li>\n<li>端到端延迟（p99 每曝光延迟变化百分比，数值越低越好）。</li>\n</ul>\n<h3 id=\"实验结果\">实验结果</h3>\n<p>如表 6 所示，OneTransL 在两个场景均取得稳定提升。</p>\n<p><img alt=\"image-8\" class=\"lazyload\" /></p>\n<h4 id=\"feeds-场景\">Feeds 场景</h4>\n<ul>\n<li>click/u：+7.737%</li>\n<li>order/u：+4.3510%</li>\n<li>gmv/u：+5.6848%</li>\n<li>p99 延迟：−3.91%</li>\n</ul>\n<h4 id=\"mall-场景\">Mall 场景</h4>\n<ul>\n<li>click/u：+5.143%</li>\n<li>order/u：+2.5772%</li>\n<li>gmv/u：+3.6696%</li>\n<li>p99 延迟：−3.26%</li>\n</ul>\n<p>结果表明，相较于强基线 RankMixer+Transformer，统一建模框架在提升核心业务指标的同时，反而降低了线上服务延迟。</p>\n<h3 id=\"泛化能力分析\">泛化能力分析</h3>\n<p>进一步观察到：</p>\n<ul>\n<li>用户 Active Days 提升 +0.7478%；</li>\n<li>冷启动商品 order/u 提升 +13.59%。</li>\n</ul>\n<p>这说明 OneTrans 在冷启动与泛化能力方面具有显著优势，能够更有效地建模稀疏或低频行为模式。</p>\n<h3 id=\"综合结论-1\">综合结论</h3>\n<p>本节验证了统一 Transformer 主干不仅在离线指标与系统效率上具有优势，在真实大规模线上环境中亦能实现：</p>\n<ol>\n<li>显著的核心业务指标提升；</li>\n<li>服务延迟下降；</li>\n<li>冷启动泛化能力增强。</li>\n</ol>\n<p>从工程与业务双重维度看，OneTrans 实现了性能、效率与泛化能力的协同提升，证明统一建模设计在工业推荐系统中的实际可行性与商业价值。</p>\n<h2 id=\"5-结论\">5 结论</h2>\n<p>本文提出 OneTrans，一种用于个性化排序的统一 Transformer 主干结构，用以替代传统的“编码–交互”（encode–then–interaction）范式。</p>\n<p>该方法通过统一的 Tokenizer 将序列特征与非序列特征转换为单一 token 序列，并通过统一的 Transformer Block 在同一框架内同时完成序列建模与特征交互。其中：</p>\n<ul>\n<li>对同质的序列 token 共享参数；</li>\n<li>对异质的非序列 token 使用专属参数，以增强表达能力。</li>\n</ul>\n<p>为保证统一架构在大规模场景下的效率：</p>\n<ul>\n<li>采用金字塔式调度策略，逐层裁剪序列 token；</li>\n<li>使用跨请求 KV 缓存复用用户侧计算；</li>\n<li>同时引入大模型风格的系统优化技术，如 FlashAttention 与混合精度计算。</li>\n</ul>\n<p>在大规模实验中，OneTrans 随模型宽度与深度扩展呈现近似对数线性（log-linear）的性能增长趋势，并在真实线上环境中取得统计显著的业务指标提升，同时保持生产级延迟要求。</p>\n<p>总体而言，该统一设计为推荐系统的规模扩展提供了一条可行路径，使其能够直接复用近年来推动大语言模型发展的系统优化技术，实现模型能力与工程效率的协同提升。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t<p class=\"postfoot\">\n\t\t\tposted on \n<span id=\"post-date\">2026-02-14 11:31</span>&nbsp;\n<a href=\"https://www.cnblogs.com/GlenTt\">GlenTt</a>&nbsp;\n阅读(<span id=\"post_view_count\">50</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</p>"
    },
    {
      "title": "发布园子的第一个B站视频！提前祝大家春节快乐！",
      "link": "https://www.cnblogs.com/cmt/p/19613749",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/cmt/p/19613749\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 22:43\">\n    <span>发布园子的第一个B站视频！提前祝大家春节快乐！</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>园子入驻B站啦，今天发布了第一个视频，提前祝大家马年春节快乐！码到成功！欢迎大家前往<a href=\"https://www.bilibili.com/video/BV1UVcEzJEUD/\" rel=\"noopener nofollow\" target=\"_blank\">B站</a>捧场！</p>\n<p></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 22:43</span>&nbsp;\n<a href=\"https://www.cnblogs.com/cmt\">博客园团队</a>&nbsp;\n阅读(<span id=\"post_view_count\">343</span>)&nbsp;\n评论(<span id=\"post_comment_count\">5</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "睡前讲一段docker编译镜像的故事",
      "link": "https://www.cnblogs.com/freephp/p/19613690",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/freephp/p/19613690\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 22:31\">\n    <span>睡前讲一段docker编译镜像的故事</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>最近在和团队的工程师开发一些创新项目，我需要把本地项目打包成docker镜像，并且推送得到AWS云服务上的镜像仓库（ECR）。<br />\n我写好Dockerfile之后就开始先编译，然后再推送。命令如下所示：</p>\n<pre><code class=\"language-bash\">docker build -t ai-service:latest .\ndocker tag ai-service:latest XXXXX.dkr.ecr.us-east-1.amazonaws.com/ai-service:latest\ndocker push XXXXX.dkr.ecr.us-east-1.amazonaws.com/ai-service:latest\n</code></pre>\n<p>一切看似顺利，但当我使用这个容器镜像来创建EC2（我选择了t3.micro）的时候，却遇到了镜像和系统平台不兼容的报错，具体如下所示：</p>\n<pre><code class=\"language-bash\">latest: Pulling from smart-inventory\nno matching manifest for linux/amd64 in the manifest list entries\n</code></pre>\n<p>可以看出EC2的系统是linux/amd64的，而我的笔记本电脑是MacOS M1芯片，是arm64的架构。Docker默认编译出来的镜像是基于所在平台的，也就是说用MacOS M1编译出的镜像也是arm架构。<br />\n解决这个问题十分简单，Docker提供了强大的buildx工具，可以在编译的时候指定编译出的架构。<br />\n例如我想编译出适合linxu/amd64的镜像，可以用如下命令：</p>\n<pre><code class=\"language-bash\">docker buildx build --platform linux/amd64 -t ai-service:latest .\n</code></pre>\n<p>这样编译出来的镜像就能在linux/amd64架构的机器上运行了。</p>\n<p>仔细再想一想，为什么之前我没有遇到这个问题呢？<br />\n原因是因为我并不是直接在我本地电脑上编译镜像并推送到远端镜像库的，我们通常是通过持续集成系统（CI/CD）来完成的。在CI/CD流程中，我们使用的是amd64架构的虚拟机来完成镜像编译，这样编译出的镜像都是amd64的。<br />\n所以如果我要坚持在本地编译镜像来推送，则需要先判断当前系统是否是MacOS，如果是，则使用“--platform linux/amd64”参数完成编译即可，用Python实现如下所示：</p>\n<pre><code class=\"language-python\">  if os.name == 'posix' and 'darwin' in os.uname().sysname.lower():\n            # On macOS, build for linux/amd64 platform\n            build_cmd = [\"docker\", \"buildx\", \"build\", \"--platform\", \"linux/amd64\", \"-t\", image_name, \".\"]\n        else:\n            build_cmd = [\"docker\", \"build\", \"-t\", image_name, \".\"]\n</code></pre>\n<p>每次遇到问题，我总是想多思考一下背后的原因，有点像挖金矿一样，收获蛮大。</p>\n<p>王阳明说：<strong>人须在事上磨，方立得住，方能静亦定，动亦定。</strong></p>\n<p>我还需要继续磨励，继续专研，把事儿做好。走好脚下路的，才能有好未来。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 22:31</span>&nbsp;\n<a href=\"https://www.cnblogs.com/freephp\">freephp</a>&nbsp;\n阅读(<span id=\"post_view_count\">96</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "MAF快速入门（16）用户智能体交互协议AG-UI（上）",
      "link": "https://www.cnblogs.com/edisontalk/p/-/quick-start-on-maf-chatper16",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/edisontalk/p/-/quick-start-on-maf-chatper16\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 22:27\">\n    <span>MAF快速入门（16）用户智能体交互协议AG-UI（上）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"MAF快速入门（16）用户智能体交互协议AG-UI（上）\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/381412/202602/381412-20260213222607884-1836311076.png\" />\n        AG-UI 全称 Agent–User Interaction Protocol 即 智能体-用户 交互协议，这是一个开放的、基于事件的协议，由 CopilotKit 团队发起，用于标准化 AI Agent 与 用户界面 的实时交互。本文介绍了AG-UI协议的基本概念，为什么会出现AG-UI协议，AG-UI和MCP，A2A的对比，随后介绍了如何在MAF中快速开发一个基于AG-UI的对话应用。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>大家好，我是Edison。</p>\n<p>最近我一直在跟着圣杰的《<a class=\"normal_text_link mp_article_text_link\" href=\"https://mp.weixin.qq.com/s?__biz=MzA4NzQzNTg4Ng==&amp;mid=2651744458&amp;idx=1&amp;sn=139f7584e81aeecd0945133bdc2b4791&amp;scene=21#wechat_redirect\" rel=\"noopener nofollow\" target=\"_blank\">.NET+AI智能体开发进阶</a>》课程学习MAF开发多智能体工作流，我强烈推荐你也上车跟我一起出发！</p>\n<p><a class=\"normal_text_link mp_article_text_link\" href=\"https://www.cnblogs.com/edisontalk/p/-/quick-start-on-maf-chatper15\" target=\"_blank\">上一篇</a>，<span><span>我们学习了MAF中<span>快速调试的利器DevUI<span><span>。本篇，我们来了解一个用户智能体交互协议：AG-UI。</span></span></span></span></span></p>\n<h1><strong>1 什么是AG-UI</strong></h1>\n<p><span><span><span><span><span><span><span><span>AG-UI 全称&nbsp;<strong><span><span>Agent–User Interaction Protocol&nbsp;<span><span>即&nbsp;<span>智能体-用户 交互协议<span>，</span></span></span></span></span></span></strong>这是一个开放的、基于事件的协议，由 CopilotKit 团队发起，用于标准化 AI Agent 与 用户界面 的实时交互。</span></span></span></span></span></span></span></span></p>\n<h3><span><span><span><span><span>为什么出现AG-UI协议？</span></span></span></span></span></h3>\n<p><span><span><span><span><span>这是因为在构建AI Agent应用的界面时，传统API模式面临很多问题和挑战：</span></span></span></span></span></p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/381412/202602/381412-20260213222047426-1470758576.png\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></p>\n<p><span><span><span><span><span><span><span>而AG-UI则是专门为AI Agent与用户界面的交互而设计的协议，其核心价值体现在：</span></span></span></span></span></span></span></p>\n<ul class=\"list-paddingleft-1\">\n<li>\n<p><span>📡&nbsp;<strong>实时流式响应</strong><span>：即时展示 Agent 输出，无需等待</span></span></p>\n</li>\n<li>\n<p><span>🎯&nbsp;<strong>事件驱动架构</strong><span>：细粒度的交互事件，精确控制 UI</span></span></p>\n</li>\n<li>\n<p><span>🔄&nbsp;<strong>状态同步机制</strong><span>：Snapshot/Delta 模式，保持 UI 与 Agent 状态一致</span></span></p>\n</li>\n<li>\n<p><span>🔧&nbsp;<strong>工具调用可视化</strong><span>：透明展示 Agent 的思考和行动过程</span></span></p>\n</li>\n</ul>\n<h3><span><span><span><span><span><span><span>三大Agent协议对比</span></span></span></span></span></span></span></h3>\n<p><span>我们之前已经学习了MCP 和 A2A两个重要的协议了，加上AG-UI，它们共同组成了Agent的三大通信协议体系。</span></p>\n<p><span>不过，它们的定位各有侧重，并非非此即彼，而是协同使用，用形象的比喻来讲：</span></p>\n<p><strong><span><span>AG-UI 像是\"客服窗口\"：</span></span></strong></p>\n<ul class=\"list-paddingleft-1\">\n<li>\n<p><span><span>用户与 Agent 之间的交互界面</span></span></p>\n</li>\n<li>\n<p><span><span>实时展示 Agent 的工作状态</span></span></p>\n</li>\n<li>\n<p><span><span>支持用户输入和反馈</span></span></p>\n</li>\n</ul>\n<p><strong><span><span>MCP 像是\"工具箱\"：</span></span></strong></p>\n<ul class=\"list-paddingleft-1\">\n<li>\n<p><span><span>Agent 调用外部工具完成任务</span></span></p>\n</li>\n<li>\n<p><span><span>工具是被动的，等待调用</span></span></p>\n</li>\n<li>\n<p><span><span>扩展 Agent 的能力边界</span></span></p>\n</li>\n</ul>\n<p><strong><span><span>A2A 像是\"同事协作\"：</span></span></strong></p>\n<ul class=\"list-paddingleft-1\">\n<li>\n<p><span><span>多个 Agent 之间的任务分发</span></span></p>\n</li>\n<li>\n<p><span><span>每个 Agent 都是自主的</span></span></p>\n</li>\n<li>\n<p><span><span>可以互相委托和协作</span></span></p>\n</li>\n</ul>\n<p><span><span><span>在实际企业场景中，<strong><strong><span><span>三大协议通常协同使用：</span></span></strong></strong></span></span></span></p>\n<ul class=\"list-paddingleft-1\">\n<li>\n<p><strong>AG-UI</strong><span>：用户通过界面与主 Agent 交互</span></p>\n</li>\n<li>\n<p><strong>MCP</strong><span>：Agent 内部使用 MCP 调用工具</span></p>\n</li>\n<li>\n<p><strong>A2A</strong><span>：复杂任务委托给专家 Agent 处理</span></p>\n</li>\n</ul>\n<p><span><span><span>下图展示了三大协议的详细对比：</span></span></span></p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/381412/202602/381412-20260213222138115-952680249.png\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></p>\n<h1><strong><span>2 快速开始：AG-UI对话应用<strong><span><br /></span></strong></span></strong></h1>\n<p>AG-UI协议定义了清晰的架构组件，包括 Server、Client 和 Agent。</p>\n<p>在MAF中提供了一个内置的AG-UI组件，我们可以非常方便地创建集成AG-UI的Agent应用。</p>\n<p>接下来，我们就一步一步完成一个AG-UI对话应用涉及到的Server 和 Client。</p>\n<h3>AG-UI Server</h3>\n<p>首先，我们创建一个ASP.NET Web应用，安装以下NuGet包：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">Microsoft.Agents.AI.Hosting.AGUI.AspNetCore\nMicrosoft.Agents.AI.OpenAI\nMicrosoft.Extensions.AI.OpenAI</span></pre>\n</div>\n<p>然后，就是整个示例的核心部分，我们一块一块来说：</p>\n<p>（1）创建应用并注册AG-UI服务</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step0. Create WebApplication builder</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">var</span> builder =<span style=\"color: rgba(0, 0, 0, 1);\"> WebApplication.CreateBuilder(args);\nbuilder.Services.AddHttpClient().AddLogging();\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step1. Register AG-UI services</span>\nbuilder.Services.AddAGUI();</pre>\n</div>\n<p><span>（2）创建一个AI Agent：</span></p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">var</span> app =<span style=\"color: rgba(0, 0, 0, 1);\"> builder.Build();\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step2. Load Configuration</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">var</span> config = <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> ConfigurationBuilder()\n    .AddJsonFile($</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">appsettings.json</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, optional: <span style=\"color: rgba(0, 0, 255, 1);\">false</span>, reloadOnChange: <span style=\"color: rgba(0, 0, 255, 1);\">true</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    .AddJsonFile($</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">appsettings.{Environment.GetEnvironmentVariable(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>ASPNETCORE_ENVIRONMENT<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">)}.json</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, optional: <span style=\"color: rgba(0, 0, 255, 1);\">true</span>, reloadOnChange: <span style=\"color: rgba(0, 0, 255, 1);\">true</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    .Build();\n</span><span style=\"color: rgba(0, 0, 255, 1);\">var</span> openAIProvider = config.GetSection(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">OpenAI</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>).Get&lt;OpenAIProvider&gt;<span style=\"color: rgba(0, 0, 0, 1);\">();\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step3. Create one ChatClient</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">var</span> chatClient = <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> OpenAIClient(\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> ApiKeyCredential(openAIProvider.ApiKey),\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">new</span> OpenAIClientOptions { Endpoint = <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> Uri(openAIProvider.Endpoint) })\n    .GetChatClient(openAIProvider.ModelId)\n    .AsIChatClient();\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step4. Create one AI Agent</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">var</span> agent =<span style=\"color: rgba(0, 0, 0, 1);\"> chatClient.AsAIAgent(\n    name: </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">AGUI-Assistant</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">,\n    instructions: </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">你是一个友好的AI助手，请使用中文回答用户的问题。</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">✅ AI Agent 创建成功</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>);</pre>\n</div>\n<p><span>（3）映射AGUI端点并启动应用：</span></p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step5. Mapping AG-UI Endpoints</span>\napp.MapAGUI(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">/</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">, agent);\n\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">🚀 AG-UI Server 已启动</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">📍 端点地址: https://localhost:8443/</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">💡 使用 Ctrl+C 停止服务</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n\napp.Run();</span></pre>\n</div>\n<p>可以看到，我们仅用一行代码 app.MapAGUI() 就启用了AG-UI协议，So Easy!</p>\n<p><span><span>启动起来的效果如下图所示：</span></span></p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/381412/202602/381412-20260213222325381-1327622437.png\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></p>\n<h3><strong><span><span>AG-UI Client</span></span></strong></h3>\n<p>首先，我们创建一个控制台应用，安装以下NuGet包：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">Microsoft.Agents.AI.AGUI\nMicrosoft.Agents.AI</span></pre>\n</div>\n<p><span><span>然后，我们创建AG-UI Client 和 AI Agent：</span></span></p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Load Configuration</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">var</span> config = <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> ConfigurationBuilder()\n    .AddJsonFile($</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">appsettings.json</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, optional: <span style=\"color: rgba(0, 0, 255, 1);\">false</span>, reloadOnChange: <span style=\"color: rgba(0, 0, 255, 1);\">true</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    .Build();\n</span><span style=\"color: rgba(0, 0, 255, 1);\">var</span> openAIProvider = config.GetSection(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">OpenAI</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>).Get&lt;OpenAIProvider&gt;<span style=\"color: rgba(0, 0, 0, 1);\">();\n</span><span style=\"color: rgba(0, 0, 255, 1);\">var</span> serverEndpoint = config.GetValue&lt;<span style=\"color: rgba(0, 0, 255, 1);\">string</span>&gt;(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">AGUI_SERVER_URL</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span>?? <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">https://localhost:8443</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">;\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">🚀 AG-UI 客户端已启动</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.WriteLine($</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">📍 服务端地址: {serverEndpoint}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n</span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step1. Create HTTP Client</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">using</span> HttpClient httpClient = <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\">()\n{\n    Timeout </span>= TimeSpan.FromSeconds(<span style=\"color: rgba(128, 0, 128, 1);\">60</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n};\n</span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step2. Create AG-UI Client</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">var</span> chatClient = <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> AGUIChatClient(httpClient, serverEndpoint);\n</span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step3. Create AI Agent</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">var</span> agent =<span style=\"color: rgba(0, 0, 0, 1);\"> chatClient.AsAIAgent(\n    name: </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">agui-client</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">,\n    description: </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">AG-UI Client Agent</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>);</pre>\n</div>\n<p><span>然后，准备开始对话：</span></p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Step4. Prepare for Conversation</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">var</span> session = <span style=\"color: rgba(0, 0, 255, 1);\">await</span><span style=\"color: rgba(0, 0, 0, 1);\"> agent.GetNewSessionAsync();\n</span><span style=\"color: rgba(0, 0, 255, 1);\">var</span> messages = <span style=\"color: rgba(0, 0, 255, 1);\">new</span> List&lt;ChatMessage&gt;<span style=\"color: rgba(0, 0, 0, 1);\">()\n{\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">new</span> ChatMessage(ChatRole.System, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">你是一个友好的AI助手，使用中文回答用户的问题。</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n};\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">💬 开始对话（输入 :q 或 quit 退出）\\n</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">while</span> (<span style=\"color: rgba(0, 0, 255, 1);\">true</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n{\n    Console.Write(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">👤 用户: </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">string</span>? message =<span style=\"color: rgba(0, 0, 0, 1);\"> Console.ReadLine();\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> (<span style=\"color: rgba(0, 0, 255, 1);\">string</span><span style=\"color: rgba(0, 0, 0, 1);\">.IsNullOrWhiteSpace(message)) \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">continue</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> (message <span style=\"color: rgba(0, 0, 255, 1);\">is</span> <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">:q</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> or <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">quit</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">) \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 添加用户消息</span>\n    messages.Add(<span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> ChatMessage(ChatRole.User, message));\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 流式接收响应</span>\n    Console.Write(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">🤖 助手: </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">await</span> <span style=\"color: rgba(0, 0, 255, 1);\">foreach</span> (<span style=\"color: rgba(0, 0, 255, 1);\">var</span> update <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> agent.RunStreamingAsync(messages, session))\n    {\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">foreach</span> (AIContent content <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> update.Contents)\n        {\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">switch</span><span style=\"color: rgba(0, 0, 0, 1);\"> (content)\n            {\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">case</span><span style=\"color: rgba(0, 0, 0, 1);\"> TextContent textContent:\n                    Console.Write(textContent.Text);\n                    </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">case</span><span style=\"color: rgba(0, 0, 0, 1);\"> UsageContent usageContent:\n                    Console.WriteLine($</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n[📊 Tokens: {usageContent.Details.TotalTokenCount}]</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n                    </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">default</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                    Console.Write(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Unknown content!</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n                    </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n            }\n        }\n    }\n    Console.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n}\nConsole.WriteLine(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">👋 再见！</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nConsole.ReadKey();</span></pre>\n</div>\n<p><span>最终的调试效果如下图所示：</span></p>\n<p><span><img alt=\"agui-gif-demo-001\" src=\"https://img2024.cnblogs.com/blog/381412/202602/381412-20260213222507160-684244773.gif\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></span></p>\n<p><span><span>可以看到，我们很容易就创建了一个用户友好的对话客户端，实时的流式响应也不需要我们写过多代码实现。</span></span></p>\n<h1><span><strong><span>3 小结</span></strong></span></h1>\n<p><span>本文介绍了AG-UI协议的基本概念，为什么会出现AG-UI协议，AG-UI和MCP，A2A的对比，随后介绍了如何在MAF中快速开发一个基于AG-UI的对话应用。</span></p>\n<h1>示例源码</h1>\n<p>GitHub:&nbsp;<a href=\"https://github.com/EdisonTalk/MAFD\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/EdisonTalk/MAFD</a></p>\n<h1>参考资料</h1>\n<p><span><span>圣杰，《<a href=\"https://www.cnblogs.com/sheng-jie/p/19200934\" target=\"_blank\">.NET + AI 智能体开发进阶</a>》（推荐指数：★★★★★）</span></span></p>\n<p><span><span>Microsoft Learn，<span>《<a href=\"https://learn.microsoft.com/en-us/agent-framework/overview/agent-framework-overview?wt.mc_id=MVP_397012\" rel=\"noopener nofollow\" target=\"_blank\">Agent Framework Tutorials</a>》</span></span></span></p>\n<div><span><span>&nbsp;</span></span></div>\n<p style=\"text-align: center;\"><img alt=\"\" src=\"https://images.cnblogs.com/cnblogs_com/edisonchou/1647700/o_200902144330EdisonTalk-Footer.jpg\" /></p>\n<div id=\"Copyright\">\n<p>作者：<span style=\"text-decoration: underline;\">爱迪生</span></p>\n<p>出处：<a href=\"https://edisontalk.cnblogs.com\" target=\"_blank\" title=\"from\">https://edisontalk.cnblogs.com</a></p>\n<p>本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文链接。</p>\n</div>\n\n</div>\n<div id=\"MySignature\">\n    <div align=\"center\"><a href=\"https://weibo.com/u/2068032061?s=6uyXnP\" target=\"_blank\"><img border=\"0\" src=\"http://service.t.sina.com.cn/widget/qmd/2068032061/d643d182/10.png\" /></a></div>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 22:27</span>&nbsp;\n<a href=\"https://www.cnblogs.com/edisontalk\">EdisonZhou</a>&nbsp;\n阅读(<span id=\"post_view_count\">36</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "收藏！RAG核心工具大全: 7大解析工具+向量模型+数据库+检索排序",
      "link": "https://www.cnblogs.com/aifrontiers/p/19613559",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/aifrontiers/p/19613559\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 21:04\">\n    <span>收藏！RAG核心工具大全: 7大解析工具+向量模型+数据库+检索排序</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>原文: <a href=\"https://mp.weixin.qq.com/s/5XAWHqjZspU9xtC_CckV3w\" rel=\"noopener nofollow\" target=\"_blank\">https://mp.weixin.qq.com/s/5XAWHqjZspU9xtC_CckV3w</a></p>\n<p><strong>关注gzh: AI-Frontiers</strong></p>\n<p><strong>RAG往期文章推荐</strong></p>\n<p><a href=\"https://mp.weixin.qq.com/s/VV29xpdOMEkbz4iXmD_szg\" rel=\"noopener nofollow\" target=\"_blank\">RAG效果差？7个指标让你的准确率大幅提升</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/am89yasxAvuYUToEAWNyTA\" rel=\"noopener nofollow\" target=\"_blank\">RAG评测完整指南：指标、测试和最佳实践</a></p>\n<p>检索增强生成（Retrieval-Augmented Generation, RAG）架构已成为LLM落地企业级应用的核心范式。但，在实际部署中，普遍面临「垃圾进，垃圾出」（Garbage In, Garbage Out）的困境。RAG系统的上限往往不由模型（如GPT-4或DeepSeek-V3）决定，而是由上游的数据处理流水线（ETL Pipeline）所限制。</p>\n<p>本文旨在对构建高性能开源RAG系统的关键模块进行详尽的技术拆解，并附带所有核心工具的GitHub、官方文档及模型下载地址。我们将深入剖析七大主流开源解析工具（Unstructured, Marker, PyMuPDF, Docling, MinerU, PaddleOCR, DeepSeek-OCR）的架构原理与性能特征。随后，将沿着数据流向，系统性梳理切块、向量化、检索及排序等下游模块的最新开源技术进展，意在为构建企业级、高精度的RAG系统提供理论依据与实战参考。</p>\n<h1 id=\"核心模块深度解析文档解析与版面分析\">核心模块深度解析：文档解析与版面分析</h1>\n<p>文档解析模块的任务是将非结构化的文档，如PDF/Images/PPT/word/excel，还原为机器可理解的结构化文本，即Markdown/JSON。该过程涉及OCR（Optical Character Recognition，光学字符识别）、OLA（ Layout Analysis，版面分析）、TSR（Table Structure Recognition，表格结构识别）及阅读顺序重构等多个复杂子任务。</p>\n<h2 id=\"unstructured全能型etl中间件架构\">Unstructured：全能型ETL中间件架构</h2>\n<ul>\n<li>\n<p>官方文档: <a href=\"https://docs.unstructured.io/\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.unstructured.io</a></p>\n</li>\n<li>\n<p>Github: <a href=\"https://github.com/Unstructured-IO/unstructured\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/Unstructured-IO/unstructured</a></p>\n</li>\n<li>\n<p><strong>适用场景:</strong> 企业级通用ETL流程，处理多源异构数据（邮件、办公文档、PDF混合）</p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>目前RAG生态中覆盖面最广的通用ETL框架，其设计哲学是提供一个标准化的归一化层，将包括PDF、HTML、Email、PPTX在内的25种以上异构格式转换为统一的JSON Schema</p>\n<p><strong>架构原理与分区策略</strong></p>\n<p>Unstructured的核心是分区机制，该机制并非依赖单一模型，而是根据文档类型动态匹配不同处理管线：</p>\n<ul>\n<li>\n<p><strong>基于规则的快速解析（Fast Strategy）</strong>：针对原生数字PDF，通过pdfminer.six等底层库直接提取文本流，速度快、CPU开销低，但无法处理扫描件，且易丢失复杂双栏阅读顺序；</p>\n</li>\n<li>\n<p><strong>高精度视觉解析（Hi-Res Strategy）</strong>：作为处理复杂文档的核心，借助YOLOX/Detectron2架构的目标检测模型，将页面分割为标题、正文、列表项、表格、图片等语义区块，能精准识别并剔除页眉页脚，避免干扰RAG 上下文；</p>\n</li>\n<li>\n<p><strong>表格处理子系统</strong>：检测到表格区块时触发专属识别模块，开源版本依赖Tesseract OCR或简单HTML转换，商业 API则集成更高级视觉模型恢复复杂行列结构。</p>\n</li>\n</ul>\n<p><strong>局限性与生态位分析</strong></p>\n<p>Unstructured因格式支持广泛成为RAG初学者首选，但开源版与商业版性能差距显著：开源版缺少针对金融报表、学术论文等特定领域微调的OCR模型，无法使用最新VLM（视觉语言模型）功能；hi_res策略处理长文档计算成本高，且依赖Tesseract作为OCR引擎，处理非英语文档时精度受限。不过其标准化的元数据输出（含父子节点关系、页码坐标），为下游混合切块提供了优质数据基础。</p>\n<h2 id=\"marker专注于科学文献的高精度转换管线\">Marker：专注于科学文献的高精度转换管线</h2>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/VikParuchuri/marker\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/VikParuchuri/marker</a></p>\n</li>\n<li>\n<p>适用场景: 学术论文、教科书、技术手册（公式/代码密集型文档）</p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>由Vik Paruchuri开发，专为将PDF转换为高质量Markdown而设计，特别针对数学公式、代码块和学术排版进行了深度优化。</p>\n<p><strong>深度学习流水线机制</strong></p>\n<ul>\n<li>\n<p><strong>Surya版面分析</strong>: 作为高精度OCR与版面分析模型，能精准检测文本行、阅读顺序、列边界，还可通过视觉特征判断文本逻辑流向，解决多栏排版（如双栏论文）的乱序问题。</p>\n</li>\n<li>\n<p><strong>Texify公式引擎</strong>: 针对科学文献的数学公式痛点，可将位图/PDF绘制指令形式的公式转换为标准LaTeX代码，让Marker处理arXiv论文、技术手册时语义完整性远超传统OCR。</p>\n</li>\n<li>\n<p><strong>混合****OCR</strong>：优先提取PDF内嵌文本层保证速度，对公式区域/扫描图片自动切换视觉模型识别，兼顾精度与吞吐量。</p>\n</li>\n</ul>\n<p><strong>增强的后处理与LLM融合</strong></p>\n<p>Marker新增可选--use_llm参数，可在后处理阶段调用轻量级 LLM（如 Gemini Flash、本地模型），解决文档解析 「最后一公里」问题：合并跨页表格、修正 OCR 乱码、从复杂表单提取结构化键值对。测试表明，开启LLM辅助后，Marker表格还原度优于单一模型。</p>\n<h2 id=\"pymupdf-fitz极致速度的底层流式解析\">PyMuPDF (Fitz)：极致速度的底层流式解析</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/pymupdf/PyMuPDF\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/pymupdf/PyMuPDF</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://pymupdf.readthedocs.io/\" rel=\"noopener nofollow\" target=\"_blank\">https://pymupdf.readthedocs.io</a></p>\n</li>\n<li>\n<p>适用场景: 海量原生数字PDF清洗，纯CPU环境，对速度要求极高（毫秒级）的场景。</p>\n</li>\n</ul>\n<p>PyMuPDF是MuPDF引擎的Python绑定，代表了文档解析的另一个极端，极致的工程化效率。与基于AI的视觉模型不同，PyMuPDF直接操作PDF文件的内部对象结构和渲染流。</p>\n<p><strong>文本块与字典模式的技术原理</strong></p>\n<p>PyMuPDF的核心能力在于其对PDF底层指令的解析：</p>\n<ul>\n<li>\n<p><code>get_text(\"blocks\")</code>：通过分析文本在页面上的物理位置坐标，利用启发式算法将相邻的文本行聚类为段落，该方法速度极快（毫秒级），但对排版的理解是浅层的，容易将页眉页脚误判为正文。</p>\n</li>\n<li>\n<p><code>get_text(\"dict\")</code>：更为精细的提取模式，返回一个层级化的JSON对象：<code>Page -&gt; Block -&gt; Line -&gt; Span -&gt; Char</code>。其中，Span（文本跨度）是包含相同字体、大小和颜色的最小文本单元。这一层级信息对于RAG至关重要，开发者可以通过分析Span的字体大小来区分标题和正文，或通过颜色过滤掉水印。</p>\n</li>\n</ul>\n<p><strong>局限性与适用场景</strong></p>\n<p>PyMuPDF不支持扫描件（需外接Tesseract），也无原生语义理解能力（仅能识别文本位置，无法区分摘要、参考文献等语义）。但在处理海量原生数字化合同、财报、电子书时，其纯CPU运行、超高吞吐量的优势，使其成为清洗大规模预训练语料的首选；对于简单RAG应用，通过定制Python脚本过滤页眉页脚后，PyMuPDF的性价比也最高。</p>\n<h2 id=\"doclingibm的企业级多模态文档理解框架\">Docling：IBM的企业级多模态文档理解框架</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/docling-project/docling\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/docling-project/docling</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://docling-project.github.io/docling/\" rel=\"noopener nofollow\" target=\"_blank\">https://docling-project.github.io/docling/</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/ibm-granite/granite-docling-258M\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/ibm-granite/granite-docling-258M</a></p>\n</li>\n<li>\n<p>适用场景: Agentic RAG（需要理解文档结构供Agent调用），高精度表格还原</p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>Docling是IBM Research推出的新一代文档处理库，旨在打通传统文档与生成式AI的壁垒，特别强调对表格结构和文档层级的还原。Docling不仅仅是一个解析器，定义了一种统一的文档对象模型，旨在为Agentic RAG（代理式RAG）提供结构化支撑。</p>\n<p><strong>统一文档表示与VLM集成</strong></p>\n<ul>\n<li>\n<p><strong>DoclingDocument 对象模型</strong>：将PDF、DOCX、HTML等输入转为统一内部表示，保留Section、Group、Body、Furniture层级结构，支持RAG应用「基于结构的切块」，如仅检索指定章节表格，而非盲目文本切片。</p>\n</li>\n<li>\n<p><strong>Granite VLM流水线</strong>：Docling集成IBM自研Granite视觉语言模型，以端到端方式理解页面，除文字转录外，还能解析图表（如将柱状图/折线图转为数据描述），适配金融研报分析需求。</p>\n</li>\n<li>\n<p>表格结构恢复：采用TableFormer等变体算法，高保真重建含合并单元格、无框线的复杂表格，支持导出为 HTML/Markdown 格式。</p>\n</li>\n</ul>\n<p><strong>与Agent生态的深度融合</strong></p>\n<p>Docling设计高度适配Agentic Workflow，提供MCP（Model Context Protocol）服务，可让Claude Desktop等 AI代理直接调用其文档解析能力。在构建复杂RAG Agent时，Docling可作为工具被动态调用，按用户意图提取指定信息。</p>\n<h2 id=\"mineru-pdf-extract-kit面向llm训练的高质量语料清洗\">MinerU (PDF-Extract-Kit)：面向LLM训练的高质量语料清洗</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/opendatalab/MinerU\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/opendatalab/MinerU</a></p>\n</li>\n<li>\n<p>HuggingFace: <a href=\"https://huggingface.co/opendatalab\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/opendatalab</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://opendatalab.github.io/MinerU/\" rel=\"noopener nofollow\" target=\"_blank\">https://opendatalab.github.io/MinerU/</a></p>\n</li>\n<li>\n<p>适用场景: 构建高质量知识库，处理包含复杂数学符号和双栏排版的学术文献</p>\n</li>\n</ul>\n<p>MinerU是OpenDataLab（上海人工智能实验室）为支持InternLM（书生·浦语）大模型预训练而开发的专用工具。其核心目标是从最复杂的科学文献中提取出零噪声、语义连贯的Markdown数据。</p>\n<p><strong>PDF-Extract-Kit与高精度管线</strong></p>\n<p>MinerU的后端引擎被称为PDF-Extract-Kit，这是一个集成了多种SOTA模型的综合工具包：</p>\n<ul>\n<li>\n<p><strong>布局分析</strong>：利用基于YOLO架构改进的模型，精确区分正文、标题、图片、表格、脚注和边注。MinerU特别强调对边注和页眉页脚的剔除，确保生成的Markdown文本流在语义上是连续的，不会被页面元数据打断。</p>\n</li>\n<li>\n<p><strong>公式与符号转换</strong>：针对学术论文中的数学符号，MinerU内置了强大的转换逻辑，能够将复杂的数学表达式还原为LaTeX。这解决了传统OCR将\"$$x^2$$\"识别为\"x2\"的常见错误，对于构建理工科知识库具有决定性意义。</p>\n</li>\n<li>\n<p><strong>乱码自动检测</strong>：在处理PDF时，经常遇到编码崩坏（Garbled Text）的情况。MinerU内置了自动检测机制，一旦发现直接提取的文本乱码率过高，会立即无缝切换至OCR模式，利用视觉信息重构文本，保证了极高的召回率 。</p>\n</li>\n</ul>\n<p><strong>产学研结合的工程优化</strong></p>\n<p>MinerU支持CUDA加速，还适配华为昇腾NPU、Apple Silicon MPS，适配性广泛；其 多模态Markdown输出格式保留图片占位符并关联高分辨率图像，适合构建多模态RAG（MM-RAG）系统。</p>\n<h2 id=\"paddleocr-pp-structure工业级表格与版面还原\">PaddleOCR (PP-Structure)：工业级表格与版面还原</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/PaddlePaddle/PaddleOCR\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/PaddlePaddle/PaddleOCR</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://paddlepaddle.github.io/PaddleOCR/\" rel=\"noopener nofollow\" target=\"_blank\">https://paddlepaddle.github.io/PaddleOCR/</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/PaddlePaddle/PaddleOCR-VL\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/PaddlePaddle/PaddleOCR-VL</a></p>\n</li>\n<li>\n<p>适用场景: 金融报表识别、票据处理、需要私有化部署到边缘设备的场景。</p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>PaddleOCR 是百度飞桨体系下的明星项目，其PP-Structure模块代表了工业界在文档结构化领域的最高水平。与学术界的实验性模型不同，PaddleOCR极其强调模型在服务器、移动端及嵌入式设备上的部署能力 。</p>\n<p><strong>PP-StructureV2/V3核心技术</strong></p>\n<ul>\n<li>\n<p><strong>版面分析（Layout Analysis）</strong>：PP-Structure视版面分析为典型的计算机视觉任务，利用轻量级的主干网络（如MobileNet）配合FPN结构，快速检测页面元素。其优势在于拥有庞大的中文及多语言预训练数据，对中文文档的版面理解能力尤为突出。</p>\n</li>\n<li>\n<p><strong>表格识别（Table Recognition）</strong>：这是PaddleOCR的杀手锏。它采用了SLANet（Structure-Location Alignment Network）等先进算法，将表格识别解耦为结构预测和单元格坐标回归。即使是扭曲、倾斜或光照不均的拍照文档表格，PP-Structure也能还原出精确的Excel或HTML结构。</p>\n</li>\n<li>\n<p><strong>键值对提取（KIE）</strong>：针对发票、表单等半结构化文档，PP-Structure集成了SER（语义实体识别）和RE（关系抽取）模型，能够直接提取「姓名-张三」、「金额-100元」等键值对关系，这对于财务RAG系统极具价值。</p>\n</li>\n</ul>\n<h2 id=\"deepseek-ocr端到端的生成式解析革命\">DeepSeek-OCR：端到端的生成式解析革命</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/deepseek-ai/DeepSeek-OCR</a></p>\n</li>\n<li>\n<p>Github: <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR-2\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/deepseek-ai/DeepSeek-OCR-2</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/deepseek-ai/DeepSeek-OCR</a></p>\n</li>\n<li>\n<p>适用场景: 传统OCR无法处理的极复杂排版（如报纸、杂志、手稿），需要视觉理解的任务</p>\n</li>\n</ul>\n<p>DeepSeek-OCR代表了文档解析技术的最新范式转移，从流水线式向端到端生成式演进。基于DeepSeek-VL2多模态大模型，该系统不再将解析拆解为检测、识别、拼接等步骤，而是直接看图说话。</p>\n<p><strong>视觉因果流（Visual Causal Flow）</strong></p>\n<ul>\n<li>\n<p><strong>视觉Token化</strong>：DeepSeek-VL2引入了动态分辨率策略，将高分辨率的文档图像切片并编码为视觉Token序列。这些Token与文本Token共享同一个Transformer嵌入空间。</p>\n</li>\n<li>\n<p><strong>生成式输出</strong>：模型接收文档图像作为输入，直接自回归地生成Markdown代码。这种方法的革命性在于它具备了推理能力。例如，在解析一个复杂的流程图时，传统OCR只能输出零散的文字，而DeepSeek-OCR可以根据箭头和布局生成描述性的文本：步骤A导致了步骤B。它能理解复选框旁边的文字是标签，从而输出\"Checked: Yes\"。</p>\n</li>\n<li>\n<p><strong>上下文光学压缩</strong>：为了处理长文档，DeepSeek设计了视觉压缩机制，在保留高频细节（如文字笔画）的同时压缩低频背景信息，使得在有限的Context Window内处理多页文档成为可能。</p>\n</li>\n</ul>\n<h2 id=\"文档解析模块对比总结表\">文档解析模块对比总结表</h2>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>特性/工具</td>\n<td>Unstructured</td>\n<td>Marker</td>\n<td>PyMuPDF</td>\n<td>Docling</td>\n<td>MinerU</td>\n<td>PaddleOCR</td>\n<td>DeepSeek-OCR</td>\n</tr>\n<tr>\n<td>核心架构</td>\n<td>混合策略（规则+视觉模型）</td>\n<td>深度学习流水线 (Surya + Texify)</td>\n<td>底层PDF流解析 (C++绑定)</td>\n<td>混合架构 (统一DOM + VLM)</td>\n<td>深度学习流水线 (PDF-Extract-Kit)</td>\n<td>深度学习 (PP-Structure / OCR)</td>\n<td>端到端生成式VLM (MoE架构)</td>\n</tr>\n<tr>\n<td>解析策略</td>\n<td>分区</td>\n<td>版面检测 -&gt; Markdown生成</td>\n<td>文本块/跨度提取</td>\n<td>对象模型重构 -&gt; 导出</td>\n<td>布局分析 -&gt; 多模态MD</td>\n<td>检测+识别+结构化回归</td>\n<td>视觉Token -&gt; 文本生成</td>\n</tr>\n<tr>\n<td>最佳适用场景</td>\n<td>通用ETL，多格式混合处理</td>\n<td>科学论文、数学公式、书籍</td>\n<td>海量原生数字PDF清洗</td>\n<td>企业级文档、Agentic RAG</td>\n<td>学术文献、LLM预训练数据</td>\n<td>表单、票据、复杂表格</td>\n<td>极复杂排版、视觉推理任务</td>\n</tr>\n<tr>\n<td>表格处理能力</td>\n<td>HTML/CSV (开源版能力一般)</td>\n<td>高精度 (支持跨页合并)</td>\n<td>基础 (仅依赖文本位置)</td>\n<td>高精度 (结构恢复强)</td>\n<td>高精度 (转HTML)</td>\n<td>SOTA (擅长扭曲/无框线表)</td>\n<td>生成式 (语义描述/结构化)</td>\n</tr>\n<tr>\n<td>公式/数学支持</td>\n<td>基础 (依赖OCR)</td>\n<td>卓越 (Texify转LaTeX)</td>\n<td>无 (仅原始字符)</td>\n<td>良好 (支持LaTeX)</td>\n<td>卓越 (LaTeX转换)</td>\n<td>良好 (字符识别)</td>\n<td>卓越 (生成LaTeX)</td>\n</tr>\n<tr>\n<td>处理速度</td>\n<td>中等 (取决于策略)</td>\n<td>中等 (建议GPU)</td>\n<td>极快 (纯CPU)</td>\n<td>中等偏慢 (VLM模式)</td>\n<td>中等 (建议GPU)</td>\n<td>快 (提供轻量级模型)</td>\n<td>慢 (大模型推理开销)</td>\n</tr>\n<tr>\n<td>输入模态</td>\n<td>多格式 (PDF, PPT, HTML, Email)</td>\n<td>PDF, EPUB, Images</td>\n<td>PDF, XPS, Ebook</td>\n<td>多格式 (PDF, DOCX, Images)</td>\n<td>PDF, Images</td>\n<td>Images, PDF</td>\n<td>Images, PDF</td>\n</tr>\n<tr>\n<td>输出格式</td>\n<td>JSON (标准化Schema)</td>\n<td>Markdown, JSON, HTML</td>\n<td>Dict, String</td>\n<td>Markdown, JSON, Doctags</td>\n<td>Markdown (多模态), JSON</td>\n<td>JSON, Excel, HTML</td>\n<td>Markdown, JSON</td>\n</tr>\n<tr>\n<td>主要依赖</td>\n<td>Tesseract, Poppler, NLTK</td>\n<td>PyTorch, Surya, Texify</td>\n<td>MuPDF (无外部依赖)</td>\n<td>PyTorch, Granite Model</td>\n<td>PyTorch, YOLO, Ray</td>\n<td>PaddlePaddle</td>\n<td>PyTorch, FlashAttention</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"切块模块从文本流到语义单元\">切块模块：从文本流到语义单元</h1>\n<p>解析后的数据必须经过切块才能进入向量空间。切块策略直接决定了检索的粒度与上下文完整性。</p>\n<h2 id=\"策略演进\">策略演进</h2>\n<ul>\n<li>\n<p><strong>固定窗口切块</strong>：这是最基础的方法，利用LangChain的<code>RecursiveCharacterTextSplitter</code>按字符数（如512 tokens）切分，并设置重叠（Overlap）。优点是稳定，缺点是容易切断语义，例如将一句话截断在两个块中。</p>\n</li>\n<li>\n<p><strong>语义切块</strong>：利用嵌入模型计算句与句之间的余弦相似度。当相似度骤降时，意味着话题发生了转换，系统在此处进行切分。这种方法能保证每个块包含一个完整的语义主题。开源库Chonkie和SemChunk提供了轻量级的实现，支持在不依赖重型框架的情况下快速进行语义切分。</p>\n</li>\n<li>\n<p><strong>层级切块</strong>：利用Docling或MinerU输出的结构化信息（Header, Section），先按章节切大块，再在大块内切小块。检索时匹配小块，但返回大块（Parent Document Retrieval），兼顾了检索的精准度与生成的上下文丰富度。</p>\n</li>\n</ul>\n<h2 id=\"智能agent切块\">智能Agent切块</h2>\n<p>最新的趋势是Agentic Chunking，即利用LLM将文本重写为独立的「命题」。例如，将「张三，百度的工程师，去了北京」拆解为「张三是百度的工程师」和「张三去了北京」两个独立事实。虽然成本高，但能显著提升复杂问题的检索召回率。</p>\n<h2 id=\"资源链接\">资源链接</h2>\n<p><strong>Chonkie</strong></p>\n<ul>\n<li>\n<p>一个轻量级、极速的RAG切块库，专注于语义切分。</p>\n<ul>\n<li>\n<p><strong>GitHub</strong>: <a href=\"https://github.com/chonkie-inc/chonkie\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/chonkie-inc/chonkie</a></p>\n</li>\n<li>\n<p><strong>Docs</strong>: <a href=\"https://docs.chonkie.ai/\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.chonkie.ai/</a></p>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>Semantic</strong> <strong>Router</strong></p>\n<ul>\n<li>\n<p>虽然主要用于路由，但也包含强大的语义分析层，可用于高级切块。</p>\n<ul>\n<li>GitHub: <a href=\"https://github.com/aurelio-labs/semantic-router\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/aurelio-labs/semantic-router</a></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"向量化模块语义空间的构建\">向量化模块：语义空间的构建</h1>\n<p>向量化是将文本投影到高维语义空间的过程。在2025-2026年，MTEB (Massive Text Embedding Benchmark) 排行榜成为了衡量模型性能的黄金标准。开源模型在这一领域已经全面追平甚至超越了闭源商业模型（如OpenAI text-embedding-3）</p>\n<h2 id=\"bge-baai-general-embedding-系列\">BGE (BAAI General Embedding) 系列</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/FlagOpen/FlagEmbedding\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/FlagOpen/FlagEmbedding</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/BAAI/bge-m3\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/BAAI/bge-m3</a></p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://bge-model.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://bge-model.com/</a></p>\n</li>\n<li>\n<p>参考资料：<a href=\"https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models\" rel=\"noopener nofollow\" target=\"_blank\">https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models</a></p>\n</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<p>由北京智源人工智能研究院开发。BGE-M3是目前的SOTA模型，支持多语言（100+）、多功能（稠密、稀疏、多向量检索）及长文本（8192 tokens）。其独特的混合检索能力使其成为构建多路召回系统的首选。该模型集成了三种检索范式：</p>\n<ul>\n<li>\n<p><strong>密集检索（Dense Retrieval）</strong>：基于语义向量。</p>\n</li>\n<li>\n<p><strong>稀疏检索（****Sparse</strong> <strong>Retrieval）</strong>：类似于BM25的词汇匹配，用于弥补密集检索在专有名词匹配上的不足。</p>\n</li>\n<li>\n<p><strong>多向量检索（Multi-Vector/ColBERT）</strong>：保留每个Token的向量进行细粒度交互。 这种“全能型”设计使得 BGE-M3 能够适应多语言（100+）、长文本（8192 Token）的复杂场景。</p>\n</li>\n</ul>\n<h2 id=\"qwen-embedding\">Qwen-Embedding</h2>\n<ul>\n<li>Huggingface: <a href=\"https://huggingface.co/collections/Qwen/qwen3-embedding\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/collections/Qwen/qwen3-embedding</a></li>\n</ul>\n<p>阿里巴巴推出的Qwen3-Embedding系列模型在MTEB榜单上表现优异，特别是在多语言任务和长上下文任务中。</p>\n<ul>\n<li><strong>弹性维度（Matryoshka Representation Learning, MRL）</strong>：Qwen模型支持弹性输出维度。用户可以根据存储预算，灵活选择使用前1024维甚至512维向量，而性能损失微乎其微。这使得RAG系统可以在速度和精度之间进行动态权衡。</li>\n</ul>\n<h2 id=\"e5系列\">E5系列</h2>\n<ul>\n<li>Hugging Face: <a href=\"https://huggingface.co/intfloat/multilingual-e5-large\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/intfloat/multilingual-e5-large</a></li>\n</ul>\n<p>E5系列 (EmbEddings from bidirEcTional Encoder rEpresentations)模型，如multilingual-e5-large，通过指令微调优化了非对称搜索任务（短Query找长Passage）。使用时需添加前缀<code>query:</code>和<code>passage:</code>，在MTEB榜单上长期霸榜。</p>\n<h2 id=\"jina-embeddings-v3\"><strong>Jina Embeddings v3</strong></h2>\n<ul>\n<li>Hugging Face: <a href=\"https://huggingface.co/jinaai/jina-embeddings-v3\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/jinaai/jina-embeddings-v3</a></li>\n</ul>\n<p>专为长文档和代码设计，利用ALiBi位置编码外推上下文长度，非常适合技术文档库的RAG。</p>\n<h1 id=\"检索模块retrieval向量数据库选型\">检索模块（Retrieval）：向量数据库选型</h1>\n<p>向量数据库是RAG系统的长时记忆。当前市场已经形成了以Milvus、Qdrant和Weaviate为代表的开源三巨头，以及pgvector这种依托于PostgreSQL的轻量化方案。</p>\n<h2 id=\"milvus云原生时代的巨舰\">Milvus：云原生时代的巨舰</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/milvus-io/milvus\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/milvus-io/milvus</a></p>\n</li>\n<li>\n<p>文档: <a href=\"https://milvus.io/docs\" rel=\"noopener nofollow\" target=\"_blank\">https://milvus.io/docs</a></p>\n</li>\n</ul>\n<p>由Zilliz开发的Milvus采用了存储与计算分离的云原生架构，专为处理十亿级（Billion-scale）向量数据而设计。</p>\n<ul>\n<li><strong>核心特性</strong>：支持分布式部署、Kubernetes 编排、多租户隔离以及多种索引类型（HNSW, DiskANN）。它适合需要极高吞吐量和大规模数据分片的企业级应用。</li>\n</ul>\n<h2 id=\"qdrant高性能与灵活性的平衡\">Qdrant：高性能与灵活性的平衡</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/qdrant/qdrant\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/qdrant/qdrant</a></p>\n</li>\n<li>\n<p>文档: <a href=\"https://qdrant.tech/documentation/\" rel=\"noopener nofollow\" target=\"_blank\">https://qdrant.tech/documentation/</a></p>\n</li>\n</ul>\n<p>Qdrant基于Rust语言开发，以高性能和低延迟著称。</p>\n<ul>\n<li><strong>核心特性</strong>：将向量索引与Payload（元数据）索引紧密结合，支持强大的过滤查询（Filtering）。与传统的先检索后过滤不同，Qdrant能够在索引遍历过程中应用过滤条件，极大提升了混合查询的效率。其推荐系统和去重功能也非常完善。</li>\n</ul>\n<h2 id=\"weaviateai原生的模块化数据库\">Weaviate：AI原生的模块化数据库</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/weaviate/weaviate\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/weaviate/weaviate</a></p>\n</li>\n<li>\n<p>文档: <a href=\"https://weaviate.io/developers/weaviate\" rel=\"noopener nofollow\" target=\"_blank\">https://weaviate.io/developers/weaviate</a></p>\n</li>\n</ul>\n<p>Weaviate不仅仅是一个数据库，更像是一个AI中间件平台。</p>\n<ul>\n<li><strong>核心特性</strong>：内置了大量的模块，可以直接在数据库层面集成OpenAI、HuggingFace等模型的向量化能力。用户只需存入文本，Weaviate会自动调用模型生成向量。其GraphQL接口也为复杂的数据关联查询提供了便利。</li>\n</ul>\n<h2 id=\"pgvectorpostgresql用户的务实之选\">pgvector：PostgreSQL用户的务实之选</h2>\n<ul>\n<li>GitHub: <a href=\"https://github.com/pgvector/pgvector\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/pgvector/pgvector</a></li>\n</ul>\n<p>pgvector是PostgreSQL的一个开源扩展，为现有的关系型数据库添加了向量存储和相似度搜索功能。</p>\n<ul>\n<li><strong>核心特性</strong>：允许向量数据与业务数据（如用户信息、订单记录）存储在同一张表中，天然支持ACID事务和复杂的SQL Join操作。对于数据量在千万级别以下，且已有Postgres基础设施的团队，这是成本最低的方案。</li>\n</ul>\n<h1 id=\"排序模块精度的最后一道防线\">排序模块：精度的最后一道防线</h1>\n<p>检索模块通常返回 Top-K（如50-100）个候选片段，但这些片段是基于粗略的向量相似度获取的。为了让LLM获得最精准的上下文，需要引入重排序模块。重排序模型通常采用交叉编码器架构，它将查询和文档同时输入模型，计算它们之间的深层交互，从而给出极高精度的相关性评分。</p>\n<h2 id=\"bge-reranker开源重排序的标杆\">BGE-Reranker：开源重排序的标杆</h2>\n<ul>\n<li>\n<p>GitHub: <a href=\"https://github.com/FlagOpen/FlagEmbedding\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/FlagOpen/FlagEmbedding</a></p>\n</li>\n<li>\n<p>Hugging Face: <a href=\"https://huggingface.co/BAAI/bge-reranker-v2-m3\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/BAAI/bge-reranker-v2-m3</a></p>\n</li>\n</ul>\n<p>BGE-Reranker系列（v2, v2.5）是目前开源社区中最强大的重排序模型之一。</p>\n<ul>\n<li>\n<p><strong>多语言能力</strong>：支持中英等多语言混合排序。</p>\n</li>\n<li>\n<p><strong>轻量化蒸馏</strong>：最新的BGE-Reranker-v2.5-Gemma2-Lightweight通过层级蒸馏技术，在保持LLM级排序能力的同时，大幅降低了推理延迟，使其能够部署在实时性要求较高的生产环境中。</p>\n</li>\n</ul>\n<h2 id=\"rankgpt利用llm进行列表排序\">RankGPT：利用LLM进行列表排序</h2>\n<ul>\n<li><strong>GitHub</strong>: <a href=\"https://github.com/sunnweiwei/RankGPT\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/sunnweiwei/RankGPT</a></li>\n</ul>\n<p>RankGPT探索了一种不同的路径：利用生成式 LLM（如 GPT-4, Qwen）的推理能力进行排序。</p>\n<ul>\n<li><strong>列表式（Listwise）排序</strong>：不同于Cross-Encoder 对每对 (Query, Doc) 单独打分，RankGPT将Query和一组文档（如10个）同时输入LLM，并提示模型「请按相关性对这些文档进行排序」。这种方法考虑了文档之间的相对关系，往往能获得比Pairwise方法更高的MAP指标，但延迟和成本也显著增加。</li>\n</ul>\n<h2 id=\"flashrank极致轻量化的cpu方案\">FlashRank：极致轻量化的CPU方案</h2>\n<ul>\n<li>GitHub: <a href=\"https://github.com/PrithivirajDamodaran/FlashRank\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/PrithivirajDamodaran/FlashRank</a></li>\n</ul>\n<p><strong>FlashRank</strong> 是一个专为无 GPU 环境设计的 Python 库。</p>\n<ul>\n<li><strong>核心特性</strong>：基于量化后的TinyBERT等微型模型，能够在普通CPU上实现毫秒级的重排序。这对于部署在AWS Lambda等Serverless环境或边缘设备上的RAG应用至关重要 。</li>\n</ul>\n<h1 id=\"选型建议\">选型建议</h1>\n<p>构建开源RAG系统已不再是简单的模型堆砌，而是对数据处理全链路的精细化工程。</p>\n<ul>\n<li>\n<p><strong>解析层</strong>：对于通用场景，推荐使用Unstructured进行快速原型开发；对于科学文献和复杂报表，Marker和MinerU是目前的最佳实践；若需处理海量原生PDF，PyMuPDF不可或缺；而对于追求极致结构化和Agent交互的企业级应用，Docling展现了巨大的潜力。</p>\n</li>\n<li>\n<p><strong>数据流层</strong>：应摒弃固定的字符切块，转向语义切块或层级切块。</p>\n</li>\n<li>\n<p><strong>模型层</strong>：BGE-M3（Embedding）配合BGE-Reranker-v2-m3（Reranking）构成了目前最强的开源语义理解组合，配合Milvus或Qdrant可构建出性能媲美商业闭源方案的RAG系统。</p>\n</li>\n</ul>\n<p>随着DeepSeek-OCR等生成式解析技术的成熟，未来的文档解析将逐渐具备推理能力，进一步模糊感知与认知的边界，为RAG系统注入更深层的智能。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 21:04</span>&nbsp;\n<a href=\"https://www.cnblogs.com/aifrontiers\">AI-Frontiers</a>&nbsp;\n阅读(<span id=\"post_view_count\">114</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Volcano v1.14 重磅发布！迈向 AI 统一调度新纪元",
      "link": "https://www.cnblogs.com/huaweiyun/p/19612682",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/huaweiyun/p/19612682\" id=\"cb_post_title_url\" title=\"发布于 2026-02-13 16:35\">\n    <span>Volcano v1.14 重磅发布！迈向 AI 统一调度新纪元</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>本文分享自华为云社区《<a href=\"https://bbs.huaweicloud.com/blogs/474065?utm_source=zhihu&amp;utm_medium=bbs-ex&amp;utm_campaign=other&amp;utm_content=content\" rel=\"noopener nofollow\" target=\"_blank\">Volcano v1.14 重磅发布！迈向 AI 统一调度新纪元</a>》</p>\n<p>北京时间2026年1月，<strong>Volcano</strong>[1] v1.14 正式发布。随着 AI 业务形态从单一的离线训练向在线推理、Agent 智能体等多元化场景延伸，调度系统面临着前所未有的挑战。<strong>v1.14</strong>[2] 通过架构级的创新，在保持大规模批量计算优势的同时，补齐了对延迟敏感型业务的调度短板，向着 <strong>“AI训推、RL、Agent全场景统一调度平台”</strong> 的目标迈出了坚实一步。</p>\n<p><img alt=\"download\" class=\"lazyload\" /></p>\n<h1><span class=\"prefix\"><span class=\"content\"><strong>版本亮点</strong></span></span></h1>\n<p>v1.14.0 版本带来以下重磅更新：</p>\n<p><strong>统一调度平台架构</strong></p>\n<ul>\n<li>多调度器架构升级：动态节点分片机制 (Alpha)</li>\n<li>AI Agent 工作负载极速调度能力 (Alpha)</li>\n</ul>\n<p><strong>网络拓扑感知调度增强</strong></p>\n<ul>\n<li>HyperNode 级 Binpack 策略</li>\n<li>SubGroup 级精细化拓扑感知</li>\n<li>PodGroup 与 SubGroup 多层级 Gang Scheduling</li>\n<li>Volcano Job 分区支持</li>\n</ul>\n<p><strong>混部能力增强</strong></p>\n<ul>\n<li>全面支持通用操作系统（Ubuntu、CentOS 等）</li>\n<li>Cgroup V2 全面适配</li>\n<li>CPU 动态压制</li>\n<li>基于 Cgroup V2 的内存 QoS</li>\n<li>支持 CPU Burst</li>\n<li>支持 systemd driver 自动检测</li>\n</ul>\n<p><strong>异构硬件支持</strong></p>\n<ul>\n<li>昇腾 vNPU 调度（支持 MindCluster 和 HAMi 模式）</li>\n</ul>\n<p><strong>Volcano Global 增强</strong></p>\n<ul>\n<li>HyperJob 多集群作业自动拆分</li>\n<li>数据感知多集群调度</li>\n</ul>\n<p><strong>Volcano Dashboard 增强</strong></p>\n<ul>\n<li>PodGroup 全景可视化</li>\n<li>Job / Queue 全生命周期管理</li>\n</ul>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>多调度器架构升级：动态节点分片机制 (Alpha)</strong></span></span></h2>\n<p>随着 Volcano 承载的工作负载类型日益丰富、规模持续扩大，单一调度器架构逐渐显露瓶颈。批量训练、AI Agent、微服务等不同类型的工作负载，对调度时延、资源利用模式的诉求各不相同。单调度器难以兼顾，而静态资源划分又会造成利用率低下。<br />\n新引入的 Sharding Controller 构建了一套可扩展的多调度器架构，能够根据集群实时状态，为每个调度器动态计算候选节点池。与传统的静态分区不同，Sharding Controller 采用动态计算的方式划分资源，而非强制硬隔离。这种灵活机制让 Volcano 真正成为\"一个平台调度所有负载\"的统一调度平台，同时保持高吞吐、低时延。<br />\n</p>\n<p><strong>核心能力</strong>：</p>\n<ul>\n<li><strong>动态分片策略</strong>：支持多种策略来计算动态候选节点池。当前版本率先支持基于 CPU 利用率的分片策略，并采用了可扩展的架构设计，以便未来轻松集成更多分片算法。</li>\n<li><strong>节点池化管理</strong>：引入 NodeShard CRD，为特定调度器管理专属的动态候选节点池。</li>\n<li><strong>支持大规模集群</strong>：通过在多个调度器之间灵活分配负载，天然适配大规模集群场景。</li>\n<li><strong>多调度器协同</strong>：支持多种调度器组合的无缝协作。无论是部署多个 Batch Scheduler 进行负载分担，还是混合部署 Agent Scheduler 与 Batch Scheduler 以应对不同业务需求，都能灵活适配。</li>\n\n</ul>\n<p>配置示例：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\"># Sharding Controller 启动参数\n--scheduler-configs=\"volcano:volcano:0.0:0.6:false:2:100,agent-scheduler:agent:0.7:1.0:true:2:100\"\n--shard-sync-period=60s\n--enable-node-event-trigger=true\n\n# 参数格式: name:type:min_util:max_util:prefer_warmup:min_nodes:max_nodes</pre>\n</div>\n<p>相关 PR：https://github.com/volcano-sh/volcano/pull/4777<br />\n设计文档：<strong>Sharding Controller Design</strong>[3]<br />\n感谢社区开发者：@ssfffss, @Haoran, @qi-min</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>AI Agent 工作负载极速调度 (Alpha)</strong></span></span></h2>\n<p>AI Agent 类业务对时延极度敏感，任务创建频繁且生命周期短，对调度器的响应速度和吞吐量提出了严苛要求。原生 Volcano Batch Scheduler 专为批量计算设计，按固定周期处理 Pod，难以满足 Agent 场景的毫秒级响应需求。<br />\n为打造兼容批量计算与延迟敏感型业务的统一调度平台，v1.14 引入了专用的 <strong>Agent Scheduler</strong>。它通过 Sharding Controller 与批量调度器协同工作，各司其职又无缝配合，真正实现\"一个平台、多种负载\"。<br />\n</p>\n<p><strong>核心能力</strong>：</p>\n<ul>\n<li><strong>极速调度通道</strong>：专为延迟敏感型负载（如 AI Agent）打造的独立调度器，提供极致响应速度。</li>\n<li><strong>多 Worker 并行处理</strong>：采用多 Worker 并发消费调度队列的架构，大幅提升调度吞吐量。</li>\n<li><strong>乐观并发控制</strong>：引入 Conflict-Aware Binder 机制，在实际绑定前预先解决冲突，减少无效操作。</li>\n<li><strong>增强型调度队列</strong>：优化队列机制，支持紧急任务重试，确保关键任务不阻塞。</li>\n<li><strong>统一平台融合</strong>：通过 Sharding Controller 与批量调度器无缝协作，共享集群资源。</li>\n\n</ul>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4804,<br />\nhttps://github.com/volcano-sh/volcano/pull/4801,<br />\nhttps://github.com/volcano-sh/volcano/pull/4805<br />\n设计文档：<strong>Agent Scheduler Design</strong>[4]<br />\n感谢社区开发者：@qi-min, @JesseStutler, @handan-yxh</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>网络拓扑感知调度增强</strong></span></span></h2>\n<p>Volcano v1.14.0 对网络拓扑感知调度进行了进一步增强，满足分布式工作负载（包括 LLM 训练、推理、HPC 和其他网络密集型应用）日益增长的需求。<br />\n</p>\n<p><strong>核心增强</strong>：</p>\n<ul>\n<li><strong>SubGroup 级精细化拓扑感知</strong>：支持在 SubGroup / Partition 粒度设置网络拓扑约束，调度颗粒度更精细。</li>\n<li><strong>灵活的网络层级约束</strong>：新增 highestTierName，支持按名称指定允许跨越的最高网络层级。</li>\n<li><strong>多层级 Gang Scheduling</strong>：同时支持 PodGroup 级别和 SubGroup 级别的 Gang Scheduling，确保分布式任务的整体性。</li>\n<li><strong>Volcano Job 分区</strong>：支持将 Job 拆分为多个分区（Partition），便于管理 TP/PP/DP 等并行策略，并优化网络亲和性。</li>\n<li><strong>HyperNode 级 Binpacking</strong>：在 HyperNode（如交换机、机架）层级进行资源装箱，减少网络碎片，提升通信效率。</li>\n\n</ul>\n<p>配置示例 - Volcano Job：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">apiVersion: batch.volcano.sh/v1alpha1\nkind: Job\nmetadata:\nname: llm-training-job\nspec:\nnetworkTopology:\nmode: hard\nhighestTierAllowed: 2 # 整个 Job 最多跨越 Tier 2 HyperNode\ntasks:\n- name: trainer\nreplicas: 8\npartitionPolicy:\ntotalPartitions: 2 # 拆分为 2 个分区\npartitionSize: 4 # 每个分区 4 个 Pod\nminPartitions: 2 # 至少需要 2 个分区\nnetworkTopology:\nmode: hard\nhighestTierAllowed: 1 # 单个分区必须在 Tier 1 内\ntemplate:\nspec:\ncontainers:\n- name: trainer\nimage: training-image:v1\nresources:\nrequests:\nnvidia.com/gpu: 8</pre>\n</div>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4721,<br />\nhttps://github.com/volcano-sh/volcano/pull/4810,<br />\nhttps://github.com/volcano-sh/volcano/pull/4795,<br />\nhttps://github.com/volcano-sh/volcano/pull/4785,<br />\nhttps://github.com/volcano-sh/volcano/pull/4889</p>\n<p>设计文档：<strong>Network Topology Aware Scheduling</strong>[5]<br />\n感谢社区开发者：@ouyangshengjia, @3sunny, @zhaoqi, @wangyang0616, @MondayCha, @Tau721</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>混部能力全面升级：支持通用操作系统</strong></span></span></h2>\n<p>本次发布对 Volcano 的混部能力进行了全面改进，其中一个重要里程碑是：Volcano 混部能力<strong>正式支持通用操作系统</strong>（Ubuntu、CentOS 等），不再局限于 OpenEuler。这意味着更多用户可以使用 Volcano Agent 实现在离线混部，提升集群整体资源利用率。</p>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>CPU 动态压制 (CPU Suppression)</strong></span></span></h3>\n<p>在线业务流量通常具有潮汐特性。为了在保障在线业务 SLA 的同时最大化资源利用，离线 Pod 的 CPU 配额需要随在线用量动态调整：在线用量高时压制离线配额，用量回落时逐步恢复，实现自适应的资源分配。</p>\n<p>核心设计：</p>\n<ul>\n<li>根据节点可分配 CPU 和实时用量，动态调整 BestEffort root cgroup 的 CPU 配额。</li>\n<li>采用\"监控-事件-处理\"架构，并实施保守更新策略，有效抑制抖动。</li>\n\n</ul>\n<p>配置示例：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">cpuThrottlingConfig:\n\nenable: true\n\ncpuThrottlingThreshold: 80 # BE 配额上限为可分配 CPU 的 80%\n\ncpuJitterLimitPercent: 1 # 配额变化超过 1% 才触发更新\ncpuRecoverLimitPercent: 10 # 单次恢复上限 10%</pre>\n</div>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>内存 QoS (Cgroup V2)</strong></span></span></h3>\n<p>基于 Cgroup V2 实现混部场景的内存隔离。新增 ColocationConfiguration CRD，支持为指定工作负载配置内存 QoS 策略。</p>\n<p>核心能力：</p>\n<ul>\n<li><strong>New API</strong>：通过标签选择器定义内存隔离策略的 ColocationConfiguration CRD</li>\n<li><strong>动态计算</strong>：\n<ul>\n<li>memory.high = pod.limits.memory * highRatio %</li>\n<li>memory.low = pod.requests.memory * lowRatio %</li>\n<li>memory.min = pod.requests.memory * minRatio %</li>\n</ul>\n</li>\n<li><strong>统一接口</strong>：可靠检测和支持 Cgroup V2 环境</li>\n</ul>\n<p>使用示例：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">apiVersion: config.volcano.sh/v1alpha1\n\nkind: ColocationConfiguration\n\nmetadata:\n\nname: colo-config1\n\nspec:\n\nselector:\n\n*matchLabels:*\n\n  *app: offline-test*\nmemoryQos:\n\n*highRatio: 100  \\# memory.high \\= memory.limits \\* 100%*\n\n*lowRatio: 50    \\# memory.low \\= memory.requests \\* 50%*  \n*minRatio: 0     \\# memory.min \\= memory.requests \\* 0%*</pre>\n</div>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>CPU Burst 与 Cgroup V2 全面支持</strong></span></span></h3>\n<p>CPU Burst 能力已扩展至通用操作系统。同时，Volcano Agent 现已全面适配 Cgroup V2 环境，支持自动检测 Cgroup 版本及驱动类型（如 systemd driver），无需人工干预即可在现代 Linux 发行版上无缝运行。</p>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4632,</p>\n<p>https://github.com/volcano-sh/volcano/pull/4945,</p>\n<p>https://github.com/volcano-sh/volcano/pull/4913,</p>\n<p>https://github.com/volcano-sh/volcano/pull/4984</p>\n<p>设计文档：<strong>CPU Throttle Design</strong>[6], <strong>Agent Cgroup V2 Adaptation</strong>[7]</p>\n<p>感谢社区开发者：@Haibara-Ai97, @JesseStutler, @ouyangshengjia</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>昇腾 vNPU 调度</strong></span></span></h2>\n<p>v1.14 原生集成了昇腾 vNPU（虚拟 NPU）调度能力，实现昇腾 AI 处理器在多个工作负载之间的高效算力复用。提供两种模式，灵活适配不同部署场景。</p>\n<p><strong>支持模式</strong>：</p>\n<p><strong>1. MindCluster 模式</strong></p>\n<ul>\n<li>集成自 Ascend MindCluster 调度插件：https://gitcode.com/Ascend/mind-cluster\n<ul>\n<li>支持昇腾 310P 系列的动态虚拟化</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. HAMi 模式</strong></p>\n<ul>\n<li>由 HAMi 社区开发\n<ul>\n<li>同时支持昇腾 310 和 910 系列</li>\n<li>支持异构昇腾集群（910A、910B2、910B3、310P）</li>\n</ul>\n</li>\n</ul>\n<p>调度器配置：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\"># MindCluster 模式\n\n- name: deviceshare\n\narguments:\n\n*deviceshare.AscendMindClusterVNPUEnable: true*\n# HAMi 模式\n\n- name: deviceshare\n\narguments:\n\n*deviceshare.AscendHAMiVNPUEnable: true*  \n*deviceshare.SchedulePolicy: binpack  \\# 或 spread*</pre>\n</div>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4656,<br />\nhttps://github.com/volcano-sh/volcano/pull/4717<br />\n使用文档：<strong>How to Use vNPU</strong>[8]<br />\n感谢社区开发者：@JackyTYang, @DSFans2014</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>Volcano Global 增强</strong></span></span></h2>\n<p>Volcano Global v0.3.0 引入了两个重要功能，通过基于计算资源和数据局部性的智能调度，显著扩展了 Volcano Global 对 AI/ML 和大数据工作负载的能力。</p>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>HyperJob：多集群作业自动拆分</strong></span></span></h3>\n<p>随着 AI 训练工作负载规模和复杂性的增长，企业越来越面临跨多个异构集群管理大规模训练作业的挑战。HyperJob 是构建在 Volcano Job 之上的更高级抽象。它组合多个 Volcano Job 模板，将训练能力扩展到单集群边界之外，同时保留每个集群内现有 Volcano Job 的全部能力。</p>\n<p><strong>核心能力</strong>：</p>\n<ul>\n<li><strong>Karmada 深度集成</strong>：自动生成 PropagationPolicy，精准配置集群亲和性与副本调度。</li>\n<li><strong>状态统一聚合</strong>：将各集群子任务状态汇总为统一的 HyperJob 状态，全局可观测。</li>\n<li><strong>自动资源生成</strong>：根据 ReplicatedJob 定义自动创建 VCJob 和 PropagationPolicy。</li>\n\n</ul>\n<p>HyperJob 资源示例（跨 2 个集群拆分大规模训练作业，共 256 个 GPU）：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">apiVersion: training.volcano.sh/v1alpha1\n\nkind: HyperJob\n\nmetadata:\n\nname: llm-training\n\nspec:\n\nreplicatedJobs:\n\n- name: trainer\n\nreplicas: 2\n\ntemplateSpec:\n\n  tasks:\n\n  \\- name: worker\n\n    replicas: 128\n\n    template:\n\n      spec:\n\n        containers:\n\n        \\- name: trainer\n\n          image: training-image:v1\n\n          resources:\n\n            requests:  \n              nvidia.com/gpu: 1</pre>\n</div>\n<h3><span class=\"prefix\"><span class=\"content\"><strong>数据感知调度</strong></span></span></h3>\n<p>在 AI 训练和大数据分析等高性能计算场景中，任务执行不仅依赖计算资源，还严重依赖数据资源。在多集群环境中，调度器可能会将任务分派到与数据源物理距离较远的集群，导致跨地域带宽成本过高和 I/O 延迟过高。</p>\n<p>数据感知调度框架引入 <strong>DataDependencyController</strong>，打通了逻辑数据需求与物理集群分布的壁垒。通过外部插件（如 Amoro）实时获取数据分布信息，自动将调度约束注入 Karmada，实现\"计算随数据而动\"的全自动工作流。</p>\n<p><strong>核心能力</strong>：</p>\n<ul>\n<li><strong>插件化架构</strong>：可扩展支持 Amoro、Hive、S3 等多种数据系统。</li>\n<li><strong>声明式 API</strong>：DataSourceClaim / DataSource CRD，采用\"声明-缓存\"模式。</li>\n<li><strong>自动亲和注入</strong>：将数据局部性转化为 ClusterAffinity 约束，注入 ResourceBinding。</li>\n</ul>\n<p>详见： <strong>Volcano Global v0.3.0 Release Notes</strong>[9]<br />\n感谢社区开发者：@JesseStutler, @fx147, @Monokaix, @zhoujinyu, @anryko, @tanberBro</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>Volcano Dashboard v0.2.0</strong></span></span></h2>\n<p>Volcano Dashboard v0.2.0 对资源管理能力进行了重大增强，使得通过 Web 界面管理 Volcano 资源更加便捷。<br />\n<strong>核心增强</strong>：</p>\n<ul>\n<li><strong>PodGroup 全景可视化</strong>：跨命名空间查看、搜索、过滤 PodGroup，支持 YAML 语法高亮。</li>\n<li><strong>Job 生命周期管理</strong>：直接在界面创建、删除 Volcano Job，操作更便捷。</li>\n<li><strong>Queue 管理增强</strong>：在线编辑 Queue 配额、权重，支持 YAML 直接修改。</li>\n<li><strong>安全加固</strong>：默认配置 SELinux、Seccomp、非 root 运行及禁止提权，保障生产安全。</li>\n\n</ul>\n<p>详见： <strong>Volcano Dashboard v0.2.0 Release Notes</strong>[10]<br />\n感谢社区开发者：@vzhou-p, @Shrutim1505, @JesseStutler, @karanBRAVO, @Sayan4444, @jayesh9747, @Alivestars24, @kuldeep, @Monokaix</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>调度器稳定性与性能提升</strong></span></span></h2>\n<p><strong>Reclaim 重构与增强</strong><br />\n对 Reclaim Action 进行了全面重构，并修复了 Capacity Plugin 中的关键逻辑问题，大幅提升多租户集群资源回收的准确性、稳定性和性能。<br />\n主要改进：</p>\n<ul>\n<li><strong>Reclaim Action 重构</strong>：重构了 reclaim 工作流，提高代码可读性、可维护性和测试覆盖率。</li>\n<li><strong>增强的 Capacity Plugin 逻辑</strong>：修复了 reclaimableFn 和 preemptiveFn，正确处理标量资源并防止错误的抢占决策。</li>\n<li><strong>稳定性提升</strong>：解决了资源计算中的边缘情况，防止调度死循环和误驱逐。</li>\n\n</ul>\n<p>相关 PR：</p>\n<p>https://github.com/volcano-sh/volcano/pull/4794,<br />\nhttps://github.com/volcano-sh/volcano/pull/4659,<br />\nhttps://github.com/volcano-sh/volcano/pull/4919<br />\n感谢社区开发者：@guoqinwill, @hajnalmt</p>\n<h2><span class=\"prefix\"><span class=\"content\"><strong>支持 Kubernetes 1.34</strong></span></span></h2>\n<p>Volcano 版本紧跟 Kubernetes 社区。v1.14 已全面支持最新的 Kubernetes v1.34，并通过完整的单元测试和 E2E 测试保障功能与稳定性。<br />\n相关 PR：https://github.com/volcano-sh/volcano/pull/4704<br />\n感谢社区开发者：@suyiiyii, @tunedev</p>\n<h1><span class=\"prefix\"><span class=\"content\"><strong>总结：Volcano v1.14.0 — AI 时代的统一调度平台</strong></span></span></h1>\n<p>Volcano v1.14 是一个里程碑式的版本。通过引入多调度器架构和 Agent Scheduler，Volcano 正式迈入统一调度平台新阶段，既能高效处理批量 AI 训练，又能满足 AI Agent 的极致时延要求。网络拓扑感知增强、通用操作系统混部支持、昇腾 vNPU 集成，进一步夯实了 Volcano 在 AI 基础设施领域的领先地位。<br />\n同时，Volcano Global v0.3.0 通过 HyperJob 实现大规模分布式训练和数据感知调度，扩展了多集群能力。Volcano Dashboard v0.2.0 通过全面的资源管理功能显著改善了用户体验。</p>\n<p><strong>立即体验 Volcano v1.14，共启 AI 时代统一调度新篇章！</strong></p>\n<p><strong>v1.14.0 发布地址：</strong>https://github.com/volcano-sh/volcano/releases/tag/v1.14.0<br />\n<strong>Volcano Global v0.3.0 发布地址：</strong>https://github.com/volcano-sh/volcano-global/releases/tag/v0.3.0<br />\n<strong>Volcano Dashboard v0.2.0 发布地址：</strong>https://github.com/volcano-sh/dashboard/releases/tag/v0.2.0</p>\n<h1><span class=\"prefix\"><span class=\"content\"><strong>致   谢</strong></span></span></h1>\n<p>Volcano v1.14 生态版本（含 Volcano Global v0.3.0、Dashboard v0.2.0）共有 55 位社区贡献者参与。衷心感谢每一位贡献者：</p>\n<table>\n<tbody>\n<tr><th>@3sunny</th><th>@3th4novo</th><th>@acsoto</th>\n</tr>\n<tr>\n<td>@Alivestars24</td>\n<td>@Aman-Cool</td>\n<td>@anryko</td>\n\n\n</tr>\n<tr>\n<td>@archlitchi</td>\n<td>@dafu-wu</td>\n<td>@DSFans2014</td>\n\n\n</tr>\n<tr>\n<td>@FAUST-BENCHOU</td>\n<td>@fengruotj</td>\n<td>@Freshwlnd</td>\n\n\n</tr>\n<tr>\n<td>@fx147</td>\n<td>@goyalpalak18</td>\n<td>@guoqinwill</td>\n\n\n</tr>\n<tr>\n<td>@Haibara-Ai97</td>\n<td>@hajnalmt</td>\n<td>@halcyon-r</td>\n\n\n</tr>\n<tr>\n<td>@handan-yxh</td>\n<td>@JackyTYang</td>\n<td>@jayesh9747</td>\n\n\n</tr>\n<tr>\n<td>@JesseStutler</td>\n<td>@jiahuat</td>\n<td>@karanBRAVO</td>\n\n\n</tr>\n<tr>\n<td>@kingeasternsun</td>\n<td>@kiritoxkiriko</td>\n<td>@kube-gopher</td>\n\n\n</tr>\n<tr>\n<td>@kuldeep</td>\n<td>@LiZhenCheng9527</td>\n<td>@medyagh</td>\n\n\n</tr>\n<tr>\n<td>@MondayCha</td>\n<td>@Monokaix</td>\n<td>@mvinchoo</td>\n\n\n</tr>\n<tr>\n<td>@neeraj542</td>\n<td>@nitindhiman314e</td>\n<td>@ouyangshengjia</td>\n\n\n</tr>\n<tr>\n<td>@PersistentJZH</td>\n<td>@qi-min</td>\n<td>@rhh777</td>\n\n\n</tr>\n<tr>\n<td>@ruanwenjun</td>\n<td>@RushabhMehta2005</td>\n<td>@sailorvii</td>\n\n\n</tr>\n<tr>\n<td>@Sayan4444</td>\n<td>@Shrutim1505</td>\n<td>@ssfffss</td>\n\n\n</tr>\n<tr>\n<td>@suyiiyii</td>\n<td>@tanberBro</td>\n<td>@Tau721</td>\n\n\n</tr>\n<tr>\n<td>@vzhou-p</td>\n<td>@wangyang0616</td>\n<td>@weapons97</td>\n\n\n</tr>\n<tr>\n<td>@Wonki4</td>\n<td>@zhaoqi612</td>\n<td>@zhengchenyu</td>\n\n\n</tr>\n<tr>\n<td>@zhoujinyu</td>\n<td>@zjj2wry</td>\n<td>&nbsp;</td>\n\n\n</tr>\n\n\n</tbody>\n\n\n</table>\n<p><strong>相关链接</strong></p>\n<p>[1] Volcano: <em>https://volcano.sh/en/</em><br />\n[2] Volcano v1.14.0: <em>https://github.com/volcano-sh/volcano/releases/tag/v1.14.0</em><br />\n[3] Sharding Controller Design: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/sharding_controller.md</em><br />\n[4] Agent Scheduler Design: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/agent-scheduler.md</em><br />\n[5] Network Topology Aware Scheduling: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/Network%20Topology%20Aware%20Scheduling.md</em><br />\n[6] CPU Throttle Design: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/cpu-throttle-design.md</em><br />\n[7] Agent Cgroup V2 Adaptation: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/design/agent-cgroup-v2-adaptation.md</em><br />\n[8] How to Use vNPU: <em>https://github.com/volcano-sh/volcano/blob/v1.14.0/docs/user-guide/how_to_use_vnpu.md</em><br />\n[9] Volcano Global v0.3.0 Release Notes: <em>https://github.com/volcano-sh/volcano-global/releases/tag/v0.3.0</em><br />\n[10] Volcano Dashboard v0.2.0 Release Notes: <em>https://github.com/volcano-sh/dashboard/releases/tag/v0.2.0</em></p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-13 16:35</span>&nbsp;\n<a href=\"https://www.cnblogs.com/huaweiyun\">华为云开发者联盟</a>&nbsp;\n阅读(<span id=\"post_view_count\">87</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "为什么现代 C++ 库都用 PIMPL？一场关于封装、依赖与安全的演进",
      "link": "https://www.cnblogs.com/charlee44/p/19616660",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/charlee44/p/19616660\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 21:27\">\n    <span>为什么现代 C++ 库都用 PIMPL？一场关于封装、依赖与安全的演进</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        系统阐述了在 C++ 工程中如何通过 PIMPL 惯用法，在坚守 RAII 资源安全的前提下，有效解耦头文件依赖、提升编译效率并保持接口简洁。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>在 C++ 的工程实践中，如何在保证资源安全管理的同时，又避免头文件污染和不必要的编译依赖？这个问题贯穿了现代 C++ 库设计的核心。本文将沿着一条清晰的技术演进路径，探讨从 RAII 封装出发，历经值语义、裸指针、智能指针等阶段，最终走向 PIMPL（Pointer to Implementation） 这一成熟且优雅的解决方案。</p>\n</blockquote>\n<h1 id=\"1-raii资源管理的基石\">1. RAII——资源管理的基石</h1>\n<p>C++ 的核心哲学之一是 RAII（Resource Acquisition Is Initialization）：资源（内存、文件句柄、网络连接等）的生命周期应由对象的构造与析构自动管理。例如：</p>\n<pre><code class=\"language-cpp\">class FileHandle {\n    FILE* fp;\npublic:\n    FileHandle(const char* path) : fp(fopen(path, \"r\")) {}\n    ~FileHandle() { if (fp) fclose(fp); }\n};\n</code></pre>\n<p>RAII 让资源管理变得安全：利用类对象的生命周期，在构造函数中申请资源，在析构函数中释放资源。如果这个类对象是基于栈的值对象，那么就可以自动实现资源的管理。因此，在现代 C++ 中，相比传统的指针语义，更加提倡使用基于 RAII 的值语义。</p>\n<h1 id=\"2-值语义的诱惑与代价\">2. 值语义的诱惑与代价</h1>\n<p>但是，当我们把这种思想用于封装复杂组件（如 ONNX 模型会话、数据库连接池）时，问题出现了。理想情况下，我们希望像使用 std::string 一样，用“值语义”操作一个封装对象：</p>\n<pre><code class=\"language-cpp\">class Embedder {\n    Ort::Session session; // 值成员\npublic:\n    std::vector&lt;float&gt; embed(const std::string&amp; text);\n};\n</code></pre>\n<p>这看起来非常简洁、高效、符合现代 C++ 风格。但也有另外一个问题：破坏了封装，导致不必要的环境依赖。最直观的问题就是 <code>Ort::Session</code> 的完整定义必须出现在头文件中，这意味着使用者必须包含 onnxruntime ，而这个头文件可能重达数 MB ，依赖数十个系统库。这就会造成如下问题：</p>\n<ul>\n<li>编译时间暴增，微小的改动都需要编译很长的时间。</li>\n<li>头文件耦合严重，调用者使用不方便，甚至造成环境污染。</li>\n<li>ABI 极其脆弱，内部改动导致所有用户重编译。</li>\n</ul>\n<h1 id=\"3-指针语义的回退\">3. 指针语义的回退</h1>\n<p>为了解耦，一个比较好的办法就是使用前置声明 + 指针语义：</p>\n<pre><code class=\"language-cpp\">// header\nclass SessionImpl; // 前置声明\nclass Embedder {\n    SessionImpl* pimpl;\npublic:\n    Embedder();\n    ~Embedder(); // 必须手动 delete\n};\n</code></pre>\n<p>这样做确实切断了编译依赖，但也引入了新的问题。那就是需要按照 RAII 原则写好构造函数和析构函数。而一旦要写析构函数，也往往意味着需要写另外四个特殊的成员函数：</p>\n<ol>\n<li>拷贝构造函数（Copy Constructor）</li>\n<li>拷贝赋值运算符（Copy Assignment Operator）</li>\n<li>移动构造函数（Move Constructor）</li>\n<li>移动赋值运算符（Move Assignment Operator）</li>\n</ol>\n<p>这样做要写非常多的样板代码，而且也很容易出问题。为了封装牺牲安全，得不偿失。</p>\n<h1 id=\"4-使用智能指针\">4. 使用智能指针</h1>\n<p>使用裸指针又麻烦又不安全，那么就可以使用 C++11 引入的智能指针：std::unique_ptr 和 std::shared_ptr；智能指针同样是基于 RAII 的：</p>\n<pre><code class=\"language-cpp\">class SessionImpl;\nclass Embedder {\n    std::unique_ptr&lt;SessionImpl&gt; pimpl;\n};\n</code></pre>\n<p>这里为什么使用 <code>std::unique_ptr</code> 而不使用 <code>std::shared_ptr</code> 呢？其实也可以，不过在现代 C++ 中，更推荐使用 <code>std::unique_ptr</code> 。<code>std::shared_ptr</code> 是用来共享资源的所有权，会对引用资源进行计数，但是有可能会造成相互循环引用造成不能释放资源的问题；而<code>std::unique_ptr</code> 则表示独占资源的所有权，不仅开销更低（无引用计数），也更加安全（只能通过 <code>std::move</code> 转移所有权 ）。</p>\n<p>不过有一点需要注意：<code>std::unique_ptr</code> 和 <code>std::shared_ptr</code> 在处理不完整类型（incomplete type）时的行为截然不同。具体来说，当在头文件中使用前置声明（如 <code>class Impl;</code>）并用智能指针持有它时，<code>Impl</code> 是一个不完整类型。</p>\n<ul>\n<li><code>std::shared_ptr</code> 可以安全地在头文件中默认析构，因为它在构造时（通常在 <code>.cpp</code> 文件中）会捕获一个完整的删除器（deleter），即使析构发生在头文件上下文中，也能正确调用 <code>delete</code>。</li>\n<li>而 <code>std::unique_ptr</code> 的删除器是其类型的一部分（通常是默认的 <code>std::default_delete&lt;Impl&gt;</code>），它要求在析构点（即类的析构函数被实例化的地方）<code>Impl</code> 必须是完整类型。如果在头文件中写 <code>~Embedder() = default;</code>，此时 <code>Impl</code> 仍是不完整的，编译器可能不会报错，但会导致未定义行为（通常是链接失败或运行时崩溃）。</li>\n</ul>\n<p>因此，使用 <code>std::unique_ptr&lt;Impl&gt;</code> 时，必须将主类的析构函数定义移到 <code>.cpp</code> 文件中，确保 <code>Impl</code> 已被完整定义：</p>\n<pre><code class=\"language-cpp\">// Embedder.cpp\nclass Embedder::Impl {\n    // 完整定义...\n};\n\nEmbedder::~Embedder() = default; // ✅ 此时 Impl 完整，安全析构\n</code></pre>\n<h1 id=\"5-封装与效率的平衡pimpl\">5. 封装与效率的平衡：PIMPL</h1>\n<p>使用智能指针虽然好，但是总归是比不上值语义方便。当类中只有一个需要隐藏的成员还好，如果有很多个需要隐藏的成员，每一个都写前置声明，并用智能指针来管理，那就实在太繁琐了。并且，从编程品味上来说，C++ 智能指针的写法说不上优雅：智能指针是由传染性的，当满屏都是 <code>std::shared_ptr</code> 或者 <code>std::unique_ptr</code> 的时候，实在很影响阅读性。</p>\n<p>另外，作为对外的接口，最好是提供像 Java / C# 那样的接口，C++ 的纯虚函类也行，隐藏掉所有的细节，包括私有函数和数据成员。这样有非常多的好处：</p>\n<ol>\n<li>最小化依赖环境，提升编译速度。</li>\n<li>调用者使用方便，不会污染环境。</li>\n<li>ABI 稳定，可以只更新库而不用更新整个程序。</li>\n</ol>\n<p>那么要怎么进行优化呢？很简单，我们可以实现一个名为 <code>Impl</code> 的类中类 ，使用<code>std::unique_ptr</code>进行管理。<code>Impl</code> 是实现在 cpp 中的，可以将一切实现的细节，比说私有函数和数据成员，都放在这个 <code>Impl</code> 中。更重要的是，<code>Impl</code> 中的数据成员完全可以使用值类型！如下所示：</p>\n<pre><code class=\"language-cpp\">// 头文件\nclass Embedder {\n    class Impl;\n    std::unique_ptr&lt;Impl&gt; impl;\npublic:\n    Embedder(const std::string&amp; model);\n    ~Embedder(); // 声明但不在头文件定义！\n    std::vector&lt;float&gt; embed(std::string_view text) const;\n};\n</code></pre>\n<pre><code class=\"language-cpp\">// 源文件\nclass Embedder::Impl {\n    Ort::Session session;\n    hf::Tokenizer tokenizer;\n    int64_t dim;\npublic:\n    Impl(const std::string&amp; path, const hf::Tokenizer&amp; tok) \n        : session(...), tokenizer(tok) { /* init */ }\n    std::vector&lt;float&gt; embed(std::string_view text) const { /* ... */ }\n};\n\nEmbedder::Embedder(const std::string&amp; path) \n    : impl(std::make_unique&lt;Impl&gt;(path, global_tokenizer)) {}\n\nEmbedder::~Embedder() = default; // 此时 Impl 完整，安全！\n</code></pre>\n<p>这个实现，就是所谓的 PIMPL（Pointer to IMPLementation）惯用法，也常被称作 “编译防火墙”（Compilation Firewall） 或 “Opaque Pointer” 模式。不得不说，这种 PIMPL 设计模式确实精妙——它在安全性、封装性、编译效率与接口简洁性之间取得了近乎完美的平衡，既坚守了 RAII 的资源管理原则，又有效隔离了实现细节，堪称现代 C++ 工程实践中“高内聚、低耦合”的典范。</p>\n<h1 id=\"6-没有银弹只有权衡\">6. 没有银弹，只有权衡</h1>\n<p>PIMPL 使用了前置声明。是否使用前置声明一直是 C++ 中比较争议的一点，Qt 遵循前置声明的原则实现了非常强大、优雅且高效的 C++ 运行时框架。Google 则经历了从推荐使用前置声明到不推荐使用前置声明的转变。个人认为，PIMPL 解决的就是 C++ 中两个重要原则矛盾的问题：</p>\n<ul>\n<li>推荐使用值语义，但是会引入更多环境依赖</li>\n<li>封装需要尽可能隐藏不必要的细节</li>\n</ul>\n<p>如果两者只能选择其中一个，那么还是尽量使用值语义的原则更加重要，毕竟这涉及到安全问题，而资源管理的安全问题贯穿 C++ 程序的始终。事实上，如果不是提供对外接口，或者实现比较小，那么直接使用值语义即可（第2节中的内容）——值语义永远是最简洁安全的实现。</p>\n<p>另外，如果实现 C++20 Modules ，那么就不必要使用 PIMPL 了，完全可以回归值语义实现，因为 C++20 Modules 在语言层面已经实现了 PIMPL 的诸多优点。</p>\n<h1 id=\"7-示例代码\">7. 示例代码</h1>\n<p>最后放出笔者自己实现的基于 PIMPL 的嵌入器的完整代码供读者参考：</p>\n<pre><code class=\"language-cpp\">// BgeOnnxEmbedder.h\n#pragma once\n\n#include &lt;memory&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n\nnamespace embedding {\n\nnamespace hf {\nclass Tokenizer;\n}\n\nclass BgeOnnxEmbedder {\n public:\n  explicit BgeOnnxEmbedder(const std::string&amp; modelPath,\n                           const hf::Tokenizer&amp; tokenizer);\n  ~BgeOnnxEmbedder();\n\n  const int64_t&amp; EmbeddingDim() const;\n\n  std::vector&lt;float&gt; Embed(const std::string&amp; text) const;\n\n private:\n  class Impl;  // 前向声明\n  std::unique_ptr&lt;Impl&gt; impl;\n};\n\n}  // namespace embedding\n</code></pre>\n<pre><code class=\"language-cpp\">//BgeOnnxEmbedder.cpp\n#include \"BgeOnnxEmbedder.h\"\n\n#include &lt;onnxruntime_cxx_api.h&gt;\n\n#include \"HfTokenizer.h\"\n#include \"Util/StringEncode.h\"\n\nnamespace embedding {\n\nclass BgeOnnxEmbedder::Impl {\n public:\n  Ort::Env&amp; GetOrtEnv() {\n    static Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"BgeOnnxEmbedder\");\n    return env;\n  }\n\n  const int64_t&amp; EmbeddingDim() const { return embeddingDim; }\n\n  explicit Impl(const std::string&amp; modelPath, const hf::Tokenizer&amp; tokenizer)\n      : session{GetOrtEnv(),\n#ifdef _WIN32\n                util::StringEncode::Utf8StringToWideString(modelPath).c_str(),\n#else\n                modelPath.c_str(),\n#endif\n                Ort::SessionOptions()},\n        memInfo{Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU)},\n        tokenizer(tokenizer),\n        embeddingDim(0) {\n\n    //\n    const auto&amp; outputInfo = session.GetOutputTypeInfo(0);\n    const auto&amp; tensorInfo = outputInfo.GetTensorTypeAndShapeInfo();\n    const auto&amp; shape = tensorInfo.GetShape();\n\n    // 假设输出是 [batch, seq, dim] 或 [batch, dim]\n    // 我们取最后一个非 -1 的维度\n    for (auto it = shape.rbegin(); it != shape.rend(); ++it) {\n      if (*it != -1) {\n        embeddingDim = *it;\n        break;\n      }\n    }\n\n    if (embeddingDim == 0) {\n      throw std::runtime_error(\n          \"Failed to infer embedding dimension from ONNX model.\");\n    }\n  }\n\n  std::vector&lt;float&gt; Embed(const std::string&amp; text) const {\n    hf::Tokenizer::ResultPtr result = tokenizer.Encode(text);\n    if (!result) {\n      throw std::runtime_error(\"tokenizer_encode failed\");\n    }\n\n    // 定义张量维度\n    int64_t seqLen = static_cast&lt;int64_t&gt;(result-&gt;length);\n    std::vector&lt;int64_t&gt; inputShape = {1, seqLen};\n    size_t dataByteCount = sizeof(int64_t) * seqLen;\n\n    Ort::Value inputIdsTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;input_ids, dataByteCount, inputShape.data(),\n        inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    Ort::Value attentionMaskTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;attention_mask, dataByteCount,\n        inputShape.data(), inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    Ort::Value tokenTypeIdsTensor = Ort::Value::CreateTensor(\n        memInfo.GetConst(), result-&gt;token_type_ids, dataByteCount,\n        inputShape.data(), inputShape.size(),\n        ONNXTensorElementDataType::ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64);\n\n    // 输入名必须与模型定义一致\n    const char* inputNames[] = {\"input_ids\", \"attention_mask\",\n                                \"token_type_ids\"};\n    const char* outputNames[] = {\"last_hidden_state\"};\n\n    // 把三个输入张量放进数组\n    std::vector&lt;Ort::Value&gt; inputs;\n    inputs.push_back(std::move(inputIdsTensor));\n    inputs.push_back(std::move(attentionMaskTensor));\n    inputs.push_back(std::move(tokenTypeIdsTensor));\n\n    // 执行推理\n    auto outputs = session.Run(Ort::RunOptions(),  // 运行选项（通常 nullptr）\n                               inputNames,         // 输入名数组\n                               inputs.data(),  // 输入张量数组\n                               inputs.size(),  // 输入数量（3）\n                               outputNames,    // 输出名数组\n                               1               // 输出数量（1）\n    );\n\n    // 获取输出信息\n    auto&amp; output_tensor = outputs[0];\n    auto output_shape = output_tensor.GetTensorTypeAndShapeInfo().GetShape();\n    if (output_shape.size() != 3 || output_shape[0] != 1) {\n      throw std::runtime_error(\"Unexpected output shape\");\n    }\n\n    // 获取输出张量的原始 float 指针\n    const float* outputData = outputs[0].GetTensorData&lt;float&gt;();\n\n    // 提取 [CLS] token 的 embedding（第0个token）\n    int64_t hiddenSize = output_shape[2];\n    std::vector&lt;float&gt; embedding(outputData, outputData + hiddenSize);\n\n    // L2 归一化（BGE 要求）\n    float norm = 0.0f;\n    for (float v : embedding) norm += v * v;\n    norm = std::sqrt(norm);\n    if (norm &gt; 1e-8) {\n      for (float&amp; v : embedding) v /= norm;\n    }\n\n    return embedding;\n  }\n\n private:\n  mutable Ort::Session session;\n  Ort::MemoryInfo memInfo;\n  const hf::Tokenizer&amp; tokenizer;\n  int64_t embeddingDim;\n};\n\nBgeOnnxEmbedder::BgeOnnxEmbedder(const std::string&amp; modelPath,\n                                 const hf::Tokenizer&amp; tokenizer)\n    : impl(std::make_unique&lt;Impl&gt;(modelPath, tokenizer)) {}\n\nBgeOnnxEmbedder::~BgeOnnxEmbedder() = default;  // 此时 Impl 已定义，可安全析构\n\nconst int64_t&amp; BgeOnnxEmbedder::EmbeddingDim() const {\n  return impl-&gt;EmbeddingDim();\n}\n\nstd::vector&lt;float&gt; BgeOnnxEmbedder::Embed(const std::string&amp; text) const {\n  return impl-&gt;Embed(text);\n}\n\n}  // namespace embedding\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 21:27</span>&nbsp;\n<a href=\"https://www.cnblogs.com/charlee44\">charlee44</a>&nbsp;\n阅读(<span id=\"post_view_count\">6</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "linux字符设备驱动",
      "link": "https://www.cnblogs.com/cear/p/19616290",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/cear/p/19616290\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 17:57\">\n    <span>linux字符设备驱动</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        Linux字符设备驱动中的内核函数讲解和使用实例\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<h2>1.字符设备驱动相关系统函数简介</h2>\n<h3>1.1 container_of</h3>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\">*\n * container_of 通过一个结构体成员的指针，获取包含该成员的结构体的起始地址\n * @ptr:        变量的指针\n * @type:       指针指向的结构体类型\n * @member:     结构体中的变量类型\n </span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">#define</span> container_of(ptr, type, member) ({              \\\n    <span style=\"color: rgba(0, 0, 255, 1);\">void</span> *__mptr = (<span style=\"color: rgba(0, 0, 255, 1);\">void</span> *<span style=\"color: rgba(0, 0, 0, 1);\">)(ptr);                   \\\n    BUILD_BUG_ON_MSG(</span>!__same_type(*(ptr), ((type *)<span style=\"color: rgba(128, 0, 128, 1);\">0</span>)-&gt;member) &amp;&amp;<span style=\"color: rgba(0, 0, 0, 1);\">   \\\n                     </span>!__same_type(*(ptr), <span style=\"color: rgba(0, 0, 255, 1);\">void</span><span style=\"color: rgba(0, 0, 0, 1);\">),            \\\n                     </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">pointer type mismatch in container_of</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">); \\\n    ((type </span>*)(__mptr - offsetof(type, member))); })</pre>\n</div>\n<p>为了简化理解，可以将&nbsp;<span class=\"cnblogs_code\">container_of</span>&nbsp;函数理解为如下表示</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">#define</span> container_of(ptr, type, member) \\<span style=\"color: rgba(0, 0, 0, 1);\">\n    ((type </span>*)((<span style=\"color: rgba(0, 0, 255, 1);\">char</span> *)(ptr) - offsetof(type, member)))</pre>\n</div>\n<p>该函数根据 结构体成员变量的指针，通过该变量相对于结构体的偏移，得到了该变量对应的结构体的指针。</p>\n<p>这是一个非常灵活的用法，通过保存某个结构体变量的指针，可以通过该指针反向推出该结构体的指针。在后续案例中有详细解释。</p>\n<h3>1.2 register_chrdev_region</h3>\n<p class=\"ds-markdown-paragraph\">&nbsp;<span class=\"cnblogs_code\">register_chrdev_region</span> 是 Linux 内核中用于注册字符设备编号范围的函数。该函数为驱动程序预留一段连续的设备号（主设备号 + 起始次设备号），后续将字符设备（通过 &nbsp;<span class=\"cnblogs_code\">cdev_add</span>&nbsp;）绑定到这些设备号上，函数原型如下</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\">*\n * first：dev_t 类型，指定要注册的起始设备号。使用 MKDEV(major, minor) 宏生成\n * count：需要注册的连续设备号数量（次设备号的范围）\n * name：设备的名称，该名称会出现在 /proc/devices 文件中，用于标识该组设备号属于哪个驱动。\n * 返回值: 成功返回0\n *        参数无效返回     -EINVAL\n *        设备号被占用返回 -EBUSY\n</span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">int</span> register_chrdev_region(dev_t first, unsigned <span style=\"color: rgba(0, 0, 255, 1);\">int</span> count, <span style=\"color: rgba(0, 0, 255, 1);\">const</span> <span style=\"color: rgba(0, 0, 255, 1);\">char</span> *name);</pre>\n</div>\n<h3>1.3 cdev_init</h3>\n<p>&nbsp;<span class=\"cnblogs_code\"><span style=\"color: rgba(0, 0, 255, 1);\">struct</span> cdev</span>&nbsp;是内核表示字符设备的对象，每个字符设备驱动都需要创建&nbsp;<span class=\"cnblogs_code\">cdev</span> 实例，并将其注册到内核</p>\n<p>&nbsp;<span class=\"cnblogs_code\">cdev_init</span>&nbsp;&nbsp;负责设置 <span class=\"cnblogs_code\">cdev</span> 的基本字段，为后续的 <span class=\"cnblogs_code\">cdev_add</span>做准备</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\">\n * dev：指向要初始化的 struct cdev 结构体的指针。\n *       该结构体可以由驱动静态分配 (kmalloc) ，也可以动态分配 (cdev_alloc)\n * fops：指向 struct file_operations 结构体的指针\n *       该结构体包含了设备支持的各种操作函数（如 open、read、write、ioctl 等）\n</span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">void</span> cdev_init(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> cdev *cdev, <span style=\"color: rgba(0, 0, 255, 1);\">const</span> <span style=\"color: rgba(0, 0, 255, 1);\">struct</span> file_operations *<span style=\"color: rgba(0, 0, 0, 1);\">fops)\n{\n    memset(cdev, </span><span style=\"color: rgba(128, 0, 128, 1);\">0</span>, <span style=\"color: rgba(0, 0, 255, 1);\">sizeof</span> *<span style=\"color: rgba(0, 0, 0, 1);\">cdev);\n    INIT_LIST_HEAD(</span>&amp;cdev-&gt;<span style=\"color: rgba(0, 0, 0, 1);\">list);\n    kobject_init(</span>&amp;cdev-&gt;kobj, &amp;<span style=\"color: rgba(0, 0, 0, 1);\">ktype_cdev_default);\n    cdev</span>-&gt;ops =<span style=\"color: rgba(0, 0, 0, 1);\"> fops;\n}</span></pre>\n</div>\n<h3>1.4 cdev_add</h3>\n<p>该函数建立了设备号与字符设备驱动的关联，使 VFS（虚拟文件系统）能够通过设备号找到对应的驱动程序，从而响应用户空间的打开、读写等操作</p>\n<ul>\n<li>将&nbsp;<span class=\"cnblogs_code\"><span style=\"color: rgba(0, 0, 255, 1);\">struct</span> cdev</span>&nbsp;对象与设备号绑定，在内核的字符设备映射表中建立关联</li>\n<li>将&nbsp;<span class=\"cnblogs_code\">cdev</span>&nbsp;对象加入内核的全局字符设备链表或哈希表，使 VFS 能够根据设备号找到对应的<span class=\"cnblogs_code\">cdev</span>&nbsp;</li>\n<li>激活设备：调用&nbsp;<span class=\"cnblogs_code\">cdev_add</span>&nbsp;后，该设备便可以被用户空间访问（需要使用<code>mknod</code>关联到<code>/dev/mydevname</code>）</li>\n</ul>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\">\n * p：指向 cdev_init() 已经初始化的 struct cdev 对象的指针。\n * dev：分配给该设备的起始设备号，应该与 register_chrdev_region() 向内核申请的设备号位置一致\n * count：与该设备关联的次设备号数量\n * 返回值：成功 0\n          参数无效    -EINVAL\n          设备号被占用 -EBUSY （理论上在分配设备号时已经检查过，但这里仍可能发生，如动态添加时竞争）\n          内存不足    -ENOMEM\n</span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">int</span> cdev_add(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> cdev *p, dev_t dev, unsigned count);</pre>\n</div>\n<h2>2. 使用字符设备驱动实现共享内存</h2>\n<p>下面是使用字符设备驱动实现共享内存的案例，用来快速熟悉驱动函数的使用方法，代码参考书籍为Linux设备驱动详解</p>\n<div class=\"cnblogs_code\">\n<pre>#include &lt;linux/init.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/errno.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/mm.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/sched.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/module.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/ioctl.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/io.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/fs.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/cdev.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/uaccess.h&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n#include </span>&lt;linux/slab.h&gt;\n\n<span style=\"color: rgba(0, 0, 255, 1);\">#define</span> GLOBALMEM_SIZE    1024\n<span style=\"color: rgba(0, 0, 255, 1);\">#define</span> GLOBALMEM_MAGIC    'M'\n<span style=\"color: rgba(0, 0, 255, 1);\">#define</span> MEM_CLEAR        _IO(GLOBALMEM_MAGIC, 0)\n\n<span style=\"color: rgba(0, 0, 255, 1);\">struct</span><span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_dev \n{\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 字符设备</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">struct</span><span style=\"color: rgba(0, 0, 0, 1);\"> cdev m_cdev;\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 共享内存</span>\n    unsigned <span style=\"color: rgba(0, 0, 255, 1);\">char</span><span style=\"color: rgba(0, 0, 0, 1);\"> mem[GLOBALMEM_SIZE];\n};\n\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">int</span> globalmem_major = <span style=\"color: rgba(128, 0, 128, 1);\">266</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 存放两个字符设备私有数据</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> globalmem_dev*<span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_devp;\n\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\"> user open fd </span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">int</span> globalmem_open(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> inode* inode, <span style=\"color: rgba(0, 0, 255, 1);\">struct</span> file*<span style=\"color: rgba(0, 0, 0, 1);\"> filp) {\n    \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">struct</span> globalmem_dev*<span style=\"color: rgba(0, 0, 0, 1);\"> dev;\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\"> 下面是一种常用的区分次设备号的方法\n     * 通过 globalmem_init 初始化时传入不同的 cdev 指针实现区分 </span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n    <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> inode 中保存的 i_cdev 指针是 globalmem_init 函数中传入的，globalmem_dev 结构体变量 m_cdev 的指针\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 所以通过 inode-&gt;i_cdev 指针，即 m_cdev 成员变量的指针，可以找到其对应的结构体指针</span>\n    dev = container_of(inode-&gt;i_cdev, <span style=\"color: rgba(0, 0, 255, 1);\">struct</span><span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_dev, m_cdev);\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 将设备结构体指针传给文件私有数据指针，提供给其他函数调用时使用</span>\n    filp-&gt;private_data =<span style=\"color: rgba(0, 0, 0, 1);\"> dev;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n}\n\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\"> user release fd</span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">int</span> globalmem_release(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> inode* inode, <span style=\"color: rgba(0, 0, 255, 1);\">struct</span> file*<span style=\"color: rgba(0, 0, 0, 1);\"> filp) {\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n}\n\n\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\"> user read fd </span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">static</span> ssize_t globalmem_read(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> file* filp, <span style=\"color: rgba(0, 0, 255, 1);\">char</span> __user* buf, size_t count, loff_t*<span style=\"color: rgba(0, 0, 0, 1);\"> ppos) {\n    unsigned </span><span style=\"color: rgba(0, 0, 255, 1);\">long</span> p = *<span style=\"color: rgba(0, 0, 0, 1);\">ppos;\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 获得设备结构体的指针（这是通过open函数传入的指针，实现不同次设备使用不同的共享内存）</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">struct</span> globalmem_dev* dev = filp-&gt;<span style=\"color: rgba(0, 0, 0, 1);\">private_data;    \n\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span>(p &gt;=<span style=\"color: rgba(0, 0, 0, 1);\"> GLOBALMEM_SIZE)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span>(count &gt; GLOBALMEM_SIZE -<span style=\"color: rgba(0, 0, 0, 1);\"> p)\n        count </span>= GLOBALMEM_SIZE -<span style=\"color: rgba(0, 0, 0, 1);\"> p;\n    \n    copy_to_user(buf, (</span><span style=\"color: rgba(0, 0, 255, 1);\">void</span>*)(dev-&gt;mem +<span style=\"color: rgba(0, 0, 0, 1);\"> p), count);\n    </span>*ppos = p +<span style=\"color: rgba(0, 0, 0, 1);\"> count;\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> count;\n}\n\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\"> user write fd </span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span> \n<span style=\"color: rgba(0, 0, 255, 1);\">static</span> ssize_t globalmem_write(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> file* filp, <span style=\"color: rgba(0, 0, 255, 1);\">const</span> <span style=\"color: rgba(0, 0, 255, 1);\">char</span> __user* buf, size_t count, loff_t*<span style=\"color: rgba(0, 0, 0, 1);\"> ppos) {\n    unsigned </span><span style=\"color: rgba(0, 0, 255, 1);\">long</span> p = *<span style=\"color: rgba(0, 0, 0, 1);\">ppos;\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 获得设备结构体的指针</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">struct</span> globalmem_dev* dev = filp-&gt;<span style=\"color: rgba(0, 0, 0, 1);\">private_data;    \n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span>(p &gt;=<span style=\"color: rgba(0, 0, 0, 1);\"> GLOBALMEM_SIZE)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span>(count &gt; GLOBALMEM_SIZE -<span style=\"color: rgba(0, 0, 0, 1);\"> p)\n        count </span>= GLOBALMEM_SIZE -<span style=\"color: rgba(0, 0, 0, 1);\"> p;\n\n    copy_from_user(dev</span>-&gt;mem +<span style=\"color: rgba(0, 0, 0, 1);\"> p, buf, count);\n    </span>*ppos = p +<span style=\"color: rgba(0, 0, 0, 1);\"> count;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> count;\n}\n\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\"> user lseek fd </span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">static</span> loff_t globalmem_llseek(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> file* filp, loff_t offset, <span style=\"color: rgba(0, 0, 255, 1);\">int</span><span style=\"color: rgba(0, 0, 0, 1);\"> orig) {\n    loff_t ret;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">switch</span><span style=\"color: rgba(0, 0, 0, 1);\">(orig) {\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 从起始位置开始移动指针</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">case</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span>(offset &lt; <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">) {\n            ret </span>= -<span style=\"color: rgba(0, 0, 0, 1);\">EINVAL;\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n        }\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span>((unsigned <span style=\"color: rgba(0, 0, 255, 1);\">int</span>)offset &gt;<span style=\"color: rgba(0, 0, 0, 1);\"> GLOBALMEM_SIZE) {\n            ret </span>= -<span style=\"color: rgba(0, 0, 0, 1);\">EINVAL;\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n        }\n        filp</span>-&gt;f_pos = (unsigned <span style=\"color: rgba(0, 0, 255, 1);\">int</span><span style=\"color: rgba(0, 0, 0, 1);\">)offset;\n        ret </span>= filp-&gt;<span style=\"color: rgba(0, 0, 0, 1);\">f_pos;\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 从当前位置开始移动指针</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">case</span> <span style=\"color: rgba(128, 0, 128, 1);\">1</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span>((filp-&gt;f_pos + offset) &gt;<span style=\"color: rgba(0, 0, 0, 1);\"> GLOBALMEM_SIZE) {\n            ret </span>= -<span style=\"color: rgba(0, 0, 0, 1);\">EINVAL;\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n        }\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span>((filp-&gt;f_pos + offset) &lt; <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">) {\n            ret </span>= -<span style=\"color: rgba(0, 0, 0, 1);\">EINVAL;\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n        }\n        filp</span>-&gt;f_pos +=<span style=\"color: rgba(0, 0, 0, 1);\"> offset;\n        ret </span>= filp-&gt;<span style=\"color: rgba(0, 0, 0, 1);\">f_pos;\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">default</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        ret </span>= -<span style=\"color: rgba(0, 0, 0, 1);\">EINVAL;\n    }\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> ret;\n}\n\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\"> user ioctl fd </span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">int</span> globalmem_ioctl(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span> inode* inodep, <span style=\"color: rgba(0, 0, 255, 1);\">struct</span> file* filp, unsigned <span style=\"color: rgba(0, 0, 255, 1);\">int</span> cmd, unsigned <span style=\"color: rgba(0, 0, 255, 1);\">long</span><span style=\"color: rgba(0, 0, 0, 1);\"> arg) {\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 获取设备结构体指针    </span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">struct</span> globalmem_dev* dev = filp-&gt;<span style=\"color: rgba(0, 0, 0, 1);\">private_data;    \n    \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">switch</span><span style=\"color: rgba(0, 0, 0, 1);\">(cmd) {\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">case</span><span style=\"color: rgba(0, 0, 0, 1);\"> MEM_CLEAR:\n        memset(dev</span>-&gt;mem, <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">, GLOBALMEM_SIZE);\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">default</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> -<span style=\"color: rgba(0, 0, 0, 1);\">EINVAL;\n    }\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n}\n\n\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">const</span> <span style=\"color: rgba(0, 0, 255, 1);\">struct</span> file_operations globalmem_fops =<span style=\"color: rgba(0, 0, 0, 1);\"> {\n    .owner </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> THIS_MODULE,\n    .open </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_open,\n    .release </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_release,\n    .llseek </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_llseek,\n    .read </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_read,\n    .write </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_write,\n    .unlocked_ioctl </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_ioctl\n};\n\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">/*</span><span style=\"color: rgba(0, 128, 0, 1);\"> 设备驱动模块insmod加载函数 </span><span style=\"color: rgba(0, 128, 0, 1);\">*/</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">int</span> globalmem_init(<span style=\"color: rgba(0, 0, 255, 1);\">void</span><span style=\"color: rgba(0, 0, 0, 1);\">) {\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 向 Linux 内核中注册字符设备编号范围</span>\n    register_chrdev_region(MKDEV(globalmem_major, <span style=\"color: rgba(128, 0, 128, 1);\">0</span>), <span style=\"color: rgba(128, 0, 128, 1);\">2</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">globalmem</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 为2个次设备以及共享内存分配内存</span>\n    globalmem_devp = kmalloc(<span style=\"color: rgba(128, 0, 128, 1);\">2</span> * <span style=\"color: rgba(0, 0, 255, 1);\">sizeof</span>(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span><span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_dev), GFP_KERNEL);\n    memset(globalmem_devp, </span><span style=\"color: rgba(128, 0, 128, 1);\">0</span>, <span style=\"color: rgba(128, 0, 128, 1);\">2</span> * <span style=\"color: rgba(0, 0, 255, 1);\">sizeof</span>(<span style=\"color: rgba(0, 0, 255, 1);\">struct</span><span style=\"color: rgba(0, 0, 0, 1);\"> globalmem_dev));\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 初始化字符设备0的基本字段</span>\n    cdev_init(&amp;(globalmem_devp[<span style=\"color: rgba(128, 0, 128, 1);\">0</span>].m_cdev), &amp;<span style=\"color: rgba(0, 0, 0, 1);\">globalmem_fops);\n    globalmem_devp[</span><span style=\"color: rgba(128, 0, 128, 1);\">0</span>].m_cdev.owner =<span style=\"color: rgba(0, 0, 0, 1);\"> THIS_MODULE;\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 将主设备号globalmem_major次设备号0，与字符设备驱动的关联</span>\n    cdev_add(&amp;(globalmem_devp[<span style=\"color: rgba(128, 0, 128, 1);\">0</span>].m_cdev), MKDEV(globalmem_major, <span style=\"color: rgba(128, 0, 128, 1);\">0</span>), <span style=\"color: rgba(128, 0, 128, 1);\">1</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 初始化字符设备1的基本字段</span>\n    cdev_init(&amp;(globalmem_devp[<span style=\"color: rgba(128, 0, 128, 1);\">1</span>].m_cdev), &amp;<span style=\"color: rgba(0, 0, 0, 1);\">globalmem_fops);\n    globalmem_devp[</span><span style=\"color: rgba(128, 0, 128, 1);\">1</span>].m_cdev.owner =<span style=\"color: rgba(0, 0, 0, 1);\"> THIS_MODULE;\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 将主设备号globalmem_major次设备号1，与字符设备驱动的关联</span>\n    cdev_add(&amp;(globalmem_devp[<span style=\"color: rgba(128, 0, 128, 1);\">1</span>].m_cdev), MKDEV(globalmem_major, <span style=\"color: rgba(128, 0, 128, 1);\">1</span>), <span style=\"color: rgba(128, 0, 128, 1);\">1</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n}\n\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">int</span> globalmem_exit(<span style=\"color: rgba(0, 0, 255, 1);\">void</span><span style=\"color: rgba(0, 0, 0, 1);\">) {\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 注销cdev</span>\n    cdev_del(&amp;(globalmem_devp[<span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">].m_cdev));\n    cdev_del(</span>&amp;(globalmem_devp[<span style=\"color: rgba(128, 0, 128, 1);\">1</span><span style=\"color: rgba(0, 0, 0, 1);\">].m_cdev));\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 释放设备结构体内存</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">    kfree(globalmem_devp);    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 释放设备号</span>\n    dev_t devno = MKDEV(globalmem_major, <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n    unregister_chrdev_region(devno, </span><span style=\"color: rgba(128, 0, 128, 1);\">2</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n}\n\n\nMODULE_AUTHOR(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cear</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\nMODULE_LICENSE(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">GPL</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n\nmodule_param(globalmem_major, </span><span style=\"color: rgba(0, 0, 255, 1);\">int</span><span style=\"color: rgba(0, 0, 0, 1);\">, S_IRUGO);\nmodule_init(globalmem_init);\nmodule_exit(globalmem_exit);</span></pre>\n</div>\n<p>&nbsp;</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 17:57</span>&nbsp;\n<a href=\"https://www.cnblogs.com/cear\">cear</a>&nbsp;\n阅读(<span id=\"post_view_count\">6</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "从0到1，无代码微调并部署本地大语言模型LLM",
      "link": "https://www.cnblogs.com/ClownLMe/p/19615980",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ClownLMe/p/19615980\" id=\"cb_post_title_url\" title=\"发布于 2026-02-14 15:40\">\n    <span>从0到1，无代码微调并部署本地大语言模型LLM</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"前言\">前言</h1>\n<p><strong>LLM模型微调</strong> 能让大模型掌握特定行业的深度知识，能够实现AI虚拟主播，AI医生，AI程序员，AI网络安全工程师等特定领域的延展。更重要的是，当有本地部署的硬件条件限制时，能够让微调后小的大语言模型等效百亿级的大语言模型</p>\n<p><strong>测试环境：windows11，RTX4070显卡</strong><br />\n<strong>下面将手把手带你跑通无代码模型微调的全过程</strong></p>\n<h1 id=\"环境安装\">环境安装</h1>\n<h3 id=\"必要的工具\">必要的工具：</h3>\n<ul>\n<li>git： <a href=\"https://git-scm.cn/\" rel=\"noopener nofollow\" target=\"_blank\">https://git-scm.cn/</a> （方便拉取资源）</li>\n<li>python： <a href=\"https://www.python.org/\" rel=\"noopener nofollow\" target=\"_blank\">https://www.python.org/</a> （微调和运行必要环境）</li>\n</ul>\n<h3 id=\"流程\">流程：</h3>\n<ol>\n<li>创建文件夹，并拉取 llama-factory项目</li>\n</ol>\n<pre><code class=\"language-bash\">mkdir D:/LLM-Tuning\ncd D:/LLM-Tuning\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\n</code></pre>\n<ol start=\"2\">\n<li>安装LLaMA-Factory需要的环境</li>\n</ol>\n<pre><code class=\"language-bash\">pip install -e \".[torch,metrics]\"\npip install modelscope\n</code></pre>\n<ol start=\"3\">\n<li>验证环境</li>\n</ol>\n<pre><code class=\"language-bash\">python -c \"import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))\"\n</code></pre>\n<p>正常输出如下：</p>\n<p><img alt=\"微调环境验证\" class=\"lazyload\" /></p>\n<blockquote>\n<p>错误：正常来说安装完后验证环境会显示显卡型号，但是我在安装时，会出现报错，原因是它安装了错误的cuda版本，需要重新安装<code>torch</code><br />\n解决方法如下：</p>\n<pre><code>pip uninstall torch torchvision torchaudio\n\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n</code></pre>\n<p>如果其他版本请参考官网： <a href=\"https://pytorch.org/get-started/locally/\" rel=\"noopener nofollow\" target=\"_blank\">https://pytorch.org/get-started/locally/</a></p>\n</blockquote>\n<h1 id=\"微调\">微调</h1>\n<p>这里用于演示，只对模型做一个自我认知的微调</p>\n<h3 id=\"准备数据集\">准备数据集</h3>\n<p><strong>拉取数据集</strong></p>\n<pre><code class=\"language-bash\">git clone https://www.modelscope.cn/datasets/DanKe123abc/yuki_identity_sft.git\n</code></pre>\n<p><strong>修改数据集</strong><br />\n下载完后，目录结构如下：</p>\n<p><img alt=\"微调数据集1\" class=\"lazyload\" /></p>\n<p>我们需要关注的是<code>yuki_identity_sft.jsonl</code>文件，用编辑器将下列文字全局替换：</p>\n<pre><code>Yuki =&gt; 陈千语\nDanKe =&gt; 管理员\n</code></pre>\n<p>效果图如下：</p>\n<p><img alt=\"微调数据集替换\" class=\"lazyload\" /></p>\n<h3 id=\"准备本地模型\">准备本地模型</h3>\n<p>这里使用的是<code>qwen2.5_1.5B</code>用于演示<br />\n<strong>下载模型</strong></p>\n<pre><code class=\"language-python\">from modelscope import snapshot_download\n\ndownload_dir = \"D:\\\\Models\\\\Qwen2.5-1.5B-Instruct\"\n\nmodel_dir = snapshot_download(\n    'qwen/Qwen2.5-1.5B-Instruct', \n    cache_dir=download_dir, \n    revision='master'\n)\n\nprint(f\"下载完成！模型路径为: {model_dir}\")\n</code></pre>\n<h3 id=\"微调-1\">微调</h3>\n<p><strong>配置数据集信息</strong></p>\n<p><img alt=\"配置文件\" class=\"lazyload\" /></p>\n<ol>\n<li>打开<code>D:\\LLM-Tuning\\LLaMA-Factory\\data</code>文件，将刚刚修改好的数据集<code>yuki_identity_sft.jsonl</code>文件拖入文件夹中</li>\n<li>打开<code>dataset_info.json</code>文件，添加新配置：</li>\n</ol>\n<p><img alt=\"数据集配置\" class=\"lazyload\" /></p>\n<pre><code class=\"language-json\">\"MytestData\": {\n&nbsp; &nbsp; \"file_name\":\"yuki_identity_sft.jsonl\",\n&nbsp; &nbsp; \"columns\": {\n&nbsp; &nbsp; &nbsp; \"messages\": \"conversations\"\n&nbsp; &nbsp; },\n&nbsp; &nbsp; \"tags\": {\n&nbsp; &nbsp; &nbsp; \"role_tag\": \"role\",\n&nbsp; &nbsp; &nbsp; \"content_tag\": \"content\",\n&nbsp; &nbsp; &nbsp; \"user_tag\": \"user\",\n&nbsp; &nbsp; &nbsp; \"assistant_tag\": \"assistant\"\n&nbsp; &nbsp; },\n&nbsp; &nbsp; \"formatting\": \"sharegpt\"\n&nbsp; },\n</code></pre>\n<p><strong>打开LLamaFactory微调面板</strong></p>\n<pre><code class=\"language-bash\">python -m llamafactory.cli webui\n</code></pre>\n<p>设置参数如图，其他的默认就行：</p>\n<p><img alt=\"微调参数设置\" class=\"lazyload\" /></p>\n<p>设置完后直接点击开始，模型就开始训练了，训练完后会出现下面提示：</p>\n<p><img alt=\"微调完成\" class=\"lazyload\" /></p>\n<h1 id=\"验证模型\">验证模型</h1>\n<h3 id=\"加载训练完后的lora模型\">加载训练完后的lora模型</h3>\n<p><img alt=\"验证模型\" class=\"lazyload\" /></p>\n<h3 id=\"训练前后的大模型对比\">训练前后的大模型对比</h3>\n<p><strong>训练前</strong></p>\n<p><img alt=\"微调前\" class=\"lazyload\" /></p>\n<p><strong>训练后</strong></p>\n<p><img alt=\"微调后\" class=\"lazyload\" /></p>\n<p><strong>观察图片可以发现，微调后qwen2.5认为自己是陈千语，自己由管理员开发的</strong></p>\n<h1 id=\"大模型部署\">大模型部署</h1>\n<p><strong>下面不是新手向</strong><br />\n如果只是希望学习微调的在这里已经结束了，下面是本系列教程的后续，如何用<code>langchain</code>部署本地的LLM微调大语言模型</p>\n<h3 id=\"环境配置\">环境配置</h3>\n<p>安装需要的环境</p>\n<pre><code class=\"language-bash\">pip install peft langchain langchain-huggingface\n</code></pre>\n<h3 id=\"下面是样例代码\">下面是样例代码</h3>\n<p>代码流程如下：<br />\n<strong>加载基座模型-&gt;加载 LoRA 权重-&gt;正在合并权重-&gt;构建Langchain通道-&gt;调用模型</strong></p>\n<pre><code class=\"language-python\">import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom peft import PeftModel\nfrom langchain_huggingface import HuggingFacePipeline\nfrom langchain_core.prompts import PromptTemplate\n\nBASE_MODEL_PATH = r'D:\\Models\\Qwen2.5-1.5B-Instruct\\qwen\\Qwen2___5-1___5B-Instruct'\nLORA_PATH = r'D:\\D_MyProject\\LLM-Tuning\\LLaMA-Factory\\saves\\Qwen2.5-1.5B\\lora\\train_2026-02-13-23-16-50\\checkpoint-260'\n\nprint(\"1. 正在加载基座模型...\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_PATH,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",  \n    trust_remote_code=True\n)\n\nprint(\"2. 正在加载 LoRA 权重 ...\")\nmodel = PeftModel.from_pretrained(base_model, LORA_PATH)\n\nprint(\"3. 正在合并权重 ...\")\nmodel = model.merge_and_unload()\n\nprint(\"4. 构建 LangChain 管道...\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=200,    \n    do_sample=True,        \n    temperature=0.7,      \n    repetition_penalty=1.1 \n)\n\nllm = HuggingFacePipeline(pipeline=pipe)\n\nprint(\"\\n=== 陈千语上线 ===\\n\")\n\nrespone = llm.invoke('你好，你是谁？')\nprint(f\"{respone}\")\n</code></pre>\n<h3 id=\"演示效果\">演示效果</h3>\n<p><img alt=\"langchain演示效果\" class=\"lazyload\" /></p>\n<p><strong>至此，我们成功的实现了大模型LLM从微调到部署，把之前的langchain串起来...</strong></p>\n<p><strong>如果❤喜欢❤本系列教程，就点个关注吧，后续不定期更新~</strong></p>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/ClownLMe/\" target=\"_blank\">ClownLMe</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/ClownLMe/p/19615980\" target=\"_blank\">https://www.cnblogs.com/ClownLMe/p/19615980</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-14 15:40</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ClownLMe\">ClownLMe</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}