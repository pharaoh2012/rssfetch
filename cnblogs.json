{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "Python 高效实现 Excel 转 TXT 文本",
      "link": "https://www.cnblogs.com/jazz-z/p/19547652",
      "published": "",
      "description": "<div class=\"postcontent\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在数据处理场景中，将Excel文件转换为纯文本（TXT）格式便成为了一个常见的需求。传统的手动复制粘贴，不仅效率低下，更容易因格式不兼容、数据量庞大而引发错误。本文将解析如何通过 Spire.XLS for Python 实现 Excel 转 TXT 的高效、无依赖操作，提升数据处理效率和灵活性。</p>\n<blockquote>\n<p>安装指令：<code>pip install spire.xls​​</code><br />\n免费版：<code>​​pip install spire.xls.free​​ </code></p>\n</blockquote>\n<h2 id=\"基础示例单工作表-excel-转-txt\">基础示例：单工作表 Excel 转 TXT</h2>\n<p>以下是将一个 Excel 文件中的第一个工作表转换为 TXT 的完整步骤：</p>\n<h3 id=\"1-加载并读取excel文件\">1. 加载并读取Excel文件</h3>\n<pre><code class=\"language-python\">from spire.xls import *\nfrom spire.xls.common import *\n\nworkbook = Workbook()\nworkbook.LoadFromFile(\"示例.xlsx\")\n</code></pre>\n<h3 id=\"2-执行转换并保存\">2. 执行转换并保存</h3>\n<pre><code class=\"language-python\">sheet = workbook.Worksheets[0]\nsheet.SaveToFile(\"output.txt\", \"\\t\", Encoding.get_UTF8())\n</code></pre>\n<h3 id=\"参数说明\">参数说明</h3>\n<table>\n<thead>\n<tr>\n<th>参数类型</th>\n<th>示例值</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>分隔符</td>\n<td><code>\\t</code> (制表符)</td>\n<td>定义 TXT 文件中各列数据之间的分隔方式，也可使用逗号 <code>,</code>、分号 <code>;</code> 等。</td>\n</tr>\n<tr>\n<td>编码方式</td>\n<td><code>Encoding.get_UTF8()</code>（国际通用编码）</td>\n<td>指定文本文件的编码格式，推荐使用 <code>UTF-8</code> 以保证中文等字符正常显示。</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"进阶处理多工作表分别导出为-txt\">进阶处理：多工作表分别导出为 TXT</h2>\n<p>如果你的 Excel 文件包含多个工作表，并且希望将每个工作表保存为单独的 TXT 文件，可以使用以下代码：</p>\n<pre><code class=\"language-python\">from spire.xls import *\nfrom spire.xls.common import *\n\ndef excel_sheets_to_txt(input_file, output_folder, delimiter=\"\\t\"):\n  \n    # 创建Workbook对象\n    workbook = Workbook()\n    # 加载Excel文件\n    workbook.LoadFromFile(input_file)\n        \n    # 遍历所有工作表\n    for i in range(workbook.Worksheets.Count):\n        sheet = workbook.Worksheets[i]\n            \n        # 构建输出文件路径\n        output_file = f\"sheet_{i+1}_{sheet.Name}.txt\"\n        output_path = os.path.join(output_folder, output_file)\n            \n        # 将工作表内容保存为TXT文件\n        sheet.SaveToFile(output_path, delimiter, Encoding.get_UTF8())\n</code></pre>\n<h2 id=\"excel-转-txt-在自动化流程中的应用\">Excel 转 TXT 在自动化流程中的应用</h2>\n<p>将 Excel 转换为 TXT 不仅是格式的简单转换，更是实现数据自动化流程的重要环节。结合 Spire.XLS for Python，可轻松构建以下应用：</p>\n<ul>\n<li><strong>自动化报告生成</strong>： 从 Excel 模板中提取数据，生成纯文本格式的报告摘要，方便邮件发送或系统集成。</li>\n<li><strong>数据清洗与预处理</strong>： 将复杂格式的 Excel 数据转换为 TXT，作为其他数据分析工具（如Spark、Hadoop）的输入源，进行更深层次的处理。</li>\n<li><strong>配置管理</strong>： 将 Excel 中维护的系统配置、参数表等导出为 TXT 格式，便于脚本直接读取，同时更适合使用 Git 等工具进行版本管理。</li>\n</ul>\n<hr />\n<p>通过以上方法与示例，您可以快速掌握使用 Python 实现 Excel 到 TXT 的高效转换。该方案不仅提升了数据导出效率，也为后续的数据集成与自动化处理奠定了可靠基础。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"itemdesc\">\n                发表于 \n<span id=\"post-date\">2026-01-29 12:01</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jazz-z\">LAYONTHEGROUND</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n            </div>"
    },
    {
      "title": "前端倒计时活动，为什么不推荐直接用 setTimeout / setInterval？",
      "link": "https://www.cnblogs.com/zxlh1529/p/19545350",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/zxlh1529/p/19545350\" id=\"cb_post_title_url\" title=\"发布于 2026-01-29 11:44\">\n    <span>前端倒计时活动，为什么不推荐直接用 setTimeout / setInterval？</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在商城项目中，「倒计时活动」几乎是绕不开的需求：<br />\n秒杀、限时优惠、拼团、支付剩余时间……</p>\n<p>我相信很多都跟我一样一开始写出类似这样的代码：</p>\n<pre><code class=\"language-js\">setInterval(() =&gt; {\n  remainTime--\n}, 1000)\n</code></pre>\n<h2 id=\"功能能跑但线上问题也会跟着跑出来\"><strong>功能能跑，但线上问题也会跟着跑出来。</strong></h2>\n<h2 id=\"一为什么-settimeout--setinterval-不适合活动倒计时\">一、为什么 setTimeout / setInterval 不适合活动倒计时？</h2>\n<h3 id=\"它们天生就不准\">它们天生就不准</h3>\n<p>很多人对定时器有一个误解：</p>\n<blockquote>\n<p><code>setInterval(fn, 1000)</code> ≠ 每 1000ms 准时执行</p>\n</blockquote>\n<p>原因只有一个：<br />\n<strong>JavaScript 是单线程的。</strong></p>\n<p>到这你先回想一下事件循环机制，有哪些？然后再往下看；如果实在想不起来就先上网搜下，或者看看我的<a href=\"https://www.cnblogs.com/zxlh1529/p/18829778\" target=\"_blank\">单线程原理</a>。</p>\n<ul>\n<li>事件循环机制</li>\n<li>主线程被渲染卡住</li>\n<li>执行大量 JS</li>\n<li>GC、布局、重绘</li>\n</ul>\n<p>都会导致定时器回调被<strong>延后执行</strong>。</p>\n<p>倒计时的表现就是：</p>\n<ul>\n<li>跳秒</li>\n<li>变慢</li>\n<li>和真实时间对不上</li>\n</ul>\n<hr />\n<h3 id=\"浏览器会故意降级定时器\">浏览器会「故意」降级定时器</h3>\n<p>这是活动倒计时最容易翻车的一点。</p>\n<p>当页面进入以下状态：</p>\n<ul>\n<li>后台 Tab</li>\n<li>页面最小化</li>\n<li>手机锁屏</li>\n</ul>\n<p>浏览器会主动做这些事：</p>\n<ul>\n<li>延长定时器触发间隔</li>\n<li>甚至直接暂停执行</li>\n</ul>\n<p>结果就是：</p>\n<blockquote>\n<p>用户切个 Tab 回来，<br />\n倒计时还显示 10 秒，<br />\n实际活动已经结束。</p>\n</blockquote>\n<p><strong>对活动类业务，这是不能接受的。</strong></p>\n<hr />\n<h3 id=\"settimeout-递归本质问题没变\">setTimeout 递归，本质问题没变</h3>\n<p>可能有的会写成这样：</p>\n<pre><code class=\"language-js\">function tick() {\n  setTimeout(() =&gt; {\n    remainTime--\n    tick()\n  }, 1000)\n}\n</code></pre>\n<p>看起来比 <code>setInterval</code> 稳一点，但实际上：</p>\n<ul>\n<li>依然受线程影响</li>\n<li>依然受浏览器限流</li>\n<li>依然不可靠</li>\n</ul>\n<p>只是“写法高级了”，问题没解决。</p>\n<hr />\n<h2 id=\"二倒计时的核心思路必须反过来\">二、倒计时的核心思路必须反过来</h2>\n<h3 id=\"错误思路很多第一版代码\">错误思路（很多第一版代码）</h3>\n<blockquote>\n<p>“我现在有 60 秒，每秒减 1”</p>\n</blockquote>\n<h3 id=\"正确思路真实业务\">正确思路（真实业务）</h3>\n<blockquote>\n<p>“活动有一个<strong>确定的结束时间点</strong>，我只计算<strong>当前时间与结束时间的差值</strong>”</p>\n</blockquote>\n<hr />\n<h3 id=\"正确的倒计时模型\">正确的倒计时模型</h3>\n<ol>\n<li>后端返回活动结束时间戳（<code>endTime</code>）</li>\n<li>前端永远不存“剩余秒数”</li>\n<li>每次渲染时：</li>\n</ol>\n<pre><code class=\"language-js\">const remain = endTime - Date.now()\n</code></pre>\n<blockquote>\n<p>前提是： <code>Date.now()</code> 是可信的</p>\n</blockquote>\n<p>这样做的好处是：</p>\n<ul>\n<li>页面卡顿不影响</li>\n<li>切 Tab 不影响</li>\n<li>页面刷新不影响</li>\n<li>时间一定是真实世界的时间</li>\n</ul>\n<hr />\n<h2 id=\"三requestanimationframe-在倒计时里的正确用法\">三、requestAnimationFrame 在倒计时里的正确用法</h2>\n<p>那么有的就得来犟一下：</p>\n<blockquote>\n<p>那是不是可以用 requestAnimationFrame？</p>\n</blockquote>\n<h3 id=\"nonono是这样的\">nonono，是这样的</h3>\n<blockquote>\n<p><strong>requestAnimationFrame 适合“展示型倒计时”，不适合直接当计时器。</strong></p>\n</blockquote>\n<hr />\n<h3 id=\"为什么-raf-比-setinterval-好一点\">为什么 rAF 比 setInterval 好一点？</h3>\n<ul>\n<li>跟随浏览器刷新节奏（通常 60fps）</li>\n<li>页面不可见时自动暂停（省性能）</li>\n<li>不会出现多个定时器竞争</li>\n</ul>\n<p>但它的问题也很明显：</p>\n<ul>\n<li>后台直接停</li>\n<li>不保证时间间隔</li>\n<li>本质还是“帧驱动”，不是“时间驱动”</li>\n</ul>\n<hr />\n<h3 id=\"正确用法raf--时间戳差值\">正确用法：rAF + 时间戳差值</h3>\n<pre><code class=\"language-js\">function startCountdown(endTime, update) {\n  function loop() {\n    const remain = endTime - Date.now()\n\n    if (remain &lt;= 0) {\n      update(0)\n      return\n    }\n\n    update(remain)\n    requestAnimationFrame(loop)\n  }\n\n  loop()\n}\n</code></pre>\n<ol>\n<li>rAF 只负责触发更新</li>\n<li>时间完全由 <code>Date.now()</code> 决定</li>\n<li>不用 rAF 去「数秒」</li>\n</ol>\n<p>这种方式非常适合：</p>\n<ul>\n<li>大屏倒计时</li>\n<li>动画数字变化</li>\n<li>强 UI 表现的倒计时</li>\n</ul>\n<hr />\n<h2 id=\"四真实业务里的性能坑\">四、真实业务里的性能坑</h2>\n<h3 id=\"多个倒计时--多个定时器\">多个倒计时 = 多个定时器</h3>\n<p>列表页如果有 20 个活动：</p>\n<ul>\n<li>20 个 <code>setInterval</code></li>\n<li>页面性能直线下降</li>\n</ul>\n<h3 id=\"组件卸载忘记清理\">组件卸载忘记清理</h3>\n<ul>\n<li>内存泄漏</li>\n<li>幽灵定时器</li>\n<li>难以排查的线上问题</li>\n</ul>\n<h3 id=\"前后端时间不同步\">前后端时间不同步</h3>\n<ul>\n<li>前端显示没结束</li>\n<li>后端接口已判定结束</li>\n<li>用户点击直接报错</li>\n</ul>\n<hr />\n<h2 id=\"五重点来了第三方方案怎么选\">五、重点来了：第三方方案怎么选？</h2>\n<h3 id=\"dayjs--date-fns强烈推荐\">dayjs / date-fns（强烈推荐）</h3>\n<p>它们<strong>不是倒计时库</strong>，但非常适合做倒计时。</p>\n<p>示例（dayjs）：</p>\n<pre><code class=\"language-js\">import dayjs from 'dayjs'\n\nconst endTime = dayjs('2026-01-30 20:00:00')\n\nsetInterval(() =&gt; {\n  const diff = endTime.diff(dayjs(), 'second')\n  console.log(diff &gt; 0 ? diff : 0)\n}, 1000)\n</code></pre>\n<ol>\n<li>时间计算可靠</li>\n<li>不依赖定时器精度</li>\n<li>和后端时间模型一致</li>\n</ol>\n<p><strong>这是我线上最常用的方案之一。</strong></p>\n<hr />\n<h3 id=\"自己封装一个全局时间驱动器\">自己封装一个「全局时间驱动器」</h3>\n<p>这是很多成熟项目最终都会走到的一步。</p>\n<p>核心思想：</p>\n<ul>\n<li>全局只存在一个 timer / rAF</li>\n<li>所有倒计时组件订阅它</li>\n<li>统一调度、统一销毁</li>\n</ul>\n<p>简单示意：</p>\n<pre><code class=\"language-js\">const listeners = new Set()\n\nsetInterval(() =&gt; {\n  const now = Date.now()\n  listeners.forEach(fn =&gt; fn(now))\n}, 1000)\n\nexport function subscribe(fn) {\n  listeners.add(fn)\n  return () =&gt; listeners.delete(fn)\n}\n</code></pre>\n<p>组件只关心：</p>\n<pre><code class=\"language-js\">subscribe(now =&gt; {\n  remain.value = endTime - now\n})\n</code></pre>\n<ol>\n<li>性能稳定</li>\n<li>行为一致</li>\n<li>易维护</li>\n</ol>\n<hr />\n<h3 id=\"ui-倒计时组件慎用\">UI 倒计时组件（慎用）</h3>\n<p>很多组件库提供：</p>\n<ul>\n<li><code>&lt;Countdown /&gt;</code></li>\n<li><code>&lt;Timer /&gt;</code></li>\n</ul>\n<p>适合：</p>\n<ul>\n<li>展示</li>\n<li>Demo</li>\n<li>非关键业务</li>\n</ul>\n<p>不适合：</p>\n<ul>\n<li>活动判定</li>\n<li>支付</li>\n<li>风控相关逻辑</li>\n</ul>\n<p><strong>展示可以用，业务别依赖。</strong></p>\n<hr />\n<h2 id=\"六so\">六、SO</h2>\n<blockquote>\n<p><strong>倒计时不是在“数秒”，而是点对点的时间差。</strong></p>\n</blockquote>\n<ul>\n<li><code>setTimeout / setInterval</code><br />\n只能当“触发器”</li>\n<li><code>requestAnimationFrame</code><br />\n只负责“渲染节奏”</li>\n<li>真正的时间<br />\n永远来自<strong>时间戳差值</strong></li>\n</ul>\n<p>如果一个倒计时：</p>\n<ul>\n<li>切 Tab 就不准</li>\n<li>刷新就重置</li>\n<li>和后端状态对不上</li>\n</ul>\n<p>那它<strong>大概率不是 UI 问题，而是时间模型错了。</strong></p>\n<p>再记住三个原则：</p>\n<ol>\n<li>前端时间永远不能当权威</li>\n<li>用「服务端时间差」而不是本地时间（服务端多分布只允许一个地方定义）</li>\n<li>关键状态以接口返回为准</li>\n</ol>\n<p>为什么说这三个，大家可以好好思考下，评论区欢迎大家讨论！</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-29 11:44</span>&nbsp;\n<a href=\"https://www.cnblogs.com/zxlh1529\">幼儿园技术家</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "美的以OceanBase为基构建云中立数字化基座破局多云孤岛",
      "link": "https://www.cnblogs.com/OceanBase/p/19547433",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/OceanBase/p/19547433\" id=\"cb_post_title_url\" title=\"发布于 2026-01-29 11:30\">\n    <span>美的以OceanBase为基构建云中立数字化基座破局多云孤岛</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        美的面临传统数据中心技术栈陈旧、公有云成本高昂、多云孤岛等挑战，选择以 OceanBase 为关键支撑构建云中立的多云统一数字化底座。OceanBase 可以全面兼容原数据库，具备多租户能力，实行大集群管理模式。其成功承载核心业务，既解决多云管理难题，又实现降本节能与安全合规，为企业全球化提供坚实支撑。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p><strong>摘要：</strong></p>\n<p><em><strong>美的面临传统数据中心技术栈陈旧、公有云成本高昂、多云孤岛等挑战，选择以&nbsp;OceanBase&nbsp;为关键支撑构建云中立的多云统一数字化底座。OceanBase&nbsp;可以全面兼容原数据库，具备多租户能力，实行大集群管理模式。其成功承载核心业务，既解决多云管理难题，又实现降本节能与安全合规，为企业全球化提供坚实支撑。</strong></em></p>\n<p>&nbsp;</p>\n<p>美的集团是全球家电行业的领军企业。2024&nbsp;年，集团全年营收达&nbsp;4091&nbsp;亿元人民币，位列《财富》世界&nbsp;500&nbsp;强第&nbsp;246&nbsp;位。经过多年的发展，其业务版图早已超越家电，延伸至楼宇科技、工业技术、机器人与自动化、医疗健康及智慧物流等众多领域。美的不仅致力于提升其在国内市场的优势，同时也积极布局海外市场，推动国内外业务的均衡发展。</p>\n<p>&nbsp;</p>\n<p>为实现这一战略目标，美的积极推动数字化转型，并以&nbsp;OceanBase&nbsp;数据库为关键支撑，构建了一套云中立、多云统一的数字化底座，更以云原生、统一架构与安全合规为核心能力，为&nbsp;AI&nbsp;应用落地、全球业务敏捷拓展及系统持续演进奠定了坚实基础。</p>\n<p>&nbsp;</p>\n<h1><strong>当全球化业务遇上传统&nbsp;IT&nbsp;架构</strong></h1>\n<p>&nbsp;</p>\n<p>对于年营收超&nbsp;4000&nbsp;亿元且正处于全球化进程中的美的而言，一套既能灵活响应业务变化，又能实现统一管控的数字化系统至关重要。这正是刘向阳的核心任务之一。</p>\n<p>&nbsp;</p>\n<p>作为美的集团首席信息安全官（CISO）兼软件工程院院长，刘向阳同时负责集团数字化底座的整体建设与信息安全体系。然而，这条路并不好走。无论是传统的自建数据中心，还是直接采用公有云，都无法完全满足美的的复杂业务需求。</p>\n<p>&nbsp;</p>\n<p>刘向阳介绍，传统数据中心建设虽初期成本较低，但往往面临技术栈陈旧、产品组合杂乱、系统稳定性不足以及安全隐患较多等问题。而公有云虽具备弹性与灵活性，却同样存在成本高昂、多云环境互操作性差、数据孤岛以及运维复杂等挑战。</p>\n<p>&nbsp;</p>\n<p>“我们的核心数字化系统要部署在自建数据中心，海外业务则依托公有云作为基础设施。这自然带来了数据中心与多云管理的复杂性。”刘向阳解释道。</p>\n<p>&nbsp;</p>\n<p>美的没有二选一，而是选择了一条融合之路——构建云中立的多云统一数字化底座。该平台融合私有数据中心和公有云两者的优势，实现了技术体系的统一：它既可以部署在数据中心内，其技术栈与当前主流的公有云原生体系保持一致；同时，它也能直接部署在公有云上，并确保“云下云上”架构一致，升级时无需进行代码改造。</p>\n<p>&nbsp;</p>\n<p>“更重要的是，这是一个操作系统级别的统一底座。它实现了软硬件的解耦，能将分布在所有公有云及数据中心的资源整合成单一资源池，实现统一调度、管理与纳管，甚至提供统一的地址空间。对上层应用而言，仿佛始终运行在同一个数据中心内。”刘向阳强调。</p>\n<p>&nbsp;</p>\n<p>同时，美的的数字化底座是一个全功能的企业级云平台，不仅有交付资源的&nbsp;IaaS&nbsp;层、为应用提供支持的&nbsp;PaaS&nbsp;层，还有负责运维和安全的运维管理系统等。其中，数据库平台是核心模块，而&nbsp;OceanBase&nbsp;的引入极大提升了该平台的成熟度，并为后续各类应用与服务提供了强劲支撑。</p>\n<p>&nbsp;</p>\n<h1><strong>从传统集中式数据库到&nbsp;OceanBase&nbsp;的架构升级</strong></h1>\n<p>&nbsp;</p>\n<p>美的早期的应用系统主要基于传统集中式数据库开发。随着美的全面推进云化以及加强技术的自主掌控，近年来新增应用已经全部转向自主研发数据库。在此背景下，OceanBase&nbsp;进入了美的视野，成为数据库管理平台的核心组件，承接关键业务的数据负载。</p>\n<p>&nbsp;</p>\n<p>刘向阳表示，升级至&nbsp;OceanBase&nbsp;，除了国产升级这个大的行业背景外，更有现实需求：原来的&nbsp;IOE&nbsp;架构整体&nbsp;TCO&nbsp;高、扩展能力有上限，导致业务系统的处理逻辑复杂。而作为关键组件的传统集中式单机数据库面对互联网和&nbsp;5G&nbsp;时代下不断激增的用户数和并发量，难以满足未来业务弹性扩缩容。</p>\n<p>&nbsp;</p>\n<p>基于上述原因，美的决定对数据库系统进行升级，选择&nbsp;OceanBase&nbsp;来承担这一重任，主要基于以下考量：</p>\n<p>&nbsp;</p>\n<p><strong>首先，&nbsp;OceanBase&nbsp;全面兼容原数据库，</strong>功能完全覆盖原有功能点，并且提供迁移工具与解决方案，避免了上千万行代码重写，实现应用真正平滑升级，高效完成核心数据库升级改造；</p>\n<p>&nbsp;</p>\n<p><strong>其次，&nbsp;OceanBase&nbsp;具备多租户能力，</strong>支持动态弹性调整租户计算资源，能敏捷地应对业务负载变化，轻松应对年度“开门红”等业务高峰；</p>\n<p>&nbsp;</p>\n<p><strong>第三，&nbsp;OceanBase&nbsp;大集群管理模式，</strong>通过集中化管控、平台化管理，整合竖井式应用，避免集中式架构的资源浪费。同时，多个集群使用同一套&nbsp;OCP&nbsp;集群进行运维管控，提高了管理效率。</p>\n<p>&nbsp;</p>\n<p>此外，OceanBase&nbsp;的云中立特性也非常契合美的构建“云中立的多云统一数字化底座”这一目标。目前，OB Cloud&nbsp;已稳定运行于阿里云、华为云、腾讯云、百度智能云、AWS、Azure、GCP&nbsp;七大主流云基础设施，应用无需改造即可“一套架构、全球运行”。这一能力正成为美的出海的关键支撑，为其全球化布局提供了充分的灵活性。</p>\n<p>&nbsp;</p>\n<h1><strong>不止于降本，更是业务敏捷与安全的基石</strong></h1>\n<p>&nbsp;</p>\n<p>经过周密部署与精心筹备，OceanBase&nbsp;数据库已成功在美的集团落地，并顺利承载了工厂&nbsp;MES&nbsp;系统、供应链管理系统、中台系统等多个核心业务系统的运行。其稳定高效的表现，为美的集团带来了显著的商业价值。</p>\n<p>&nbsp;</p>\n<p><strong>1.成本节约。</strong>用普通&nbsp;PC&nbsp;服务器替代传统的小型机加集中式存储架构，大幅降低了硬件采购成本。同时，OceanBase&nbsp;数据库卓越的数据压缩技术将数据存储容量缩减了&nbsp;88%，进一步节省了存储成本，实现了软硬件成本的双重优化。</p>\n<p>&nbsp;</p>\n<p><strong>2.强大平台化管理功能。</strong>OceanBase&nbsp;数据库提供强大平台化管理功能，实现了应用的集中化管控、统一资源分配与高效运维，提升了管理效率和资源利用率。</p>\n<p>&nbsp;</p>\n<p><strong>3.&nbsp;高可用性与安全。</strong>OceanBase&nbsp;数据库实现了&nbsp;RPO=0&nbsp;的高级别跨城市容灾能力，能满足金融行业的严苛要求。同时，其全链路透明加密技术为数据提供了全方位的安全防护，有效防止了数据泄露风险，保障了企业数据资产的安全。</p>\n<p>&nbsp;</p>\n<p><strong>4.&nbsp;绿色节能。</strong>OceanBase&nbsp;数据库的应用还带来了显著的绿色节能效果。通过提高数据库服务器及存储机柜的数量利用率，降低了设备功率消耗，节约了大量电力资源，积极响应了国家“双碳”战略，为企业可持续发展贡献了力量。</p>\n<p>&nbsp;</p>\n<h1><strong>结语</strong></h1>\n<p>&nbsp;</p>\n<p>通过构建云中立、多云统一的数字化底座，并引入&nbsp;OceanBase&nbsp;作为核心数据库支撑，美的不仅实现了传统系统的平滑升级与全面云化，更在成本控制、弹性扩展、安全加固及全球业务响应速度上取得了显著提升。</p>\n<p>&nbsp;</p>\n<p>如今，这一数字化底座不仅服务于美的自身的数字化转型，也正通过商业化输出，为更多制造企业提供可借鉴的云原生、智能化、安全合规的数字化底座范本，助力更多企业在&nbsp;AI&nbsp;时代的竞争中构筑坚实底座。</p>\n<p>&nbsp;</p>\n<p><em>欢迎访问&nbsp;OceanBase&nbsp;官网获取更多信息：</em><a href=\"https://www.oceanbase.com/\" rel=\"noopener nofollow\"><em>https://www.oceanbase.com/</em></a></p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-29 11:30</span>&nbsp;\n<a href=\"https://www.cnblogs.com/OceanBase\">OceanBase数据库</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "istio初探以及解决http-426的问题",
      "link": "https://www.cnblogs.com/MrVolleyball/p/19547269",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/MrVolleyball/p/19547269\" id=\"cb_post_title_url\" title=\"发布于 2026-01-29 11:19\">\n    <span>istio初探以及解决http-426的问题</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"前言\">前言</h2>\n<p>在之前的文章中，我们花了大量的篇幅，从记录后端pod真实ip开始说起，然后引入envoy，再解决了各种各样的需求：配置自动重载、流量劫持、sidecar自动注入，到envoy的各种能力：熔断、流控、分流、透明代理、可观测性等等，已经可以支撑起一个完整的服务治理框架了</p>\n<p>而今天介绍的istio，正是前面提到的这些所有功能的集大成者，从本文开始，我们将详细介绍istio，并且与之前手搓的功能做一个详细的对比，为大家以后选择服务治理的某个功能提供参考</p>\n<h2 id=\"istio架构\">istio架构</h2>\n<pre><code>           ┌──────────────┐\n           │   istiod     │   ← 控制面\n           │ (Pilot+CA)   │\n           └──────┬───────┘\n                  │ xDS (gRPC / TLS)\n                  │\n┌────────────┐    │    ┌────────────┐\n│  Envoy     │◄───┼───►│   Envoy    │  ← 数据面\n│ (Sidecar)  │         │ (Sidecar)  │\n└─────▲──────┘         └─────▲──────┘\n      │ iptables             │\n      │                      │\n   App Pod                App Pod\n\n</code></pre>\n<ul>\n<li>数据面就是之前一直在研究的envoy，包括4/7代理、熔断、限流、可观测性等等，envoy就是执行由控制面下发的配置</li>\n<li>控制面istiod主要的职责：将配置下发到每一个envoy去。由于istio中配置以crd的形式成为了k8s的资源，所以要不断的监听k8s apiserver，将资源的变化翻译成envoy看得懂的配置，并且下发到envoy去</li>\n</ul>\n<p>至于其余istio的资源，我们后面详细介绍</p>\n<h2 id=\"istio安装\">istio安装</h2>\n<p>不说废话，先把istio安装上去再说</p>\n<p>首先准备好k8s集群，其次下载istio（这一步有可能需要上网）</p>\n<pre><code>curl -L https://istio.io/downloadIstio | sh -\ncd istio-*\nsudo ln -s $PWD/istioctl /usr/local/bin/istioctl\n</code></pre>\n<p>验证兼容性</p>\n<pre><code>istioctl x precheck\n</code></pre>\n<p>开始安装</p>\n<pre><code>istioctl install --set profile=default -y\n</code></pre>\n<p>由于镜像仓库没法直接使用，所以需要一些特殊的方法，具体可以看这篇文章： <a href=\"https://mp.weixin.qq.com/s/Vwesi9_8ThdODfkMi1033Q\" rel=\"noopener nofollow\" target=\"_blank\">快速拉取docker镜像</a></p>\n<p>需要的镜像有：</p>\n<pre><code>docker.io/istio/pilot:1.28.2\ndocker.io/istio/proxyv2:1.28.2\n</code></pre>\n<p>安装完成：</p>\n<pre><code>▶ kubectl -n istio-system get pod\nNAME                                    READY   STATUS    RESTARTS   AGE\nistio-ingressgateway-865c448856-qs8s2   1/1     Running   0          8s\nistiod-86c75775bb-j7qbg                 1/1     Running   0          12s\n\n</code></pre>\n<p>安装完成，要从哪儿开始呢？</p>\n<h2 id=\"istio的自动注入\">istio的自动注入</h2>\n<pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre>\n<p>同之前envoy一样，给namespace打上标签之后，重启服务即可</p>\n<pre><code>kubectl rollout restart deploy nginx-test\n</code></pre>\n<p>重启之后sidecar已经注入进去了，我们来观察一下istio注入到底做了什么事情</p>\n<p>先describe看看events</p>\n<pre><code>Events:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  8s    default-scheduler  Successfully assigned default/nginx-test-6f855b9bb9-9phsv to wilson\n  Normal  Pulled     8s    kubelet            Container image \"docker.io/istio/proxyv2:1.28.2\" already present on machine\n  Normal  Created    8s    kubelet            Created container: istio-init\n  Normal  Started    8s    kubelet            Started container istio-init\n  Normal  Pulled     8s    kubelet            Container image \"docker.io/istio/proxyv2:1.28.2\" already present on machine\n  Normal  Created    8s    kubelet            Created container: istio-proxy\n  Normal  Started    8s    kubelet            Started container istio-proxy\n  Normal  Pulled     6s    kubelet            Container image \"registry.cn-beijing.aliyuncs.com/wilsonchai/nginx:latest\" already present on machine\n  Normal  Created    6s    kubelet            Created container: nginx-test\n  Normal  Started    5s    kubelet            Started container nginx-test\n\n</code></pre>\n<p>1个initContainer，1个业务container和1个sidecar</p>\n<p>其中initContainer：</p>\n<pre><code>Init Containers:\n  istio-init:\n    Container ID:  containerd://2bf56cd37703d82a2a43e94e8c8d683ed66b0afe22bf7148a597d67b89a727a8\n    Image:         docker.io/istio/proxyv2:1.28.2\n    Image ID:      docker.m.daocloud.io/istio/proxyv2@sha256:39065152d6bd3e7fbf6bb04be43c7a8bbd16b5c7181c84e3d78fa164a945ae7f\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Args:\n      istio-iptables\n      -p\n      15001\n      -z\n      15006\n      -u\n      1337\n      -m\n      REDIRECT\n      -i\n      *\n      -x\n\n      -b\n      *\n      -d\n      15090,15021,15020\n      --log_output_level=default:info\n...\n</code></pre>\n<p>和之前envoy中劫持流量的做法一样，istio依然是使用iptables将端口流量导入到代理之中处理</p>\n<p>尝试访问一下：</p>\n<pre><code>▶ curl 10.22.12.178:30785/test\ni am backend in backend-6d76f54494-g6srz\n</code></pre>\n<p>成功，再次查看istio-proxy日志。空的？为了调试方便，将其打开并且输出至控制台</p>\n<pre><code>kubectl -n istio-system edit cm istio\n\napiVersion: v1\ndata:\n  mesh: |-\n    accessLogFile: /dev/stdout\n  ...\n</code></pre>\n<p>至此，istio的第一个功能探索完毕，自动注入sidecar container并且完成了流量劫持</p>\n<h2 id=\"upgrade-required-426-的问题\">Upgrade Required 426 的问题</h2>\n<p>当前的架构是左图，现在要前进到右图</p>\n<p><img alt=\"watermarked-istio_1\" class=\"lazyload\" /></p>\n<p>其实就是在backend注入istio-proxy，直接重启就好</p>\n<pre><code>▶ kubectl get pod -owide\nNAME                          READY   STATUS        RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES\nbackend-5d4d7b598c-f7852      2/2     Running       0          13s     10.244.0.49   wilson   &lt;none&gt;           &lt;none&gt;\nnginx-test-6f855b9bb9-9phsv   2/2     Running       0          58m     10.244.0.48   wilson   &lt;none&gt;           &lt;none&gt;\n\n</code></pre>\n<p>注入完成，测试一下</p>\n<pre><code>▶ curl 10.22.12.178:30785/test\nUpgrade Required\n</code></pre>\n<pre><code>▶ kubectl logs -f -l app=nginx-test -c istio-proxy\n[2026-01-26T07:54:42.977Z] \"GET /test HTTP/1.1\" 426 - upstream=10.244.0.48:80 duration=6ms route=default\n[2026-01-26T07:54:42.978Z] \"- - -\" 0 - upstream=10.105.148.194:10000 duration=9ms route=-\n\n</code></pre>\n<p>在nginx注入istio-proxy，backend没有注入的时候并没有报错。而一旦nginx与backend都注入的时候就会出现Upgrade Required (426)错误，Nginx Sidecar 发现目标（Backend）是一个纯文本服务，它会回退到“透明代理”模式，简单地把 Nginx 发出的流量透传出去</p>\n<p>Nginx Sidecar 发现目标也有 Sidecar，它会尝试建立一个高度优化的、基于 mTLS 的隧道（关于mTLS后面会详细介绍）。如果此时 Nginx 发出的请求头（比如缺少 Host 字段，或者使用了 HTTP/1.0）不符合 Envoy 对这种隧道<br />\n协议的预期，Envoy 可能会向 Nginx 发送一个特殊的响应，或者 Nginx 在尝试通过这种隧道通信时，因为某些 Header 冲突（如 Connection: close）自发产生了 426 错误</p>\n<p>想要解决这个问题有两种方法</p>\n<h4 id=\"改造nginx中加入标记\">改造nginx中加入标记</h4>\n<pre><code>        location /test {\n            proxy_http_version 1.1; # 必须添加这一行\n            proxy_set_header Host $host; # 这一行也是必须的\n            proxy_pass http://backend_ups;\n        }\n\n</code></pre>\n<p>Nginx 的 proxy_pass 默认使用 HTTP/1.0。在 Istio 环境中，HTTP/1.0 不支持长连接（Keep-Alive）以及一些现代的协议协商，这与 Istio Sidecar（Envoy）默认的 L7 代理行为冲突，Istio 需要 HTTP/1.1 来支持复杂连接管理问题</p>\n<h4 id=\"改造backend-service\">改造backend service</h4>\n<p>如果nginx改造有难度，那也可以尝试改造backend-service</p>\n<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\n  namespace: default\nspec:\n  ports:\n  - name: tcp-80 # 原为 http-80 改为 tcp-80\n    port: 10000\n    protocol: TCP\n    targetPort: 10000\n  selector:\n    app: backend\n\n</code></pre>\n<p>Istio 只有在识别到流量是 HTTP 时才会进行深度的协议检查和转换。如果你把这个服务声明为 TCP，Istio 就会将其视为原始字节流进行透传，不再关心它是 HTTP/1.0 还是 1.1。优点就是彻底解决 426 问题，无需改 Nginx。<br />\n缺点则是你会失去 Istio 针对该服务的 HTTP 监控指标（如请求数、4xx/5xx 统计）、分布式追踪以及基于路径的路由功能</p>\n<h2 id=\"http-10-与-http-11\">http 1.0 与 http 1.1</h2>\n<p>这里再简单介绍一下两个协议版本的区别</p>\n<ul>\n<li>\n<p>连接管理（最显著的区别）</p>\n<ul>\n<li>HTTP 1.0：短连接 (Short-lived)，默认情况下，客户端每发起一个请求，都要与服务器建立一次 TCP 三次握手。请求结束并收到响应后，TCP 连接立即关闭。如果页面有 10 张图片，浏览器就要建立 10 次 TCP 连接。这带来了极高的延迟和资源开销。</li>\n<li>HTTP 1.1：持久连接 (Persistent Connection / Keep-Alive)。默认开启 Connection: keep-alive。一个 TCP 连接可以被多个请求复用。只有在明确声明 Connection: close 或连接超时后才会关闭。</li>\n<li>在 Istio 中： Envoy 极度依赖持久连接来维持高性能的 Sidecar 间隧道。HTTP 1.0 的频繁断开会让 Envoy 感到“压力山大”，甚至认为这是一种非标准的协议行为。</li>\n</ul>\n</li>\n<li>\n<p>Host Header</p>\n<ul>\n<li>HTTP 1.0：人们认为一个 IP 对应一个网站，所以请求头里不需要带域名信息。</li>\n<li>HTTP 1.1：随着虚拟主机（一个 IP 跑多个网站）的流行，HTTP 1.1 规定请求头必须包含 Host 字段。</li>\n<li>在 K8s/Istio 中： Istio 的路由决策、Service 的匹配完全依赖 Host 头。这也是为什么 Nginx 使用 HTTP 1.0 转发时，如果不手动补全 Host 头，后端往往会返回 404 或协议错误。</li>\n</ul>\n</li>\n</ul>\n<p>以上是istio必须要求HTTP 1.1最主要的两个因素，当然还有其他非常重要的区别</p>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>HTTP 1.0</th>\n<th>HTTP 1.1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>连接模型</td>\n<td>默认短连接，每次请求新开 TCP</td>\n<td>默认持久连接 (Keep-Alive)，复用 TCP</td>\n</tr>\n<tr>\n<td>Host 头部</td>\n<td>可选 (导致无法支持虚拟主机)</td>\n<td>必须 (支持一 IP 多域名)</td>\n</tr>\n<tr>\n<td>流水线 (Pipelining)</td>\n<td>不支持</td>\n<td>支持 (但在实际应用中受限)</td>\n</tr>\n<tr>\n<td>断点续传</td>\n<td>不支持</td>\n<td>支持 (通过 Range 头部)</td>\n</tr>\n<tr>\n<td>缓存控制</td>\n<td>简单 (Expires)</td>\n<td>复杂且强大 (Cache-Control, ETag)</td>\n</tr>\n<tr>\n<td>默认协议版本</td>\n<td>许多旧软件(如 Nginx proxy)的默认值</td>\n<td>现代 Web 应用的基石标准</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"小结\">小结</h2>\n<p>本章内容算是一个开胃小菜，成功安装了istio，并且解决了一个非常常见的426问题，至于怎么把之前在envoy的那些最佳实践搬迁到istio，那就是后面的内容了，敬请期待</p>\n<h2 id=\"后记\">后记</h2>\n<p>如果整个namespace都已经有了注入标签<code>istio-injection=enabled</code>，但是某个deployment不想让istio注入</p>\n<pre><code>kubectl patch deployment nginx -p '{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"sidecar.istio.io/inject\":\"false\"}}}}}'\n</code></pre>\n<h2 id=\"联系我\">联系我</h2>\n<ul>\n<li>联系我，做深入的交流</li>\n</ul>\n<p><img alt=\"\" class=\"lazyload\" height=\"200\" width=\"500\" /></p>\n<hr />\n<p>至此，本文结束<br />\n在下才疏学浅，有撒汤漏水的，请各位不吝赐教...</p>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/MrVolleyball/\" target=\"_blank\">it排球君</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/MrVolleyball/p/19547269\" target=\"_blank\">https://www.cnblogs.com/MrVolleyball/p/19547269</a></p>\n<div>本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须在文章页面给出原文连接，否则保留追究法律责任的权利。 </div>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-29 11:19</span>&nbsp;\n<a href=\"https://www.cnblogs.com/MrVolleyball\">it排球君</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "StreamJsonRpc 在 HagiCode 中的深度集成与实践",
      "link": "https://www.cnblogs.com/newbe36524/p/19546448",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/newbe36524/p/19546448\" id=\"cb_post_title_url\" title=\"发布于 2026-01-29 08:46\">\n    <span>StreamJsonRpc 在 HagiCode 中的深度集成与实践</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"streamjsonrpc-在-hagicode-中的深度集成与实践\">StreamJsonRpc 在 HagiCode 中的深度集成与实践</h1>\n<blockquote>\n<p>本文详细介绍了 HagiCode（原 PCode）项目如何成功集成 Microsoft 的 StreamJsonRpc 通信库，以替换原有的自定义 JSON-RPC 实现，并解决了集成过程中的技术痛点与架构挑战。</p>\n</blockquote>\n\n<h2 id=\"背景\">背景</h2>\n<p>StreamJsonRpc 是微软官方维护的用于 .NET 和 TypeScript 的 JSON-RPC 通信库，以其强大的类型安全、自动代理生成和成熟的异常处理机制著称。在 HagiCode 项目中，为了通过 ACP (Agent Communication Protocol) 与外部 AI 工具（如 iflow CLI、OpenCode CLI）进行通信，并消除早期自定义 JSON-RPC 实现带来的维护成本和潜在 Bug，项目决定集成 StreamJsonRpc。然而，在集成过程中遇到了流式 JSON-RPC 特有的挑战，特别是在处理代理目标绑定和泛型参数识别时。</p>\n<p>为了解决这些痛点，我们做了一个大胆的决定：整个构建系统推倒重来。这个决定带来的变化，可能比你想象的还要大——稍后我会具体说。</p>\n<h2 id=\"关于-hagicode\">关于 HagiCode</h2>\n<blockquote>\n<p>先介绍一下本文的\"主角项目\"</p>\n</blockquote>\n<p>如果你在开发中遇到过这些烦恼：</p>\n<ul>\n<li>多项目、多技术栈，构建脚本维护成本高</li>\n<li>CI/CD 流水线配置繁琐，每次改都要查文档</li>\n<li>跨平台兼容性问题层出不穷</li>\n<li>想让 AI 帮忙写代码，但现有工具不够智能</li>\n</ul>\n<p>那么我们正在做的 HagiCode 可能你会感兴趣。</p>\n<p><strong>HagiCode 是什么？</strong></p>\n<ul>\n<li>一款 AI 驱动的代码智能助手</li>\n<li>支持多语言、跨平台的代码生成与优化</li>\n<li>内置游戏化机制，让编码不再枯燥</li>\n</ul>\n<p><strong>为什么在这里提它？</strong><br />\n本文分享的 StreamJsonRpc 集成方案，正是我们在开发 HagiCode 过程中实践总结出来的。如果你觉得这套工程化方案有价值，说明我们的技术品味还不错——那么 HagiCode 本身也值得关注一下。</p>\n<p><strong>想了解更多？</strong></p>\n<ul>\n<li>GitHub: <a href=\"https://github.com/HagiCode-org/site\" rel=\"noopener nofollow\" target=\"_blank\">github.com/HagiCode-org/site</a>（求 Star）</li>\n<li>官网: <a href=\"https://hagicode-org.github.io/site\" rel=\"noopener nofollow\" target=\"_blank\">hagicode-org.github.io/site</a></li>\n<li>视频演示: <a href=\"https://www.bilibili.com/video/BV1pirZBuEzq/\" rel=\"noopener nofollow\" target=\"_blank\">www.bilibili.com/video/BV1pirZBuEzq/</a>（30 分钟实战演示）</li>\n<li>安装指南: <a href=\"https://hagicode-org.github.io/site/docs/installation/docker-compose\" rel=\"noopener nofollow\" target=\"_blank\">hagicode-org.github.io/site/docs/installation/docker-compose</a></li>\n<li>公测已开始：现在安装即可参与公测</li>\n</ul>\n<h2 id=\"分析\">分析</h2>\n<p>当前项目处于 ACP 协议集成的关键阶段，面临着以下几个技术痛点和架构挑战：</p>\n<h3 id=\"1-自定义实现的局限\">1. 自定义实现的局限</h3>\n<p>原有的 JSON-RPC 实现位于 <code>src/HagiCode.ClaudeHelper/AcpImp/</code>，包含 <code>JsonRpcEndpoint</code> 和 <code>ClientSideConnection</code> 等组件。维护这套自定义代码成本高，且缺乏成熟库的高级功能（如进度报告、取消支持）。</p>\n<h3 id=\"2-streamjsonrpc-集成障碍\">2. StreamJsonRpc 集成障碍</h3>\n<p>在尝试将现有的 <code>CallbackProxyTarget</code> 模式迁移到 StreamJsonRpc 时，发现 <code>_rpc.AddLocalRpcTarget(target)</code> 方法无法识别通过代理模式创建的目标。具体表现为，StreamJsonRpc 无法自动将泛型类型 <code>T</code> 的属性拆分为 RPC 方法参数，导致服务器端无法正确处理客户端发起的方法调用。</p>\n<h3 id=\"3-架构分层混乱\">3. 架构分层混乱</h3>\n<p>现有的 <code>ClientSideConnection</code> 混合了传输层（WebSocket/Stdio）、协议层（JSON-RPC）和业务层（ACP Agent 接口），导致职责不清，且存在 <code>AcpAgentCallbackRpcAdapter</code> 方法绑定缺失的问题。</p>\n<h3 id=\"4-日志缺失\">4. 日志缺失</h3>\n<p>WebSocket 传输层缺少对原始 JSON 内容的日志输出，导致在调试 RPC 通信问题时难以定位是序列化问题还是网络问题。</p>\n<h2 id=\"解决\">解决</h2>\n<p>针对上述问题，我们采用了以下系统化的解决方案，从架构重构、库集成和调试增强三个维度进行优化：</p>\n<h3 id=\"1-全面迁移至-streamjsonrpc\">1. 全面迁移至 StreamJsonRpc</h3>\n<h4 id=\"移除旧代码\">移除旧代码</h4>\n<p>删除 <code>JsonRpcEndpoint.cs</code>、<code>AgentSideConnection.cs</code> 及相关的自定义序列化转换器（<code>JsonRpcMessageJsonConverter</code> 等）。</p>\n<h4 id=\"集成官方库\">集成官方库</h4>\n<p>引入 <code>StreamJsonRpc</code> NuGet 包，利用其 <code>JsonRpc</code> 类处理核心通信逻辑。</p>\n<h4 id=\"抽象传输层\">抽象传输层</h4>\n<p>定义 <code>IAcpTransport</code> 接口，统一处理 <code>WebSocket</code> 和 <code>Stdio</code> 两种传输模式，确保协议层与传输层解耦。</p>\n<pre><code class=\"language-csharp\">// IAcpTransport 接口定义\npublic interface IAcpTransport\n{\n    Task SendAsync(string message, CancellationToken cancellationToken = default);\n    Task&lt;string&gt; ReceiveAsync(CancellationToken cancellationToken = default);\n    Task CloseAsync(CancellationToken cancellationToken = default);\n}\n\n// WebSocket 传输实现\npublic class WebSocketTransport : IAcpTransport\n{\n    private readonly WebSocket _webSocket;\n\n    public WebSocketTransport(WebSocket webSocket)\n    {\n        _webSocket = webSocket;\n    }\n\n    // 实现发送和接收方法\n    // ...\n}\n\n// Stdio 传输实现\npublic class StdioTransport : IAcpTransport\n{\n    private readonly StreamReader _reader;\n    private readonly StreamWriter _writer;\n\n    public StdioTransport(StreamReader reader, StreamWriter writer)\n    {\n        _reader = reader;\n        _writer = writer;\n    }\n\n    // 实现发送和接收方法\n    // ...\n}\n</code></pre>\n<h3 id=\"2-修复代理目标识别问题\">2. 修复代理目标识别问题</h3>\n<h4 id=\"分析-callbackproxytarget\">分析 <code>CallbackProxyTarget</code></h4>\n<p>检查现有的动态代理生成逻辑，确定 StreamJsonRpc 无法识别的根本原因（通常是因为代理对象没有公开实际的方法签名，或者使用了 StreamJsonRpc 不支持的参数类型）。</p>\n<h4 id=\"重构参数传递\">重构参数传递</h4>\n<p>将泛型属性拆分为明确的 RPC 方法参数。不再依赖动态属性，而是定义具体的 Request/Response DTO（数据传输对象），确保 StreamJsonRpc 能通过反射正确识别方法签名。</p>\n<pre><code class=\"language-csharp\">// 原有的泛型属性方式\npublic class CallbackProxyTarget&lt;T&gt;\n{\n    public Func&lt;T, Task&gt; Callback { get; set; }\n}\n\n// 重构后的具体方法方式\npublic class ReadTextFileRequest\n{\n    public string FilePath { get; set; }\n}\n\npublic class ReadTextFileResponse\n{\n    public string Content { get; set; }\n}\n\npublic interface IAcpAgentCallback\n{\n    Task&lt;ReadTextFileResponse&gt; ReadTextFileAsync(ReadTextFileRequest request);\n    // 其他方法...\n}\n</code></pre>\n<h4 id=\"使用-attach-替代-addlocalrpctarget\">使用 <code>Attach</code> 替代 <code>AddLocalRpcTarget</code></h4>\n<p>在某些复杂场景下，手动代理 <code>JsonRpc</code> 对象并处理 <code>RpcConnection</code> 可能比直接添加目标更灵活。</p>\n<h3 id=\"3-实现方法绑定与日志增强\">3. 实现方法绑定与日志增强</h3>\n<h4 id=\"实现-acpagentcallbackrpcadapter\">实现 <code>AcpAgentCallbackRpcAdapter</code></h4>\n<p>确保该组件显式实现 StreamJsonRpc 的代理接口，将 ACP 协议定义的方法（如 <code>ReadTextFileAsync</code>）映射到 StreamJsonRpc 的回调处理器上。</p>\n<h4 id=\"集成日志记录\">集成日志记录</h4>\n<p>在 WebSocket 或 Stdio 的消息处理管道中，拦截并记录 JSON-RPC 请求和响应的原始文本。利用 <code>ILogger</code> 在解析前和序列化后输出原始 payload，以便排查格式错误。</p>\n<pre><code class=\"language-csharp\">// 日志增强的传输包装器\npublic class LoggingAcpTransport : IAcpTransport\n{\n    private readonly IAcpTransport _innerTransport;\n    private readonly ILogger&lt;LoggingAcpTransport&gt; _logger;\n\n    public LoggingAcpTransport(IAcpTransport innerTransport, ILogger&lt;LoggingAcpTransport&gt; logger)\n    {\n        _innerTransport = innerTransport;\n        _logger = logger;\n    }\n\n    public async Task SendAsync(string message, CancellationToken cancellationToken = default)\n    {\n        _logger.LogTrace(\"Sending message: {Message}\", message);\n        await _innerTransport.SendAsync(message, cancellationToken);\n    }\n\n    public async Task&lt;string&gt; ReceiveAsync(CancellationToken cancellationToken = default)\n    {\n        var message = await _innerTransport.ReceiveAsync(cancellationToken);\n        _logger.LogTrace(\"Received message: {Message}\", message);\n        return message;\n    }\n\n    public async Task CloseAsync(CancellationToken cancellationToken = default)\n    {\n        _logger.LogDebug(\"Closing connection\");\n        await _innerTransport.CloseAsync(cancellationToken);\n    }\n}\n</code></pre>\n<h3 id=\"4-架构分层重构\">4. 架构分层重构</h3>\n<h4 id=\"传输层-acprpcclient\">传输层 (<code>AcpRpcClient</code>)</h4>\n<p>封装 StreamJsonRpc 连接，负责 <code>InvokeAsync</code> 和连接生命周期管理。</p>\n<pre><code class=\"language-csharp\">public class AcpRpcClient : IDisposable\n{\n    private readonly JsonRpc _rpc;\n    private readonly IAcpTransport _transport;\n\n    public AcpRpcClient(IAcpTransport transport)\n    {\n        _transport = transport;\n        _rpc = new JsonRpc(new StreamRpcTransport(transport));\n        _rpc.StartListening();\n    }\n\n    public async Task&lt;TResponse&gt; InvokeAsync&lt;TResponse&gt;(string methodName, object parameters)\n    {\n        return await _rpc.InvokeAsync&lt;TResponse&gt;(methodName, parameters);\n    }\n\n    public void Dispose()\n    {\n        _rpc.Dispose();\n        _transport.Dispose();\n    }\n\n    // StreamRpcTransport 是对 IAcpTransport 的 StreamJsonRpc 适配器\n    private class StreamRpcTransport : IDuplexPipe\n    {\n        // 实现 IDuplexPipe 接口\n        // ...\n    }\n}\n</code></pre>\n<h4 id=\"协议层-iacpagentclient--iacpagentcallback\">协议层 (<code>IAcpAgentClient</code> / <code>IAcpAgentCallback</code>)</h4>\n<p>定义清晰的 client-to-agent 和 agent-to-client 接口，移除 <code>Func&lt;IAcpAgent, IAcpClient&gt;</code> 这种循环依赖的工厂模式，改用依赖注入或直接注册回调。</p>\n<h2 id=\"实践\">实践</h2>\n<p>基于 StreamJsonRpc 的最佳实践和项目经验，以下是实施过程中的关键建议：</p>\n<h3 id=\"1-强类型-dto-优于动态对象\">1. 强类型 DTO 优于动态对象</h3>\n<p>StreamJsonRpc 的核心优势在于强类型。不要使用 <code>dynamic</code> 或 <code>JObject</code> 传递参数。应为每个 RPC 方法定义明确的 C# POCO 类作为参数。这不仅解决了代理目标识别问题，还能在编译时发现类型错误。</p>\n<p>示例：将 <code>CallbackProxyTarget</code> 中的泛型属性替换为 <code>ReadTextFileRequest</code> 和 <code>WriteTextFileRequest</code> 等具体类。</p>\n<h3 id=\"2-显式声明-method-name\">2. 显式声明 Method Name</h3>\n<p>使用 <code>[JsonRpcMethod]</code> 特性显式指定 RPC 方法名称，不要依赖默认的方法名映射。这可以防止因命名风格差异（如 PascalCase vs camelCase）导致的调用失败。</p>\n<pre><code class=\"language-csharp\">public interface IAcpAgentCallback\n{\n    [JsonRpcMethod(\"readTextFile\")]\n    Task&lt;ReadTextFileResponse&gt; ReadTextFileAsync(ReadTextFileRequest request);\n    \n    [JsonRpcMethod(\"writeTextFile\")]\n    Task WriteTextFileAsync(WriteTextFileRequest request);\n}\n</code></pre>\n<h3 id=\"3-利用连接状态回调\">3. 利用连接状态回调</h3>\n<p>StreamJsonRpc 提供了 <code>JsonRpc.ConnectionLost</code> 事件。务必监听此事件以处理进程意外退出或网络断开的情况，这比单纯依赖 Orleans 的 Grain 失效检测更及时。</p>\n<pre><code class=\"language-csharp\">_rpc.ConnectionLost += (sender, e) =&gt;\n{\n    _logger.LogError(\"RPC connection lost: {Reason}\", e.ToString());\n    // 处理重连逻辑或通知用户\n};\n</code></pre>\n<h3 id=\"4-日志分层记录\">4. 日志分层记录</h3>\n<ul>\n<li><strong>Trace 级别</strong>：记录完整的 JSON Request/Response 原文。</li>\n<li><strong>Debug 级别</strong>：记录方法调用栈和参数摘要。</li>\n<li><strong>注意</strong>：确保日志中不包含敏感的 Authorization Token 或大文件内容的 Base64 编码。</li>\n</ul>\n<h3 id=\"5-处理流式传输的特殊性\">5. 处理流式传输的特殊性</h3>\n<p>StreamJsonRpc 原生支持 <code>IAsyncEnumerable</code>。在实现 ACP 的流式 Prompt 响应时，应直接使用 <code>IAsyncEnumerable</code> 而不是自定义的分页逻辑。这能极大简化流式处理的代码量。</p>\n<pre><code class=\"language-csharp\">public interface IAcpAgentCallback\n{\n    [JsonRpcMethod(\"streamText\")]\n    IAsyncEnumerable&lt;string&gt; StreamTextAsync(StreamTextRequest request);\n}\n</code></pre>\n<h3 id=\"6-适配器模式-adapter-pattern\">6. 适配器模式 (Adapter Pattern)</h3>\n<p>保持 <code>ACPSession</code> 和 <code>ClientSideConnection</code> 的分离。<code>ACPSession</code> 应专注于 Orleans 的状态管理和业务逻辑（如消息入队），通过组合而非继承的方式使用 StreamJsonRpc 连接对象。</p>\n<h2 id=\"总结\">总结</h2>\n<p>通过全面集成 StreamJsonRpc，HagiCode 项目成功解决了原自定义实现的维护成本高、功能局限性和架构分层混乱等问题。关键改进包括：</p>\n<ol>\n<li>采用强类型 DTO 替代动态属性，提高了代码的可维护性和可靠性</li>\n<li>实现了传输层抽象和协议层分离，提升了架构的清晰性</li>\n<li>增强了日志记录功能，便于排查通信问题</li>\n<li>引入了流式传输支持，简化了流式处理的实现</li>\n</ol>\n<p>这些改进为 HagiCode 提供了更稳定、更高效的通信基础，使其能够更好地与外部 AI 工具进行交互，并为未来的功能扩展奠定了坚实的基础。</p>\n<h2 id=\"参考资料\">参考资料</h2>\n<ul>\n<li>StreamJsonRpc 官方文档：<a href=\"https://learn.microsoft.com/en-us/dotnet/api/microsoft.visualstudio.threading.streamjsonrpc\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/dotnet/api/microsoft.visualstudio.threading.streamjsonrpc</a></li>\n<li>ACP (Agent Communication Protocol) 规范：<a href=\"https://github.com/microsoft/agentcommunicationprotocol\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/microsoft/agentcommunicationprotocol</a></li>\n<li>HagiCode 项目：<a href=\"https://github.com/HagiCode-org/site\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/HagiCode-org/site</a></li>\n<li>Orleans 官方文档：<a href=\"https://learn.microsoft.com/en-us/dotnet/orleans\" rel=\"noopener nofollow\" target=\"_blank\">https://learn.microsoft.com/en-us/dotnet/orleans</a></li>\n</ul>\n<hr />\n<p>如果本文对你有帮助：</p>\n<ul>\n<li>点个赞让更多人看到</li>\n<li>来 GitHub 给个 Star：<a href=\"https://github.com/HagiCode-org/site\" rel=\"noopener nofollow\" target=\"_blank\">github.com/HagiCode-org/site</a></li>\n<li>访问官网了解更多：<a href=\"https://hagicode-org.github.io/site\" rel=\"noopener nofollow\" target=\"_blank\">hagicode-org.github.io/site</a></li>\n<li>观看 30 分钟实战演示：<a href=\"https://www.bilibili.com/video/BV1pirZBuEzq/\" rel=\"noopener nofollow\" target=\"_blank\">www.bilibili.com/video/BV1pirZBuEzq/</a></li>\n<li>一键安装体验：<a href=\"https://hagicode-org.github.io/site/docs/installation/docker-compose\" rel=\"noopener nofollow\" target=\"_blank\">hagicode-org.github.io/site/docs/installation/docker-compose</a></li>\n<li>公测已开始，欢迎安装体验</li>\n</ul>\n<hr />\n<p>感谢您的阅读,如果您觉得本文有用,快点击下方点赞按钮👍,让更多的人看到本文。</p>\n<p>本内容采用人工智能辅助协作,经本人审核,符合本人观点与立场。</p>\n<ul>\n<li><strong>本文作者:</strong> <a href=\"https://www.newbe.pro\" rel=\"noopener nofollow\" target=\"_blank\">newbe36524</a></li>\n<li><strong>本文链接:</strong> <a href=\"https://hagicode-org.github.io/site/blog/2026/01/28/-streamjsonrpc-integration-in-hagicode-\" rel=\"noopener nofollow\" target=\"_blank\">https://hagicode-org.github.io/site/blog/2026/01/28/-streamjsonrpc-integration-in-hagicode-</a></li>\n<li><strong>版权声明:</strong> 本博客所有文章除特别声明外,均采用 BY-NC-SA 许可协议。转载请注明出处!</li>\n</ul>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-29 08:46</span>&nbsp;\n<a href=\"https://www.cnblogs.com/newbe36524\">Newbe36524</a>&nbsp;\n阅读(<span id=\"post_view_count\">62</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "FastAPI日志实战：从踩坑到优雅配置，让你的应用会“说话”",
      "link": "https://www.cnblogs.com/ymtianyu/p/19546427",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ymtianyu/p/19546427\" id=\"cb_post_title_url\" title=\"发布于 2026-01-29 08:36\">\n    <span>FastAPI日志实战：从踩坑到优雅配置，让你的应用会“说话”</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        本文分享FastAPI日志的实战配置经验。从基础概念讲起，提供可复用的日志配置代码，详解如何设置多级别、分文件、防覆盖的日志系统。重点剖析异步日志阻塞、敏感信息泄露、日志文件膨胀等常见坑点，并给出结构化日志、请求ID追踪等进阶优化建议，帮助开发者构建清晰、可靠、便于排查问题的应用日志体系。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>你的FastAPI服务跑得好好的，直到某天凌晨两点，它突然“自闭”了——没有崩溃日志，没有错误追踪，只有用户的投诉和你的满屏问号。🎯</p>\n<p>别问我怎么知道的，这坑我踩过，而且<strong style=\"color: rgba(186, 55, 42, 1);\">团队里超过60%的FastAPI初级部署都曾因为日志配置不当，在故障排查时抓瞎</strong>。今天，咱就好好聊聊日志这回事，它不是你代码里可有可无的<code style=\"color: rgba(186, 55, 42, 1);\">print</code>，而是你在数字世界安插的“耳目”。</p>\n<div style=\"background-color: rgba(248, 249, 250, 1); padding: 15px; margin: 20px 0;\">\n<p><strong>📋 本文能帮你：</strong></p>\n<div>\n<p>🔹 理解Python标准<code style=\"color: rgba(186, 55, 42, 1);\">logging</code>与FastAPI的协作原理</p>\n<p>🔹 完成一份开箱即用、结构清晰的日志配置</p>\n<p>🔹 避开输出混乱、性能拖累、日志丢失等经典大坑</p>\n<p>🔹 获得让日志真正为运维和调试服务的进阶思路</p>\n</div>\n</div>\n<h2>🎯 第一部分：别把日志当“事后烟”——它该是你的黑匣子</h2>\n<p>刚写FastAPI那会儿，我也觉得日志嘛，不就是<code style=\"color: rgba(186, 55, 42, 1);\">print(“Here!\")</code>的高级版？直到线上出了个诡异的偶发性400错误，翻遍代码一无所获，才彻底醒悟。</p>\n<p><strong style=\"color: rgba(186, 55, 42, 1);\">日志系统的核心价值，是在你无法“现场调试”的生产环境里，还原事故现场。</strong>它得告诉你：谁（IP/用户）、在什么时候、请求了什么、内部经过了哪些步骤、最终为什么失败。</p>\n<p>FastAPI本身不造轮子，它完美集成Python标准的<code style=\"color: rgba(186, 55, 42, 1);\">logging</code>模块。你的任务，就是用好这个强大的原生工具，而不是东一榔头西一棒子地乱打<code style=\"color: rgba(186, 55, 42, 1);\">print</code>。</p>\n<h2>🔧 第二部分：核心四步走，配置一个“会思考”的日志系统</h2>\n<p>好，咱们先来捋清思路。一个健壮的日志配置，通常围绕这四个问题展开：</p>\n<div>\n<h3><strong>1. 日志记到哪里？</strong> (Handlers)</h3>\n<p><strong>- 控制台：</strong>开发调试看一眼。</p>\n<p><strong>- 文件：</strong>长期保存，便于追溯。这里<strong style=\"color: rgba(186, 55, 42, 1);\">千万别学我当初偷懒只用单个文件</strong>，否则文件体积爆炸，打开都费劲。</p>\n<p><strong>- 网络/第三方服务：</strong>如Logstash, Sentry，用于集中式日志管理。</p>\n<h3><strong>2. 记录什么级别？</strong> (Levels)</h3>\n<p><span style=\"color: rgba(186, 55, 42, 1);\"><strong>DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL</strong></span>。简单说，<code style=\"color: rgba(186, 55, 42, 1);\">INFO</code>记录常规流程，<code style=\"color: rgba(186, 55, 42, 1);\">ERROR</code>记录错误异常。生产环境通常从<code style=\"color: rgba(186, 55, 42, 1);\">INFO</code>起记。</p>\n<h3><strong>3. 记录成什么格式？</strong> (Formatters)</h3>\n<p>时间、级别、模块、行号、消息……一个都不能少。格式清晰，查起来才快。</p>\n<h3><strong>4. 各个模块怎么控制？</strong> (Loggers)</h3>\n<p>你可以给FastAPI核心、SQLAlchemy、你自己的业务模块设置不同的记录级别和输出目的地，非常灵活。</p>\n</div>\n<h2>🚀 第三部分：实战！给你一份能直接“抄作业”的配置</h2>\n<p>接下来重点来了，上代码。我将一个项目中的精华配置拆解给你看。把它放在你的配置文件（如<code style=\"color: rgba(186, 55, 42, 1);\">log_config.py</code>）里。</p>\n<pre class=\"language-python highlighter-hljs\"><code>import logging\nimport logging.handlers\nfrom pathlib import Path\n\n# 1. 创建logs目录\nLOG_DIR = Path(__file__).parent.parent / \"logs\"\nLOG_DIR.mkdir(exist_ok=True)\n\n# 2. 定义格式\nDETAIL_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'\nSIMPLE_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'\n\n# 3. 配置Logger\ndef setup_logging():\n    # 根日志记录器\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)  # 全局最低级别\n\n    # 清除可能已有的处理器，防止重复（Jupyter等环境需要）\n    if logger.handlers:\n        logger.handlers.clear()\n\n    # ---- 控制台处理器 (开发时看) ----\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG)  # 控制台可以看更细\n    console_formatter = logging.Formatter(SIMPLE_FORMAT)\n    console_handler.setFormatter(console_formatter)\n    logger.addHandler(console_handler)\n\n    # ---- 文件处理器 (按天轮转，避免单个文件过大) ----\n    # 这是关键！用RotatingFileHandler或TimedRotatingFileHandler\n    file_handler = logging.handlers.TimedRotatingFileHandler(\n        filename=LOG_DIR / \"app.log\",\n        when=\"midnight\",  # 每天午夜轮转\n        interval=1,\n        backupCount=30,   # 保留最近30天\n        encoding=\"utf-8\"\n    )\n    file_handler.setLevel(logging.INFO)\n    file_formatter = logging.Formatter(DETAIL_FORMAT)  # 文件里记详细点\n    file_handler.setFormatter(file_formatter)\n    logger.addHandler(file_handler)\n\n    # ---- 错误日志单独文件 ----\n    error_file_handler = logging.handlers.RotatingFileHandler(\n        filename=LOG_DIR / \"error.log\",\n        maxBytes=10 * 1024 * 1024,  # 10MB\n        backupCount=5,\n        encoding=\"utf-8\"\n    )\n    error_file_handler.setLevel(logging.ERROR)  # 只记录ERROR及以上\n    error_file_handler.setFormatter(logging.Formatter(DETAIL_FORMAT))\n    logger.addHandler(error_file_handler)\n\n    # 4. 控制第三方库的日志噪音（比如uvicorn访问日志太吵）\n    # 官方文档虽然没强调，但根据线上经验，适当调整更清净\n    logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n    # 如果你用了SQLAlchemy，也可以这样控制SQL日志\n    # logging.getLogger(\"sqlalchemy.engine\").setLevel(logging.WARNING)\n\n    # 最后，记录一条日志表示配置完成\n    logger.info(\"日志系统初始化完成！\")\n\nif __name__ == \"__main__\":\n    setup_logging()    </code></pre>\n<p>然后，在你的FastAPI应用主文件（如<code style=\"color: rgba(186, 55, 42, 1);\">main.py</code>）开头导入并调用：</p>\n<pre class=\"language-python highlighter-hljs\"><code>from fastapi import FastAPI\nimport log_config\nimport logging\n\nlog_config.setup_logging()  # 最先初始化！\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    # 在视图里愉快地记录日志吧\n    log = logging.getLogger(__name__)  # 推荐用`__name__`获取logger\n    log.info(\"有人访问了根路径！\")\n    return {\"message\": \"Hello World\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app=app)</code></pre>\n<h2>⚠️ 第四部分：容易翻车的点 &amp; 进阶思考</h2>\n<h3><strong style=\"color: rgba(186, 55, 42, 1);\">坑1：异步代码中的日志阻塞。</strong></h3>\n<p>标准<code style=\"color: rgba(186, 55, 42, 1);\">logging</code>是同步的，如果在大量异步任务中疯狂写日志，可能会拖慢整体性能。对于超高并发场景，考虑使用像<code style=\"color: rgba(186, 55, 42, 1);\">structlog</code>+异步处理器，或者将日志先放入内存队列异步写入。</p>\n<h3><strong style=\"color: rgba(186, 55, 42, 1);\">坑2：日志格式包含敏感信息。</strong></h3>\n<p>千万注意！不要在日志里记录密码、完整Token、身份证号。可以在Formatter里做过滤，或者覆写<code style=\"color: rgba(186, 55, 42, 1);\">LogRecord</code>来清洗数据。</p>\n<h3><strong style=\"color: rgba(186, 55, 42, 1);\">坑3：过度日志导致磁盘爆炸。</strong></h3>\n<p>一定要用<code style=\"color: rgba(186, 55, 42, 1);\">RotatingFileHandler</code>或<code style=\"color: rgba(186, 55, 42, 1);\">TimedRotatingFileHandler</code>！并设置合理的<code style=\"color: rgba(186, 55, 42, 1);\">maxBytes</code>和<code style=\"color: rgba(186, 55, 42, 1);\">backupCount</code>。</p>\n<h3><strong>🎯 进阶一下：</strong></h3>\n<div>\n<p>🔹 <strong>结构化日志：</strong> 别再只输出纯文本了。输出JSON格式，方便后续用ELK（Elasticsearch, Logstash, Kibana）或Loki等工具进行检索和分析。一条好的日志应该是一个结构化的数据对象。</p>\n<p>🔹 <strong>请求ID贯穿：</strong> 为每个 incoming request 生成一个唯一ID，并让它出现在这个请求链路的所有相关日志里。这是分布式系统排查问题的“黄金线索”。可以通过中间件实现。</p>\n<p>🔹 <strong>与监控告警联动：</strong> 当出现<code style=\"color: rgba(186, 55, 42, 1);\">ERROR</code>或更高级别日志时，自动触发告警通知（发短信、发钉钉/企微）。让日志系统从“记录仪”变成“预警机”。</p>\n</div>\n<hr />\n<p>好了，关于FastAPI日志的实战心得，就先聊这么多。工具的选择，好比选螺丝刀，不是最贵的就好，而是<strong style=\"color: rgba(186, 55, 42, 1);\">最适合你当前项目阶段和团队习惯的</strong>。先从一份清晰的配置开始，让你们的应用“会说话”。</p>\n<p>如果你在配置过程中又遇到了新的妖孽问题，或者有更妙的日志实践，欢迎随时来聊聊。技术人的成长，不就是填完自己的坑，再看看别人的坑，最后一起把路铺平嘛。</p>\n<p>你的朋友一名程序媛，下次见……</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-29 08:36</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ymtianyu\">一名程序媛呀</a>&nbsp;\n阅读(<span id=\"post_view_count\">58</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "零门槛部署本地 AI 助手：Clawdbot/Meltbot 部署深度保姆级教程",
      "link": "https://www.cnblogs.com/ChenAI-TGF/p/19545952",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ChenAI-TGF/p/19545952\" id=\"cb_post_title_url\" title=\"发布于 2026-01-29 00:14\">\n    <span>零门槛部署本地 AI 助手：Clawdbot/Meltbot 部署深度保姆级教程</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        Clawdbot是一个多功能智能体（Agent），具备文件操作、代码执行、联网搜索等能力。本文详细介绍了其安装配置流程： 环境准备：全新安装Node.js（v22+/v24+）或彻底卸载旧版后安装新版，需确保环境变量配置正确； 权限设置：在PowerShell中解锁脚本执行权限； 一键安装：通过官方脚本自动部署主程序； 初始化向导：选择QuickStart模式，配置基础技能（Skills）和API（如Qwen或OpenAI），暂跳过高级选项。 完成上述步骤后即可启动Clawdbot，后续可扩展远程控制等功能\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<hr />\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<h1 id=\"前言为什么选择-clawdbot-moltbot\">前言：为什么选择 Clawdbot (Moltbot)？</h1>\n<p>Clawdbot 不仅仅是一个聊天框。它是一个 <strong>智能体（Agent）</strong>，意味着它有“手”和“脚”：</p>\n<ul>\n<li><strong>手</strong>：它可以读写你电脑上的文件、执行代码、操控命令行。</li>\n<li><strong>脚</strong>：它可以联网搜索、访问 Google、分析网页。</li>\n<li><strong>大脑</strong>：你可以接入云端的 <strong>API</strong>，也可以利用自己的 <strong>GPU</strong> 运行本地模型。</li>\n</ul>\n<hr />\n<h1 id=\"第一阶段基建工程环境准备\">第一阶段：基建工程（环境准备）</h1>\n<h2 id=\"11-解决-nodejs-安装与版本问题\">1.1 解决 Node.js 安装与版本问题</h2>\n<h3 id=\"111全新安装nodejs电脑未安装过nodejs时\">1.1.1全新安装Node.js（电脑未安装过Node.js时）</h3>\n<p>如果你的电脑上从来没装过Node.js，无需执行卸载、删残留的步骤，直接按以下流程全新安装即可，步骤适配Windows系统，新手友好、全程默认下一步即可。</p>\n<p><strong>1. 下载适配的Node.js安装包</strong></p>\n<ol>\n<li>\n<p>打开<a href=\"https://nodejs.org/\" rel=\"noopener nofollow\" target=\"_blank\">Node.js 官方官网</a>，官网页面会自动识别你的系统（Windows）；<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p>点击下载<strong>Windows Installer (64-bit)</strong> 格式的安装包（.msi后缀，64位是目前Windows电脑的主流，无需选32位）；</p>\n<blockquote>\n<p>小贴士：下载时建议保存到桌面/下载文件夹，方便找到安装包。</p>\n</blockquote>\n</li>\n</ol>\n<p><strong>2. 安装Node.js（全程新手友好，默认下一步即可）</strong></p>\n<ol>\n<li>双击刚下载的.msi安装包，弹出安装向导，点击<strong>Next</strong>；</li>\n<li>勾选同意协议（I accept the terms in the License Agreement），点击<strong>Next</strong>；</li>\n<li><strong>关键点1</strong>：确认安装路径（默认是<code>C:\\Program Files\\nodejs\\</code>，新手<strong>不要修改</strong>，避免后续环境变量出问题），直接<strong>Next</strong>；</li>\n<li><strong>关键点2</strong>：进入「Custom Setup」自定义安装页，<strong>所有选项默认勾选即可</strong>（重点确认<code>Add Node.js to PATH</code>已勾选，这一步会自动把Node.js加入系统环境变量，不用手动配置，是最关键的一步），直接<strong>Next</strong>；</li>\n<li>后续页面无需修改任何高级选项，一直点击<strong>Next</strong>，最后点击<strong>Install</strong>开始安装，等待10-30秒（安装速度看电脑配置）；</li>\n<li>安装完成后，点击<strong>Finish</strong>关闭向导即可。</li>\n</ol>\n<p><strong>3. 版本验证</strong><br />\n<strong>必须重启终端</strong>（关闭所有已打开的PowerShell/CMD，重新打开），否则系统识别不到新安装的Node.js！</p>\n<ol>\n<li>按下<code>Win+R</code>，输入<code>PowerShell</code>，打开普通权限的PowerShell即可；</li>\n<li>输入验证命令：<pre><code class=\"language-powershell\">node -v\n</code></pre>\n</li>\n<li>回车后，若显示<code>v22.x.x</code>或<code>v24.x.x</code>（比如v22.10.0、v24.4.0），说明安装成功且版本符合要求；\n<blockquote>\n<p>可选验证：输入<code>npm -v</code>，会显示配套的npm版本（Node.js安装包会自动附带npm，无需单独安装），能正常输出版本即代表环境变量配置无误。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</blockquote>\n</li>\n</ol>\n<hr />\n<h3 id=\"112卸载旧版nodejs-安装新版电脑安装过nodejs但是版本不够新时\">1.1.2卸载旧版Node.js 安装新版（电脑安装过Node.js但是版本不够新时）</h3>\n<p>如果版本不够新，执行安装命令的时候程序会尝试安装新版的Node.js，但是大概率会失败，还是手动比较好，失败的话会显示以下界面：<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n<strong>操作：</strong><br />\n1.可以直接通过新的安装包对上一个版本的Node.js进行remove<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<ol start=\"2\">\n<li>去“控制面板”卸载旧的 Node.js。</li>\n<li><strong>关键点：</strong> 手动进入 <code>C:\\Program Files\\nodejs</code> 文件夹，把残留的文件全部删干净！</li>\n<li>前往 <a href=\"https://nodejs.org/\" rel=\"noopener nofollow\" target=\"_blank\">Node.js 官网</a> 下载最新的 <strong>v22 或 v24</strong> 稳定版并安装。</li>\n<li><strong>清楚之前的旧的环境变量</strong>（这一步非常重要！）</li>\n</ol>\n<ul>\n<li><strong>验证：</strong> 重新打开 PowerShell，输入 <code>node -v</code>。看到显示 <code>v22.x</code> 或 <code>v24.x</code> 才算过关。</li>\n</ul>\n<ol start=\"6\">\n<li>版本验证（和卸载旧版后的验证步骤一致）<br />\n<strong>必须重启终端</strong>（关闭所有已打开的PowerShell/CMD，重新打开），否则系统识别不到新安装的Node.js！</li>\n<li>按下<code>Win+R</code>，输入<code>PowerShell</code>，打开普通权限的PowerShell即可；</li>\n<li>输入验证命令：<pre><code class=\"language-powershell\">node -v\n</code></pre>\n</li>\n<li>回车后，若显示<code>v22.x.x</code>或<code>v24.x.x</code>（比如v22.10.0、v24.4.0），说明安装成功且版本符合要求；\n<blockquote>\n<p>可选验证：输入<code>npm -v</code>，会显示配套的npm版本（Node.js安装包会自动附带npm，无需单独安装），能正常输出版本即代表环境变量配置无误。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"12-解锁脚本执行权限\">1.2 解锁脚本执行权限</h2>\n<ul>\n<li><strong>问题：</strong> Windows 默认禁止运行脚本，会导致安装指令失效。</li>\n<li><strong>操作：</strong> 以管理员身份运行 PowerShell，执行：<pre><code class=\"language-powershell\">Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n</code></pre>\n</li>\n</ul>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<p><strong>完成以上步骤后，你的Node.js环境就完全满足Clawdbot的要求了，可以直接进入后续的项目安装环节。</strong></p>\n<hr />\n<h1 id=\"第二阶段正式安装与初始化\">第二阶段：正式安装与初始化</h1>\n<h2 id=\"21-执行一键安装\">2.1 执行一键安装</h2>\n<ul>\n<li><strong>操作：</strong> 在 PowerShell 输入：<pre><code class=\"language-powershell\">iwr -useb https://clawd.bot/install.ps1 | iex\n</code></pre>\n</li>\n<li><strong>作用：</strong> 这行代码会自动下载主程序并配置环境变量。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n这个会持续一会才有进展，不要担心，如果太久可以按按回车看看是不是powershell刷新的问题<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n这个会持续一段时间才有进展，不要担心，</li>\n</ul>\n<h2 id=\"22-运行向导-onboarding\">2.2 运行向导 (Onboarding)</h2>\n<ul>\n<li><strong>操作：</strong> 输入 <code>moltbot onboard</code>。</li>\n<li><strong>详细选项说明：</strong>\n<ol>\n<li>\n<p><strong>Mode</strong>：选 <code>QuickStart</code>。</p>\n</li>\n<li>\n<p><strong>Provider (大脑)</strong>：建议选 <code>Qwen</code>。</p>\n<ul>\n<li><strong>注意：</strong> 此时会跳出网页让你授权，登录或者注册Qwen的账号既可。这步是为了先让机器人有个“临时大脑”跑起来。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n选择OpenAI Chat，拿到Key之后输入到界面中给Clawdbot。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n选择这个直接按回车即可。</li>\n</ul>\n</li>\n<li>\n<p><strong>Channels (远程控制)</strong>：选 <code>Skip for now</code>。我们先在本地跑通，以后再连 Telegram。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p><strong>Skills (技能)</strong>：选 <code>Yes</code>。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" />这些是最基本的Skill选择，选择Yes就好</p>\n</li>\n<li>\n<p><strong>Skill install</strong>：选 <code>npm</code>。</p>\n</li>\n</ol>\n</li>\n</ul>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<ol start=\"5\">\n<li><strong>Skill Dependencies</strong>：选 <code>Skip for now</code>。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n这些是扩展的，目前配置比较麻烦，可以后续再配置<br />\n6. 一些其他配置<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n选择No，尽量都留到后面再配置<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></li>\n</ol>\n<h2 id=\"23-成功界面\">2.3 成功界面</h2>\n<p><img alt=\"在这里插入图片描述\" class=\"lazyload\" />、<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n<hr />\n<h1 id=\"第三阶段破解命令找不到与网关未授权\">第三阶段：破解“命令找不到”与“网关未授权”</h1>\n<h2 id=\"31-解决-moltbot-命令失效\">3.1 解决 moltbot 命令失效</h2>\n<ul>\n<li>\n<p><strong>现象：</strong> 安装完输入 <code>moltbot open</code> 提示“无法识别”。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p><strong>原因：</strong> 环境变量未刷新。</p>\n</li>\n<li>\n<p><strong>对策：</strong> 重启 PowerShell 窗口。如果还不行，执行npx moltbot open。</p>\n</li>\n</ul>\n<h2 id=\"32-提取身份令牌-token-登录网页\">3.2 提取身份令牌 (Token) 登录网页</h2>\n<ul>\n<li>\n<p><strong>现象：</strong> 访问 <code>http://localhost:18789</code> 显示“未授权：网关令牌缺失”。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p><strong>操作：</strong></p>\n<ol>\n<li>\n<p>打开文件夹 <code>C:\\Users\\你的用户名\\.clawdbot</code>。</p>\n</li>\n<li>\n<p>右键点击 <code>clawdbot.json</code>，用记事本打开。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p>找到 <code>\"token\": \"xxxxxxx\"</code> 这一行，复制那串长代码。<br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p>回到浏览器，点击右上角红色状态，把 Token 贴进去。<strong>一旦变绿，恭喜你，你的助理正式上线了！</strong><br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /></p>\n</li>\n</ol>\n</li>\n</ul>\n<hr />\n<h1 id=\"第四阶段配置各种api\">第四阶段：配置各种API</h1>\n<ul>\n<li><strong>操作步骤：</strong>\n<ol>\n<li>在网页后台点击左侧 <strong>Skill(技能)</strong><br />\n<img alt=\"在这里插入图片描述\" class=\"lazyload\" /><br />\n可以看到有非常多的API可以配置，过程相对来说还是比较繁琐的，有机会的话下次专门再出一期博客来讲，目前的话他可以实现通过对话来进行一些本地的文件操作，命令操作等等。整体来说还是不错的</li>\n</ol>\n</li>\n</ul>\n<hr />\n<hr />\n<h2 id=\"总结你的-ai-现在能干什么\">总结：你的 AI 现在能干什么？</h2>\n<ol>\n<li><strong>对话</strong>：你可以问它任何问题。</li>\n<li><strong>读文件</strong>：把代码发给它，或者让它读你 F 盘的文档。</li>\n<li><strong>本地代码执行</strong>：让它用 Python 画一个股价走势图。</li>\n</ol>\n<p><strong>部署贴士总结：</strong></p>\n<ul>\n<li><strong>Node 版本必须 v22+</strong></li>\n<li><strong>Token 在配置文件里找</strong></li>\n<li><strong>本地 GPU 模型用 Ollama 桥接</strong></li>\n<li><strong>Google 搜索一定要开“搜索全网”开关</strong></li>\n</ul>\n<hr />\n<p><em>恭喜你完成了部署！现在，你的 Windows 已经不仅仅是一台电脑，而是一个拥有最强国产模型大脑和本地硬件加速的超级助手了。</em></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-29 00:14</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ChenAI-TGF\">TTGF</a>&nbsp;\n阅读(<span id=\"post_view_count\">192</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "揭秘 Codex Agent 的核心运行机制：从循环到智能决策",
      "link": "https://www.cnblogs.com/smartloli/p/19530777",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/smartloli/p/19530777\" id=\"cb_post_title_url\" title=\"发布于 2026-01-28 23:32\">\n    <span>揭秘 Codex Agent 的核心运行机制：从循环到智能决策</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<h1>1.概述</h1>\n<p>&nbsp;在人工智能快速发展的今天，AI不再仅仅是回答问题的聊天机器人，而是正在演变为能够主动完成复杂任务的智能代理。OpenAI的Codex CLI就是这一趋势的典型代表——一个跨平台的本地软件代理，能够在用户的机器上安全高效地生成高质量的软件变更。</p>\n<h1>2.内容</h1>\n<p>如果你只把 Codex 当成“更会写代码的 ChatGPT”，那你只理解了它 10% 的价值。真正让 Codex 不同的，是它背后那套完整、可运行、可反复思考的 Agent Loop（智能体循环）系统。</p>\n<h2>2.1&nbsp;Codex 到底和普通大模型有什么区别？</h2>\n<p>我们先看一个最普通的大模型交互流程：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">你：帮我写一个 Python 脚本\n模型：给你一段代码\n结束</span></pre>\n</div>\n<p>这是一次性生成，模型：</p>\n<ul>\n<li>不知道代码能不能运行</li>\n<li>不知道有没有报错</li>\n<li>更不知道“下一步该干什么”</li>\n</ul>\n<p><strong>1. Codex 的真实工作方式完全不同</strong></p>\n<p>Codex 的思路更像一个新手工程师坐在你电脑前：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 128, 1);\">1</span> <span style=\"color: rgba(0, 0, 0, 1);\">看需求\n</span><span style=\"color: rgba(0, 128, 128, 1);\">2</span> <span style=\"color: rgba(0, 0, 0, 1);\">写点代码\n</span><span style=\"color: rgba(0, 128, 128, 1);\">3</span> <span style=\"color: rgba(0, 0, 0, 1);\">运行一下\n</span><span style=\"color: rgba(0, 128, 128, 1);\">4</span> <span style=\"color: rgba(0, 0, 0, 1);\">报错了？看看错误\n</span><span style=\"color: rgba(0, 128, 128, 1);\">5</span> <span style=\"color: rgba(0, 0, 0, 1);\">改代码\n</span><span style=\"color: rgba(0, 128, 128, 1);\">6</span> <span style=\"color: rgba(0, 0, 0, 1);\">再运行\n</span><span style=\"color: rgba(0, 128, 128, 1);\">7</span> 直到成功</pre>\n</div>\n<p>这个「反复尝试」的过程，就是 Codex Agent Loop。</p>\n<h2>2.2&nbsp;什么是 Agent Loop？</h2>\n<p><strong>Agent Loop = 让模型在一个循环里，不断思考 → 行动 → 看结果 → 再思考</strong>。Codex CLI 的核心不是“一次推理”，而是反复展开这个循环，模型不是直接给答案，而是每一轮只决定：我下一步该干什么？</p>\n<p><strong>1.&nbsp;先忘掉「大模型」，把 Codex 当成一个“新人程序员”</strong></p>\n<p>想象一个<strong>刚入职的初级工程师</strong>，你给他一个任务：</p>\n<div class=\"cnblogs_code\">\n<pre>“帮我把这个项目跑起来，并写一个 README。”</pre>\n</div>\n<p>他会怎么做？一定不是：</p>\n<div class=\"cnblogs_code\">\n<pre>“我闭上眼睛，一次性把所有事情做对。”</pre>\n</div>\n<p>而是更接近下面这个过程：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 128, 1);\">1</span> <span style=\"color: rgba(0, 0, 0, 1);\">先看看项目目录结构\n</span><span style=\"color: rgba(0, 128, 128, 1);\">2</span> <span style=\"color: rgba(0, 0, 0, 1);\">猜一猜怎么运行\n</span><span style=\"color: rgba(0, 128, 128, 1);\">3</span> <span style=\"color: rgba(0, 0, 0, 1);\">真的运行一下\n</span><span style=\"color: rgba(0, 128, 128, 1);\">4</span> <span style=\"color: rgba(0, 0, 0, 1);\">发现报错\n</span><span style=\"color: rgba(0, 128, 128, 1);\">5</span> <span style=\"color: rgba(0, 0, 0, 1);\">根据报错改代码\n</span><span style=\"color: rgba(0, 128, 128, 1);\">6</span> <span style=\"color: rgba(0, 0, 0, 1);\">再运行\n</span><span style=\"color: rgba(0, 128, 128, 1);\">7</span> <span style=\"color: rgba(0, 0, 0, 1);\">直到跑通\n</span><span style=\"color: rgba(0, 128, 128, 1);\">8</span> 最后再总结，写 README</pre>\n</div>\n<p><strong>注意：</strong><br />这个过程中，每一步都依赖上一步的结果。这，就是 Agent Loop 的直觉来源。</p>\n<p><strong>2.普通 ChatBot VS Agent：根本区别在哪？</strong></p>\n<p>普通 ChatBot 的工作方式</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">输入问题\n↓\n模型“想一想”\n↓\n一次性输出答案\n↓\n结束</span></pre>\n</div>\n<p>它的特点是：</p>\n<ul>\n<li>只能“想”，不能“做”</li>\n<li>没有真实世界的反馈</li>\n<li>更像是在考试答题</li>\n</ul>\n<p>Codex Agent 的工作方式</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">目标\n↓\n想一小步\n↓\n做一小步\n↓\n看结果\n↓\n再想一小步\n↓\n……\n↓\n完成</span></pre>\n</div>\n<p>它的特点是：</p>\n<ul>\n<li>每一轮只解决一个非常小的问题</li>\n<li>每一步都基于真实执行结果</li>\n<li>更像是在真实工作</li>\n</ul>\n<p>Agent Loop，本质上就是把“一次性回答问题”，拆成了“多轮小决策”。</p>\n<p><strong>3.&nbsp;「Loop」这个词，为什么这么重要？</strong></p>\n<p>我们先看一个不展开的情况：</p>\n<div class=\"cnblogs_code\">\n<pre>模型在脑子里想 <span style=\"color: rgba(128, 0, 128, 1);\">10</span><span style=\"color: rgba(0, 0, 0, 1);\"> 步\n↓\n一次性输出最终答案</span></pre>\n</div>\n<p>这种方式的问题是：</p>\n<ul>\n<li>中间哪一步想错了，你完全不知道</li>\n<li>没有机会修正</li>\n<li>对复杂任务非常不稳定</li>\n</ul>\n<p>而 Agent Loop 是把这 10 步“摊开”：</p>\n<div class=\"cnblogs_code\">\n<pre>第 <span style=\"color: rgba(128, 0, 128, 1);\">1</span><span style=\"color: rgba(0, 0, 0, 1);\"> 步：我该不该看目录？\n↓\n第 </span><span style=\"color: rgba(128, 0, 128, 1);\">2</span><span style=\"color: rgba(0, 0, 0, 1);\"> 步：我该不该运行测试？\n↓\n第 </span><span style=\"color: rgba(128, 0, 128, 1);\">3</span><span style=\"color: rgba(0, 0, 0, 1);\"> 步：这个报错是什么意思？\n↓\n第 </span><span style=\"color: rgba(128, 0, 128, 1);\">4</span> 步：我该改哪个文件？</pre>\n</div>\n<p>这就是 unrolling the loop 的含义：把模型原本“在脑子里一次性完成的思考”，拆成一轮一轮、可执行、可观察的步骤。</p>\n<p><strong>4.为什么说「模型不是直接给答案」？</strong></p>\n<p>复杂问题，没有人能在“看都没看现场”的情况下，一次就给出完美答案。Agent Loop 的设计，正是承认了这一点。所以 Codex 的策略是：</p>\n<ul>\n<li>不追求“一次答对”</li>\n<li>而是追求：\n<ul>\n<li>每一步都有依据</li>\n<li>每一步都能被验证</li>\n<li>每一步错了都能修</li>\n</ul>\n</li>\n</ul>\n<p>假设你的项目现在是这样：</p>\n<div class=\"cnblogs_code\">\n<pre>“一个 Node 项目，运行 npm start 会报错”</pre>\n</div>\n<p>Codex 的内心独白，并不是一句话，而是类似这样一轮一轮展开：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 128, 1);\">1</span> 我不知道项目结构，我需要先看看 → 调用工具：<span style=\"color: rgba(0, 0, 255, 1);\">ls</span>\n<span style=\"color: rgba(0, 128, 128, 1);\">2</span> <span style=\"color: rgba(0, 0, 0, 1);\">看起来是 Node 项目，我应该试着运行 → 调用工具：npm start\n</span><span style=\"color: rgba(0, 128, 128, 1);\">3</span> 报错说缺依赖 → 调用工具：npm <span style=\"color: rgba(0, 0, 255, 1);\">install</span>\n<span style=\"color: rgba(0, 128, 128, 1);\">4</span> <span style=\"color: rgba(0, 0, 0, 1);\">再运行一次 → npm start\n</span><span style=\"color: rgba(0, 128, 128, 1);\">5</span> 现在跑通了，我可以总结了 → 输出最终回答</pre>\n</div>\n<h2>2.3&nbsp;把 Agent Loop 拆成 5 个步骤</h2>\n<h3>2.3.1 接收用户目标（不是马上干活）</h3>\n<p><strong>1 用户输入 ≠ 模型直接思考的内容</strong></p>\n<p>当你在 Codex CLI（或任何 Agent 系统）里输入一句话，比如：</p>\n<div class=\"cnblogs_code\">\n<pre>“帮我给这个项目补一个 README。”</pre>\n</div>\n<p>很多人会误以为：</p>\n<div class=\"cnblogs_code\">\n<pre>这句话直接被送进模型，然后模型开始思考。</pre>\n</div>\n<p>但实际上，在 Agent 系统里，这句话的角色更接近于：</p>\n<div class=\"cnblogs_code\">\n<pre>“任务目标（Goal）”</pre>\n</div>\n<p>也就是说，它只是告诉系统：</p>\n<div class=\"cnblogs_code\">\n<pre>最终你要把事情做到什么状态</pre>\n</div>\n<p><strong>2 为什么要把“目标”和“过程”分开？</strong></p>\n<p>因为 Agent Loop 的设计理念是：</p>\n<ul>\n<li>目标是稳定的</li>\n<li>过程是动态变化的</li>\n</ul>\n<p>举个生活化的例子：</p>\n<div class=\"cnblogs_code\">\n<pre>你的目标是“把房间收拾干净”</pre>\n</div>\n<p>你并不会一开始就决定：</p>\n<ul>\n<li>先扫地还是先整理桌子</li>\n<li>垃圾有多少</li>\n<li>要不要换垃圾袋</li>\n</ul>\n<p>你只是知道：最后要干净</p>\n<p>Codex 也是一样。</p>\n<div class=\"cnblogs_code\">\n<pre>用户输入只负责定义“终点”，不负责定义“路径”。</pre>\n</div>\n<h3>2.3.2&nbsp;构造当前上下文（Prompt）</h3>\n<p><strong>1.Prompt 是“模型看世界的全部信息”</strong></p>\n<p>这是 Agent Loop 里最关键、也最容易被低估的一步。</p>\n<p>我们先说一句非常重要的话：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">对模型来说，它并不知道“刚刚发生了什么”，\n除非你把这些信息放进 Prompt。</span></pre>\n</div>\n<p>所以，每一轮 Agent Loop，都会重新构造一个 Prompt。</p>\n<p><strong>2.Prompt 里通常包含哪些东西？</strong></p>\n<p>一个完整的 Prompt，通常包含：</p>\n<ol>\n<li>你是谁（系统设定）<ol>\n<li>你是一个 coding agent</li>\n<li>你可以修改文件、运行命令</li>\n</ol></li>\n<li>你能用什么工具<ol>\n<li>shell</li>\n<li>文件读写</li>\n<li>测试运行</li>\n</ol></li>\n<li>用户目标<ol>\n<li>比如：补 README</li>\n</ol></li>\n<li>到目前为止发生了什么<ol>\n<li>我刚才运行了什么命令</li>\n<li>输出结果是什么</li>\n<li>有没有报错</li>\n</ol></li>\n</ol>\n<p>对模型来说，这些内容就是它的“记忆”。</p>\n<p><strong>3.为什么每一轮都要“重新构造” Prompt？</strong></p>\n<p>举个例子：</p>\n<ul>\n<li>第一轮：你还没看过项目结构</li>\n<li>第二轮：你已经知道有哪些文件</li>\n<li>第三轮：你已经看到测试报错</li>\n</ul>\n<p>如果 Prompt 不更新，模型就会：</p>\n<ul>\n<li>永远以为自己什么都不知道</li>\n</ul>\n<p>所以 Agent Loop 的一个核心动作就是：</p>\n<ul>\n<li>把“刚刚发生的现实结果”，翻译成模型能理解的文字，再塞回 Prompt。</li>\n</ul>\n<h3>2.3.3&nbsp;让模型做“下一步决策”</h3>\n<p><strong>1.模型在这一轮，只回答一个问题</strong></p>\n<p>这是 Agent Loop 的灵魂所在。</p>\n<ul>\n<li>模型不会在这一轮里把所有事情想完。</li>\n</ul>\n<p>它只做一个非常具体、非常有限的判断：</p>\n<ul>\n<li>“在当前信息条件下，我下一步该做什么？”</li>\n</ul>\n<p><strong>2.这个“下一步”，通常只有两种可能</strong></p>\n<p><strong>情况一：我还需要更多信息 / 行动</strong></p>\n<p>模型会说类似：</p>\n<ul>\n<li>“我需要看看目录结构”</li>\n<li>“我需要跑一下测试”</li>\n<li>“我需要打开某个文件看看内容”</li>\n</ul>\n<p>在系统层面，这会被表达为：</p>\n<ul>\n<li>Tool Call（工具调用）</li>\n</ul>\n<p><strong>情况二：信息已经够了，可以结束</strong></p>\n<p>模型会说类似：</p>\n<ul>\n<li>“现在我可以写 README 了”</li>\n<li>“问题已经修复完成”</li>\n</ul>\n<p>这时，它会直接输出最终回答，Agent Loop 结束。</p>\n<p><strong>3.为什么要限制成“只想一步”？</strong></p>\n<p>因为这是控制复杂度的关键。</p>\n<p>如果模型一次性想 10 步：</p>\n<ul>\n<li>中间哪一步错了，你不知道</li>\n<li>无法插入真实反馈</li>\n<li>很难纠正</li>\n</ul>\n<p>而“一步一想”的好处是：</p>\n<ul>\n<li>每一步都可以被验证</li>\n<li>错了就马上修</li>\n<li>对复杂任务更稳</li>\n</ul>\n<h3>2.3.4&nbsp;如果要干活 → 调工具</h3>\n<p><strong>1.模型自己“不会干活”</strong></p>\n<p>模型 ≠ 能执行命令的程序，模型只能输出文字（或结构化指令），但：</p>\n<ul>\n<li>它不能真的运行 ls</li>\n<li>不能真的执行 npm install</li>\n<li>不能真的写文件</li>\n</ul>\n<p><strong>2.Tool 的作用：把“建议”变成“现实动作”</strong></p>\n<p>当模型说：“我需要运行 ls 看看目录”，Agent 系统会：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 128, 1);\">1</span> <span style=\"color: rgba(0, 0, 0, 1);\">解析模型输出\n</span><span style=\"color: rgba(0, 128, 128, 1);\">2</span> <span style=\"color: rgba(0, 0, 0, 1);\">发现这是一个 tool call\n</span><span style=\"color: rgba(0, 128, 128, 1);\">3</span> <span style=\"color: rgba(0, 0, 0, 1);\">在真实环境里执行命令\n</span><span style=\"color: rgba(0, 128, 128, 1);\">4</span> 收集真实输出</pre>\n</div>\n<h3>2.3.5&nbsp;把结果塞回上下文，继续循环</h3>\n<p><strong>1.这是 Agent Loop 最“反直觉”的一步</strong></p>\n<p>很多人会以为：工具执行完，模型“就知道结果了”，其实不然。模型并不知道工具执行结果，除非你把结果写进 Prompt。</p>\n<p><strong>2.现实 → 文本 → Prompt</strong></p>\n<p>Agent 会把刚才的执行结果，转成类似这样的内容：</p>\n<div class=\"cnblogs_code\">\n<pre>你刚刚运行了 <span style=\"color: rgba(0, 0, 255, 1);\">ls</span><span style=\"color: rgba(0, 0, 0, 1);\">\n输出是：\nsrc</span>/<span style=\"color: rgba(0, 0, 0, 1);\">\npackage.json</span></pre>\n</div>\n<p>然后：</p>\n<ul>\n<li>把这段文字加入 Prompt</li>\n<li>再发起下一轮模型推理</li>\n</ul>\n<p>这一步完成后，新的一轮 Loop 开始。</p>\n<p>我们现在可以把这 5 步，用一句非常生活化的话说清楚：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 128, 1);\">1</span> <span style=\"color: rgba(0, 0, 0, 1);\">把当前情况告诉模型\n</span><span style=\"color: rgba(0, 128, 128, 1);\">2</span> <span style=\"color: rgba(0, 0, 0, 1);\">让模型决定下一小步\n</span><span style=\"color: rgba(0, 128, 128, 1);\">3</span> <span style=\"color: rgba(0, 0, 0, 1);\">把真实结果反馈回去\n</span><span style=\"color: rgba(0, 128, 128, 1);\">4</span> 直到模型觉得“可以收工了”</pre>\n</div>\n<h1>3.Agent Loop代码示例</h1>\n<p>前面我们讲了很多概念：<br />Agent Loop、目标、Prompt、工具、反馈……<br />现在我们用一段最小但完整的代码，把这些概念全部落到实处。</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> SimpleAgent:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> <span style=\"color: rgba(128, 0, 128, 1);\">__init__</span><span style=\"color: rgba(0, 0, 0, 1);\">(self, llm):\n        self.llm </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> llm\n        self.history </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> run(self, goal):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">while</span><span style=\"color: rgba(0, 0, 0, 1);\"> True:\n            prompt </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> self.build_prompt(goal)\n            response </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> self.llm(prompt)\n\n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 如果模型说“完成了”</span>\n            <span style=\"color: rgba(0, 0, 255, 1);\">if</span> response[<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">type</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>] == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">final</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(response[<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">text</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">])\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span>\n\n            <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 如果模型要用工具</span>\n            <span style=\"color: rgba(0, 0, 255, 1);\">if</span> response[<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">type</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>] == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">tool_call</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                result </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> self.execute_tool(response)\n                self.history.append(result)\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> build_prompt(self, goal):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> {\n            </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">goal</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">: goal,\n            </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">history</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">: self.history\n        }\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> execute_tool(self, call):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> call[<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">name</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>] == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">shell</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> os.popen(call[<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">command</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>]).read()</pre>\n</div>\n<p>这段代码不是生产级，但它100%体现了 Agent Loop 的本质结构。下面我们从整体 → 局部 → 每一行的“为什么”来拆。</p>\n<h3>1.先整体理解：这段代码在干什么？</h3>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">它在做一件事：\n不断把“当前状态”交给模型，让模型决定下一步，\n然后根据结果更新状态，直到模型说“可以结束了”。</span></pre>\n</div>\n<p>可以理解为：“你先想一步 → 我帮你干 → 把结果告诉你 → 你再想一步”</p>\n<h3>2.class SimpleAgent：Agent 不是模型，而是“调度者”</h3>\n<p>Agent ≠ 模型（LLM）</p>\n<ul>\n<li>llm：负责“思考 / 决策”</li>\n<li>Agent：负责“循环 / 执行 / 状态管理”</li>\n</ul>\n<p>Agent 的角色更像是一个项目经理 + 执行助理。</p>\n<h3>3.__init__：Agent 的“长期记忆”在哪里？</h3>\n<p>self.llm 是什么？</p>\n<ul>\n<li>它是一个函数或对象</li>\n<li>输入：Prompt</li>\n<li>输出：模型的“下一步决策”</li>\n</ul>\n<p>你可以把它理解成：</p>\n<div class=\"cnblogs_code\">\n<pre>response = 大模型(prompt)</pre>\n</div>\n<h3>4.self.history 为什么这么重要？</h3>\n<p>这是整个 Agent Loop 的核心状态。</p>\n<p>history 里存的不是聊天记录，而是：</p>\n<ul>\n<li>你刚刚执行了什么命令</li>\n<li>命令输出了什么</li>\n<li>有没有报错</li>\n</ul>\n<p>它是“现实世界发生过的事情”的文本化记录</p>\n<p>如果没有 history：</p>\n<ul>\n<li>模型每一轮都会“失忆”</li>\n<li>永远不知道自己刚才干过什么</li>\n</ul>\n<h3>5.run 方法：Agent Loop 的真正入口</h3>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">def</span> run(self, goal):</pre>\n</div>\n<p>这里的 goal，就是你输入的那句：</p>\n<div class=\"cnblogs_code\">\n<pre>“帮我给这个项目加一个 README”</pre>\n</div>\n<p>它只做一件事：定义终点，不定义路径。</p>\n<h3>6.while True：为什么 Agent 必须是“死循环”？</h3>\n<p>这行代码非常关键。</p>\n<p>很多人一看到“死循环”会下意识觉得不优雅，但在 Agent 里：</p>\n<ul>\n<li>没有循环，就没有 Agent</li>\n</ul>\n<p>为什么？</p>\n<p>因为 Agent 的工作模式是：</p>\n<ul>\n<li>不知道要循环多少轮</li>\n<li>不知道什么时候信息才“足够”</li>\n<li>只能一轮一轮试</li>\n</ul>\n<p>结束条件不是写死的，而是由模型决定的。</p>\n<h3>7.build_prompt：模型“看到的世界”是怎么来的？</h3>\n<div class=\"cnblogs_code\">\n<pre>prompt = self.build_prompt(goal)</pre>\n</div>\n<p>这是 Agent Loop 中最容易被忽略，但最重要的一步。</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> build_prompt(self, goal):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> {\n        </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">goal</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">: goal,\n        </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">history</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">: self.history\n    }</span></pre>\n</div>\n<p>它做的事情非常简单，但意义非常大：把“目标 + 已发生的事实”打包，交给模型。</p>\n<h3>8.response = self.llm(prompt)：模型只做一件事</h3>\n<div class=\"cnblogs_code\">\n<pre>response = self.llm(prompt)</pre>\n</div>\n<p>这一行，看似简单，其实决定了整个 Agent 的风格。</p>\n<p>模型在这里不会：</p>\n<ul>\n<li>写完整代码</li>\n<li>一次性解决所有问题</li>\n</ul>\n<p>它只回答一个问题：</p>\n<ul>\n<li>“在当前 prompt 条件下，我下一步该做什么？”</li>\n</ul>\n<p>我们用一句完整的流程复述：</p>\n<ul>\n<li>Agent 把目标 + 历史交给模型</li>\n<li>模型说：“下一步干这个”</li>\n<li>Agent 去真实执行</li>\n<li>Agent 把结果记录下来</li>\n<li>回到第 1 步</li>\n<li>直到模型说：</li>\n<li>“可以结束了。”</li>\n</ul>\n<h1>4.总结</h1>\n<p>Codex Agent 的真正价值，并不在于它“写代码有多快”，而在于它被设计成一个可以反复思考和行动的系统。通过 Agent Loop，模型不再试图一次性给出完美答案，而是像真实工程师一样：先尝试、再观察、再修正，逐步推进目标完成。这种“思考 → 执行 → 反馈 → 再思考”的循环机制，让复杂问题被自然拆解成一连串可验证的小步骤，也让错误变成系统的一部分，而不是失败的终点。</p>\n<h1>5.结束语</h1>\n<p>这篇博客就和大家分享到这里，如果大家在研究学习的过程当中有什么问题，可以加群进行讨论或发送邮件给我，我会尽我所能为您解答，与君共勉！</p>\n<p>另外，博主出新书了《<span style=\"color: rgba(255, 0, 0, 1);\"><strong><a href=\"https://item.jd.com/14421833.html\" rel=\"noopener nofollow\" target=\"_blank\"><span style=\"color: rgba(255, 0, 0, 1);\">Hadoop与Spark大数据全景解析</span></a></strong></span>》、同时已出版的《<span style=\"color: rgba(0, 0, 255, 1);\"><strong><a href=\"https://item.jd.com/14699434.html\" rel=\"noopener nofollow\" target=\"_blank\"><span style=\"color: rgba(0, 0, 255, 1);\">深入理解Hive</span></a></strong></span>》、《<span style=\"color: rgba(0, 0, 255, 1);\"><strong><a href=\"https://item.jd.com/12455361.html\" rel=\"noopener nofollow\" target=\"_blank\"><span style=\"color: rgba(0, 0, 255, 1);\">Kafka并不难学</span></a></strong></span>》和《<span style=\"color: rgba(0, 0, 255, 1);\"><strong><a href=\"https://item.jd.com/12371763.html\" rel=\"noopener nofollow\" target=\"_blank\"><span style=\"color: rgba(0, 0, 255, 1);\">Hadoop大数据挖掘从入门到进阶实战</span></a></strong></span>》也可以和新书配套使用，喜欢的朋友或同学， 可以<span style=\"color: rgba(255, 0, 0, 1);\"><strong>在公告栏那里点击购买链接购买博主的书</strong></span>进行学习，在此感谢大家的支持。关注下面公众号，根据提示，可免费获取书籍的教学视频。</p>\n<p>&nbsp;</p>\n\n</div>\n<div id=\"MySignature\">\n    <div>\n<b class=\"b1\"></b><b class=\"b2 d1\"></b><b class=\"b3 d1\"></b><b class=\"b4 d1\"></b>\n<div class=\"b d1 k\">  \n联系方式：\n<br />\n邮箱：smartloli.org@gmail.com\n<br />\n<strong style=\"color: green;\">QQ群（Hive与AI实战【新群】）：935396818</strong>\n<br />\nQQ群（Hadoop - 交流社区1）：424769183\n<br />\nQQ群（Kafka并不难学）：825943084\n<br />\n温馨提示：请大家加群的时候写上加群理由（姓名＋公司/学校），方便管理员审核，谢谢！\n<br />\n<h3>热爱生活，享受编程，与君共勉！</h3>  \n</div>\n<b class=\"b4b d1\"></b><b class=\"b3b d1\"></b><b class=\"b2b d1\"></b><b class=\"b1b\"></b>\n</div>\n<br />\n<div>\n<b class=\"b1\"></b><b class=\"b2 d1\"></b><b class=\"b3 d1\"></b><b class=\"b4 d1\"></b>\n<div class=\"b d1 k\">\n<h3>公众号：</h3>\n<h3><img src=\"https://www.cnblogs.com/images/cnblogs_com/smartloli/1324636/t_qr.png\" style=\"width: 8%; margin-left: 10px;\" /></h3>\n</div>\n<b class=\"b4b d1\"></b><b class=\"b3b d1\"></b><b class=\"b2b d1\"></b><b class=\"b1b\"></b>\n</div>\n<br />\n<div>\n<b class=\"b1\"></b><b class=\"b2 d1\"></b><b class=\"b3 d1\"></b><b class=\"b4 d1\"></b>\n<div class=\"b d1 k\">\n<h3>作者：哥不是小萝莉 ［<a href=\"http://www.kafka-eagle.org/\" style=\"color: green;\" target=\"_blank\">关于我</a>］［<a href=\"http://www.cnblogs.com/smartloli/p/4241701.html\" style=\"color: green;\" target=\"_blank\">犒赏</a>］</h3>\n<h3>出处：<a href=\"http://www.cnblogs.com/smartloli/\" style=\"color: green;\" target=\"_blank\">http://www.cnblogs.com/smartloli/</a></h3>\n<h3>转载请注明出处，谢谢合作！</h3>\n</div>\n<b class=\"b4b d1\"></b><b class=\"b3b d1\"></b><b class=\"b2b d1\"></b><b class=\"b1b\"></b>\n</div>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-28 23:32</span>&nbsp;\n<a href=\"https://www.cnblogs.com/smartloli\">哥不是小萝莉</a>&nbsp;\n阅读(<span id=\"post_view_count\">61</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "使用 JYPPX.DeploySharp 高效部署 PaddleOCR，解锁多种高性能 OCR 文字识别方案",
      "link": "https://www.cnblogs.com/guojin-blogs/p/19545866",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/guojin-blogs/p/19545866\" id=\"cb_post_title_url\" title=\"发布于 2026-01-28 22:40\">\n    <span>使用 JYPPX.DeploySharp 高效部署 PaddleOCR，解锁多种高性能 OCR 文字识别方案</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"使用 JYPPX.DeploySharp 高效部署 PaddleOCR，解锁多种高性能 OCR 文字识别方案\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/2933426/202601/2933426-20260128223946237-441095120.png\" />\n        本文介绍了基于DeploySharp框架在.NET环境下部署PaddleOCR模型的解决方案。该框架通过统一接口封装了OpenVINO、TensorRT、ONNX Runtime等多种推理引擎，支持百毫秒级文字识别。文章详细解析了PaddleOCR三阶段工作流程（检测-分类-识别）及性能优化策略，阐述了DeploySharp\"统一接口、灵活部署\"的架构优势。演示程序支持多种推理后端，涵盖CPU/GPU不同硬件场景，提供模型加载、图片推理、性能测试等功能。通过该方案，开发者可根据实际硬件环\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"使用-jyppxdeploysharp-高效部署-paddleocr解锁多种高性能-ocr-文字识别方案\">使用 JYPPX.DeploySharp 高效部署 PaddleOCR，解锁多种高性能 OCR 文字识别方案</h1>\n<blockquote>\n<p>本文介绍如何通过 DeploySharp 框架在 .NET 环境下部署 PaddleOCR 模型，支持 OpenVINO、TensorRT、ONNX Runtime 等多种推理引擎，实现百毫秒级文字识别。</p>\n</blockquote>\n<hr />\n<h2 id=\"目录\">目录</h2>\n<ul>\n<li><a href=\"#%E4%B8%80%E5%89%8D%E8%A8%80\" rel=\"noopener nofollow\">一、前言</a></li>\n<li><a href=\"#%E4%BA%8C%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90\" rel=\"noopener nofollow\">二、核心技术原理解析</a></li>\n<li><a href=\"#%E4%B8%89deploysharp-%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8A%BF\" rel=\"noopener nofollow\">三、DeploySharp 架构优势</a></li>\n<li><a href=\"#%E5%9B%9B%E6%94%AF%E6%8C%81%E7%9A%84%E6%8E%A8%E7%90%86%E8%AE%BE%E5%A4%87\" rel=\"noopener nofollow\">四、支持的推理设备</a></li>\n<li><a href=\"#%E4%BA%94%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B%E6%8C%87%E5%8D%97\" rel=\"noopener nofollow\">五、快速开始指南</a></li>\n<li><a href=\"#%E5%85%AD%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E4%B8%8E%E5%88%86%E6%9E%90\" rel=\"noopener nofollow\">六、性能测试与分析</a></li>\n<li><a href=\"#%E4%B8%83%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94\" rel=\"noopener nofollow\">七、常见问题解答</a></li>\n<li><a href=\"#%E5%85%AB%E8%BD%AF%E4%BB%B6%E8%8E%B7%E5%8F%96\" rel=\"noopener nofollow\">八、软件获取</a></li>\n<li><a href=\"#%E4%B9%9D%E6%8A%80%E6%9C%AF%E6%94%AF%E6%8C%81\" rel=\"noopener nofollow\">九、技术支持</a></li>\n</ul>\n<hr />\n<h2 id=\"一前言\">一、前言</h2>\n<p>OCR（光学字符识别）技术在数字化办公、文档管理、票据识别等场景中发挥着重要作用。百度飞桨开源的 <strong>PaddleOCR</strong> 作为业界领先的 OCR 框架，以其优异的识别精度和丰富的功能特性深受开发者喜爱。</p>\n<p>一年前，我基于自己开发的 OpenVINO C# API 项目，在 .NET 框架下使用 OpenVINO 部署工具部署 PaddleOCR 系列模型，推出了 <strong>PaddleOCR-OpenVINO-CSharp</strong> 项目。借助 OpenVINO 在 CPU 上的强大推理优化能力，该项目成功实现了在纯 CPU 环境下完成图片文字识别、版面分析及表格识别等功能，推理速度可控制在 300 毫秒以内。</p>\n<p>随着项目的发展和应用场景的多样化，单一推理引擎已无法满足所有需求。近期，我将 OpenVINO、TensorRT、ONNX Runtime 等主流推理工具进行了统一封装，推出了 <strong>DeploySharp</strong> 开源项目。该项目的核心优势在于：</p>\n<ul>\n<li><strong>统一接口</strong>：通过底层接口抽象，实现一套代码适配多种推理引擎</li>\n<li><strong>灵活部署</strong>：开发者可根据实际硬件环境选择最优推理方案</li>\n<li><strong>性能优化</strong>：充分发挥各推理引擎的硬件加速能力</li>\n</ul>\n<p>得益于 DeploySharp 底层接口统一的优势，开发者现在可以用同一段代码在 OpenVINO、TensorRT、ONNX Runtime 等多种推理引擎间自由切换。近期，我们完成了 PaddleOCR 模型的支持更新，为 .NET 开发者提供了一套完整的 OCR 解决方案。</p>\n<p>目前，PaddleOCR 功能已集成至 DeploySharp 开源项目中（代码已上传至仓库，NuGet 包正在筹备中）。为了让大家快速体验新版 PaddleOCR 的极致性能，我们特别准备了 <strong>JYPPX.DeploySharp.OpenCvSharp.PaddleOcr.TestDemo</strong> 演示程序，支持即开即用，无需复杂配置。</p>\n<hr />\n<h2 id=\"二核心技术原理解析\">二、核心技术原理解析</h2>\n<h3 id=\"21-paddleocr-工作流程\">2.1 PaddleOCR 工作流程</h3>\n<p>PaddleOCR 采用经典的「检测-分类-识别」三阶段流水线架构：</p>\n<pre><code>输入图片\n    │\n    ▼\n┌─────────────┐\n│ 文本检测     │ → 检测图片中的文本区域位置\n│ (Detection) │\n└─────────────┘\n    │\n    ▼\n┌─────────────┐\n│ 文本方向分类 │ → 判断文本方向（180度翻转等）\n│ (Classifier)│\n└─────────────┘\n    │\n    ▼\n┌─────────────┐\n│ 文本识别     │ → 识别文本区域的具体内容\n│ (Recognition)│\n└─────────────┘\n    │\n    ▼\n输出识别结果\n</code></pre>\n<h3 id=\"22-三阶段模型详解\">2.2 三阶段模型详解</h3>\n<table>\n<thead>\n<tr>\n<th>阶段</th>\n<th>模型名称</th>\n<th>输入</th>\n<th>输出</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>检测</td>\n<td>PP-OCRv5_det</td>\n<td>原始图片 (3xHxW)</td>\n<td>文本框坐标</td>\n<td>定位文本区域</td>\n</tr>\n<tr>\n<td>分类</td>\n<td>PP-OCRv5_cls</td>\n<td>裁剪文本框 (3x80x160)</td>\n<td>方向标签</td>\n<td>纠正文本方向</td>\n</tr>\n<tr>\n<td>识别</td>\n<td>PP-OCRv5_rec</td>\n<td>裁剪文本框 (3x48xL)</td>\n<td>文本内容</td>\n<td>识别字符序列</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"23-性能优化策略\">2.3 性能优化策略</h3>\n<ol>\n<li><strong>模型量化</strong>：使用 int8 量化减小模型体积，提升推理速度</li>\n<li><strong>动态批处理</strong>：支持 Batch Size &gt; 1，提高 GPU 利用率</li>\n<li><strong>并发推理</strong>：支持多线程并发处理，充分利用多核性能</li>\n<li><strong>硬件加速</strong>：针对不同硬件选择最优计算后端</li>\n</ol>\n<hr />\n<h2 id=\"三deploysharp-架构优势\">三、DeploySharp 架构优势</h2>\n<p>DeploySharp 的核心设计理念是「<strong>统一接口，灵活部署</strong>」，其架构如下图所示：</p>\n<pre><code>┌─────────────────────────────────────────────────────────┐\n│                    应用层 (Application)                  │\n│            PaddleOCR 文字识别 / 其他模型应用              │\n└─────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌─────────────────────────────────────────────────────────┐\n│                 DeploySharp 抽象接口层                    │\n│  统一的模型加载 / 推理执行 / 资源管理接口                 │\n└─────────────────────────────────────────────────────────┘\n                            │\n            ┌───────────────┼───────────────┐\n            ▼               ▼               ▼\n┌───────────────┐ ┌───────────────┐ ┌───────────────┐\n│   OpenVINO    │ │   TensorRT    │ │ ONNX Runtime  │\n│   Engine      │ │   Engine      │ │    Engine     │\n│  (CPU 优化)   │ │ (GPU 加速)    │ │ (跨平台支持)   │\n└───────────────┘ └───────────────┘ └───────────────┘\n            │               │               │\n            ▼               ▼               ▼\n┌───────────────┐ ┌───────────────┐ ┌───────────────┐\n│  Intel CPU    │ │  NVIDIA GPU   │ │ 多种硬件设备   │\n│               │ │               │ │ (CPU/GPU/DML) │\n└───────────────┘ └───────────────┘ └───────────────┘\n</code></pre>\n<p><strong>主要优势：</strong></p>\n<ul>\n<li><strong>零代码切换</strong>：更换推理引擎无需修改业务代码</li>\n<li><strong>资源高效利用</strong>：自动管理模型生命周期和计算资源</li>\n<li><strong>扩展性强</strong>：易于添加新的推理引擎支持</li>\n<li><strong>生产就绪</strong>：经过充分测试，可直接用于生产环境</li>\n</ul>\n<hr />\n<h2 id=\"四支持的推理设备\">四、支持的推理设备</h2>\n<p>本演示程序支持多种主流推理后端，覆盖从入门级设备到高性能服务器的各种场景：</p>\n<table>\n<thead>\n<tr>\n<th>推理引擎</th>\n<th>支持设备</th>\n<th>适用场景</th>\n<th>性能特点</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>OpenVINO</strong></td>\n<td>CPU</td>\n<td>无 GPU 环境、Intel 处理器</td>\n<td>CPU 优化，启动快，稳定</td>\n</tr>\n<tr>\n<td><strong>TensorRT</strong></td>\n<td>CUDA 11/12</td>\n<td>NVIDIA GPU 高性能场景</td>\n<td>GPU 加速，极致性能，需模型转换</td>\n</tr>\n<tr>\n<td><strong>ONNX Runtime CPU</strong></td>\n<td>CPU</td>\n<td>跨平台部署</td>\n<td>通用性强，性能中等</td>\n</tr>\n<tr>\n<td><strong>ONNX Runtime CUDA</strong></td>\n<td>CUDA 12</td>\n<td>NVIDIA GPU 环境部署</td>\n<td>GPU 加速，开箱即用</td>\n</tr>\n<tr>\n<td><strong>ONNX Runtime TensorRT</strong></td>\n<td>CUDA 12</td>\n<td>NVIDIA GPU 高性能场景</td>\n<td>GPU 加速 + TensorRT 优化</td>\n</tr>\n<tr>\n<td><strong>ONNX Runtime DML</strong></td>\n<td>DML GPU</td>\n<td>Windows 平台多厂商 GPU</td>\n<td>支持 AMD/NVIDIA/Intel GPU</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p><strong>性能提示</strong>：首次加载模型和推理时会较慢，这是正常现象（模型初始化和 JIT 编译）。首次运行时请避免频繁操作，待模型预热完成后性能将显著提升。</p>\n</blockquote>\n<hr />\n<h2 id=\"五快速开始指南\">五、快速开始指南</h2>\n<h3 id=\"51-程序界面概览\">5.1 程序界面概览</h3>\n<p>运行程序后，主界面如下图所示：</p>\n<img alt=\"image-20260128211529014\" class=\"lazyload\" />\n<p><strong>核心操作说明：</strong></p>\n<table>\n<thead>\n<tr>\n<th>操作项</th>\n<th>说明</th>\n<th>注意事项</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>推理后端</td>\n<td>选择使用的推理引擎</td>\n<td>切换后需重新加载模型</td>\n</tr>\n<tr>\n<td>模型路径</td>\n<td>预置模型路径，一般无需修改</td>\n<td>支持自定义模型路径</td>\n</tr>\n<tr>\n<td>图像路径</td>\n<td>选择待识别的图片</td>\n<td>支持 JPG/PNG/BMP 等格式</td>\n</tr>\n<tr>\n<td>加载模型</td>\n<td>加载指定模型到内存</td>\n<td>首次使用必须执行</td>\n</tr>\n<tr>\n<td>推理图片</td>\n<td>执行单次图片识别</td>\n<td>首次需预热</td>\n</tr>\n<tr>\n<td>时间测试</td>\n<td>连续推理十次并统计平均耗时</td>\n<td>用于性能评估</td>\n</tr>\n<tr>\n<td>并发数量</td>\n<td>调整推理并发线程数</td>\n<td>修改后需重新加载模型</td>\n</tr>\n<tr>\n<td>BatchSize</td>\n<td>批量处理大小</td>\n<td>可动态调整</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h3 id=\"52-openvino-推理\">5.2 OpenVINO 推理</h3>\n<p>OpenVINO 是 Intel 推出的开源工具套件，针对 CPU 和Intel IGPU进行了深度优化，特别适合无 GPU 环境下的高性能推理。</p>\n<p><strong>CPU使用步骤：</strong></p>\n<p>1.运行程序</p>\n<p><img alt=\"程序主界面\" class=\"lazyload\" /></p>\n<p>2.在「推理后端」下拉框中选择 <strong>OpenVINO</strong></p>\n<p>3.点击「加载模型」</p>\n<p>4.点击「推理图片」开始识别</p>\n<img alt=\"image-20260128221119962\" class=\"lazyload\" />\n<p><strong>IGPU使用步骤：</strong></p>\n<p>英特尔集显使用流程与上述一致，主要是设备要选择<strong>GPU0</strong>：</p>\n<img alt=\"image-20260128221345165\" class=\"lazyload\" />\n<p><strong>混合设备使用步骤：</strong></p>\n<p>英特尔OpenVINO支持CPU+IGPU混合设备推理，即<strong>AUTO</strong>模式，OpenVINO会根据设备情况自主选择，使用方式与上述一致，主要是设备要选择<strong>AUTO</strong>：</p>\n<img alt=\"image-20260128221535531\" class=\"lazyload\" />\n<p><strong>适用场景：</strong></p>\n<ul>\n<li>服务器环境部署</li>\n<li>低功耗设备</li>\n<li>Intel CPU 用户</li>\n<li>对启动速度要求高的场景</li>\n</ul>\n<hr />\n<h3 id=\"53-onnx-runtime-cpu-推理\">5.3 ONNX Runtime CPU 推理</h3>\n<p>ONNX Runtime 是微软推出的跨平台推理引擎，支持多种硬件加速后端，CPU 模式无需任何依赖即可使用。</p>\n<p><strong>使用步骤：</strong></p>\n<ol>\n<li>运行程序</li>\n<li>在「推理后端」下拉框中选择 <strong>ONNX Runtime CPU</strong></li>\n<li>点击「加载模型」</li>\n<li>点击「推理图片」开始识别</li>\n</ol>\n<p><img alt=\"ONNX Runtime CPU 选择界面\" class=\"lazyload\" /></p>\n<p><strong>适用场景：</strong></p>\n<ul>\n<li>跨平台部署需求</li>\n<li>无 GPU 加速环境</li>\n<li>需要快速原型验证</li>\n</ul>\n<hr />\n<h3 id=\"54-onnx-runtime-cuda-推理\">5.4 ONNX Runtime CUDA 推理</h3>\n<p>CUDA 是 NVIDIA 提供的并行计算平台，可充分利用 GPU 的并行计算能力实现显著加速。</p>\n<h4 id=\"配置步骤\">配置步骤</h4>\n<ol>\n<li>\n<p><strong>安装 CUDA 驱动</strong></p>\n<ul>\n<li>访问 <a href=\"https://developer.nvidia.com/cuda-downloads\" rel=\"noopener nofollow\" target=\"_blank\">NVIDIA CUDA 官网</a></li>\n<li>下载并安装 CUDA 12.x 版本（测试环境：CUDA 12.3）</li>\n</ul>\n</li>\n<li>\n<p><strong>复制依赖文件</strong></p>\n<p>将以下 CUDA 相关 DLL 文件复制到程序运行目录：</p>\n<p><img alt=\"CUDA 依赖文件\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p><strong>启动推理</strong></p>\n<p>运行程序，在「推理后端」下拉框中选择 <strong>ONNX Runtime CUDA</strong></p>\n<p><img alt=\"ONNX Runtime CUDA 选择界面\" class=\"lazyload\" /></p>\n</li>\n</ol>\n<p><strong>依赖说明：</strong></p>\n<table>\n<thead>\n<tr>\n<th>NuGet 包名</th>\n<th>版本</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Microsoft.ML.OnnxRuntime.Gpu.Windows</td>\n<td>1.23.0</td>\n</tr>\n<tr>\n<td>Microsoft.ML.OnnxRuntime.Managed</td>\n<td>1.23.0</td>\n</tr>\n</tbody>\n</table>\n<p><strong>适用场景：</strong></p>\n<ul>\n<li>拥有 NVIDIA 显卡的设备</li>\n<li>对推理速度有较高要求</li>\n<li>需要快速部署无需模型转换</li>\n</ul>\n<hr />\n<h3 id=\"55-onnx-runtime-tensorrt-推理\">5.5 ONNX Runtime TensorRT 推理</h3>\n<p>TensorRT 是 NVIDIA 推出的高性能深度学习推理优化器，结合 CUDA 加速可达到极致性能。</p>\n<h4 id=\"配置步骤-1\">配置步骤</h4>\n<p>依赖文件复制方式与 CUDA 模式一致。</p>\n<h4 id=\"使用步骤\">使用步骤</h4>\n<ol>\n<li>运行程序</li>\n<li>在「推理后端」下拉框中选择 <strong>ONNX Runtime TensorRT</strong></li>\n<li>点击「加载模型」</li>\n<li>点击「推理图片」开始识别</li>\n</ol>\n<p><img alt=\"ONNX Runtime TensorRT 选择界面\" class=\"lazyload\" /></p>\n<blockquote>\n<p><strong>重要提示</strong>：首次运行推理时，TensorRT 会自动对 ONNX 模型进行优化编译，此过程可能需要数分钟，请耐心等待。编译后的引擎文件会被缓存，后续推理速度将大幅提升。</p>\n</blockquote>\n<p><strong>依赖说明：</strong></p>\n<table>\n<thead>\n<tr>\n<th>NuGet 包名</th>\n<th>版本</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Microsoft.ML.OnnxRuntime.Gpu.Windows</td>\n<td>1.23.2</td>\n</tr>\n<tr>\n<td>Microsoft.ML.OnnxRuntime.Managed</td>\n<td>1.23.2</td>\n</tr>\n</tbody>\n</table>\n<p><strong>适用场景：</strong></p>\n<ul>\n<li>对推理速度要求极高的生产环境</li>\n<li>NVIDIA GPU 设备</li>\n<li>可接受首次运行较长的编译时间</li>\n</ul>\n<hr />\n<h3 id=\"56-onnx-runtime-dml-推理\">5.6 ONNX Runtime DML 推理</h3>\n<p>DirectML（DML）是 Windows 平台的高性能硬件加速接口，支持 AMD、NVIDIA 和 Intel 多厂商显卡。</p>\n<h4 id=\"配置步骤-2\">配置步骤</h4>\n<p>将 DML 相关 DLL 文件复制到程序运行目录：</p>\n<p><img alt=\"DML 依赖文件\" class=\"lazyload\" /></p>\n<h4 id=\"使用步骤-1\">使用步骤</h4>\n<ol>\n<li>运行程序</li>\n<li>在「推理后端」下拉框中选择 <strong>ONNX Runtime DML</strong></li>\n<li>点击「加载模型」</li>\n<li>点击「推理图片」开始识别</li>\n</ol>\n<p><img alt=\"ONNX Runtime DML 选择界面\" class=\"lazyload\" /></p>\n<p><strong>适用场景：</strong></p>\n<ul>\n<li>Windows 平台用户</li>\n<li>AMD 显卡用户</li>\n<li>需要统一接口支持多品牌显卡</li>\n</ul>\n<hr />\n<h3 id=\"57-tensorrtsharp-推理\">5.7 TensorRTSharp 推理</h3>\n<p>TensorRTSharp 是对 NVIDIA TensorRT 的 C# 封装，提供原生的 TensorRT 引擎加载和推理能力，支持 FP16 精度进一步提升性能。</p>\n<h4 id=\"环境准备\">环境准备</h4>\n<p>详细的安装和配置指南请参考：</p>\n<pre><code>https://mp.weixin.qq.com/s/D0c6j5MmraJO4Eza7tWm1A\n</code></pre>\n<p>TensorRTSharp 支持 CUDA 11 和 CUDA 12 两个系列，请根据系统安装的 CUDA 版本选择对应的 DLL 文件。</p>\n<h4 id=\"配置步骤-3\">配置步骤</h4>\n<ol>\n<li>\n<p><strong>替换 DLL 文件</strong></p>\n<p>根据安装的 CUDA 版本，将对应的 TensorRT DLL 文件复制到程序目录：</p>\n<p><img alt=\"TensorRT DLL 文件\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p><strong>模型转换</strong></p>\n<p>使用 <code>trtexec</code> 工具将 ONNX 模型转换为 TensorRT 引擎文件：</p>\n<p><img alt=\"trtexec 转换工具\" class=\"lazyload\" /></p>\n</li>\n</ol>\n<h4 id=\"模型转换指令\">模型转换指令</h4>\n<p><strong>文本检测模型（Det）：</strong></p>\n<pre><code class=\"language-bash\">trtexec.exe --onnx=PP-OCRv5_mobile_det_onnx.onnx \\\n  --minShapes=x:1x3x32x32 \\\n  --optShapes=x:4x3x640x640 \\\n  --maxShapes=x:8x3x960x960 \\\n  --fp16 \\\n  --memPoolSize=workspace:1024 \\\n  --sparsity=disable \\\n  --saveEngine=PP-OCRv5_mobile_det_f16_onnx.engine\n</code></pre>\n<p><strong>文本分类模型（Cls）：</strong></p>\n<pre><code class=\"language-bash\">trtexec.exe --onnx=PP-OCRv5_mobile_cls_onnx.onnx \\\n  --minShapes=x:1x3x80x160 \\\n  --optShapes=x:8x3x80x160 \\\n  --maxShapes=x:64x3x80x160 \\\n  --fp16 \\\n  --memPoolSize=workspace:1024 \\\n  --sparsity=disable \\\n  --saveEngine=PP-OCRv5_mobile_cls_f16_onnx.engine\n</code></pre>\n<p><strong>文本识别模型（Rec）：</strong></p>\n<pre><code class=\"language-bash\">trtexec.exe --onnx=PP-OCRv5_mobile_rec_onnx.onnx \\\n  --minShapes=x:1x3x48x48 \\\n  --optShapes=x:8x3x48x1024 \\\n  --maxShapes=x:64x3x48x1024 \\\n  --fp16 \\\n  --memPoolSize=workspace:1024 \\\n  --sparsity=disable \\\n  --saveEngine=PP-OCRv5_mobile_rec_f16_onnx.engine\n</code></pre>\n<h4 id=\"开始推理\">开始推理</h4>\n<p>模型转换完成后，在程序中选择对应的 <code>.engine</code> 文件即可开始推理：</p>\n<p><img alt=\"TensorRT 引擎文件选择\" class=\"lazyload\" /></p>\n<p><strong>适用场景：</strong></p>\n<ul>\n<li>追求极致推理性能</li>\n<li>NVIDIA GPU 环境</li>\n<li>允许离线模型转换</li>\n</ul>\n<hr />\n<h2 id=\"六性能测试与分析\">六、性能测试与分析</h2>\n<h3 id=\"61-性能测试工具\">6.1 性能测试工具</h3>\n<p>演示程序内置了完整的性能测试工具，支持两种测试模式：</p>\n<ol>\n<li><strong>整体耗时统计</strong>：计算从图片输入到结果输出的完整端到端耗时</li>\n<li><strong>详细阶段分析</strong>：记录预处理、推理、后处理各阶段的具体耗时</li>\n</ol>\n<p><img alt=\"整体耗时统计\" class=\"lazyload\" /></p>\n<p><img alt=\"详细性能记录\" class=\"lazyload\" /></p>\n<h3 id=\"62-tensorrtsharp-性能示例\">6.2 TensorRTSharp 性能示例</h3>\n<p>以下为使用 TensorRTSharp 在 4 并发配置下的性能测试数据：</p>\n<pre><code>Inference time: 53 ms\n\n---- Detection ----\n\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         2.01              6.37              0.57              8.96\n2         2.23              5.51              0.68              8.43\n\n---- Classification ----\n\nDevice/Worker 0:\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         1.84              6.89              0.00              8.73\n2         1.99              6.97              0.01              8.96\n\nDevice/Worker 1:\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         1.79              6.66              0.00              8.46\n2         1.66              7.60              0.00              9.26\n\nDevice/Worker 2:\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         1.61              5.31              0.00              6.92\n2         1.51              8.01              0.00              9.53\n\nDevice/Worker 3:\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         1.24              7.73              0.00              8.98\n2         1.82              8.35              0.00              10.17\n\n---- Recognition ----\n\nDevice/Worker 0:\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         0.00              41.97             1.42              43.39\n2         0.00              14.50             2.30              16.81\n\nDevice/Worker 1:\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         0.00              47.40             6.81              54.21\n2         0.00              19.42             2.76              22.18\n\nDevice/Worker 2:\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         0.00              38.10             3.42              41.52\n2         0.00              22.36             3.37              25.73\n\nDevice/Worker 3:\nInference Time Records:\nIndex    Preprocess(ms)    Inference(ms)    Postprocess(ms)    Total(ms)\n1         0.00              109.94            4.58              114.52\n2         0.00              26.59             4.55              31.14\n</code></pre>\n<h3 id=\"63-性能对比总结\">6.3 性能对比总结</h3>\n<p>下表为使用洗发水图片，跑10次的平均时间测试：</p>\n<table>\n<thead>\n<tr>\n<th>推理引擎</th>\n<th>设备</th>\n<th>平均耗时</th>\n<th>设备类型</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>OpenVINO</td>\n<td>CPU</td>\n<td>288ms</td>\n<td>Intel(R) Core(TM) Ultra 9 288V  8核</td>\n</tr>\n<tr>\n<td>OpenVINO</td>\n<td>IGPU</td>\n<td>99ms</td>\n<td>Intel(R) Arc(TM) 140V GPU (16GB)</td>\n</tr>\n<tr>\n<td>OpenVINO</td>\n<td>混合 AUTO：IGPU+CPU</td>\n<td>100ms</td>\n<td>Intel(R) Core(TM) Ultra 9 288V  8核  <br />Intel(R) Arc(TM) 140V GPU (16GB)</td>\n</tr>\n<tr>\n<td>ONNX Runtime</td>\n<td>CPU</td>\n<td>656ms</td>\n<td>AMD Ryzen 7 5800H with Radeon Graphics 8核</td>\n</tr>\n<tr>\n<td>ONNX Runtime DML</td>\n<td>GPU</td>\n<td>114ms</td>\n<td>NVIDIA GeForce RTX 3060 Laptop GPU</td>\n</tr>\n<tr>\n<td>ONNX Runtime DML</td>\n<td>IGPU</td>\n<td>331ms</td>\n<td>Intel(R) Arc(TM) 140V GPU (16GB)</td>\n</tr>\n<tr>\n<td>ONNX Runtime CUDA</td>\n<td>GPU</td>\n<td>93ms</td>\n<td>NVIDIA GeForce RTX 3060 Laptop GPU</td>\n</tr>\n<tr>\n<td>ONNX Runtime TensorRT</td>\n<td>GPU</td>\n<td>52ms</td>\n<td>NVIDIA GeForce RTX 3060 Laptop GPU</td>\n</tr>\n<tr>\n<td>TensorRTSharp</td>\n<td>GPU</td>\n<td>51ms</td>\n<td>NVIDIA GeForce RTX 3060 Laptop GPU</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p><strong>性能测试征集</strong>：我们欢迎广大开发者分享各自的测试数据。请在评论区提供您的测试配置（硬件型号、并发数、Batch Size）和实测耗时，后续我们将整理成性能基准对比表。</p>\n</blockquote>\n<hr />\n<h2 id=\"七常见问题解答\">七、常见问题解答</h2>\n<h3 id=\"q1-首次推理为什么特别慢\">Q1: 首次推理为什么特别慢？</h3>\n<p><strong>A:</strong> 首次推理时需要进行以下操作：</p>\n<ul>\n<li>模型加载到内存</li>\n<li>推理引擎初始化</li>\n<li>JIT 编译（部分引擎）</li>\n</ul>\n<p>这是正常现象，后续推理速度会显著提升。</p>\n<hr />\n<h3 id=\"q2-如何选择合适的推理引擎\">Q2: 如何选择合适的推理引擎？</h3>\n<p><strong>A:</strong> 根据硬件环境和需求选择：</p>\n<table>\n<thead>\n<tr>\n<th>场景</th>\n<th>推荐引擎</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>无 GPU，有Intel CPU,追求稳定性</td>\n<td>OpenVINO</td>\n</tr>\n<tr>\n<td>有Intel GPU，需要跨平台</td>\n<td>OpenVINO</td>\n</tr>\n<tr>\n<td>无 GPU，需要跨平台</td>\n<td>ONNX Runtime CPU</td>\n</tr>\n<tr>\n<td>有 NVIDIA 显卡，快速部署</td>\n<td>ONNX Runtime CUDA</td>\n</tr>\n<tr>\n<td>有 NVIDIA 显卡，追求性能</td>\n<td>ONNX Runtime TensorRT / TensorRTSharp</td>\n</tr>\n<tr>\n<td>Windows 平台，AMD 显卡</td>\n<td>ONNX Runtime DML</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h3 id=\"q3-切换推理引擎时为什么需要重新加载模型\">Q3: 切换推理引擎时为什么需要重新加载模型？</h3>\n<p><strong>A:</strong> 不同推理引擎对模型格式的内部表示和优化策略不同，因此需要重新解析和加载模型。点击「加载模型」即可完成切换。</p>\n<hr />\n<h3 id=\"q4-batchsize-和并发数量有什么区别\">Q4: BatchSize 和并发数量有什么区别？</h3>\n<p><strong>A:</strong> 两个参数的作用不同：</p>\n<ul>\n<li><strong>BatchSize</strong>：单次推理处理的图片数量，提升 GPU 利用率</li>\n<li><strong>并发数量</strong>：同时运行的推理引擎数量，设置几个就会生成几个推理引擎进行同时推理，提升多核/CPU 利用率</li>\n</ul>\n<p>调整 BatchSize 不需要重新加载模型，但调整并发数量后需要重新加载。</p>\n<hr />\n<h3 id=\"q5-tensorrt-模型转换失败怎么办\">Q5: TensorRT 模型转换失败怎么办？</h3>\n<p><strong>A:</strong> 检查以下几点：</p>\n<ol>\n<li>确保 CUDA 版本与 TensorRT 版本匹配</li>\n<li>检查 ONNX 模型文件是否完整</li>\n<li>确认 <code>trtexec</code> 参数中输入尺寸范围合理</li>\n<li>如显存不足，减小 <code>--memPoolSize</code> 参数</li>\n</ol>\n<hr />\n<h3 id=\"q6-推理结果为空或识别不准确怎么办\">Q6: 推理结果为空或识别不准确怎么办？</h3>\n<p><strong>A:</strong> 常见原因和解决方法：</p>\n<ol>\n<li><strong>图片质量</strong>：检查图片是否模糊、倾斜或光照不足</li>\n<li><strong>输入尺寸</strong>：确保图片尺寸符合模型输入要求</li>\n<li><strong>语言支持</strong>：确认模型是否支持目标语言</li>\n<li><strong>模型版本</strong>：尝试使用不同版本的 PaddleOCR 模型</li>\n</ol>\n<hr />\n<h2 id=\"八软件获取\">八、软件获取</h2>\n<h3 id=\"81-源码下载\">8.1 源码下载</h3>\n<p>DeploySharp 项目已完全开源，可通过以下方式获取：</p>\n<p><strong>主仓库：</strong></p>\n<pre><code>https://github.com/guojin-yan/DeploySharp.git\n</code></pre>\n<p><strong>PaddleOCR 演示程序：</strong></p>\n<pre><code>https://github.com/guojin-yan/DeploySharp/tree/DeploySharpV1.0/applications/JYPPX.DeploySharp.OpenCvSharp.PaddleOcr\n</code></pre>\n<h3 id=\"82-可执行程序\">8.2 可执行程序</h3>\n<p>如需直接获取编译好的可执行程序，请加入技术交流群，从群文件下载最新版本。</p>\n<hr />\n<h2 id=\"九技术支持\">九、技术支持</h2>\n<h3 id=\"91-反馈与交流\">9.1 反馈与交流</h3>\n<ul>\n<li><strong>GitHub Issues</strong>：在项目仓库提交 Issue 或 Pull Request</li>\n<li><strong>QQ 交流群</strong>：加入 <strong>945057948</strong>，获取实时技术支持</li>\n</ul>\n<p><img alt=\"QQ群二维码\" class=\"lazyload\" /></p>\n<h3 id=\"92-相关资源\">9.2 相关资源</h3>\n<ul>\n<li><strong>PaddleOCR 官方项目</strong>：<a href=\"https://github.com/PaddlePaddle/PaddleOCR\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/PaddlePaddle/PaddleOCR</a></li>\n<li><strong>OpenVINO 官方文档</strong>：<a href=\"https://docs.openvino.ai/\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.openvino.ai/</a></li>\n<li><strong>TensorRT 官方文档</strong>：<a href=\"https://docs.nvidia.com/deeplearning/tensorrt/\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.nvidia.com/deeplearning/tensorrt/</a></li>\n<li><strong>ONNX Runtime 官方文档</strong>：<a href=\"https://onnxruntime.ai/docs/\" rel=\"noopener nofollow\" target=\"_blank\">https://onnxruntime.ai/docs/</a></li>\n</ul>\n<hr />\n<h2 id=\"结语\">结语</h2>\n<p>通过 DeploySharp 框架，我们成功实现了 PaddleOCR 在 .NET 环境下的高效部署。无论是纯 CPU 环境下的稳定运行，还是 GPU 加速下的极致性能，开发者都可以根据实际需求灵活选择。</p>\n<p>未来，我们将持续优化框架性能，支持更多模型类型和推理引擎，为 .NET 开发者提供更完善的 AI 模型部署解决方案。</p>\n<hr />\n<p><em>作者：Guojin Yan</em><br />\n<em>最后更新：2026年1月</em></p>\n<hr />\n<p><strong>【文章声明】</strong></p>\n<p>本文主要内容基于作者的研究与实践，部分表述借助 AI 工具进行了辅助优化。由于技术局限性，文中可能存在错误或疏漏之处，恳请各位读者批评指正。如果内容无意中侵犯了您的权益，请及时通过公众号后台与我们联系，我们将第一时间核实并妥善处理。感谢您的理解与支持！</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-28 22:40</span>&nbsp;\n<a href=\"https://www.cnblogs.com/guojin-blogs\">椒颜皮皮虾</a>&nbsp;\n阅读(<span id=\"post_view_count\">208</span>)&nbsp;\n评论(<span id=\"post_comment_count\">2</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "跨平台 UI 工程的 Agentic 转型：MCP 在 Avalonia 生态中的深度应用与架构演进",
      "link": "https://www.cnblogs.com/shanyou/p/19545779",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/shanyou/p/19545779\" id=\"cb_post_title_url\" title=\"发布于 2026-01-28 21:59\">\n    <span>跨平台 UI 工程的 Agentic 转型：MCP 在 Avalonia 生态中的深度应用与架构演进</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在人工智能辅助软件开发的演进历程中，大型语言模型（LLM）长期以来一直面临着一个核心瓶颈：由于缺乏对运行中应用程序状态的实时访问权，这些模型往往处于一种“文本真空”状态 。尽管诸如 Claude 3.5 Sonnet 或 GPT-4o 等前沿模型在生成 XAML 代码或 C# 后端逻辑方面表现出色，但它们在面对复杂的跨平台 UI 框架（如 Avalonia UI）时，依然难以在没有人工干预的情况下实现闭环调试 。随着 Anthropic 在 2024 年 11 月发布模型上下文协议（Model Context Protocol, MCP），这一现状发生了根本性的转变。Avalonia 团队通过引入 Avalonia DevTools MCP Server（以下简称 avalonia_devtools），成功地在 AI 代理与运行中的 Avalonia 应用之间建立了一座程序化桥梁，标志着 AI 对 Avalonia 的理解从静态的代码理解跨越到了动态的运行时洞察 。</p>\n<h2 id=\"一模型上下文协议-mcp-的架构逻辑与标准化革命\"><strong>一：模型上下文协议 (MCP) 的架构逻辑与标准化革命</strong></h2>\n<p>在深入探讨 Avalonia 的具体实现之前，必须理解 MCP 如何解决了长久以来的“N×M”集成难题。在 MCP 出现之前，开发者如果希望 AI 助手连接到一个特定的数据源或工具，通常需要为每一个模型和每一个工具构建定制化的连接器 。这种碎片化的生态系统不仅难以规模化，更限制了 AI 代理在处理跨系统任务时的灵活性。</p>\n<h3 id=\"mcp-的核心组件与通信机制\"><strong>MCP 的核心组件与通信机制</strong></h3>\n<p>MCP 被设计为一种开放标准，其灵感源自成功简化了 IDE 语言支持的语言服务器协议（Language Server Protocol, LSP）。该协议采用 JSON-RPC 2.0 消息流，在客户端、主机和服务器之间建立了标准化的交互语言。其架构由三个关键部分组成：</p>\n<ol>\n<li><strong>MCP 宿主 (Host)：</strong> 它是用户与 AI 交互的起点，例如 AI 原生 IDE（如 Cursor、Zed、VS Code）或对话式客户端（如 Claude Desktop）。宿主负责管理 LLM 的请求，并决定何时调用外部工具 。</li>\n<li><strong>MCP 客户端 (Client)：</strong> 位于宿主内部，充当 LLM 与服务器之间的翻译官，负责发现可用的服务器并将其能力（Tools, Resources, Prompts）暴露给模型。</li>\n<li><strong>MCP 服务器 (Server)：</strong> 这是协议的核心扩展点，它直接连接到具体的数据源（如数据库、API 或正在运行的应用程序），并将这些系统的能力转化为 LLM 可理解的结构化工具调用。</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">组件</th>\n<th style=\"text-align: left;\">功能描述</th>\n<th style=\"text-align: left;\">传输层实现</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">MCP Server</td>\n<td style=\"text-align: left;\">提供数据、工具和上下文的外部服务</td>\n<td style=\"text-align: left;\">stdio (本地) / SSE (远程)</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">MCP Client</td>\n<td style=\"text-align: left;\">负责发现服务器并协调 LLM 请求</td>\n<td style=\"text-align: left;\">JSON-RPC 2.0 消息</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">MCP Host</td>\n<td style=\"text-align: left;\">用户交互界面 (如 IDE 或 Chat 窗口)</td>\n<td style=\"text-align: left;\">集成于应用环境</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">Transport Layer</td>\n<td style=\"text-align: left;\">确保消息在不同进程或网络间安全传递</td>\n<td style=\"text-align: left;\">标准输入输出或服务器发送事件</td>\n</tr>\n</tbody>\n</table>\n<p>这种架构不仅实现了“解耦”，更赋予了 AI 代理类似于人类开发者的“手和眼” 。通过 stdio（适用于本地资源）和 SSE（适用于远程资源）两种传输方式，MCP 确保了通信的低延迟与实时性，这对于 UI 调试这种需要即时反馈的场景至关重要 。</p>\n<h2 id=\"二avalonia-devtools-mcp-server-的核心功能与工具集\"><strong>二：Avalonia DevTools MCP Server 的核心功能与工具集</strong></h2>\n<p>Avalonia 团队推出的 avalonia_devtools 并不是一个简单的文档搜索工具，而是一个能够让 AI 直接控制程序化 DevTools 的强力插件。它解决了大规模应用迁移和 UI 调整中最为枯燥、重复且耗时的任务。</p>\n<h3 id=\"运行时连接与多实例管理\"><strong>运行时连接与多实例管理</strong></h3>\n<p>在复杂的企业开发环境中，开发者往往会同时运行多个应用实例或不同版本的同一个应用。avalonia_devtools 提供了一套完整的连接与发现工具，允许 AI 代理：</p>\n<ul>\n<li>枚举当前机器上所有可用的、且已启用调试支持的运行中 Avalonia 应用程序 。</li>\n<li>在多个运行实例之间进行上下文切换，确保 AI 能够针对正确的窗口进行操作 。</li>\n<li>这种连接能力是建立在 Avalonia 既有的 Avalonia.Diagnostics 基础之上的，利用了已有的 AttachDevTools() 基础设施 。</li>\n</ul>\n<h3 id=\"树状结构检索与深度遍历\"><strong>树状结构检索与深度遍历</strong></h3>\n<p>Avalonia 的 UI 架构由复杂的视觉树（Visual Tree）和逻辑树（Logical Tree）构成。人类开发者在使用传统 DevTools 时需要手动逐层展开这些树来寻找目标元素，而 MCP 服务器赋予了 AI 秒级检索的能力。AI 可以通过以下方式进行深入分析：</p>\n<ul>\n<li><strong>全树遍历：</strong> 检索视觉树、逻辑树或合并后的树，理解父子关系和布局约束。</li>\n<li><strong>定向搜索：</strong> 利用 x:Name 或元素类型（如 Button, TextBlock）快速定位特定 UI 节点。</li>\n<li><strong>回溯分析：</strong> 从任何选定的节点开始，向上遍历直到根节点，从而识别样式的继承路径或复杂的布局溢出问题 。</li>\n</ul>\n<h3 id=\"属性操控与动态样式调试\"><strong>属性操控与动态样式调试</strong></h3>\n<p>传统的 AI 辅助开发仅限于“生成代码并由人类运行查看结果”，而 avalonia_devtools 实现了“AI 运行、AI 观察、AI 修复”的闭环。通过属性和样式操控工具，AI 代理可以：</p>\n<ul>\n<li><strong>实时读取属性：</strong> 获取任何 UI 元素的实时属性值，包括依赖项属性、绑定结果以及当前计算出的布局大小。</li>\n<li><strong>运行时动态设置：</strong> 在不重新编译代码的情况下，通过 XAML 语法直接修改属性（如 Margin, Padding, Background），并即时观察视觉变化。</li>\n<li><strong>样式追踪：</strong> 查看当前生效的样式（Styles）及其设置器（Setters），确定是否存在样式冲突或优先级覆盖问题。</li>\n<li><strong>伪类触发：</strong> 模拟和查询伪类状态（如 :pointerover, :pressed, :focus），这对于调试交互式反馈和视觉转换至关重要。</li>\n</ul>\n<h3 id=\"视觉验证截图与资源访问\"><strong>视觉验证：截图与资源访问</strong></h3>\n<p>为了实现真正的“像素级完美”复制，AI 需要具备视觉感知能力。avalonia_devtools 支持捕获任何 UI 元素的 PNG 截图。这一功能与现代 LLM 的多模态能力相结合，使 AI 能够将实时生成的 UI 截图与原始设计稿进行逐像素对比，自动发现对齐、颜色或间距上的细微差别。此外，服务器还允许 AI 列出嵌入式资源、访问特定节点的资源字典，并直接通过 avares:// URL 下载资源 。</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">类别</th>\n<th style=\"text-align: left;\">工具名称 (能力)</th>\n<th style=\"text-align: left;\">典型应用场景</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">连接发现</td>\n<td style=\"text-align: left;\">Connect / List Instances</td>\n<td style=\"text-align: left;\">在多个运行中的应用进程间切换 AI 上下文</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">树检查</td>\n<td style=\"text-align: left;\">Traverse / Search Tree</td>\n<td style=\"text-align: left;\">定位深层嵌套的控件或分析布局溢出</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">属性操作</td>\n<td style=\"text-align: left;\">Read / Set Property</td>\n<td style=\"text-align: left;\">动态调整间距、颜色或修复数据绑定失效</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">样式分析</td>\n<td style=\"text-align: left;\">Inspect Styles / Toggle Pseudo-classes</td>\n<td style=\"text-align: left;\">调试悬停状态下的视觉效果或分析样式优先级</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">视觉反馈</td>\n<td style=\"text-align: left;\">Capture Screenshot</td>\n<td style=\"text-align: left;\">自动对比设计稿与实时 UI，实现像素级回归测试</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"三从设计稿到实现的像素级ai-工作流\"><strong>三：从设计稿到实现的“像素级”AI 工作流</strong></h2>\n<p>Avalonia 引入 MCP Server 的核心动力在于改写大规模应用 porting（移植）工作的经济模型。传统的 UI 迁移工作——例如从 WPF 迁移到 Avalonia——虽然技术难度并非极高，但涉及成千上万个视图的微调，这种极高的重复劳动极易导致资深工程师的职业倦怠。</p>\n<h3 id=\"案例分析devolutions-的大规模迁移实践\"><strong>案例分析：Devolutions 的大规模迁移实践</strong></h3>\n<p>在蒙特利尔的实际演示中，Devolutions 公司展示了其 Remote Desktop Manager 迁移项目的进展 2。该应用拥有约 5,000 个视图，如果依赖人工手动调整间距、对齐和样式，将是一个以年为单位的工程 2。</p>\n<p>通过接入 MCP 的 AI 工作流，这一过程被极大地压缩：</p>\n<ol>\n<li><strong>输入：</strong> 开发者向 AI 代理（如使用 MCP 的 Claude）提供现有旧应用的截图。</li>\n<li><strong>生成：</strong> AI 代理根据其对 Avalonia XAML 的理解生成初始代码。</li>\n<li><strong>实时连接：</strong> AI 通过 avalonia_devtools 服务器连接到正在运行的预览窗口。</li>\n<li><strong>自主检查：</strong> AI 搜索其刚刚创建的元素，读取实时属性。</li>\n<li><strong>视觉比对：</strong> AI 对该特定元素进行截图，并将其与原始截图进行多模态比对。</li>\n<li><strong>迭代修复：</strong> AI 自动识别出“侧边距多出了 2 像素”，随后发出 set-property 命令进行实时修正，并更新本地源码 。</li>\n</ol>\n<p>这种工作流的价值不在于简单的速度提升，而在于“迭代闭环的质量” 2。AI 不再是盲目地猜测代码是否正确，而是在实时观察、测量和比较 2。正如 Avalonia 团队所指出的，以往需要工程师花费 30 到 60 分钟进行的繁琐调整，现在只需几分钟即可完成 2。</p>\n<h2 id=\"四avalonia-ui-parcel-与自动化交付的结合\"><strong>四：Avalonia UI Parcel 与自动化交付的结合</strong></h2>\n<p>除了针对 UI 细节的 avalonia_devtools，Avalonia 还在其 Accelerate 套件中提供了针对打包和发布的 MCP Server：Avalonia Parcel 。Parcel 是专为简化跨平台应用打包而设计的工具，通过 MCP 接口，它将复杂的打包逻辑转化为了自然语言指令。</p>\n<h3 id=\"打包生命周期的-ai-指挥中心\"><strong>打包生命周期的 AI 指挥中心</strong></h3>\n<p>Parcel MCP Server 使 AI 助理（如 GitHub Copilot 或 Cursor）具备了管理构建目标、证书签名和分发配置的能力 。</p>\n<ul>\n<li><strong>项目配置自动化：</strong> AI 可以通过 create-project 工具从现有的.NET 项目中提取上下文，自动创建 .parcel 配置文件，并配置包名、图标和显示名称 。</li>\n<li><strong>跨平台签名支持：</strong> 开发者可以简单地对 AI 说：“为我的 macOS 应用配置签名并生成 DMG 安装包”。AI 会调用 setup-apple-sign 填充 P12 证书信息，调用 setup-apple-notary 配置公证凭据，并最终执行 pack 命令 。</li>\n<li><strong>构建目标管理：</strong> 针对 Windows (NSIS/ZIP)、macOS (DMG/PKG) 和 Linux (DEB/AppImage) 的不同要求，AI 能够自动处理运行时标识符（RID）并生成符合各平台标准的产物 。</li>\n</ul>\n<h3 id=\"许可与准入门槛\"><strong>许可与准入门槛</strong></h3>\n<p>值得注意的是，Parcel MCP 及其相关的自动化功能属于 Avalonia Accelerate 商业套件的一部分 。用户需要具备有效的商业许可证（Business 或 Enterprise 许可）或正在进行 30 天免费试用，并通过环境变量 PARCEL_LICENSE_KEY 激活服务 。</p>\n<h2 id=\"五集成指南与配置深度解析\"><strong>五：集成指南与配置深度解析</strong></h2>\n<p>要使 AI 真正“理解”Avalonia，开发者必须在开发环境中正确安装和配置这些 MCP 服务器。</p>\n<h3 id=\"跨客户端的配置策略\"><strong>跨客户端的配置策略</strong></h3>\n<p>无论是使用本地部署的 Claude Desktop，还是深度集成在 IDE 中的 AI 插件，其配置逻辑通常遵循一个标准化的 JSON 定义文件7。</p>\n<h4 id=\"1-在-vs-code-中配置-github-copilot-agent-mode\"><strong>1. 在 VS Code 中配置 (GitHub Copilot Agent Mode)</strong></h4>\n<p>VS Code 现在支持通过 .vscode/mcp.json 发现项目特定的服务器 10。对于 Parcel，配置示例如下</p>\n<p>{<br />\n\"servers\": {<br />\n\"parcel\": {<br />\n\"type\": \"stdio\",<br />\n\"command\": \"parcel\",<br />\n\"args\": [\"mcp\"]<br />\n}<br />\n}<br />\n}</p>\n<p>在启用“Agent Mode”后，Copilot 会自动检测到这些工具，并在处理打包或构建请求时使用它们 。</p>\n<h4 id=\"2-在-claude-desktop-中配置\"><strong>2. 在 Claude Desktop 中配置</strong></h4>\n<p>Claude Desktop 是目前 MCP 的主要宿主环境之一 。开发者需要编辑 claude_desktop_config.json 。</p>\n<ul>\n<li><strong>macOS 路径：</strong> ~/Library/Application Support/Claude/claude_desktop_config.json</li>\n<li><strong>Windows 路径：</strong> %APPDATA%\\Claude\\claude_desktop_config.json</li>\n</ul>\n<p>一个集成了多个 Avalonia 相关能力的典型配置如下：</p>\n<p>{<br />\n\"mcpServers\": {<br />\n\"parcel\": {<br />\n\"command\": \"parcel\",<br />\n\"args\": [\"mcp\"],<br />\n\"env\": {<br />\n\"PARCEL_LICENSE_KEY\": \"YOUR_KEY_HERE\"<br />\n}<br />\n},<br />\n\"avalonia_devtools\": {<br />\n\"command\": \"dotnet\",<br />\n\"args\": [\"run\", \"--project\", \"path/to/avalonia_devtools_server\"]<br />\n}<br />\n}<br />\n}<br />\n特别需要注意的是，在 Claude Desktop 中，环境变量必须在 env 对象中显式声明，因为它通常不会继承系统的全局环境变量 。</p>\n<h3 id=\"运行时激活与验证\"><strong>运行时激活与验证</strong></h3>\n<p>一旦配置文件保存，必须完全退出并重新启动 Claude Desktop 或 IDE 以重新加载服务器进程 12。用户可以在聊天界面的“工具”或“连接器”图标下验证服务器是否在线 12。对于 avalonia_devtools，通常还需要在受调试的应用中调用 AttachDevTools() 并确保其在 DEBUG 模式下编译 9。</p>\n<h2 id=\"六企业级-avaloniauimcp-与生态系统的深度扩展\"><strong>六：企业级 AvaloniaUI.MCP 与生态系统的深度扩展</strong></h2>\n<p>除了官方提供的 DevTools 服务器，社区和第三方库（如 AvaloniaUI.MCP）也为 AI 提供了更广泛的知识深度 。这些服务器不仅仅是工具，更是完整的知识库集成 。</p>\n<h3 id=\"深度知识库与生成工具\"><strong>深度知识库与生成工具</strong></h3>\n<p>一个成熟的 Avalonia MCP 服务器（如由 decriptor 维护的版本）通常包含以下进阶功能：</p>\n<ul>\n<li><strong>500+ 控件参考：</strong> 为 LLM 提供版本化的、准确的控件 API 文档，防止其生成已经废弃的代码 。</li>\n<li><strong>架构模板生成：</strong> 能够直接生成遵循企业级 MVVM 模式、ReactiveUI 或 CommunityToolkit.Mvvm 的代码结构 。</li>\n<li><strong>高性能保障：</strong> 为了防止 AI 等待超时，优秀的 MCP 实现采用了异步操作和缓存系统，将响应时间控制在 100ms 以内 。</li>\n<li><strong>安全性增强：</strong> 在处理文件操作或 API 密钥时，提供参数校验和沙箱化处理，防止 AI 因为恶意的 Prompt 注入而执行不安全的操作 。</li>\n</ul>\n<h3 id=\"性能与合规性指标\"><strong>性能与合规性指标</strong></h3>\n<p>对于大型企业，MCP 服务器的运行效率和合规性是关键考量指标：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">指标项目</th>\n<th style=\"text-align: left;\">规格要求</th>\n<th style=\"text-align: left;\">技术实现</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\">并发支持</td>\n<td style=\"text-align: left;\">50+ 模拟用户</td>\n<td style=\"text-align: left;\">基于.NET 9.0 的无状态请求处理</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">缓存命中率</td>\n<td style=\"text-align: left;\">&gt; 80%</td>\n<td style=\"text-align: left;\">对常用 XAML 模式和文档片段的内存缓存</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">安全合规</td>\n<td style=\"text-align: left;\">审计日志记录</td>\n<td style=\"text-align: left;\">对所有 Tool Call 进行全流量跟踪与日志化</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\">错误处理</td>\n<td style=\"text-align: left;\">无敏感信息泄露</td>\n<td style=\"text-align: left;\">自定义异常拦截器，过滤堆栈追踪中的路径信息</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"七第二与第三阶洞察ai-协作模式的根本转变\"><strong>七：第二与第三阶洞察：AI 协作模式的根本转变</strong></h2>\n<p>当 AI 通过 MCP 协议“看懂”了 Avalonia 运行时的状态，这不仅仅是工具的改进，而是生产关系的重构 。</p>\n<h3 id=\"从代码编写到意图协调\"><strong>从“代码编写”到“意图协调”</strong></h3>\n<p>在传统的 AI 辅助编程中，程序员承担了大部分的上下文补全工作——复制错误日志、手动同步 UI 属性值、手动描述视觉差异 1。而 MCP 将这些任务自动化，使程序员的职责从“执行者”转变为“意图协调者” 。</p>\n<p>这意味着：</p>\n<ol>\n<li><strong>工程成本的结构性下降：</strong> 以前需要中级开发者花费数周进行的 UI 润色，现在可以由高级架构师指挥 AI 代理在几天内完成 。</li>\n<li><strong>质量的一致性：</strong> AI 在执行 XAML 验证和样式对齐时不会感到疲劳，从而消除了因为人为疏忽导致的“像素偏移”或不一致的命名规范。</li>\n<li><strong>实时反馈回路：</strong> AI 的“观察-纠正”循环是毫秒级的。当 AI 修改了一个属性并发现截图仍未对齐时，它会立即进行第二次尝试，这种极速反馈是人类手动调整无法企及的。</li>\n</ol>\n<h3 id=\"跨领域的因果影响\"><strong>跨领域的因果影响</strong></h3>\n<p>Avalonia 在 MCP 上的领先地位很可能引发 UI 框架领域的“上下文军备竞赛”。如果一个框架能够让 AI 更好地理解其运行时状态，那么该框架的开发效率将获得指数级的提升，从而吸引更多的开发者。这也暗示了未来软件架构的一个趋势：软件将不再仅仅为人类用户设计，也将为 AI 代理的可观测性和可操作性进行优化（AI-Ready Architecture）。</p>\n<h2 id=\"八安全挑战局限性与防御策略\"><strong>八：安全挑战、局限性与防御策略</strong></h2>\n<p>尽管前景光明，但 MCP 的引入也伴随着显著的风险 。</p>\n<h3 id=\"上下文膨胀与令牌消耗-context-bloat\"><strong>上下文膨胀与令牌消耗 (Context Bloat)</strong></h3>\n<p>MCP 的一个主要技术挑战是其对上下文窗口的消耗。每个启用的 MCP 服务器都会将其所有工具的描述（Definition）注入到 LLM 的系统提示词中。</p>\n<ul>\n<li><strong>问题：</strong> 如果一个服务器暴露了 100 个复杂的工具，仅工具描述就可能占用 80k 到 100k 的令牌（Tokens）。这不仅增加了每次请求的成本，还可能导致模型因为上下文过长而变得“迟钝”或忽略核心指令。</li>\n<li><strong>解决方案：</strong> 社区正在开发“子代理（Sub-agents）”架构或“延迟加载（Lazy Loading）”机制，仅在需要时向模型暴露相关的工具子集。</li>\n</ul>\n<h3 id=\"安全漏洞与攻击向量\"><strong>安全漏洞与攻击向量</strong></h3>\n<p>安全研究人员已经指出，MCP 协议本身缺乏内置的安全增强机制，其安全性高度依赖于具体的服务器实现 。</p>\n<ul>\n<li><strong>Prompt 注入：</strong> 恶意数据或工具描述可能诱导 LLM 执行越权操作，例如将私有 UI 代码发送到外部 API 。</li>\n<li><strong>工具地毯式拖取 (Rug Pulls)：</strong> 服务器可以在用户授权后动态更改工具的行为描述，从而在后台执行非法动作。</li>\n<li><strong>建议：</strong> 企业应坚持“最小权限原则”，仅为 AI 代理提供其任务所需的特定工具集，并对所有涉及文件系统或网络访问的操作设置人工确认环节（Human-in-the-Loop）。</li>\n</ul>\n<h2 id=\"九面向未来的展望ai-原生-ui-工程的终极形态\"><strong>九：面向未来的展望：AI 原生 UI 工程的终极形态</strong></h2>\n<p>Avalonia 对 MCP 的采纳只是一个开始。随着协议的进一步成熟，我们可以预见以下几个发展阶段：</p>\n<h3 id=\"阶段一自主-qa-与回归测试\"><strong>阶段一：自主 QA 与回归测试</strong></h3>\n<p>未来的 avalonia_devtools 将不仅仅用于开发，更将成为自主 QA 的基石。AI 代理可以根据自然语言编写的测试计划，自主在应用中点击、检查样式、捕获截图并分析控制台输出，在发现 Bug 后甚至能直接提出修复 PR 。这种从测试到修复的全自动化闭环将极大地提高软件的发布质量。</p>\n<h3 id=\"阶段二动态-ui-生成与自适应\"><strong>阶段二：动态 UI 生成与自适应</strong></h3>\n<p>结合 Avalonia 的灵活布局能力和 MCP 的实时反馈，应用甚至可以实现“运行时的自适应生成”。AI 代理可以根据用户的实时操作习惯和视觉反馈，动态调整 UI 布局和资源分配，从而实现真正的个性化用户体验。</p>\n<h3 id=\"阶段三跨框架的协作智能\"><strong>阶段三：跨框架的协作智能</strong></h3>\n<p>随着更多框架（如 Chrome DevTools MCP, Playwright MCP）的加入，AI 将能够执行复杂的跨端任务。例如，一个 AI 代理可以同时控制一个运行在 Web 上的管理后台和一个运行在桌面上的 Avalonia 客户端，确保两端的业务逻辑和视觉风格在实时同步中保持一致。</p>\n<h2 id=\"结论\"><strong>结论</strong></h2>\n<p>“以后 AI 不会不懂 Avalonia 了”这一断言的背后，是整个软件工程范式的深刻变革。通过 Model Context Protocol，Avalonia 成功地将 AI 从代码生成的旁观者转变为了运行时调试的合伙人 。avalonia_devtools 和 avalonia_parcel 服务器通过标准化的接口，解决了长久以来困扰开发者的跨平台 UI 调试与分发难题，为大规模应用现代化扫清了障碍。</p>\n<p>尽管在安全性、成本控制和状态管理方面仍有挑战，但这种通过程序化桥梁实现“深度上下文感知”的路径已经证明了其无与伦比的生产力价值 。对于专业的.NET 开发者而言，积极拥抱 MCP 不仅仅是为了提高编写 XAML 的速度，更是为了在 Agentic AI 时代重塑自己的工程角色，将精力从繁琐的“像素微调”中解放出来，投入到更具创造性的架构设计之中。Avalonia 的这一步跨越，为跨平台桌面开发的未来设定了全新的标准 。</p>\n<h4 id=\"引用的著作\"><strong>引用的著作</strong></h4>\n<ol>\n<li>MCP Deep Dive (Part 1): Building the Hands and Eyes of an AI Agent in C# | by Alon Fliess <a href=\"https://medium.com/@alonfliess/mcp-deep-dive-part-1-building-the-hands-and-eyes-of-an-ai-agent-in-c-b26e71ebe102\" rel=\"noopener nofollow\" target=\"_blank\">https://medium.com/@alonfliess/mcp-deep-dive-part-1-building-the-hands-and-eyes-of-an-ai-agent-in-c-b26e71ebe102</a></li>\n<li>Avalonia DevTools MCP Server - Avalonia UI <a href=\"https://avaloniaui.net/blog/avalonia-devtools-mcp-server\" rel=\"noopener nofollow\" target=\"_blank\">https://avaloniaui.net/blog/avalonia-devtools-mcp-server</a></li>\n<li>Legacy Developer Tools  <a href=\"https://docs.avaloniaui.net/docs/guides/implementation-guides/developer-tools\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.avaloniaui.net/docs/guides/implementation-guides/developer-tools</a></li>\n<li>Model Context Protocol | Avalonia Docs, <a href=\"https://docs.avaloniaui.net/accelerate/tools/parcel/mcp\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.avaloniaui.net/accelerate/tools/parcel/mcp</a></li>\n<li>Community Edition | Avalonia Docs <a href=\"https://docs.avaloniaui.net/accelerate/community\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.avaloniaui.net/accelerate/community</a></li>\n<li>MCP server for AvaloniaUI ， <a href=\"https://github.com/decriptor/AvaloniaUI.MCP\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/decriptor/AvaloniaUI.MCP</a></li>\n<li>Your AI-Powered AvaloniaUI Development Assistant - AvaloniaUI.MCP <a href=\"https://decriptor.github.io/AvaloniaUI.MCP/\" rel=\"noopener nofollow\" target=\"_blank\">https://decriptor.github.io/AvaloniaUI.MCP/</a></li>\n<li>AvaloniaUI.MCP | MCP Servers - LobeHub <a href=\"https://lobehub.com/mcp/decriptor-avaloniaui_mcp\" rel=\"noopener nofollow\" target=\"_blank\">https://lobehub.com/mcp/decriptor-avaloniaui_mcp</a></li>\n<li>Avalonia UI Expert | Claude Code Skill for C# Apps - MCP Market， <a href=\"https://mcpmarket.com/tools/skills/avalonia-ui-development\" rel=\"noopener nofollow\" target=\"_blank\">https://mcpmarket.com/tools/skills/avalonia-ui-development</a></li>\n</ol>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>欢迎大家扫描下面二维码成为我的客户，扶你上云</p>\n<img src=\"https://images.cnblogs.com/cnblogs_com/shanyou/57459/o_220125090408_%E9%82%80%E8%AF%B7%E4%BA%8C%E7%BB%B4%E7%A0%81-258px.jpeg\" width=\"170\" />\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-28 21:59</span>&nbsp;\n<a href=\"https://www.cnblogs.com/shanyou\">张善友</a>&nbsp;\n阅读(<span id=\"post_view_count\">70</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}