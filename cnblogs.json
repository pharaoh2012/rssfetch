{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "AI基于AA游戏脚本输出的传奇3成长线配置表",
      "link": "https://www.cnblogs.com/ygluu/p/19581913",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ygluu/p/19581913\" id=\"cb_post_title_url\" title=\"发布于 2026-02-06 00:16\">\n    <span>AI基于AA游戏脚本输出的传奇3成长线配置表</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"以下是-kimi-k25-agent-基于aa游戏脚本输出的传奇3成长线配置表\">以下是 Kimi K2.5 Agent 基于AA游戏脚本输出的传奇3成长线配置表：</h2>\n<h3 id=\"输出之一\">输出之一：</h3>\n<p><img alt=\"83f69baaa4c0aff2969be15d1e4b0bd9\" class=\"lazyload\" /><br />\n<img alt=\"86ce0cfbb8f1cb7ce7e98b8346e11fbd\" class=\"lazyload\" /></p>\n<h3 id=\"输出之二\">输出之二：</h3>\n<p><img alt=\"6a3f69b70b7e18f8a027b14f5f31c0e3\" class=\"lazyload\" /><br />\n<img alt=\"cd185b8d62db1b7c5373bf2b01eba36a\" class=\"lazyload\" /></p>\n<h2 id=\"以下是提示词\">以下是提示词：</h2>\n<p>你是一个经验丰富的传奇策划大佬，也擅长于使用excel配置表来配置游戏。<br />\n这个链接是一个通用游戏脚本的规范：<br />\n<a href=\"https://www.cnblogs.com/ygluu/p/19550578\" target=\"_blank\">https://www.cnblogs.com/ygluu/p/19550578</a><br />\n请认真阅读上述链接的脚本规范，并使用该脚本编写的养成线配置表。<br />\n要求：<br />\n1、模块名：赏金猎人<br />\n2、档数：10档<br />\n3、符合传奇3版本特征<br />\n4、低级猎人仅判断杀怪数量即可<br />\n5、中级猎人结合怪物品质计算杀怪数量<br />\n6、高级猎人结合地图和怪物品质等计算怪物数量。<br />\n请以表格形式输出：<br />\n变量配置表<br />\n模块配置表</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-06 00:16</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ygluu\">码客-ygluu</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "LLVM的混淆之旅(五)-手动实现控制流平坦化混淆",
      "link": "https://www.cnblogs.com/ClownLMe/p/19581710",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ClownLMe/p/19581710\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 22:54\">\n    <span>LLVM的混淆之旅(五)-手动实现控制流平坦化混淆</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"简介\">简介</h1>\n<p>之前的教学中，简单的演示了LLVM的基本用法，下面，展示一个实战项目。</p>\n<h1 id=\"编译目标\">编译目标</h1>\n<p>本次的实验编译样例是下面判断正数，负数，和零的代码</p>\n<pre><code class=\"language-c\">#include &lt;stdio.h&gt;\n\nint main() {\n    int a = 9;\n    scanf_s(\"%d\", &amp;a);\n    if (a &lt; 0) {\n        printf(\"Negative number!\\n\");\n    } else if(a &gt; 0) {\n        printf(\"Positive number!\\n\");\n    } else {\n        printf(\"Zero!\\n\");\n    }\n    \n    printf(\"Done.\\n\");\n    return 0;\n} \n</code></pre>\n<h1 id=\"一控制流混淆平坦化\">一，控制流混淆平坦化</h1>\n<h3 id=\"简介-1\">简介</h3>\n<p>什么是控制流平坦化？简单来说，就是让原本垂直的流程分支平摊到水平方向上，使用这种方法可以提高逆向难度，软件更耐造。</p>\n<pre><code>            +-----------------------+\n            |        [开始]          |\n            |    设定初始状态 = 1     |\n            +-----------|-----------+\n                        |\n      +----------------&gt;V&lt;----------------+\n      |       +-------------------+       |\n      |       |    循环控制中心     |       | \n      |       |     (分发器)       |       |\n      |       +---------|---------+       |\n      |                 |                 |\n      |        _________V_________        |\n      |       |                   |       |\n      |       |  switch(状态变量)  |       | \n      |       |___________________|       |\n      |          /      |      \\          |\n      |         /       |       \\         |\n      |   [状态 1]   [状态 2]    [退出]     |\n      |   +-----+    +-----+    +-----+   |\n      |   | 块 1|    | 块 2 |    |结束 |   |\n      |   |     |    |     |    +-----+   |\n      |   |更新 |    | 更新  |             |\n      |   |状态 |    | 状态  |             |\n      |   +--|--+    +--|--+              |\n      |      |          |                 |\n      +------+----------+-----------------+\n</code></pre>\n<h3 id=\"实现控制流平坦化pass的核心代码\">实现控制流平坦化Pass的核心代码</h3>\n<p><strong>代码流程：</strong><br />\n识别与收集分支路径（代码块）-&gt; 构建控制骨架 -&gt; 初始化状态变量 -&gt; 配置分发器 -&gt; 重构跳转逻辑<br />\n我将代码的解释都标注在注释中</p>\n<pre><code class=\"language-cpp\">namespace{\n    struct mypass : public PassInfoMixin&lt;mypass&gt;{\n        PreservedAnalyses run(Function &amp;F, FunctionAnalysisManager &amp;AM){\n\t        //这里为了演示只混淆main函数\n            if(F.getName() != \"main\"){\n                return PreservedAnalyses::all();\n            }\n\n            errs() &lt;&lt; \"My Flattening Function:\" &lt;&lt; F.getName() &lt;&lt; \"\\n\";\n\t\t\t\n\t\t\t//通过遍历，获取main函数中的所有代码块\n            std::vector&lt;BasicBlock*&gt; OrigBBs;\n            DenseMap&lt;BasicBlock*, int&gt; BBtoID;\n            BasicBlock &amp;EntryBB = F.getEntryBlock();\n            int id_counter = 0;\n\n            for(BasicBlock &amp;BB : F){\n\t            //这里去除首代码块\n                if(&amp;BB == &amp;EntryBB) continue;\n                //记录代码块并给代码块标序号\n                //这里主要方便case中使用，可以是很复杂的独一无二的数字\n                OrigBBs.push_back(&amp;BB);\n                BBtoID[&amp;BB] = id_counter++;\n            }\n\t\t\t\n\t\t\t//判断收集到的分支是否存在，如果没有分支，就\n            if(OrigBBs.empty()){\n                return PreservedAnalyses::all();\n            }\n\t\t\t\n\t\t\t//创建循环控制代码块\n            BasicBlock *LoopEntry = BasicBlock::Create(F.getContext(), \"loop_entry\", &amp;F);\n            //控制流分发器代码块\n            BasicBlock *SwitchBB = BasicBlock::Create(F.getContext(), \"switch_block\", &amp;F);\n            //循环分发器尾部（用于兜底，可以不使用）\n            BasicBlock *LoopEnd = BasicBlock::Create(F.getContext(), \"loop_end\", &amp;F);\n\t\t\t\n\t\t\t//在首代码块中，创建初始状态变量，用于获取刚开始跳转的状态量\n            IRBuilder&lt;&gt; builderEntry(&amp;EntryBB);\n            Instruction *EntryTerm = EntryBB.getTerminator();\n            if (EntryTerm) builderEntry.SetInsertPoint(EntryTerm);\n            AllocaInst *SwitchVar = builderEntry.CreateAlloca(builderEntry.getInt32Ty(), nullptr, \"switchVar\");\n\t\t\t//获取跳转指令\n            if(BranchInst *BI = dyn_cast_or_null&lt;BranchInst&gt;(EntryTerm)){\n\t            //判断是否是条件跳转\n\t            //如果是，则直接获取跳转后的代码块序号\n                if(BI-&gt;isUnconditional()){\n                    BasicBlock *Target = BI-&gt;getSuccessor(0);\n                    if(BBtoID.count(Target)){\n                        builderEntry.CreateStore(builderEntry.getInt32(BBtoID[Target]), SwitchVar);\n                    }\n                    //如果是条件跳转\n                    //创建一条条件判断指令\n                }else if(BI-&gt;isConditional()){\n                    Value *Cond = BI-&gt;getCondition();\n                    BasicBlock *TrueBB = BI-&gt;getSuccessor(0);\n                    BasicBlock *FalseBB = BI-&gt;getSuccessor(1);\n                    if(BBtoID.count(TrueBB) &amp;&amp; BBtoID.count(FalseBB)){\n                        Value*Select = builderEntry.CreateSelect(\n                            Cond, \n                            builderEntry.getInt32(BBtoID[TrueBB]), \n                            builderEntry.getInt32(BBtoID[FalseBB]),\n                            \"init_state\"\n                        );\n                        builderEntry.CreateStore(Select, SwitchVar);\n                    }\n                }\n            }\n\t\t\t\n\t\t\t//首部创建跳转-&gt;跳转到循环控制入口\n            builderEntry.CreateBr(LoopEntry);\n            //删除旧的跳转指令\n            if(EntryTerm) EntryTerm-&gt;eraseFromParent();\n\t\t\t\n\t\t\t//创建状态变量，用于控制分发器的走向\n            IRBuilder&lt;&gt; builderLoop(LoopEntry);\n            LoadInst *CurrState = builderLoop.CreateLoad(builderLoop.getInt32Ty(), SwitchVar, \"curr_state\");\n            builderLoop.CreateBr(SwitchBB);\n\t\t\t\n\t\t\t//创建switch指令\n            IRBuilder&lt;&gt; builderSwitch(SwitchBB);\n            SwitchInst *SwitchI = builderSwitch.CreateSwitch(CurrState, LoopEnd, OrigBBs.size());\n            //创建case\n            //其中的标识符是上面收集到的代码块的序号\n            for(BasicBlock *BB :OrigBBs){\n\t            //所有都跳转到尾部\n                BB-&gt;moveBefore(LoopEnd);\n                SwitchI-&gt;addCase(builderSwitch.getInt32(BBtoID[BB]), BB);\n            }\n\t\t\t\n\t\t\t//用于兜底，跳转回循环控制头部\n            IRBuilder&lt;&gt; builderEnd(LoopEnd);\n            builderEnd.CreateBr(LoopEntry);\n\t\t\t\n\t\t\t//遍历每个代码块，插入修改状态变量的指令\n\t\t\t//这里根据下一个跳转到的代码块来标识状态变量\n\t\t\t//这里的代码跟获取初次跳转数值一模一样\n            for(BasicBlock *BB : OrigBBs){\n                Instruction *Term = BB-&gt;getTerminator();\n                IRBuilder&lt;&gt; builder(BB);\n\t\t\t\t//由于跳转分为有条件跳转和无条件跳转\n\t\t\t\t//这里照样要做个判断\n                if(BranchInst*BI = dyn_cast_or_null&lt;BranchInst&gt;(Term)){\n\t                \n                    if(BI-&gt;isUnconditional()){\n                        BasicBlock *Target = BI-&gt;getSuccessor(0);\n                        if(BBtoID.count(Target)){\n                            builder.SetInsertPoint(Term);\n                            builder.CreateStore(builderSwitch.getInt32(BBtoID[Target]), SwitchVar);\n                            builder.CreateBr(LoopEntry);\n                            Term-&gt;eraseFromParent();\n                        }\n                    }\n                    else if(BI-&gt;isConditional()){\n                        BasicBlock *TrueBB = BI-&gt;getSuccessor(0);\n                        BasicBlock *FalseBB = BI-&gt;getSuccessor(1);\n                        if(BBtoID.count(TrueBB) &amp;&amp; BBtoID.count(FalseBB)){\n                            builder.SetInsertPoint(Term);\n                            Value *Select = builder.CreateSelect(\n                                BI-&gt;getCondition(),\n                                builder.getInt32(BBtoID[TrueBB]),\n                                builder.getInt32(BBtoID[FalseBB]),\n                                \"next_state\"\n                            );\n                            builder.CreateStore(Select, SwitchVar);\n                            builder.CreateBr(LoopEntry);\n                            Term-&gt;eraseFromParent();\n                        }\n                    }\n                }\n            }\n        }\n    };\n}\n</code></pre>\n<h1 id=\"编译和使用\">编译和使用</h1>\n<p><strong>这部分之前文章讲过，这里就不浪费篇幅，不懂的可以翻看</strong></p>\n<h1 id=\"使用效果演示\">使用效果演示</h1>\n<h3 id=\"未使用时\">未使用时</h3>\n<p><img alt=\"控制流3\" class=\"lazyload\" /><br />\n<img alt=\"控制流4\" class=\"lazyload\" /></p>\n<h3 id=\"使用后\">使用后</h3>\n<p><img alt=\"控制流1\" class=\"lazyload\" /><br />\n<img alt=\"控制流2\" class=\"lazyload\" /></p>\n<p><strong>至此，我们成功实现了控制流平坦化</strong></p>\n<p><strong>如果❤喜欢❤本系列教程，就点个关注吧，后续不定期更新~</strong></p>\n\n\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/ClownLMe/\" target=\"_blank\">ClownLMe</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/ClownLMe/p/19581710\" target=\"_blank\">https://www.cnblogs.com/ClownLMe/p/19581710</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 22:54</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ClownLMe\">ClownLMe</a>&nbsp;\n阅读(<span id=\"post_view_count\">1</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战",
      "link": "https://www.cnblogs.com/algieba/p/19581327",
      "published": "",
      "description": "<div class=\"postcontent\">\n\t\t\t    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260205193318638-616281954.png\" />\n        会跑代码还不够，我们要懂原理。本文从 Transformer 的底层视角出发，图解从位置编码到注意力机制的全流程；并基于 Kaggle 平台，深入拆解 HuggingFace Transformers 库的“铁三角”组件与生成参数的玄机。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"大模型实战-03-拆解-transformers从原理图解到-huggingface-transformers-实战\">[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战</h1>\n<blockquote>\n<p><strong>核心摘要 (TL;DR)</strong></p>\n<ul>\n<li><strong>原理</strong>：图解 Transformer 是如何通过“注意力机制”和“位置编码”来理解人类语言的。</li>\n<li><strong>实战</strong>：在 <strong>Kaggle (双 T4 GPU)</strong> 环境下，拆解 HuggingFace 代码的“铁三角”（Config, Tokenizer, Model）。</li>\n<li><strong>技巧</strong>：掌握 <code>Temperature</code> 和 <code>Top_p</code>，学会控制 AI 的“创造力”。</li>\n</ul>\n</blockquote>\n<h2 id=\"前言\">前言</h2>\n<p>各位友人们，大家好，这里是阿尔。在上一节的“炼丹”环境搭建中，咱们成功地将Qwen2.5模型运行了起来，跑通了。 但是相信大家对运行的时候的那些参数都代表着什么，都还是懵的。 这篇博客就是为此准备的，我们打算先快速大概地了解一下当前的大模型的底层原理，再结合在一起介绍通用的transformers库, 去看看代码和如何对应这些理论的。</p>\n<h2 id=\"1-transformer-极简原理大模型是怎么思考的\">1. Transformer 极简原理：大模型是怎么思考的？</h2>\n<p>大模型和普通的机器学习模型一样，本质也是一个函数，不同的是，传统机器学习可能输入的是一些整理好的数据，比如房子的尺寸，地段，购买时长，和市中心的距离等等数据，输出一个预测的放假，而大模型输入的是我们的问题，拿到的是大模型给出的回答。但，咱们究其根本，它们都可以看作是一个函数，我们输入一些东西，经过运算之后，输出给我们一些东西。</p>\n<p>对于大模型，它的本质就是一个<strong>下一个字符预测器</strong>，我们输入一些文字，它只负责根据它所训练的海量数据，输出最符合，最有可能的下一个字符，就像一个<strong>文字接龙机器</strong>，其核心任务只有一个：<strong>根据上文，猜下一个字是什么。</strong></p>\n<p>为了实现这个目标，Transformer 架构经历了一个精密的流水线：<br />\n<img alt=\"包含Transformer的5个步骤的总览图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/01_cover.png\" /><br />\n下面，我们会稍微详细一点地介绍一下各个步骤。</p>\n<h3 id=\"11-第一步token-化-tokenization--查字典\">1.1 第一步：Token 化 (Tokenization) —— 查字典</h3>\n<p>大模型既然是一个函数，那么肯定是针对数字进行处理的，所以，我们就需要一个法子，去将我们的文字字符（甚至是图片）变成大模型认识的数字（或者说张量，向量）。 这就是Token化，去做这一步操作的函数，或者模块就是<strong>分词器（Tokenizer）</strong>。</p>\n<p>那么<strong>Tokenizer</strong>具体是如何工作的？它实际上就是将一个个的字符，对应成一个个数字，或者说ID，就像ASCII码一样，不过它做得更高级。</p>\n<p>把每一个字都找一个数字对应，有点奢侈，特别是对于英语,act, acting,action,actor其实都有相同的词根，主要的语义来源于其词根act，所以分词器是按照词元（token）去拆分的，能把有些词拆成词根、前缀和后缀等等，当然具体如何拆，取决于字典如何定义，字典有多大。</p>\n<p>这里我们给一个简单的例子</p>\n<ul>\n<li><strong>输入</strong>：“我爱 AI”</li>\n<li><strong>动作</strong>：切分 -&gt; <code>[\"我\", \"爱\", \"AI\"]</code> -&gt; 查表 -&gt; <code>[2301, 452, 1083]</code><br />\n<img alt=\"Token化的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.1_tokenization.png\" /></li>\n</ul>\n<h3 id=\"12-第二步embedding--位置编码--赋予含义与顺序\">1.2 第二步：Embedding &amp; 位置编码 —— 赋予含义与顺序</h3>\n<p>有了token之后，我们想知道词和词的关系，我们想要通过一个可以量化的量去判断两个词是否是有关系的，关系多大。这就引入了我们的下一个模块，词嵌入模块（Embedding）。</p>\n<ul>\n<li><strong>Embedding (词向量)</strong>：把每个 ID 变成一个长长的向量（比如 4096 维的数组）。这个向量在模型训练之前是随机的，其后随着海量的训练数据洗礼，越相关的词向量越靠近，越不相关的词越远离。这个向量代表了词的<strong>含义</strong>。比如“猫”和“狗”的向量在空间里距离很近，“苹果”和“手机”在某种语境下也更近。</li>\n</ul>\n<p>光知道词和词的关系还不够，“我爱你”和“你爱我”的每一个词都是相同的，但是它们确实可以不相关的两个句子，“小狗咬了我”和“我咬了小狗”，也会被人视作“可以正常理解”和“这人好像不对劲”两种完全不同的理解。显而易见，词语在句中的位置是一个非常重要的信息，我们不能弄丢它，也需要将这一部分信息传递给模型训练时候去学习。</p>\n<ul>\n<li><strong>Positional Encoding (位置编码)</strong>：Transformer 是并行计算的（它一眼看完所有词），这导致它不知道“我爱你”和“你爱我”的区别。所以，我们需要给每个词贴上一个“座位号”，告诉模型谁在前面，谁在后面。<br />\n<img alt=\"Embedding &amp; 位置编码 —— 赋予含义与顺序示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.2_ebedding.png\" /></li>\n</ul>\n<h3 id=\"13-第三步self-attention-自注意力--寻找关系\">1.3 第三步：Self-Attention (自注意力) —— 寻找关系</h3>\n<p>接下来就是大模型的灵魂，我们想知道一句话里的每一个词元和其他词元的<strong>关联度</strong>，其实就是上下文的联系。当模型处理“苹果”这个词时，如果上下文里有“手机”、“发布会”，注意力机制会告诉模型：“嘿，这里的‘苹果’指的是科技公司，不是水果！”,自注意力机制，让模型能理解上下文。<br />\n<img alt=\"Self-Attention 示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.3-self-attention.png\" /></p>\n<h3 id=\"14-第四步mlp-前馈神经网络--消化吸收\">1.4 第四步：MLP (前馈神经网络) —— 消化吸收</h3>\n<p>如果说注意力机制是“看”，那么 MLP 就是“想”。它包含多层神经元，负责对提取到的信息进行复杂的非线性变换和逻辑推理,也就是传统的深度学习。<br />\n<img alt=\"MLP (前馈神经网络)示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.4-mlp.png\" /></p>\n<h3 id=\"15-第五步decoder--softmax--输出概率\">1.5 第五步：Decoder &amp; Softmax —— 输出概率</h3>\n<p>经过层层计算，模型最终会输出一个包含了所有词汇（比如 15 万个词）的<strong>概率列表</strong>。</p>\n<ul>\n<li><code>AI</code> (80%)</li>\n<li><code>吃</code> (10%)</li>\n<li><code>睡</code> (5%)<br />\n...<br />\n最后，我们根据这个概率列表，选择下一个词。<br />\n<img alt=\"输出概率的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.5-decoder&amp;softmax.png\" /></li>\n</ul>\n<hr />\n<p>这就是大模型词语接龙的原理了。</p>\n<h2 id=\"2-拆解模型文件夹下载下来的到底是什么\">2. 拆解模型文件夹：下载下来的到底是什么？</h2>\n<p>理论讲完了，我们来看看在 <strong>Kaggle</strong> 的文件系统里，这些理论变成了什么文件。</p>\n<p>当你下载一个模型（以 Qwen2.5-7B 为例）时，文件夹结构如下：</p>\n<p><img alt=\"safetensor格式的文件组织示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/safetensor.png\" /></p>\n<h3 id=\"21-核心架构configjson\">2.1 核心架构：config.json</h3>\n<p><strong>config.json</strong>: 是大模型的身份证，也可以说是体检表，它其中就是真正的我们的模型，具体由哪些层构成，是什么架构类型，隐藏层有多深，注意力头的数量有几个，词表的大小是多少。对于大模型而言，它就是模型本身，也是<strong>骨架</strong>，因为大模型重要的是训练完的参数，模型本身是很小的。<br />\n<img alt=\"config.json的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/config.json.png\" /></p>\n<h3 id=\"22-行为预设generation_configjson\">2.2 行为预设：generation_config.json</h3>\n<p><strong>generation_config.json</strong>:是模型的出厂默认设置。<br />\n<img alt=\"generation_config的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/generation_config_.png\" /></p>\n<h3 id=\"23-大脑本身safetensors和indexjson\">2.3 大脑本身：*.safetensors和*.index.json</h3>\n<p>有了config.json中的躯体，我们再从<em>.safetensors和</em>.index.json载入灵魂，这才是我们能说会道的大模型。</p>\n<ul>\n<li><strong><code>model-xxxxx.safetensors</code></strong>：这里面存的是实打实的<strong>张量（Tensor）数据</strong>，即数十亿个参数的浮点数。为了方便存储和加载，通常会被切分成多个 2GB-5GB 的小文件（Shard）。</li>\n<li><strong><code>model.safetensors.index.json</code></strong>：这是一张<strong>藏宝图</strong>。因为权重被切分了，模型需要知道“第 5 层的权重”到底藏在哪个文件里。\n<ul>\n<li><strong>内部长这样</strong>：<pre><code class=\"language-json\">{\n  \"metadata\": { \"total_size\": 15423653888 },\n  \"weight_map\": {\n    \"model.layers.0.self_attn.q_proj.weight\": \"model-00001-of-00004.safetensors\",\n    \"model.layers.20.mlp.gate_proj.weight\": \"model-00003-of-00004.safetensors\"\n  }\n}\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"数字和文字的翻译官tokenizer相关文件\">数字和文字的翻译官：tokenizer相关文件</h3>\n<ul>\n<li><strong><code>vocab.json</code> / <code>merges.txt</code></strong>：这是最原始的<strong>生词表</strong>。记录了所有字、词根对应的 ID。</li>\n<li><strong><code>tokenizer.json</code></strong>：这是一个<strong>编译后</strong>的高效字典文件，包含了分词的所有逻辑（Pre-tokenization, Normalization 等），加载速度比读原始文本快得多。</li>\n<li><strong><code>tokenizer_config.json</code> (至关重要)</strong>：这是分词器的<strong>配置文件</strong>。\n<ul>\n<li>它定义了<strong>特殊符号</strong>（Special Tokens）：比如哪个 ID 代表“开始”，哪个代表“结束”。</li>\n<li>它包含了 <strong>Chat Template (聊天模板)</strong>：这是一段 Jinja2 代码，决定了 <code>apply_chat_template</code> 如何工作。</li>\n<li><strong>内部长这样</strong>：<pre><code class=\"language-json\">{\n  \"chat_template\": \"{% for message in messages %}...&lt;|im_start|&gt;...\",\n  \"eos_token\": \"&lt;|im_end|&gt;\",\n  \"pad_token\": \"&lt;|endoftext|&gt;\"\n}\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-transformers库实战代码中的铁三角\">3. Transformers库实战：代码中的“铁三角”</h2>\n<p>在Transforms库中，我们永远绕不开三个核心类。</p>\n<p><strong>环境准备：</strong><br />\n在 Kaggle 右侧设置中，确保 <strong>Internet: On</strong> 且 <strong>Accelerator: GPU T4 x2</strong>。</p>\n<pre><code class=\"language-python\">!pip install -U transformers accelerate bitsandbytes\n</code></pre>\n<h3 id=\"31-autotokenizer-翻译官\">3.1 AutoTokenizer (翻译官)</h3>\n<p>对应理论中的 <strong>Token 化</strong> 步骤,和模型文件中的tokenizer相关文件。</p>\n<pre><code class=\"language-python\">from transformers import AutoTokenizer\n\nmodel_path = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1/\"\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\ntext = \"Transformer is amazing\"\n# 1. 编码 (Encode): 文本 -&gt; 数字 ID\ninput_ids = tokenizer.encode(text)\nprint(f\"原文: {text}\")\nprint(f\"数字 ID: {input_ids}\")\n\n# 2. 解码 (Decode): 数字 ID -&gt; 文本\ndecoded_text = tokenizer.decode(input_ids)\nprint(f\"还原: {decoded_text}\")\n</code></pre>\n<p>得到的结果将会是这样</p>\n<pre><code class=\"language-bash\">原文: Transformer is amazing\n数字 ID: [46358, 374, 7897]\n还原: Transformer is amazing\n</code></pre>\n<p><strong>对话格式：Chat Template</strong><br />\n大模型需要特定的对话格式（Prompt）。来将模型的回答，和用户的问题做区分, 我们一般都可以通过载入语言模型对应模板（不同家的模型，可能模板会有不同），甚至去拼装历史记录。</p>\n<pre><code class=\"language-python\">messages = [\n    {\"role\": \"system\", \"content\": \"你是一个物理学家。\"},\n    {\"role\": \"user\", \"content\": \"用一句话解释相对论。\"}\n]\n\n# 自动应用聊天模板\nformatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nprint(\"模型实际看到的输入:\\n\", formatted_prompt)\n</code></pre>\n<p>运行结果如下</p>\n<pre><code class=\"language-bash\">模型实际看到的输入:\n &lt;|im_start|&gt;system\n你是一个物理学家。&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n用一句话解释相对论。&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></pre>\n<p>看到这里，我们也能更好地理解，为什么一个本质是词语接龙的模型，能够区分问题，然后做出回答。因为在训练的过程中，我们会让他知道&lt;|im_start&gt;表示一个message的开始，其中会告诉模型，这句话是谁说的，这句话在什么位置结束，它接龙的时候也会带上开始和结束符号，在推理模型中，甚至会带上思考的标签。</p>\n<h3 id=\"32-automodel-大脑本体\">3.2 AutoModel (大脑本体)</h3>\n<p>对应理论中的 <strong>Embedding -&gt; Attention -&gt; MLP</strong> 计算过程,通过从config.json中加载模型躯体，在加载上模型的safetensor灵魂数据以及generation_config.json的默认初始化,<br />\n在 Kaggle 上，我们拥有 <strong>双 T4 (15GB x 2)</strong>，一定要利用 <code>device_map=\"auto\"</code> 让库自动分配显存。</p>\n<pre><code class=\"language-python\">from transformers import AutoModelForCausalLM\nimport torch\n\n# 加载模型\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",      # 关键！自动将模型切分到两张 T4 显卡上\n    torch_dtype=torch.float16, # 使用半精度，节省显存\n    trust_remote_code=True\n)\n\nprint(f\"模型加载成功！显存分布: {model.hf_device_map}\")\n</code></pre>\n<p>输出结果为</p>\n<pre><code class=\"language-bash\">模型加载成功！显存分布: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n</code></pre>\n<h2 id=\"4-掌控生成的调节旋钮\">4. 掌控生成的“调节旋钮”</h2>\n<p>在模型输出的过程中，我们有一些参数可以对输出结果进行调节，对应于我们讲理论部分的中的 <strong>第五步 (Softmax 概率输出)</strong>, 我们有一堆下一个词元的概率分布了，但是我们应该如何去选择呢？</p>\n<h3 id=\"41-temperature-温度\">4.1 Temperature (温度)</h3>\n<p>我们可以设定的Temperature参数，我们在generation_config.json中也看见过它</p>\n<p>值越大，更热，更具创造性，更容易输出各种天马行空的词, 会缩小所有词元的差距，<strong>雨露均沾</strong>，以达到创造性，让低概率的词，也有机会被选中，当然,也更容易胡说八道，出现幻觉.<br />\n值越小，更冷，更严肃，在温度为0的时候甚至会固定输出最高的那个词，它会拉大高概率和低概率的差距，<strong>赢家通吃</strong>，让模型的回答更稳定，严谨。</p>\n<h3 id=\"42-top_p-核采样\">4.2 Top_p (核采样)</h3>\n<p>和温度不同，我们还有另一种方式，这种方式更类似于拉网，我们只要可能性前80%的词，在那些词里进行挑选，这个可能性就是P，这个选词（采样）方法又叫top_p(核采样).<br />\n简而言之：其只在累积概率达到 P (e.g., 0.9) 的前几个词里选。直接切掉尾部那些极低概率的离谱词。</p>\n<h3 id=\"43-实验一下\">4.3 实验一下</h3>\n<p>我们这里先定义一个函数，以温度和top_p为参数去测试不同的参数对回答的影响</p>\n<pre><code class=\"language-python\"># 定义一个测试函数\ndef test_generation(temp, top_p, prompt_text):\n    messages = [\n        {\"role\": \"system\", \"content\": \"你是一个前卫的科幻小说家。\"},\n        {\"role\": \"user\", \"content\": prompt_text}\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    print(f\"\\n======== 设置: Temperature={temp}, Top_p={top_p} ========\")\n\n    try:\n        generated_ids = model.generate(\n            **inputs,\n            max_new_tokens=100,  # 限制长度，方便快速看结果\n            temperature=temp,\n            top_p=top_p,\n            do_sample=True,      # 必须开启采样\n            pad_token_id=tokenizer.eos_token_id\n        )\n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        # 只打印回答部分\n        print(response.split('assistant')[-1].strip())\n    except Exception as e:\n        print(f\"生成出错: {e}\")\n</code></pre>\n<h3 id=\"温度实验\">温度实验</h3>\n<p>然后再其下新建code block去测试，看看效果</p>\n<pre><code class=\"language-python\">prompt = \"请用这三个词写一个微故事：量子、失恋、炒饭。\"\n\n# 1. 低温模式 (0.1)：严谨、死板\ntest_generation(temp=0.1, top_p=0.9, prompt_text=prompt)\n\n# 2. 适中模式 (0.7)：正常、流畅\ntest_generation(temp=0.7, top_p=0.9, prompt_text=prompt)\n\n# 3. 高温模式 (1.5)：疯狂、混乱\n# 注意：可能会输出乱码或完全不通顺的句子\ntest_generation(temp=1.5, top_p=0.9, prompt_text=prompt)\n</code></pre>\n<p>这是我的运行结果</p>\n<pre><code class=\"language-bash\">======== 设置: Temperature=0.1, Top_p=0.9 ========\n在量子世界里，时间与空间的概念变得模糊不清，而李明的世界也因为一段失败的恋情而变得一片混沌。他尝试着用量子纠缠理论来修复自己破碎的心灵，却意外地将自己从现实世界送入了一个平行宇宙。\n\n在这个平行宇宙中，李明发现了一家特别的餐馆，这里的厨师是一位曾经的恋人，她正在为一位顾客准备一道特别的炒饭。这道炒饭不仅色香味俱全，还\n\n======== 设置: Temperature=0.7, Top_p=0.9 ========\n在量子世界的边缘，李明独自一人坐在一家不起眼的小餐馆里，面前是一碗普通的炒饭。他和女朋友分手的原因，是因为她沉迷于虚拟现实中的量子世界，而他却对现实世界充满了留恋。\n\n就在几天前，他们最后一次争吵后，她告诉他：“我找到了真正的自我——一个穿梭在量子世界的探索者。”她留下了一盘未吃完的炒饭，然后消失在了虚拟现实中。\n\n李明望着那盘炒饭，\n\n======== 设置: Temperature=1.5, Top_p=0.9 ========\n标题：时空泡饭\n\n李明最近失恋了，每天只能煮一大锅泡饭来消磨时间。\n\n这天李明正在做饭，他忽然接收到一条量子信号。他惊奇地发现那是自己失恋前女友的坐标位置。想到能与自己相爱过的人共度时光是多么美妙的事情，他便将自己煮了一锅泡饭送进了坐标传送器。\n\n结果却是一锅炒饭。\n\n李明百思不解。\n</code></pre>\n<h3 id=\"top_p实验\">top_p实验</h3>\n<p>接下来是top_p</p>\n<pre><code class=\"language-python\">prompt_2 = \"请给一种不存在的颜色起个名字，并描述它的样子。\"\n\n# 1. 极窄采样 (0.01)：只选概率最高的那个词（近似贪婪搜索）\ntest_generation(temp=0.8, top_p=0.01, prompt_text=prompt_2)\n\n# 2. 宽广采样 (0.95)：允许罕见词出现\ntest_generation(temp=0.8, top_p=0.95, prompt_text=prompt_2)\n</code></pre>\n<p>输出结果为</p>\n<pre><code class=\"language-bash\">======== 设置: Temperature=0.8, Top_p=0.01 ========\n这种不存在的颜色我命名为“星尘紫”。它是一种梦幻般的颜色，介于紫色和银色之间，仿佛是宇宙中无数微小的星辰碎片在闪烁时所散发出的光芒。在不同的光线下，星尘紫会呈现出不同的色调，有时偏紫，有时偏银，有时又像是掺杂了点点星光的淡蓝色。它既神秘又优雅，仿佛能让人感受到宇宙的浩瀚与深邃。\n\n======== 设置: Temperature=0.8, Top_p=0.95 ========\n这种不存在的颜色我称之为“星际幻彩”（Stellar Mirage）。在视觉上，它并非单一色调，而是一种动态变化的色彩组合，像是无数微小的光点在眼前闪烁变幻，这些光点包含了所有可见光谱的颜色，同时又带着一种神秘的、不可名状的色彩。\n\n当观察者注视着“星际幻彩”的时候，他可以看到蓝、绿、紫等颜色快速地在眼前切换和混合，它们以一种几\n</code></pre>\n<h2 id=\"5-完整-transformers-代码实战\">5. 完整 Transformers 代码实战</h2>\n<p>把所有积木搭在一起，这就是一段标准的模型推理代码：</p>\n<pre><code class=\"language-python\">import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 1. 设置模型 ID\nmodel_id = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1/\"\n\n# 2. 加载翻译官\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n\n# 3. 加载大脑 (双卡模式)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\n\n# 4. 准备输入\nprompt = \"请用这三个词写一个微小说：Kaggle、深夜、爆显存\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是一个幽默的程序员。\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# 5. 生成 (调节参数！)\nprint(\" 正在生成...\")\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512,\n    temperature=0.8,  # 稍微有点创意\n    top_p=0.9,        # 剔除离谱词\n    do_sample=True    # 必须开启采样，温度才生效\n)\n\n# 6. 解码并输出\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(\"-\" * 20)\nprint(f\" 回答:\\n{response}\")\nprint(\"-\" * 20)\n</code></pre>\n<p>这是输出结果</p>\n<pre><code class=\"language-bash\">Loading checkpoint shards: 100%\n 4/4 [00:13&lt;00:00,  3.27s/it]\nThe module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\nWARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n 正在生成...\n--------------------\n 回答:\n在一个寒冷的深夜，李雷坐在他那间堆满咖啡罐和代码文件的房间里，屏幕上是Kaggle竞赛的数据集。他正尝试训练一个复杂的深度学习模型。然而，就在他认为胜利在望的时候，“嘶~”的一声，显示器瞬间变成了一片漆黑，伴随着一声悲壮的“爆显存了”。\n\n李雷揉了揉眼睛，看着眼前一片空白的屏幕，心中充满了挫败感，但他转念一想：“还好不是‘爆内存了’，否则我这台老旧电脑可能就要彻底退休了。”于是，他又开始调整参数，希望能在这个深夜里找到那个隐藏在数据海洋中的宝藏。\n--------------------\n</code></pre>\n<h3 id=\"hint\">Hint:</h3>\n<p><strong>所有的代码，都可以在 <a href=\"https://www.kaggle.com/code/thaodinhoio/llm03-transformers\" rel=\"noopener nofollow\" target=\"_blank\">这个笔记本</a>中直接获取运行哦</strong></p>\n<h2 id=\"6-常见问题-qa\">6. 常见问题 (Q&amp;A)</h2>\n<p><strong>Q: 在 Kaggle 上 <code>device_map=\"auto\"</code> 是必须的吗？</strong><br />\n<strong>A:</strong> 如果你使用单卡 T4 (15GB) 跑 7B 模型（约 14GB），勉强能塞进一张卡。但如果你开启了 Kaggle 的 <strong>T4 x2</strong>，为了利用全部 30GB 显存，<strong>必须</strong>加这个参数，否则模型只会塞进第一张卡，导致第一张爆满，第二张围观。</p>\n<p><strong>Q: 为什么生成的每一句话都不一样？</strong><br />\n<strong>A:</strong> 因为我们开启了 <code>do_sample=True</code> 并且设置了 <code>temperature &gt; 0</code>。模型在选择下一个词时是<strong>按概率随机抽取</strong>的。如果你想让结果每次都一样（比如做数学题），请设置 <code>do_sample=False</code>（此时温度失效，变为贪婪解码）。</p>\n<p><strong>Q: 什么是 Logits？</strong><br />\n<strong>A:</strong> 在代码深处，模型输出的那个“概率表”在变成百分比之前，叫 Logits（未归一化的数值）。Softmax 函数的作用就是把 Logits 变成概率。你可以把 Logits 理解为模型对每个词的“原始打分”。</p>\n<p><strong>Q: Token 和字是一一对应的吗？</strong><br />\n<strong>A:</strong> 不一定。</p>\n<ul>\n<li>英文：通常一个单词是一个 Token，长单词可能被切分。</li>\n<li>中文：通常一个汉字是一个 Token，但常见词（如“你好”）可能会合并为一个 Token。</li>\n<li>平均来说，<strong>0.75 个英文单词 ≈ 1 Token</strong>，<strong>1 个汉字 ≈ 1.5 - 2 Token</strong>（取决于分词器效率，Qwen 的中文压缩率很高）。</li>\n</ul>\n<hr />\n<p><strong>本文作者：</strong> Algieba<br />\n<strong>本文链接：</strong> <a href=\"https://blog.algieba12.cn/llm03-know-more-about-transformer-lib/\" rel=\"noopener nofollow\" target=\"_blank\">https://blog.algieba12.cn/llm03-know-more-about-transformer-lib/</a><br />\n<strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</p>\n<pre><code>\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"itemdesc\">\n\t\t\t发表于 \n<span id=\"post-date\">2026-02-05 19:33</span>&nbsp;\n<a href=\"https://www.cnblogs.com/algieba\">阿尔的代码屋</a>&nbsp;\n阅读(<span id=\"post_view_count\">33</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</div>"
    },
    {
      "title": "吴恩达深度学习课程五：自然语言处理  第三周：序列模型与注意力机制 课后习题与代码实践",
      "link": "https://www.cnblogs.com/Goblinscholar/p/19579899",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Goblinscholar/p/19579899\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 16:34\">\n    <span>吴恩达深度学习课程五：自然语言处理  第三周：序列模型与注意力机制 课后习题与代码实践</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>此分类用于记录吴恩达深度学习课程的学习笔记。<br />\n课程相关信息链接如下：</p>\n<ol>\n<li>原课程视频链接：<a href=\"https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&amp;from_spmid=playlist.playlist-detail.0.0&amp;is_story_h5=false&amp;mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&amp;plat_id=116&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&amp;share_source=COPY&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1713085655&amp;unique_k=DfBgvFW&amp;up_id=8654113&amp;vd_source=e035e9878d32f414b4354b839a4c31a4\" rel=\"noopener nofollow\" target=\"_blank\">[双语字幕]吴恩达深度学习deeplearning.ai</a></li>\n<li>github课程资料，含课件与笔记:<a href=\"https://github.com/robbertliu/deeplearning.ai-andrewNG\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达深度学习教学资料</a></li>\n<li>课程配套练习（中英）与答案：<a href=\"https://blog.csdn.net/u013733326/article/details/79827273\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达深度学习课后习题与答案</a></li>\n</ol>\n<p>本篇为第五课第三周的课后习题和代码实践部分。</p>\n<hr />\n<h1 id=\"1理论习题\">1.理论习题</h1>\n<p><a href=\"https://blog.csdn.net/u013733326/article/details/90144255\" rel=\"noopener nofollow\" target=\"_blank\">【中英】【吴恩达课后测验】Course 5 - 序列模型 - 第三周测验</a></p>\n<p>在这一周的习题内容中有一道题的相关内容需要展开，先看一下题面：</p>\n<blockquote>\n<p>网络通过学习注意力分数来决定「把注意力放在哪里」，这些值是由一个小神经网络计算得到的。<br />\n这里，我们<strong>不能</strong>把 <span class=\"math inline\">\\(s^{&lt;t-1&gt;}\\)</span> 替换成 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span> 作为这个神经网络的输入，<br />\n原因如下：因为 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span> 依赖于当前的注意力权重，而当前的注意力权重又依赖于注意力分数，因此在需要计算注意力分数的时刻，<span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span> <strong>尚未被计算出来</strong>， 此时只能使用前一时刻的隐藏状态 <span class=\"math inline\">\\(s^{&lt;t-1&gt;}\\)</span>，而无法使用当前的 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span>。<br />\n<strong>答案：正确</strong></p>\n</blockquote>\n<p>这道题一眼看下来是很迷惑的，这是<strong>因为这道题讲的是最早的注意力机制的逻辑，和我们在理论部分介绍的主流逻辑有所不同</strong>。<br />\n在<a href=\"https://www.cnblogs.com/Goblinscholar/p/19563950\" target=\"_blank\">理论部分</a>，我们介绍的注意力机制，学习注意力分数的网络的输入就是 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span> ，但在最早的注意力机制中，它的输入是 <span class=\"math inline\">\\(s^{&lt;t-1&gt;}\\)</span>。<br />\n其传播逻辑可以用一句话总结：<strong>在当前时间步中，原始注意力机制将由上一时刻解码状态计算得到的上下文向量 <span class=\"math inline\">\\(c^{&lt;t&gt;}\\)</span> 作为解码器输入的一部分，用于计算当前解码状态 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span>。</strong><br />\n我们展开如下：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162801516-704399919.png\" /><br />\n14 年的原论文使用的就是这种传播方式，具有开创性价值。但在现代，这种将上下文向量直接纳入状态递推的做法更多只具有历史意义，了解即可。</p>\n<h1 id=\"2代码实践\">2.代码实践</h1>\n<p><a href=\"https://blog.csdn.net/u013733326/article/details/97619187\" rel=\"noopener nofollow\" target=\"_blank\">【中文】【吴恩达课后编程作业】Course 5 - 序列模型 - 第三周作业</a><br />\n还是先摆链接，这篇里博主就机器翻译和触发词检测两部分内容，非常详细地演示了本周的内容，但还是要注意 Keras 的导包更新问题。<br />\n我们同样使用框架来演示这部分内容，主要内容如下：</p>\n<ol>\n<li><strong>使用编码解码框架进行日期格式翻译</strong></li>\n<li><strong>在编码解码框架下对比贪心解码和束搜索性能</strong></li>\n<li><strong>使用带注意力机制的编码解码框架进行日期格式翻译</strong></li>\n</ol>\n<h2 id=\"21-数据准备\">2.1 数据准备</h2>\n<p>需要说明的是，由于 <strong>seq2seq 模型自身的编码–解码结构</strong> 以及 <strong>序列数据的时序特性</strong>，这类任务相比许多常规监督学习任务，通常<strong>对计算资源有更高的要求</strong>。<br />\n以机器翻译任务为例，网络上存在大量公开数据集，如英法翻译、中英翻译等。由于这类数据获取相对容易，同时自然语言本身具有<strong>词汇规模大、表达形式多样</strong>的特点，模型在训练过程中往往需要同时维护<strong>源语言词典和目标语言词典</strong>，其规模通常至少达到 <strong>数万级别</strong>。这直接导致模型在参数规模、计算量以及显存占用等方面的成本显著提升。<br />\n因此，这次我们选用一种相对简化的机器翻译任务——<strong>日期格式翻译</strong>，该类任务的主要特点在于<strong>词典规模小、输入输出结构清晰、语义歧义极少</strong>，可以较高效率地演示本周内容。</p>\n<p>这次我们不使用网上公开的数据集，而是拓展吴恩达老师在编程作业里的逻辑，进行<strong>人工合成数据</strong>。<br />\n我们通过设计好的脚本，生成 <strong>10000 条</strong> 以下格式的相关数据和对应标签并保存为文件：</p>\n<pre><code class=\"language-python\">HUMAN_TEMPLATES = [  \n    \"{day} {month} {year}\",  \n    \"{month} {day}, {year}\",  \n    \"{day} of {month}, {year}\",  \n    \"{month} {day} {year}\",  \n    \"{day}/{month_num}/{year_short}\",  \n    \"{month_num}/{day}/{year_short}\",  \n    \"{weekday}, {day} {month_abbr} {year}\",  \n    \"{month_abbr} {day}th {year_short}\",  \n    \"{year}-{month_num}-{day}\",  \n    \"The {day}{day_suffix} of {month}, {year}\",  \n    \"{day}th {month_abbr}, {year}\",  \n    \"{month} the {day}{day_suffix}, {year}\",  \n    \"Date: {year}/{month_num}/{day}\",  \n    \"{day} in Roman: {roman_day} {month} {year}\",  \n    \"{month_abbr}. {day}, '{year_short}\",  \n]\n</code></pre>\n<p>完整代码放在附录，打印几条生成数据如下：</p>\n<pre><code>[1] The 28th of April, 1975  ==&gt;  1975-04-28\n[2] Date: 2017/05/5  ==&gt;  2017-05-05\n[3] Jan 10th 44  ==&gt;  1944-01-10\n[4] Tuesday, 6 Nov 2096  ==&gt;  2096-11-06\n[5] The 26th of August, 2089  ==&gt;  2089-08-26\n[6] Date: 2061/08/18  ==&gt;  2061-08-18\n[7] 08/4/35  ==&gt;  1935-08-04\n[8] 27 of May, 1948  ==&gt;  1948-05-27\n[9] 8 of April, 1907  ==&gt;  1907-04-08\n[10] November 11, 1937  ==&gt;  1937-11-11\n</code></pre>\n<p>准备好数据后，我们便可以进行下一步内容。</p>\n<h2 id=\"22-模型设计\">2.2 模型设计</h2>\n<p>在<a href=\"https://www.cnblogs.com/Goblinscholar/p/19547898\" target=\"_blank\">本周第一篇</a>内容里我们就提到过：编码解码框架下的网络结构实际上是分别设计的两个子网络，在代码逻辑中，就是<strong>先分别定义设计编码器和解码器，再在整体网络中调用</strong>。我们分点展开如下：</p>\n<h4 id=\"1编码器\">（1）编码器</h4>\n<pre><code class=\"language-python\">class Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim):\n        super().__init__()\n        # 嵌入层\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        # 使用双向 LSTM\n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n    def forward(self, x):\n        embedded = self.embedding(x) \n        outputs, (hidden, cell) = self.lstm(embedded)\n        # outputs: LSTM 对每个时间步的输出,用于注意力的相关计算。\n        # (hidden,cell): 最后时间步的隐藏状态和细胞状态，是解码器的初始输入。\n        return outputs, (hidden, cell)\n\n</code></pre>\n<h4 id=\"2解码器\">（2）解码器</h4>\n<pre><code class=\"language-python\">class Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, use_attention=False):\n        super().__init__()\n        # 设置参数，默认不使用注意力机制\n        self.use_attention = use_attention\n        # 嵌入层\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        # LSTM \n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n        if use_attention:\n            # 注意力评分层，将解码器 hidden 与编码器 outputs 拼接计算注意力分数\n            self.attn = nn.Linear(hidden_dim + hidden_dim * 2, 1)\n            # 注意力结合层：将解码器 hidden 与上下文向量融合为最终 LSTM 输出\n            self.attn_combine = nn.Linear(hidden_dim + hidden_dim * 2, hidden_dim)\n\n        # 输出层：将 LSTM 输出映射到词表维度，预测下一个 token\n        self.out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_step, hidden_cell, encoder_outputs):\n        embedded = self.embedding(input_step)\n        # LSTM 前向计算，得到当前时间步输出和新的 hidden/cell\n        lstm_out, hidden_cell = self.lstm(embedded, hidden_cell)\n        # 去掉时间维度，方便注意力计算\n        query = lstm_out.squeeze(1)\n\n        if self.use_attention:\n            # 扩展 query，实际上就是把当前解码隐藏状态复制到和编码隐藏状态数相同。\n            query_exp = query.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n            # 计算注意力分数\n            attn_scores = self.attn(torch.cat((query_exp, encoder_outputs), dim=-1)).squeeze(-1)\n            # 转为注意力权重\n            attn_weights = F.softmax(attn_scores, dim=-1).unsqueeze(1)\n            # 计算上下文向量 bmm:批量矩阵乘法\n            context = torch.bmm(attn_weights, encoder_outputs).squeeze(1)\n            # 将 query 与上下文向量融合\n            combined = torch.cat((query, context), dim=1)\n            output = torch.tanh(self.attn_combine(combined))\n        else:\n            # 不使用注意力时，直接用 LSTM 输出预测\n            output = query\n\n        # 输出层\n        output = self.out(output)\n        return output, hidden_cell\n</code></pre>\n<h4 id=\"3整体网络\">（3）整体网络</h4>\n<pre><code class=\"language-python\">class Seq2Seq(nn.Module):  \n    def __init__(self, encoder, decoder):  \n        super().__init__()  \n        # 编码器和解码器直接作为初始化参数，在主函数中创建传入\n        self.encoder = encoder  \n        self.decoder = decoder \n         \n    def forward(self, src, tgt):  \n        # src:样本 tgt:标签\n        # 编码器\n        enc_outputs, (hidden, cell) = self.encoder(src)\n\n        # 这里是合并双向 LSTM 的 hidden/cell\n        # 将两个方向的 hidden/cell 在层维度上分开，再求和或拼接作为解码器初始状态\n        hidden = hidden.view(2, -1, hidden.size(-1)).sum(dim=0, keepdim=True)\n        cell   = cell.view(2, -1, cell.size(-1)).sum(dim=0, keepdim=True)\n        dec_hidden = (hidden, cell)  # 解码器初始状态\n\n        outputs = []  # 存储每个时间步的输出\n\n        # 解码器\n        for t in range(tgt.size(1)):\n            # 训练逻辑：每步使用真实标签作为输入\n            out, dec_hidden = self.decoder(\n                tgt[:, t].unsqueeze(1), dec_hidden, enc_outputs\n            )\n            # tgt[:, t] 因为批次训练，每次要去取第 t 列\n            # out: 当前时间步预测的 token 概率分布 [batch_size, vocab_size]\n            # dec_hidden: 更新后的解码器隐藏状态\n            outputs.append(out)  # 保存当前输出\n\n        # 将所有时间步输出堆叠为最终结果。\n        return torch.stack(outputs, dim=1)\n\n</code></pre>\n<p>这样我们就完成了模型的设计，继续下一部分内容。</p>\n<h2 id=\"23-解码函数\">2.3 解码函数</h2>\n<p>还是理论部分提到的，在推理阶段，解码器的输出其实是一系列的概率分布，我们<strong>使用解码函数，就是设计相应的搜索策略来寻找最大的联合概率，得到最终输出。</strong><br />\n这里，我们定义完整的解码函数，<strong>主逻辑为贪心解码，补充束搜索的相关逻辑并通过参数选择是否使用</strong>：</p>\n<pre><code class=\"language-python\">def decode(model, input_tensor, use_beam_search=False, beam_width=3, max_len=20):\n    \"\"\"\n    参数：\n    - model: 已训练的 Seq2Seq 模型（包含 encoder 和 decoder）\n    - input_tensor: 输入序列的 tensor（一个样本）\n    - use_beam_search: 是否使用束搜索\n    - beam_width: 束搜索时的候选数量\n    - max_len: 解码最大长度\n    \"\"\"\n    model.eval()  # 评估模式\n    with torch.no_grad():  \n        input_tensor = input_tensor.unsqueeze(0) \n        # 编码器传播\n        enc_outputs, (hidden, cell) = model.encoder(input_tensor)\n        hidden = hidden.view(2, -1, hidden.size(-1)).sum(dim=0, keepdim=True)\n        cell   = cell.view(2, -1, cell.size(-1)).sum(dim=0, keepdim=True)\n        dec_hidden = (hidden, cell)  # 解码器初始状态\n        \n        # 贪心解码\n        if not use_beam_search:\n            # 初始解码输入：SOS：初始符，这里是简化为了 \\t\n            dec_input = torch.tensor([[target_token_index['\\t']]], device=device)\n            result = []\n            for _ in range(max_len):\n                # 解码器传播\n                out, dec_hidden = model.decoder(dec_input, dec_hidden, enc_outputs)\n                # 取当前时间步最大概率的 token\n                pred = out.argmax(dim=-1)\n                token = pred.item()\n                if token == target_token_index['\\n']:  # 遇到 EOS 停止，同样简化为\\n\n                    break\n                result.append(reverse_target_char_index[token])  # 保存字符\n                dec_input = pred.unsqueeze(0)  # 下一步输入为当前预测，即自回归。\n\n            return ''.join(result)  # 返回解码后的字符串\n\n       \n        # 束搜索\n        candidates = [([target_token_index['\\t']], 0.0, dec_hidden)]  # 初始化候选序列\n        for _ in range(max_len):\n            new_cands = []\n            for seq, score, h in candidates:\n                # 如果序列已遇到 EOS，保留候选\n                if seq[-1] == target_token_index['\\n']:\n                    new_cands.append((seq, score, h))\n                    continue\n                # 当前时间步输入\n                inp = torch.tensor([[seq[-1]]], device=device)\n                out, new_h = model.decoder(inp, h, enc_outputs)\n                # 计算 log 概率\n                log_probs = F.log_softmax(out, dim=-1).squeeze(0)\n                # 取 top-k 候选\n                top_vals, top_idx = log_probs.topk(beam_width)\n                for v, idx in zip(top_vals, top_idx):\n                    new_seq = seq + [idx.item()]  # 扩展序列\n                    new_score = score + v.item()  # 更新累积 log 概率\n                    new_cands.append((new_seq, new_score, new_h))\n            # 按累积概率排序，保留前 beam_width 个序列\n            candidates = sorted(new_cands, key=lambda x: x[1], reverse=True)[:beam_width]\n        # 最终选择概率最高的序列\n        best = candidates[0][0]\n        # 去掉控制符，转换为最终字符\n        return ''.join(reverse_target_char_index.get(i, '') \n                       for i in best[1:] if i != target_token_index['\\n'])\n\n</code></pre>\n<p>解码的逻辑也被使用在之后的评估环节中。</p>\n<h2 id=\"24-评估指标belu-和完全匹配率\">2.4 评估指标：BELU 和完全匹配率</h2>\n<p>自然，在完成训练后，我们要有相应的指标来进行评估，首先就是我们在理论部分介绍的 BELU ，它通过计算输出和标签在不同尺度上的匹配度来评估模型效果。<br />\n但是，我们说：BELU 这类语言指标使用的原因是因为存在”同义不同形“的情况。<br />\n而<strong>在日期翻译里，翻译结果是唯一的：错一个数字就是完全不同的一天。</strong><br />\n因此，我们再手工定义一个完全匹配率：输出和标签只有相同和不相同两种情况。<br />\n这样，<strong>我们使用 BELU 来评估序列模型拟合的大致效果，再用完全匹配率来观察在日期翻译任务中模型的真正性能。</strong></p>\n<pre><code class=\"language-python\">from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n# BLEU\ndef evaluate_bleu(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):\n    total_bleu = 0\n    # BLEU 平滑方法，避免短句导致分数过低\n    smoother = SmoothingFunction().method4\n\n    for i in range(len(test_enc_input)):\n        # 使用 decode 函数生成预测序列\n        pred = decode(model, test_enc_input[i], use_beam_search, beam_width)\n        # 去掉目标序列的起始符和结束符\n        true = test_targets[i][1:-1]\n        # BLEU 接受列表形式：ref 是参考序列列表，hyp 是预测序列\n        ref = [list(true)]\n        hyp = list(pred)\n        # 计算当前样本 BLEU 分数\n        bleu = sentence_bleu(ref, hyp, smoothing_function=smoother)\n        total_bleu += bleu\n\n    # 平均 BLEU 分数，并转换为百分比\n    avg_bleu = total_bleu / len(test_enc_input) * 100\n    print(f\"BLEU: {avg_bleu:.2f}\")\n    return avg_bleu\n\n# 完全匹配率\ndef evaluate_exact_match(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):\n    correct = 0\n    total = len(test_enc_input)\n    for i in range(total):\n        pred = decode(model, test_enc_input[i], use_beam_search, beam_width)\n        true = test_targets[i][1:-1]\n        # 判断预测与真实是否完全一致\n        if pred == true:\n            correct += 1\n    acc = correct / total * 100\n    print(f\"完全匹配率: {acc:.2f}%\")\n    return acc\n\n</code></pre>\n<p><strong>这里分开写两个指标方便演示和单独调用，实际上，把二者的前半部分逻辑合在一起是更高效的做法。</strong><br />\n到此，需要强调的部分就全部定义完毕，我们开始正式运行。</p>\n<h2 id=\"25-运行效果\">2.5 运行效果</h2>\n<p>我们设置参数如下：</p>\n<pre><code class=\"language-python\">EMB_DIM = 64  \nHIDDEN_DIM = 256  \nEPOCHS = 10  \nBATCH_SIZE = 128  \nLR = 0.001\n</code></pre>\n<p>下面，就来分情况看看效果：</p>\n<h4 id=\"1不使用注意力机制使用贪心解码\">（1）不使用注意力机制，使用贪心解码</h4>\n<p>在这个条件下，我们定义主函数如下：</p>\n<pre><code class=\"language-python\">if __name__ == \"__main__\":  \n\tEMB_DIM = 64  \n\tHIDDEN_DIM = 256  \n\tEPOCHS = 10  \n\tBATCH_SIZE = 128  \n\tLR = 0.001\n\t# 关键设置\n    USE_ATTENTION    = False  \n    USE_BEAM_SEARCH  = False  \n    BEAM_WIDTH       = 3  \n  \n    print(f\"\\n=== 配置 ===\")  \n    print(f\"使用注意力: {USE_ATTENTION}\")  \n    print(f\"使用束搜索: {USE_BEAM_SEARCH} (beam width = {BEAM_WIDTH if USE_BEAM_SEARCH else 'No'})\")  \n  \n    encoder = Encoder(num_encoder_tokens, EMB_DIM, HIDDEN_DIM).to(device)  \n    decoder = Decoder(num_decoder_tokens, EMB_DIM, HIDDEN_DIM, use_attention=USE_ATTENTION).to(device)  \n    model = Seq2Seq(encoder, decoder).to(device)  \n  \n    print(\"\\n开始训练...\")  \n    train_time = train(model, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)  \n  \n    print(\"\\n评估中...\")  \n    bleu = evaluate_bleu(model, test_encoder_input, test_targets,  \n                         use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n    exact_acc = evaluate_exact_match(model, test_encoder_input, test_targets,  \n                                     use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n  \n\tprint(\"\\n=== 抽取测试样本输出 ===\")  \n\tsample_indices = random.sample(range(len(test_encoder_input)), 10)  \n\tfor i in sample_indices:  \n\t    src = test_inputs[i]  \n\t    true = test_targets[i][1:-1]  \n\t    pred,_ = decode(model, test_encoder_input[i],  \n\t                  use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n\t    print(f\"Human  : {src} | True  : {true} | Predicted  : {pred}\")\n</code></pre>\n<p>结果如下：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162800621-1772145123.png\" /></p>\n<p>得益于任务本身和合成数据较为简单，可以发现指标还不错，我们不着急分析 ，继续下一步设置进行对比。</p>\n<h4 id=\"2不使用注意力机制使用束搜索\">（2）不使用注意力机制，使用束搜索</h4>\n<p>这次，设置主函数参数如下：</p>\n<pre><code class=\"language-python\">USE_ATTENTION    = False  \nUSE_BEAM_SEARCH  = True # 使用束搜索 \nBEAM_WIDTH       = 3 # 束宽\n</code></pre>\n<p>看看结果：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162801820-1581242608.png\" /></p>\n<p>指标好像有所提升，但也可能是训练中的偶然性，我们再增加束宽看看：</p>\n<pre><code class=\"language-python\">BEAM_WIDTH       = 10 \n</code></pre>\n<p><img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162800857-1336184228.png\" /><br />\n可以发现，<strong>随着束宽的增加，解码时间明显增加，而指标浮动不大</strong>，这说明限制指标的关键因素可能不是搜索策略。<br />\n由此，我们进入下一步：</p>\n<h4 id=\"3使用注意力机制\">（3）使用注意力机制</h4>\n<p>现在，设置参数如下：</p>\n<pre><code class=\"language-python\">USE_ATTENTION    = True # 使用注意力机制  \nUSE_BEAM_SEARCH  = True  \nBEAM_WIDTH       = 3 \n</code></pre>\n<p>看看结果：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162800597-1996349670.png\" /></p>\n<p>好像也没什么用啊？ 你会发现：<strong>使用注意力机制后，训练时间因相关计算明显增加，但是指标并没有明显提升。</strong><br />\n实际上，这和我们的日期翻译任务本身的特点有关：</p>\n<ol>\n<li><strong>输入输出序列较短</strong>：每个样本通常只有 5~15 个字符左右，LSTM 很容易在没有注意力的情况下就捕捉到序列整体信息。</li>\n<li><strong>信息对齐较简单</strong>：日期翻译本质上是字符级的对齐映射，例如 “Jan 5, 2003” → “2003-01-05”，几乎不存在复杂的长距离依赖。</li>\n</ol>\n<p>注意力机制最明显的作用是在序列较长或输入输出关系复杂时，帮助解码器聚焦到相关的编码状态。而对于现在这种短序列、规则性强的任务，其计算成本增加，而带来的性能提升很有限。</p>\n<p>在这个任务中，想要指标继续增加，有一个很简单的方法就是<strong>在脚本中增加我们人工合成的数据量</strong>，就不再展示了。</p>\n<p><strong>至此，吴恩达深度学习课程五课十五周的内容就全部结束了，之后会出一个完整的目录。</strong></p>\n<h1 id=\"3-附录\">3. 附录</h1>\n<h2 id=\"31-数据合成\">3.1 数据合成</h2>\n<pre><code class=\"language-python\">import torch\nimport random\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPAD_TOKEN = \"&lt;PAD&gt;\"\n\nMONTHS_FULL = [\n    \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n    \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n]\nMONTHS_ABBR = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n               \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n\nHUMAN_TEMPLATES = [\n    \"{day} {month} {year}\",\n    \"{month} {day}, {year}\",\n    \"{day} of {month}, {year}\",\n    \"{month} {day} {year}\",\n    \"{day}/{month_num}/{year_short}\",\n    \"{month_num}/{day}/{year_short}\",\n    \"{weekday}, {day} {month_abbr} {year}\",\n    \"{month_abbr} {day}th {year_short}\",\n    \"{year}-{month_num}-{day}\",\n    \"The {day}{day_suffix} of {month}, {year}\",\n    \"{day}th {month_abbr}, {year}\",\n    \"{month} the {day}{day_suffix}, {year}\",\n    \"Date: {year}/{month_num}/{day}\",\n    \"{day} in Roman: {roman_day} {month} {year}\",\n    \"{month_abbr}. {day}, '{year_short}\",\n]\n\ndef roman_numeral(n):\n    vals = (10, 9, 5, 4, 1)\n    nums = ('X', 'IX', 'V', 'IV', 'I')\n    result = \"\"\n    for v, r in zip(vals, nums):\n        while n &gt;= v:\n            result += r\n            n -= v\n    return result\n\ndef random_date(start_year=1900, end_year=2100):\n    start = datetime(start_year, 1, 1)\n    end = datetime(end_year, 12, 31)\n    delta = end - start\n    return start + timedelta(days=random.randint(0, delta.days))\n\ndef human_readable_date(date):\n    template = random.choice(HUMAN_TEMPLATES)\n    day = date.day\n    month_idx = date.month - 1\n    month_full = MONTHS_FULL[month_idx]\n    month_abbr = MONTHS_ABBR[month_idx]\n    year = date.year\n    month_num = str(date.month).zfill(2)\n    year_short = str(year % 100).zfill(2)\n    weekday = date.strftime(\"%A\")\n\n    day_suffix = \"th\"\n    if day % 10 == 1 and day != 11: day_suffix = \"st\"\n    elif day % 10 == 2 and day != 12: day_suffix = \"nd\"\n    elif day % 10 == 3 and day != 13: day_suffix = \"rd\"\n\n    day_str = str(day)\n    roman_day = roman_numeral(day) if random.random() &lt; 0.2 else day_str\n\n    return template.format(\n        day=day_str,\n        day_suffix=day_suffix,\n        month=month_full,\n        month_abbr=month_abbr,\n        year=year,\n        month_num=month_num,\n        year_short=year_short,\n        weekday=weekday,\n        roman_day=roman_day\n    )\n\ndef machine_readable_date(date):\n    return date.strftime(\"%Y-%m-%d\")\n\ndef generate_date_pair():\n    d = random_date()\n    return human_readable_date(d), machine_readable_date(d)\n\n\nNUM_SAMPLES = 10000\ninput_texts, target_texts = [], []\n\nfor _ in range(NUM_SAMPLES):\n    human, machine = generate_date_pair()\n    input_texts.append(human)\n    target_texts.append('\\t' + machine + '\\n')\n\ntrain_inputs, test_inputs, train_targets, test_targets = train_test_split(\n    input_texts, target_texts, test_size=0.2, random_state=42\n)\n\nall_input_chars = [PAD_TOKEN] + sorted(set(''.join(input_texts)))\nall_target_chars = [PAD_TOKEN] + sorted(set(''.join(target_texts)))\n\ninput_token_index = {c: i for i, c in enumerate(all_input_chars)}\ntarget_token_index = {c: i for i, c in enumerate(all_target_chars)}\n\nreverse_input_char_index = {i: c for c, i in input_token_index.items()}\nreverse_target_char_index = {i: c for c, i in target_token_index.items()}\n\nmax_encoder_len = max(len(txt) for txt in input_texts)\nmax_decoder_len = max(len(txt) for txt in target_texts)\n\nnum_encoder_tokens = len(all_input_chars)\nnum_decoder_tokens = len(all_target_chars)\n\nprint(f\"max enc len: {max_encoder_len}, max dec len: {max_decoder_len}\")\nprint(f\"input vocab size: {num_encoder_tokens}, target vocab size: {num_decoder_tokens}\")\n\ndef texts_to_tensor(texts, token_index, max_len):\n    pad_idx = token_index[PAD_TOKEN]\n    data = np.full((len(texts), max_len), pad_idx, dtype=np.int64)\n    for i, txt in enumerate(texts):\n        for t, char in enumerate(txt):\n            if t &lt; max_len:\n                data[i, t] = token_index[char]\n    return torch.tensor(data, dtype=torch.long, device=device)\n\ntrain_encoder_input = texts_to_tensor(train_inputs, input_token_index, max_encoder_len)\ntrain_decoder_input = texts_to_tensor(train_targets, target_token_index, max_decoder_len)\n\ntest_encoder_input = texts_to_tensor(test_inputs, input_token_index, max_encoder_len)\ntest_decoder_input = texts_to_tensor(test_targets, target_token_index, max_decoder_len)\n\n\ndataset_pt = {\n    'train_encoder_input': train_encoder_input,\n    'train_decoder_input': train_decoder_input,\n    'test_encoder_input': test_encoder_input,\n    'test_decoder_input': test_decoder_input,\n    'input_token_index': input_token_index,\n    'target_token_index': target_token_index,\n    'reverse_input_char_index': reverse_input_char_index,\n    'reverse_target_char_index': reverse_target_char_index,\n    'max_encoder_len': max_encoder_len,\n    'max_decoder_len': max_decoder_len,\n    'num_encoder_tokens': num_encoder_tokens,\n    'num_decoder_tokens': num_decoder_tokens\n}\n\ntorch.save(dataset_pt, \"date_dataset.pt\")\nprint(\"已保存：date_dataset.pt\")\n\n\n\ndef tensor_to_numpy(t):\n    return t.detach().cpu().numpy()\n\ndataset_npz = {\n    \"train_encoder_input\": tensor_to_numpy(train_encoder_input),\n    \"train_decoder_input\": tensor_to_numpy(train_decoder_input),\n    \"test_encoder_input\": tensor_to_numpy(test_encoder_input),\n    \"test_decoder_input\": tensor_to_numpy(test_decoder_input),\n\n    \"input_token_index\": np.array(input_token_index, dtype=object),\n    \"target_token_index\": np.array(target_token_index, dtype=object),\n    \"reverse_input_char_index\": np.array(reverse_input_char_index, dtype=object),\n    \"reverse_target_char_index\": np.array(reverse_target_char_index, dtype=object),\n\n    \"max_encoder_len\": max_encoder_len,\n    \"max_decoder_len\": max_decoder_len,\n    \"num_encoder_tokens\": num_encoder_tokens,\n    \"num_decoder_tokens\": num_decoder_tokens\n}\n\nnp.savez(\"date_dataset.npz\", **dataset_npz)\nprint(\"已保存：date_dataset.npz\\n\")\n\ndef decode(tensor_row, reverse_dict):\n    return ''.join(\n        reverse_dict[idx.item()]\n        for idx in tensor_row\n        if reverse_dict[idx.item()] != PAD_TOKEN\n    )\n\nprint(\"示例数据：\")\nfor i in range(20):\n    src = decode(train_encoder_input[i], reverse_input_char_index)\n    tgt = decode(train_decoder_input[i], reverse_target_char_index)\n    print(f\"[{i+1}] {src}  ==&gt;  {tgt.strip()}\")\n</code></pre>\n<h2 id=\"32-日期格式翻译-pytorch版\">3.2 日期格式翻译-PyTorch版</h2>\n<pre><code class=\"language-python\">import random  \nimport torch  \nimport torch.nn as nn  \nimport torch.nn.functional as F  \nfrom torch import optim  \nimport time  \nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n  \ndata = torch.load(\"date_dataset.pt\", map_location=device)  \n  \ntrain_encoder_input = data['train_encoder_input']  \ntrain_decoder_input = data['train_decoder_input']  \ntest_encoder_input  = data['test_encoder_input']  \ntest_decoder_input  = data['test_decoder_input']  \n  \ninput_token_index = data['input_token_index']  \ntarget_token_index = data['target_token_index']  \nreverse_target_char_index = data['reverse_target_char_index']  \n  \nnum_encoder_tokens = data['num_encoder_tokens']  \nnum_decoder_tokens = data['num_decoder_tokens']  \n  \nmax_encoder_len = data['max_encoder_len']  \nmax_decoder_len = data['max_decoder_len']  \n  \nPAD_TOKEN = \"&lt;PAD&gt;\"  \nreverse_input_char_index = {i: c for c, i in input_token_index.items()}  \n  \ndef tensor_to_text(tensor_row, reverse_dict):  \n    return ''.join(  \n        reverse_dict[idx.item()]  \n        for idx in tensor_row  \n        if reverse_dict[idx.item()] != PAD_TOKEN  \n    )  \n  \ntest_inputs = [tensor_to_text(test_encoder_input[i], reverse_input_char_index)  \n               for i in range(len(test_encoder_input))]  \n  \ntest_targets = [tensor_to_text(test_decoder_input[i], reverse_target_char_index)  \n                for i in range(len(test_decoder_input))]  \n  \nclass Encoder(nn.Module):  \n    def __init__(self, vocab_size, emb_dim, hidden_dim):  \n        super().__init__()  \n        self.embedding = nn.Embedding(vocab_size, emb_dim)  \n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)  \n  \n    def forward(self, x):  \n        embedded = self.embedding(x)  \n        outputs, (hidden, cell) = self.lstm(embedded)  \n        return outputs, (hidden, cell)  \n  \nclass Decoder(nn.Module):  \n    def __init__(self, vocab_size, emb_dim, hidden_dim, use_attention=False):  \n        super().__init__()  \n        self.use_attention = use_attention  \n        self.embedding = nn.Embedding(vocab_size, emb_dim)  \n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)  \n  \n        if use_attention:  \n            self.attn = nn.Linear(hidden_dim + hidden_dim * 2, 1)  \n            self.attn_combine = nn.Linear(hidden_dim + hidden_dim * 2, hidden_dim)  \n  \n        self.out = nn.Linear(hidden_dim, vocab_size)  \n  \n    def forward(self, input_step, hidden_cell, encoder_outputs):  \n        embedded = self.embedding(input_step)  \n        lstm_out, hidden_cell = self.lstm(embedded, hidden_cell)  \n  \n        query = lstm_out.squeeze(1)  \n  \n        if self.use_attention:  \n            query_exp = query.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)  \n            attn_scores = self.attn(torch.cat((query_exp, encoder_outputs), dim=-1)).squeeze(-1)  \n            attn_weights = F.softmax(attn_scores, dim=-1).unsqueeze(1)  \n            context = torch.bmm(attn_weights, encoder_outputs).squeeze(1)  \n            combined = torch.cat((query, context), dim=1)  \n            output = torch.tanh(self.attn_combine(combined))  \n        else:  \n            output = query  \n  \n        output = self.out(output)  \n        return output, hidden_cell  \n  \nclass Seq2Seq(nn.Module):  \n    def __init__(self, encoder, decoder):  \n        super().__init__()  \n        self.encoder = encoder  \n        self.decoder = decoder  \n  \n    def forward(self, src, tgt):  \n        enc_outputs, (hidden, cell) = self.encoder(src)  \n        hidden = hidden.view(2, -1, hidden.size(-1)).sum(dim=0, keepdim=True)  \n        cell   = cell.view(2, -1, cell.size(-1)).sum(dim=0, keepdim=True)  \n        dec_hidden = (hidden, cell)  \n        outputs = []  \n  \n        for t in range(tgt.size(1)):  \n            out, dec_hidden = self.decoder(tgt[:, t].unsqueeze(1), dec_hidden, enc_outputs)  \n            outputs.append(out)  \n  \n        return torch.stack(outputs, dim=1)  \n  \ndef train(model, epochs=10, batch_size=128, lr=0.001):  \n    enc_optimizer = optim.Adam(model.encoder.parameters(), lr=lr)  \n    dec_optimizer = optim.Adam(model.decoder.parameters(), lr=lr)  \n    criterion = nn.CrossEntropyLoss(ignore_index=0)  \n  \n    start_time = time.time()  \n  \n    for epoch in range(epochs):  \n        total_loss = 0  \n        num_batches = 0  \n        perm = torch.randperm(len(train_encoder_input))  \n        for i in range(0, len(train_encoder_input), batch_size):  \n            idx = perm[i:i+batch_size]  \n            enc_in = train_encoder_input[idx]  \n            dec_in_full = train_decoder_input[idx]  \n            dec_in = dec_in_full[:, :-1]  \n            dec_target = dec_in_full[:, 1:]  \n  \n            enc_optimizer.zero_grad()  \n            dec_optimizer.zero_grad()  \n  \n            outputs = model(enc_in, dec_in)  \n  \n            loss = criterion(outputs.reshape(-1, num_decoder_tokens), dec_target.reshape(-1))  \n            loss.backward()  \n            enc_optimizer.step()  \n            dec_optimizer.step()  \n  \n            total_loss += loss.item()  \n            num_batches += 1  \n  \n        avg_loss = total_loss / num_batches  \n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")  \n  \n    train_time = time.time() - start_time  \n    print(f\"训练总时间: {train_time:.2f} 秒\")  \n    return train_time  \n  \nimport time  \n  \ndef decode(model, input_tensor, use_beam_search=False, beam_width=3, max_len=20):  \n    model.eval()  \n    start_time = time.time()    \n    with torch.no_grad():  \n        input_tensor = input_tensor.unsqueeze(0)  \n        enc_outputs, (hidden, cell) = model.encoder(input_tensor)  \n        hidden = hidden.view(2, -1, hidden.size(-1)).sum(dim=0, keepdim=True)  \n        cell   = cell.view(2, -1, cell.size(-1)).sum(dim=0, keepdim=True)  \n        dec_hidden = (hidden, cell)  \n  \n        if not use_beam_search:  \n            dec_input = torch.tensor([[target_token_index['\\t']]], device=device)  \n            result = []  \n            for _ in range(max_len):  \n                out, dec_hidden = model.decoder(dec_input, dec_hidden, enc_outputs)  \n                pred = out.argmax(dim=-1)  \n                token = pred.item()  \n                if token == target_token_index['\\n']:  \n                    break  \n                result.append(reverse_target_char_index[token])  \n                dec_input = pred.unsqueeze(0)  \n            decoded = ''.join(result)  \n  \n        else:  \n            candidates = [([target_token_index['\\t']], 0.0, dec_hidden)]  \n            for _ in range(max_len):  \n                new_cands = []  \n                for seq, score, h in candidates:  \n                    if seq[-1] == target_token_index['\\n']:  \n                        new_cands.append((seq, score, h))  \n                        continue  \n                    inp = torch.tensor([[seq[-1]]], device=device)  \n                    out, new_h = model.decoder(inp, h, enc_outputs)  \n                    log_probs = F.log_softmax(out, dim=-1).squeeze(0)  \n                    k = min(beam_width, log_probs.size(0))    \n                    top_vals, top_idx = log_probs.topk(k)  \n                    for v, idx in zip(top_vals, top_idx):  \n                        new_seq = seq + [idx.item()]  \n                        new_score = score + v.item()  \n                        new_cands.append((new_seq, new_score, new_h))  \n                candidates = sorted(new_cands, key=lambda x: x[1], reverse=True)[:beam_width]  \n  \n            best = candidates[0][0]  \n            decoded = ''.join(reverse_target_char_index.get(i, '') for i in best[1:] if i != target_token_index['\\n'])  \n  \n    decode_time = time.time() - start_time  \n    return decoded, decode_time  \n  \n  \ndef evaluate_bleu(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):  \n    total_bleu = 0  \n    total_time = 0  \n    smoother = SmoothingFunction().method4  \n  \n    for i in range(len(test_enc_input)):  \n        pred, dt = decode(model, test_enc_input[i], use_beam_search, beam_width)  \n        total_time += dt  \n        true = test_targets[i][1:-1]  \n        ref = [list(true)]  \n        hyp = list(pred)  \n        bleu = sentence_bleu(ref, hyp, smoothing_function=smoother)  \n        total_bleu += bleu  \n  \n    avg_bleu = total_bleu / len(test_enc_input) * 100  \n    avg_time = total_time / len(test_enc_input)  \n    print(f\"BLEU: {avg_bleu:.2f} | 平均解码时间: {avg_time:.4f} 秒/样本\")  \n    return avg_bleu, avg_time  \n  \n  \ndef evaluate_exact_match(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):  \n    correct = 0  \n    total = len(test_enc_input)  \n  \n    for i in range(total):  \n        pred, _ = decode(model, test_enc_input[i], use_beam_search, beam_width)    \n        true = test_targets[i][1:-1]    \n        if pred == true:  \n            correct += 1  \n  \n    acc = correct / total * 100  \n    print(f\"完全匹配率: {acc:.2f}%\")  \n    return acc  \n  \nif __name__ == \"__main__\":  \n    EMB_DIM = 64  \n    HIDDEN_DIM = 256  \n    EPOCHS = 10  \n    BATCH_SIZE = 128  \n    LR = 0.001  \n  \n    USE_ATTENTION    = True  \n    USE_BEAM_SEARCH  = True  \n    BEAM_WIDTH       = 3  \n  \n    print(f\"\\n=== 配置 ===\")  \n    print(f\"使用注意力: {USE_ATTENTION}\")  \n    print(f\"使用束搜索: {USE_BEAM_SEARCH} (beam width = {BEAM_WIDTH if USE_BEAM_SEARCH else 'No'})\")  \n  \n    encoder = Encoder(num_encoder_tokens, EMB_DIM, HIDDEN_DIM).to(device)  \n    decoder = Decoder(num_decoder_tokens, EMB_DIM, HIDDEN_DIM, use_attention=USE_ATTENTION).to(device)  \n    model = Seq2Seq(encoder, decoder).to(device)  \n  \n    print(\"\\n开始训练...\")  \n    train_time = train(model, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)  \n  \n    print(\"\\n评估中...\")  \n    bleu = evaluate_bleu(model, test_encoder_input, test_targets,  \n                         use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n    exact_acc = evaluate_exact_match(model, test_encoder_input, test_targets,  \n                                     use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n  \n    print(\"\\n=== 抽取测试样本输出 ===\")  \n    sample_indices = random.sample(range(len(test_encoder_input)), 10)  \n    for i in sample_indices:  \n        src = test_inputs[i]  \n        true = test_targets[i][1:-1]  \n        pred,_ = decode(model, test_encoder_input[i],  \n                      use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n        print(f\"Human  : {src} | True  : {true} | Predicted  : {pred}\")\n</code></pre>\n<h2 id=\"33-日期格式翻译-tf版\">3.3 日期格式翻译-TF版</h2>\n<pre><code class=\"language-python\">import numpy as np  \nimport tensorflow as tf  \nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional  \nimport random  \nimport time  \nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  \n  \ndataset = np.load(\"date_dataset.npz\", allow_pickle=True)  \n  \ntrain_encoder_input = dataset['train_encoder_input']  \ntrain_decoder_input = dataset['train_decoder_input']  \ntest_encoder_input = dataset['test_encoder_input']  \ntest_decoder_input = dataset['test_decoder_input']  \n  \ninput_token_index = dataset['input_token_index'].item()  \ntarget_token_index = dataset['target_token_index'].item()  \nreverse_target_char_index = dataset['reverse_target_char_index'].item()  \n  \nnum_encoder_tokens = int(dataset['num_encoder_tokens'])  \nnum_decoder_tokens = int(dataset['num_decoder_tokens'])  \nmax_encoder_len = int(dataset['max_encoder_len'])  \nmax_decoder_len = int(dataset['max_decoder_len'])  \n  \nreverse_input_char_index = {i: c for c, i in input_token_index.items()}  \n  \nPAD_TOKEN = \"&lt;PAD&gt;\"    \n  \ndef tensor_to_text(tensor_row, reverse_dict):  \n    return ''.join(  \n        reverse_dict.get(int(idx), '')  \n        for idx in tensor_row  \n        if reverse_dict.get(int(idx), '') != PAD_TOKEN  \n    )  \n  \n  \ntest_inputs = [tensor_to_text(test_encoder_input[i], reverse_input_char_index)  \n               for i in range(len(test_encoder_input))]  \n  \ntest_targets = [tensor_to_text(test_decoder_input[i], reverse_target_char_index)  \n                for i in range(len(test_decoder_input))]  \n  \nclass Encoder(tf.keras.Model):  \n    def __init__(self, vocab_size, emb_dim, hidden_dim):  \n        super(Encoder, self).__init__()  \n        self.embedding = Embedding(vocab_size, emb_dim, mask_zero=True)  \n        self.lstm = Bidirectional(  \n            LSTM(hidden_dim, return_sequences=True, return_state=True)  \n        )  \n  \n    def call(self, x):  \n        embedded = self.embedding(x)  \n        outputs, fw_h, fw_c, bw_h, bw_c = self.lstm(embedded)  \n        hidden = tf.concat([fw_h, bw_h], axis=-1)  \n        cell = tf.concat([fw_c, bw_c], axis=-1)  \n        return outputs, (hidden, cell)  \n  \n  \nclass Decoder(tf.keras.layers.Layer):  \n    def __init__(self, vocab_size, emb_dim, hidden_dim, use_attention=False):  \n        super(Decoder, self).__init__()  \n        self.use_attention = use_attention  \n        self.embedding = Embedding(vocab_size, emb_dim, mask_zero=True)  \n        self.lstm = LSTM(hidden_dim, return_sequences=False, return_state=True)  \n        self.out = Dense(vocab_size)  \n  \n        if use_attention:  \n            self.attn_W1 = Dense(hidden_dim)                 \n            self.attn_W2 = Dense(hidden_dim)               \n            self.attn_V  = Dense(1)  \n            self.attn_combine = Dense(hidden_dim)  \n  \n    def call(self, input_step, hidden_cell, encoder_outputs):  \n   \n        embedded = self.embedding(input_step)            \n        lstm_out, h, c = self.lstm(embedded, initial_state=hidden_cell)  \n        query = lstm_out                                  \nif self.use_attention:  \n            score = self.attn_V(  \n                tf.tanh(  \n                    self.attn_W1(encoder_outputs) +  \n                    self.attn_W2(query[:, tf.newaxis, :])  \n                )  \n            )                                             \n  \n            attn_weights = tf.nn.softmax(score, axis=1)   \n  \n            context = tf.reduce_sum(encoder_outputs * attn_weights, axis=1)  \n            combined = tf.concat([query, context], axis=-1)  \n            output = tf.tanh(self.attn_combine(combined))  \n        else:  \n            output = query  \n  \n        logits = self.out(output)                         \n        return logits, (h, c)  \n  \n  \nclass Seq2Seq(tf.keras.Model):  \n    def __init__(self, encoder, decoder):  \n        super(Seq2Seq, self).__init__()  \n        self.encoder = encoder  \n        self.decoder = decoder  \n  \n    def call(self, src, tgt, training=True):  \n        enc_outputs, (hidden, cell) = self.encoder(src)  \n  \n        hidden_fw, hidden_bw = tf.split(hidden, num_or_size_splits=2, axis=-1)  \n        cell_fw, cell_bw = tf.split(cell, num_or_size_splits=2, axis=-1)  \n  \n        hidden = hidden_fw + hidden_bw  \n        cell = cell_fw + cell_bw  \n  \n        dec_hidden = (hidden, cell)  \n  \n        outputs = []  \n        for t in range(tgt.shape[1]):  \n            step_input = tgt[:, t:t + 1]  \n            logits, dec_hidden = self.decoder(  \n                step_input, dec_hidden, enc_outputs  \n            )  \n            outputs.append(logits)  \n  \n        return tf.stack(outputs, axis=1)  \n  \ndef train(model, epochs=10, batch_size=128, lr=0.001):  \n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)  \n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  \n  \n    start_time = time.time()  \n    n_samples = len(train_encoder_input)  \n  \n    for epoch in range(epochs):  \n        total_loss = 0.0  \n        num_batches = 0  \n  \n        indices = np.random.permutation(n_samples)  \n        enc = train_encoder_input[indices]  \n        dec = train_decoder_input[indices]  \n  \n        for i in range(0, n_samples, batch_size):  \n            enc_batch = enc[i:i + batch_size]  \n            dec_batch = dec[i:i + batch_size]  \n  \n            dec_in = dec_batch[:, :-1]  \n            dec_tgt = dec_batch[:, 1:]  \n  \n            with tf.GradientTape() as tape:  \n                logits = model(enc_batch, dec_in, training=True)  \n                loss_value = loss_fn(dec_tgt, logits)  \n                loss_value += tf.reduce_sum(model.losses)  \n  \n            grads = tape.gradient(loss_value, model.trainable_variables)  \n            optimizer.apply_gradients(zip(grads, model.trainable_variables))  \n  \n            total_loss += float(loss_value)  \n            num_batches += 1  \n  \n        avg_loss = total_loss / num_batches  \n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")  \n  \n    train_time = time.time() - start_time  \n    print(f\"训练用时: {train_time:.2f} 秒\")  \n    return train_time  \n  \n@tf.function  \ndef greedy_decode_step(decoder, dec_input, dec_hidden, enc_outputs):  \n    logits, dec_hidden = decoder(dec_input, dec_hidden, enc_outputs)  \n    return logits, dec_hidden  \n  \n  \ndef decode(model, input_tensor, use_beam_search=False, beam_width=3, max_len=20):  \n    dummy_src = tf.zeros((1, max_encoder_len), dtype=tf.int32)  \n    dummy_tgt = tf.zeros((1, 1), dtype=tf.int32)  \n    _ = model(dummy_src, dummy_tgt, training=False)  \n  \n    start_time = time.time()  \n  \n    input_tensor = tf.expand_dims(input_tensor, 0)  \n    enc_outputs, (hidden, cell) = model.encoder(input_tensor)  \n  \n    hidden_fw, hidden_bw = tf.split(hidden, num_or_size_splits=2, axis=-1)  \n    cell_fw, cell_bw = tf.split(cell, num_or_size_splits=2, axis=-1)  \n  \n    hidden = hidden_fw + hidden_bw  \n    cell = cell_fw + cell_bw  \n  \n    dec_hidden = (hidden, cell)  \n  \n    if not use_beam_search:  \n        dec_input = tf.constant([[target_token_index['\\t']]], dtype=tf.int32)  \n        result = []  \n        for _ in range(max_len):  \n            logits, dec_hidden = greedy_decode_step(  \n                model.decoder, dec_input, dec_hidden, enc_outputs  \n            )  \n            pred = tf.argmax(logits, axis=-1)  \n            token = int(pred[0])  \n            if token == target_token_index.get('\\n', -1):  \n                break  \n            result.append(reverse_target_char_index.get(token, ''))  \n            dec_input = pred[:, tf.newaxis]  \n  \n        decoded = ''.join(result)  \n  \n    else:  \n        candidates = [([target_token_index['\\t']], 0.0, dec_hidden)]  \n        for _ in range(max_len):  \n            new_cands = []  \n            for seq, score, h_c in candidates:  \n                if seq[-1] == target_token_index.get('\\n', -1):  \n                    new_cands.append((seq, score, h_c))  \n                    continue  \n                inp = tf.constant([[seq[-1]]], dtype=tf.int32)  \n                logits, new_h_c = model.decoder(inp, h_c, enc_outputs)  \n                log_probs = tf.nn.log_softmax(logits, axis=-1)[0]  \n                top_vals, top_idx = tf.math.top_k(log_probs, k=beam_width)  \n                for v, idx in zip(top_vals, top_idx):  \n                    new_seq = seq + [int(idx)]  \n                    new_score = score + float(v)  \n                    new_cands.append((new_seq, new_score, new_h_c))  \n  \n            candidates = sorted(new_cands, key=lambda x: x[1], reverse=True)[:beam_width]  \n  \n        best_seq = candidates[0][0]  \n        decoded = ''.join(  \n            reverse_target_char_index.get(i, '') for i in best_seq[1:] if i != target_token_index.get('\\n', -1))  \n  \n    decode_time = time.time() - start_time  \n    return decoded, decode_time  \n  \n  \ndef evaluate_bleu(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):  \n    total_bleu = 0  \n    total_time = 0  \n    smoother = SmoothingFunction().method4  \n  \n    for i in range(len(test_enc_input)):  \n        pred, dt = decode(model, test_enc_input[i], use_beam_search, beam_width)  \n        total_time += dt  \n        true = test_targets[i][1:-1]    \n        ref = [list(true)]  \n        hyp = list(pred)  \n        bleu = sentence_bleu(ref, hyp, smoothing_function=smoother)  \n        total_bleu += bleu  \n  \n    avg_bleu = total_bleu / len(test_enc_input) * 100  \n    avg_time = total_time / len(test_enc_input)  \n    print(f\"BLEU: {avg_bleu:.2f} | 平均解码时间: {avg_time:.4f} 秒/样本\")  \n    return avg_bleu, avg_time  \n  \n  \ndef evaluate_exact_match(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):  \n    correct = 0  \n    total = len(test_enc_input)  \n  \n    for i in range(total):  \n        pred, _ = decode(model, test_enc_input[i], use_beam_search, beam_width)  \n        true = test_targets[i][1:-1]  \n        if pred == true:  \n            correct += 1  \n  \n    acc = correct / total * 100  \n    print(f\"完全匹配率: {acc:.2f}%\")  \n    return acc  \n  \n  \nif __name__ == \"__main__\":  \n    EMB_DIM = 64  \n    HIDDEN_DIM = 256   \n    BATCH_SIZE = 128  \n    LR = 0.001  \n  \n    USE_ATTENTION = True  \n    USE_BEAM_SEARCH = True  \n    BEAM_WIDTH = 3  \n  \n    print(f\"\\n=== 配置 ===\")  \n    print(f\"使用注意力: {USE_ATTENTION}\")  \n    print(f\"使用束搜索: {USE_BEAM_SEARCH} (beam width = {BEAM_WIDTH if USE_BEAM_SEARCH else 'No'})\")  \n  \n    encoder = Encoder(num_encoder_tokens, EMB_DIM, HIDDEN_DIM)  \n    decoder = Decoder(num_decoder_tokens, EMB_DIM, HIDDEN_DIM, use_attention=USE_ATTENTION)  \n    model = Seq2Seq(encoder, decoder)  \n  \n    print(\"\\n开始训练...\")  \n    train_time = train(model, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)  \n  \n    print(\"\\n评估中...\")  \n    bleu = evaluate_bleu(model, test_encoder_input, test_targets,  \n                         use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n    exact_acc = evaluate_exact_match(model, test_encoder_input, test_targets,  \n                                     use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n  \n    print(\"\\n=== 抽取测试样本输出 ===\")  \n    sample_indices = random.sample(range(len(test_encoder_input)), 10)  \n    for i in sample_indices:  \n        src = test_inputs[i]  \n        true = test_targets[i][1:-1]  \n        pred, _ = decode(model, test_encoder_input[i],  \n                         use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n        print(f\"Human  : {src} | True  : {true} | Predicted  : {pred}\")\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 16:34</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Goblinscholar\">哥布林学者</a>&nbsp;\n阅读(<span id=\"post_view_count\">73</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "鸿蒙应用开发UI基础第三节：UIAbility生命周期全解析",
      "link": "https://www.cnblogs.com/san-xiu/p/19579716",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/san-xiu/p/19579716\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 15:55\">\n    <span>鸿蒙应用开发UI基础第三节：UIAbility生命周期全解析</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"学习目标\">【学习目标】</h2>\n<ol>\n<li>掌握UIAbility核心生命周期方法的触发时机、系统行为及约束规则；</li>\n<li>理解生命周期与WindowStage的深度联动逻辑，明确页面加载、事件订阅的时机；</li>\n<li>掌握<code>onDestroy</code>回调的特殊触发规则（含API 13一键清理、调试模式、terminateSelf调用场景）；</li>\n<li>能通过实操验证生命周期执行顺序，掌握WindowStage事件监听的正确方式，避免新手常见混淆；</li>\n<li>区分“应用退后台”与“应用销毁”的核心差异，规避生命周期相关的开发误区。</li>\n</ol>\n<h2 id=\"本节重点\">【本节重点】</h2>\n<h3 id=\"1-核心问题导入\">1. 核心问题导入</h3>\n<ul>\n<li>应用启动、切后台、切前台、退出时，UIAbility会经历哪些状态变化？</li>\n<li>窗口的获焦/失焦、前台/后台状态该如何通过WindowStage事件监听？</li>\n<li>WindowStage的事件订阅与解绑，分别应在哪个生命周期方法中执行？</li>\n<li>不同关闭应用的方式（手动调用API、一键清理、调试模式移除任务）对<code>onDestroy</code>触发有何影响？</li>\n<li>为什么页面只能在<code>onWindowStageCreate</code>中加载，不能在<code>onCreate</code>中加载？</li>\n</ul>\n<h3 id=\"2-核心概念\">2. 核心概念</h3>\n<p>UIAbility生命周期是指UIAbility从创建到销毁的全流程，由系统统一调度，包含创建、窗口管理、前后台切换、销毁等核心阶段。通过重写生命周期方法，可在特定时机完成初始化、资源申请/释放、页面加载等操作；</p>\n<p>窗口的活动状态（获焦/失焦、前台/后台）需通过<code>WindowStage</code>事件监听实现，而非生命周期方法。<br />\n<code>onDestroy</code>作为生命周期最后一个方法，其触发与否受关闭应用的方式、API版本、应用类型（有无实况窗）、运行模式（调试/正式）等因素影响。</p>\n<blockquote>\n<p>本节内容延用<code>FirstApplication</code>工程,无新增文件。 使用默认启动模式singleton。</p>\n</blockquote>\n<h2 id=\"二uiability生命周期核心阶段与方法\">二、UIAbility生命周期核心阶段与方法</h2>\n<h3 id=\"1-生命周期可视化示意图\">1. 生命周期可视化示意图</h3>\n<p>生命周期的完整流转逻辑可通过以下示意图直观理解，涵盖启动、前后台切换、关闭全流程的方法触发顺序与WindowStage事件关联：</p>\n<p>示意图说明：左侧为UIAbility核心生命周期方法，右侧为关联的WindowStage事件，窗口的获焦/失焦、前台/后台状态需通过<code>windowStageEvent</code>事件监听实现；销毁阶段的<code>onDestroy</code>触发规则需结合关闭应用的方式判断。</p>\n<p><img alt=\"UIAbility的生命周期示意图\" class=\"lazyload\" /></p>\n<h3 id=\"2-核心方法\">2. 核心方法</h3>\n<p>以下是UIAbility核心生命周期方法的详细说明，聚焦各方法的触发时机、核心作用与执行次数：</p>\n<h4 id=\"oncreatewant-launchparam\"><code>onCreate(want, launchParam)</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility实例首次创建时触发；</li>\n<li><strong>核心作用</strong>：完成应用全局初始化工作，比如建立数据库连接、初始化全局配置参数、日志模块、网络配置、创建全局通用工具类等，这些资源会在<code>onDestroy</code>中对应释放；</li>\n<li><strong>执行次数</strong>：整个UIAbility实例生命周期内仅触发1次。</li>\n</ul>\n<h4 id=\"onwindowstagecreatewindowstage\"><code>onWindowStageCreate(windowStage)</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility实例创建完成后、应用进入前台前，且系统首次创建WindowStage（窗口容器）时触发；</li>\n<li><strong>核心作用</strong>：负责页面加载、订阅WindowStage相关事件（如窗口焦点变化、显示/隐藏事件）；</li>\n<li><strong>执行次数</strong>：单实例（singleton）模式下仅触发1次（多实例模式下每次创建新实例都会触发）。</li>\n</ul>\n<h4 id=\"onforeground\"><code>onForeground()</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility从后台切换至前台、界面即将可见之前触发；</li>\n<li><strong>核心作用</strong>：恢复前台运行所需资源，比如重启暂停的定时器、重新开启定位服务、恢复网络请求轮询等；</li>\n<li><strong>执行次数</strong>：可多次触发（每次切前台都会执行）。</li>\n</ul>\n<h4 id=\"onbackground\"><code>onBackground()</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility界面完全不可见（如按Home键切后台、打开其他应用覆盖当前界面）后触发；</li>\n<li><strong>核心作用</strong>：暂停前台资源以节省系统开销，比如停止定时器、关闭定位服务、保存用户操作数据（作为<code>onDestroy</code>未触发时的兜底方案）；</li>\n<li><strong>执行次数</strong>：可多次触发（每次切后台都会执行）。</li>\n</ul>\n<h4 id=\"onwindowstagewilldestroywindowstage\"><code>onWindowStageWillDestroy(windowStage)</code></h4>\n<ul>\n<li><strong>触发时机</strong>：WindowStage（窗口容器）即将被销毁前触发；</li>\n<li><strong>核心作用</strong>：解绑在<code>onWindowStageCreate</code>中订阅的WindowStage事件、清理窗口相关缓存资源，避免内存泄漏；</li>\n<li><strong>执行次数</strong>：仅在应用“优雅销毁”（如调用<code>terminateSelf()</code>、正常退出）时触发1次，一键清理等强制销毁场景不触发。</li>\n</ul>\n<h4 id=\"onwindowstagedestroy\"><code>onWindowStageDestroy()</code></h4>\n<ul>\n<li><strong>触发时机</strong>：WindowStage（窗口容器）销毁完成后触发；</li>\n<li><strong>核心作用</strong>：确认窗口相关资源已释放，做最终的窗口状态校验；</li>\n<li><strong>执行次数</strong>：仅在应用“优雅销毁”时触发1次，强制销毁场景不触发。</li>\n</ul>\n<h4 id=\"ondestroy\"><code>onDestroy()</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility实例即将被销毁前触发；</li>\n<li><strong>核心作用</strong>：释放<code>onCreate</code>中初始化的全局资源，比如关闭数据库连接、清理全局缓存、保存最终的应用状态数据；</li>\n<li><strong>执行次数</strong>：仅在“优雅销毁”时触发1次（API 13+中，无实况窗应用被一键清理、调试模式移除任务时，系统直接终止进程，该方法不触发）。</li>\n</ul>\n<h4 id=\"onnewwantwant-launchparam\"><code>onNewWant(want, launchParam)</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility实例已启动（未销毁）、再次被外部调用（如其他页面/应用跳转）时触发；</li>\n<li><strong>核心作用</strong>：处理新的启动参数，比如接收跳转传参、更新页面展示内容；</li>\n<li><strong>执行次数</strong>：按需触发（每次复用实例调用都会执行）。</li>\n<li></li>\n</ul>\n<p><strong>注意</strong>：</p>\n<ul>\n<li><code>onDestroy</code>触发特殊规则：API 13及以上版本中，无实况窗应用被一键清理、调试模式下移除任务时，系统直接终止进程，该方法不会触发；仅<code>terminateSelf()</code>调用、正常返回退出、有实况窗应用被一键清理时触发。</li>\n</ul>\n<h3 id=\"3-windowstage事件详解监听窗口活动状态\">3. WindowStage事件详解（监听窗口活动状态）</h3>\n<p>窗口的所有活动状态均通过<code>windowStageEvent</code>事件监听，核心事件说明：</p>\n<ul>\n<li><code>SHOWN</code>：窗口从后台切换到前台（可见），代表应用切前台；</li>\n<li><code>HIDDEN</code>：窗口从前台切换到后台（不可见），代表应用切后台；</li>\n<li><code>ACTIVE</code>：窗口获得焦点（可接收点击/输入），处于可交互状态；</li>\n<li><code>INACTIVE</code>：窗口失去焦点（无法接收输入），处于不可交互状态；</li>\n<li><code>RESUMED</code>：窗口进入前台可交互状态，应用正常运行；</li>\n<li><code>PAUSED</code>：窗口进入前台不可交互状态。</li>\n</ul>\n<h2 id=\"三完整生命周期代码示例\">三、完整生命周期代码示例</h2>\n<pre><code class=\"language-javascript\">import { UIAbility, AbilityConstant, Want, common } from '@kit.AbilityKit';\nimport { window } from '@kit.ArkUI';\nimport { hilog } from '@kit.LogKit';\nimport { BusinessError } from '@ohos.base';\n\nconst TAG = 'UIAbility_Lifecycle';\nconst DOMAIN = 0x0000;\n\nexport default class EntryAbility extends UIAbility {\n  // 1. 创建阶段（仅1次）\n  onCreate(want: Want, launchParam: AbilityConstant.LaunchParam) {\n    hilog.info(DOMAIN, TAG, '--- onCreate 触发（全局初始化）---');\n    // 资源初始化：全局配置、数据库连接等（对应释放：onDestroy）\n  }\n\n  // 2. 窗口创建阶段（窗口首次创建/重建）\n  onWindowStageCreate(windowStage: window.WindowStage): void {\n    hilog.info(DOMAIN, TAG, '--- onWindowStageCreate 触发（加载页面）---');\n    // 订阅WindowStage事件\n    this.registerWindowStageEvent(windowStage);\n    // 加载页面（唯一合法时机）\n    windowStage.loadContent('pages/Index').then(()=&gt;{\n      hilog.info(DOMAIN, TAG, 'Index页面加载成功');\n    }).catch((err:BusinessError)=&gt;{\n      hilog.error(DOMAIN, TAG, `页面加载失败：code=${err.code}, message=${err.message}`);\n    })\n  }\n\n  // 3. 前台阶段（切前台时）\n  onForeground() {\n    hilog.info(DOMAIN, TAG, '--- onForeground 触发（恢复前台资源）---');\n    // 启动前台专属资源（定时器、定位等）\n  }\n\n  // 4. 后台阶段（切后台时）\n  onBackground() {\n    hilog.info(DOMAIN, TAG, '--- onBackground 触发（释放后台资源）---');\n    // 暂停前台资源，保存关键数据（兜底）\n  }\n\n  // 5. 窗口预销毁阶段（窗口即将销毁）\n  onWindowStageWillDestroy(windowStage: window.WindowStage) {\n    hilog.info(DOMAIN, TAG, '--- onWindowStageWillDestroy 触发（窗口预销毁）---');\n    // 解绑WindowStage事件，避免内存泄漏\n    this.unregisterWindowStageEvent(windowStage);\n  }\n\n  // 6. 窗口销毁阶段（窗口已销毁）\n  onWindowStageDestroy() {\n    hilog.info(DOMAIN, TAG, '--- onWindowStageDestroy 触发（销毁窗口）---');\n    // 窗口实例失效，无需额外操作\n  }\n\n  // 7. 销毁阶段（仅1次，存在不触发场景）\n  onDestroy() {\n    hilog.info(DOMAIN, TAG, '--- onDestroy 触发（销毁应用）---');\n    // 释放全局资源、保存最终数据\n  }\n\n  // 8. 新参数阶段（已启动后接收新请求）\n  onNewWant(want: Want, launchParam: AbilityConstant.LaunchParam) {\n    hilog.info(DOMAIN, TAG, `--- onNewWant 触发（新参数：${JSON.stringify(want)}）---`);\n    // 处理新的启动参数\n  }\n  /**\n   * 订阅WindowStage事件（监听窗口活动状态）\n   * @param windowStage - 当前窗口实例\n   */\n  private registerWindowStageEvent(windowStage: window.WindowStage): void {\n    try {\n      windowStage.on('windowStageEvent', (data) =&gt; {\n        const stageEventType: window.WindowStageEventType = data;\n        switch (stageEventType) {\n          case window.WindowStageEventType.SHOWN:\n            hilog.info(DOMAIN, TAG, `windowStage foreground`);\n            break;\n          case window.WindowStageEventType.ACTIVE:\n            hilog.info(DOMAIN, TAG, `windowStage active`);\n            break;\n          case window.WindowStageEventType.INACTIVE:\n            hilog.info(DOMAIN, TAG, `windowStage inactive`);\n            break;\n          case window.WindowStageEventType.HIDDEN:\n            hilog.info(DOMAIN, TAG, `windowStage background`);\n            break;\n          case window.WindowStageEventType.RESUMED:\n            hilog.info(DOMAIN, TAG, `windowStage resumed`);\n            break;\n          case window.WindowStageEventType.PAUSED:\n            hilog.info(DOMAIN, TAG, `windowStage paused.`);\n            break;\n          default:\n            break;\n        }\n      });\n      hilog.info(DOMAIN, TAG, 'WindowStage事件订阅成功');\n    } catch (exception) {\n      hilog.error(DOMAIN, TAG, `订阅窗口事件失败：${JSON.stringify(exception)}`);\n    }\n  }\n\n  /**\n   * 解绑WindowStage事件（资源释放）\n   */\n  private unregisterWindowStageEvent(windowStage: window.WindowStage): void {\n    try {\n      windowStage.off('windowStageEvent');\n      hilog.info(DOMAIN, TAG, '窗口事件解绑成功');\n    } catch (err) {\n      hilog.error(DOMAIN, TAG, `解绑窗口事件失败：code=${err.code}, message=${err.message}`);\n    }\n  }\n}\n\n/**\n * 手动停止当前UIAbility实例（触发onDestroy）\n */\nexport function stopCurrentAbility(context: common.UIAbilityContext): void {\n  context.terminateSelf().then(()=&gt;{\n    hilog.info(DOMAIN, TAG, 'terminateSelf调用成功，将触发onDestroy');\n  }).catch((err: BusinessError) =&gt; {\n    hilog.error(DOMAIN, TAG, `停止UIAbility失败：code=${err.code}, message=${err.message}`);\n  });\n}\n</code></pre>\n<h2 id=\"四新增手动关闭应用indexets\">四、新增手动关闭应用Index.ets</h2>\n<pre><code class=\"language-javascript\">import { common } from '@kit.AbilityKit';\nimport { stopCurrentAbility } from '../entryability/EntryAbility';\n\n@Entry\n@Component\nstruct Index {\n  @State message: string = '第一个应用';\n  private context: common.UIAbilityContext = this.getUIContext().getHostContext() as common.UIAbilityContext;\n\n  aboutToAppear(): void {}\n\n  build() {\n    Column() {\n      Text(this.message)\n        .fontSize($r('app.float.page_text_font_size'))\n        .fontWeight(FontWeight.Bold);\n\n      Button(\"关闭应用程序\")\n        .onClick(()=&gt;{\n          stopCurrentAbility(this.context);\n        });\n    }\n    .height('100%')\n    .width('100%');\n  }\n}\n</code></pre>\n<h2 id=\"五实战实操验证生命周期执行顺序\">五、实战实操：验证生命周期执行顺序</h2>\n<h3 id=\"目标\">目标</h3>\n<p>通过手动操作应用，在Logcat中观察生命周期方法和WindowStage事件的触发顺序，重点验证不同场景下<code>onDestroy</code>的触发情况。</p>\n<blockquote>\n<p>环境说明：开发工具最低支持API 13，无法测试API 12及以下版本，所有测试基于API 13 手机模拟器。<br />\n不同类型的设备监听窗口活动状态输出日志会有差异。</p>\n</blockquote>\n<h3 id=\"场景1启动应用--返回桌面--再次打开应用\">场景1：启动应用 → 返回桌面 → 再次打开应用</h3>\n<p><strong>日志输出</strong>：</p>\n<pre><code>&lt;!-- 启动应用 --&gt;\n--- onCreate 触发（全局初始化）---\n--- onWindowStageCreate 触发（加载页面）---\nWindowStage事件订阅成功\n--- onForeground 触发（恢复前台资源）---\nwindowStage foreground\nwindowStage active\nIndex页面加载成功\n\n&lt;!-- 返回桌面 --&gt;\nwindowStage paused.\nwindowStage inactive\n--- onBackground 触发（释放后台资源）---\nwindowStage background\n\n\n&lt;!-- 再次打开应用 --&gt;\n--- onNewWant 触发（新参数：{\"deviceId\":\"\",\"bundleName\":\"com.example.FirstApplication\",\"abilityName\":\"EntryAbility\",\"moduleName\":\"entry\",\"uri\":\"\",\"type\":\"\",\"flags\":0,\"action\":\"action.system.home\",\"parameters\":{\"debugApp\":true,\"moduleName\":\"entry\",\"ohos.aafwk.param.displayId\":0},\"fds\":{},\"entities\":[\"entity.system.home\"]}）---\n--- onForeground 触发（恢复前台资源）---\nwindowStage foreground\nwindowStage active\n</code></pre>\n<h3 id=\"场景2点击关闭应用程序按钮调用terminateself\">场景2：点击“关闭应用程序”按钮（调用terminateSelf）</h3>\n<p><strong>日志输出</strong>：</p>\n<pre><code>&lt;!-- terminateSelf调用成功，将触发onDestroy --&gt;\nwindowStage inactive\n--- onBackground 触发（释放后台资源）---\nwindowStage background\n--- onWindowStageWillDestroy 触发（窗口预销毁）---\n窗口事件解绑成功\n--- onWindowStageDestroy 触发（销毁窗口）---\n--- onDestroy 触发（销毁应用）---\n</code></pre>\n<h3 id=\"场景3启动应用--切后台--一键清理无实况窗应用\">场景3：启动应用 → 切后台 → 一键清理无实况窗应用</h3>\n<p><strong>日志输出</strong>：</p>\n<pre><code>--- onCreate 触发（全局初始化）---\n--- onWindowStageCreate 触发（加载页面）---\nWindowStage事件订阅成功\n--- onForeground 触发（恢复前台资源）---\nwindowStage foreground\nwindowStage active\nIndex页面加载成功\nwindowStage inactive\n--- onBackground 触发（释放后台资源）---\nwindowStage background\n</code></pre>\n<p><strong>关键结论</strong>：一键清理时系统直接终止进程，未触发<code>onWindowStageWillDestroy</code>/<code>onWindowStageDestroy</code>/<code>onDestroy</code>。</p>\n<h2 id=\"六内容总结\">六、内容总结</h2>\n<ol>\n<li><strong>生命周期分工</strong>：<code>onCreate</code>做全局初始化（仅1次），<code>onWindowStageCreate</code>负责页面加载/事件订阅（单实例模式下仅首次启动触发1次），<code>onDestroy</code>释放全局资源（API 13+一键清理无实况窗应用不触发）；</li>\n<li><strong>WindowStage核心规则</strong>：事件订阅/解绑必须成对出现在<code>onWindowStageCreate</code>/<code>onWindowStageWillDestroy</code>，避免内存泄漏；</li>\n<li><strong>数据安全兜底</strong>：因<code>onDestroy</code>存在不触发场景，关键数据需在<code>onBackground</code>中保存；</li>\n<li><strong>前后台切换链路</strong>：切后台先触发PAUSED/INACTIVE→<code>onBackground</code>→HIDDEN；切前台先触发SHOWN→<code>onForeground</code>→ACTIVE/RESUMED。</li>\n</ol>\n<h3 id=\"核心问题解答为什么页面只能在onwindowstagecreate中加载\">核心问题解答：为什么页面只能在<code>onWindowStageCreate</code>中加载？</h3>\n<p>页面加载的本质是将 UI 组件挂载到系统的窗口容器（WindowStage）上，两个生命周期阶段的核心差异决定了加载时机：</p>\n<ul>\n<li><code>onCreate</code>阶段：UIAbility 实例刚创建，系统尚未分配 WindowStage（无页面承载载体），此时调用<code>loadContent</code>会因无窗口容器而失败，甚至导致应用崩溃；</li>\n<li><code>onWindowStageCreate</code>阶段：系统已创建 WindowStage 并作为参数传入，此时拥有了页面渲染所需的窗口容器，是加载页面的时机。</li>\n</ul>\n<h3 id=\"新手避坑指南\">新手避坑指南</h3>\n<ul>\n<li>禁止在<code>onCreate</code>中加载页面（无WindowStage实例）；</li>\n<li>不要依赖<code>onDestroy</code>保存关键数据，优先在<code>onBackground</code>兜底；</li>\n<li>生命周期回调是在应用主线程执行，为了确保应用性能，建议在生命周期回调中，仅执行必要的轻量级操作。对于耗时任务，推荐采用异步处理或交由子线程执行，避免阻塞主线程。</li>\n<li>如果需要感知UIAbility生命周期变化，开发者可以使用ApplicationContext注册接口监听UIAbility生命周期变化。</li>\n</ul>\n<pre><code class=\"language-javascript\">  // 定义生命周期ID\n  private lifecycleId: number = -1;\n  \n  // 定义生命周期回调对象\n  let abilityLifecycleCallback: AbilityLifecycleCallback = {\n    // 各回调方法\n  }\n    // 获取应用上下文\n  let applicationContext = this.context.getApplicationContext();\n   // 注册应用内生命周期回调\n  this.lifecycleId = applicationContext.on('abilityLifecycle', abilityLifecycleCallback);\n  \n</code></pre>\n<h2 id=\"七代码仓库\">七、代码仓库</h2>\n<ul>\n<li>工程名称：FirstApplication</li>\n<li>仓库地址：<a href=\"https://gitee.com/HarmonyOS-UI-Basics/harmony-os-ui-basics.git\" rel=\"noopener nofollow\" target=\"_blank\">https://gitee.com/HarmonyOS-UI-Basics/harmony-os-ui-basics.git</a></li>\n</ul>\n<h2 id=\"八下节预告\">八、下节预告</h2>\n<p>下一节我们将系统学习UIAbility的全量启动模式，重点掌握：</p>\n<ol>\n<li>multiton（多实例）、singleton（单实例）、specified（指定实例）三种启动模式的核心差异与实例创建规则；</li>\n<li>不同启动模式的配置方法（module.json5配置+代码层面参数传递）和适用业务场景；</li>\n<li>多实例/指定实例模式与生命周期的联动关系（分析<code>onCreate</code>/<code>onNewWant</code>/<code>onWindowStageCreate</code>的差异化触发逻辑）；</li>\n<li>实操验证不同启动模式的效果，解决实例冲突、参数传递异常等开发常见问题。</li>\n</ol>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 15:55</span>&nbsp;\n<a href=\"https://www.cnblogs.com/san-xiu\">鸿蒙-散修</a>&nbsp;\n阅读(<span id=\"post_view_count\">81</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI开发-python-langchain框架（1-11 返回枚举-格式解析器）",
      "link": "https://www.cnblogs.com/yclh/p/19578042",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/yclh/p/19578042\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 10:56\">\n    <span>AI开发-python-langchain框架（1-11 返回枚举-格式解析器）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>跟上一篇内容一样，这次我们来看如何限定大模型返回的结果值是枚举类型的。</p>\n<p>先看代码：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:python;gutter:true;\"># 导入必要的模块\nfrom langchain.prompts import PromptTemplate          # 用于创建和管理提示模板\nfrom langchain_openai import ChatOpenAI               # 用于调用OpenAI兼容的聊天模型（如DeepSeek）\nfrom langchain.output_parsers.enum import EnumOutputParser  # 用于将LLM输出解析为枚举类型\nfrom enum import Enum                                 # Python标准库，用于定义枚举类型\nimport os                                             # 用于读取环境变量\n\n# 定义颜色枚举类，限定LLM输出必须为以下五种颜色之一\nclass Colors(Enum):\n    RED = \"红色\"\n    BROWN = \"棕色\"\n    BLACK = \"黑色\"\n    WHITE = \"白色\"\n    YELLOW = \"黄色\"\n\n# 创建枚举输出解析器，强制LLM输出必须匹配Colors枚举中的值\noutput_parser = EnumOutputParser(enum=Colors)\n\n# 获取格式化指令，告诉LLM如何正确格式化输出（例如：\"输出必须是：红色、棕色...\"）\nformat_instructions = output_parser.get_format_instructions()\nprint(format_instructions)  # 打印格式要求，用于调试或提示用户\nprint('###########')         # 分隔线\n\n# 创建提示模板：包含两个占位符 {person}（人物）和 {instructions}（输出格式要求）\npromptTemplate = PromptTemplate.from_template(\n    \"\"\"{person}的皮肤主要是什么颜色？\n{instructions}\"\"\"\n)\n\n# 固定instructions部分的内容，避免每次调用时重复传入\ninstructions = \"响应的结果请选择以下选项之一：红色、棕色、黑色、白色、黄色。\"\nprompt = promptTemplate.partial(instructions=instructions)  # partial用于预填充模板中的部分变量\n\n# 输出完整提示词示例（以\"亚洲人\"为例），用于调试查看实际发送给LLM的内容\nprint(prompt.invoke({\"person\": \"亚洲人\"}).text)\nprint('--------------')\n\n# 初始化聊天模型（使用DeepSeek API）\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),            # 从环境变量读取API密钥\n    base_url=os.getenv(\"BASE_URL\"),                   # 从环境变量读取API基础URL（如 https://api.deepseek.com）\n    model=\"deepseek-v3:671b\",                         # 指定使用的模型版本\n    temperature=0.7,                                  # 生成随机性控制：0.7 适中创造性\n    max_tokens=1024                                   # 单次响应最大token数\n)\n\n# 构建处理链：提示模板 → 大语言模型 → 枚举解析器\n# 实现端到端流程：生成提示 → 调用LLM → 强制输出为枚举类型\nchain = prompt | llm | output_parser\n\n# 调用链，传入\"亚洲人\"作为输入\nresult = chain.invoke({\"person\": \"亚洲人\"})\n\n# 输出解析后的结果（Enum类型）\nprint(result)        # 打印枚举对象（如：Colors.YELLOW）\nprint(result.name)   # 打印枚举成员名称（如：\"YELLOW\"）\nprint(result.value)  # 打印枚举成员值（如：\"黄色\"）\n</pre>\n</div>\n<p>&nbsp;返回值：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:python;gutter:true;\">Select one of the following options: 红色, 棕色, 黑色, 白色, 黄色\n###########\n亚洲人的皮肤主要是什么颜色？\n响应的结果请选择以下选项之一：红色、棕色、黑色、白色、黄色。\n--------------\nColors.YELLOW\nYELLOW\n黄色\n</pre>\n</div>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 18pt;\"><strong>重点说明：</strong></span></p>\n<div class=\"response-message-content t2t phase-answer\">\n<div class=\"custom-qwen-markdown\">\n<div class=\"qwen-markdown\">\n<div class=\"qwen-markdown-paragraph\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">精准控制大模型输出范围</span></strong><br /><span class=\"qwen-markdown-text\">本方案的核心目标是强制大语言模型返回预定义的枚举类型值，彻底解决自由文本输出的不确定性问题。通过结构化约束机制，确保模型响应严格限定在业务允许的选项集合内（如仅返回“红色、棕色、黑色、白色、黄色”），从源头杜绝“浅黄”“米白”等非标准值，显著提升系统可靠性与数据一致性。</span></div>\n<div class=\"qwen-markdown-space\">&nbsp;</div>\n<div class=\"qwen-markdown-paragraph\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">双重约束保障输出合规性</span></strong><br /><span class=\"qwen-markdown-text\">枚举输出解析器（EnumOutputParser）构建前后端双重防护：前端通过<code class=\"qwen-markdown-codespan\" style=\"cursor: pointer;\">get_format_instructions()</code><span class=\"qwen-markdown-text\">自动生成格式指令并注入提示词，明确引导模型“仅可选择以下选项之一”；后端在解析阶段对返回结果进行强校验，任何超出枚举范围的值将立即抛出<code class=\"qwen-markdown-codespan\" style=\"cursor: pointer;\">OutputParserException</code><span class=\"qwen-markdown-text\">异常，形成从生成到解析的全链路约束。</span></span></span></div>\n<div class=\"qwen-markdown-space\">&nbsp;</div>\n<div class=\"qwen-markdown-paragraph\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">枚举类型定义数据契约</span></strong><br /><span class=\"qwen-markdown-text\">通过Python的Enum类预先声明合法值集合（如<code class=\"qwen-markdown-codespan\" style=\"cursor: pointer;\">Colors</code><span class=\"qwen-markdown-text\">枚举），建立系统与模型之间的标准化数据契约。该设计不仅明确业务规则边界，更使模型输出直接转换为类型安全的枚举对象，开发者可直接通过<code class=\"qwen-markdown-codespan\" style=\"cursor: pointer;\">.name</code><span class=\"qwen-markdown-text\">（如\"YELLOW\"）和<code class=\"qwen-markdown-codespan\" style=\"cursor: pointer;\">.value</code><span class=\"qwen-markdown-text\">（如\"黄色\"）属性获取结构化结果，彻底规避字符串清洗与匹配的繁琐处理。</span></span></span></span></div>\n&nbsp;\n<div class=\"qwen-markdown-paragraph\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">典型应用场景与业务价值</span></strong><br /><span class=\"qwen-markdown-text\">该方案特别适用于需严格输出控制的场景：情感分析限定{正面/中性/负面}、审批状态约束{待审/通过/拒绝}、颜色识别规范标准色系等。相比传统依赖正则表达式或关键词匹配的后处理方案，枚举控制从设计层面根除脏数据风险，降低90%以上的数据清洗成本，为关键业务系统提供确定性保障。</span></div>\n<div class=\"qwen-markdown-space\">&nbsp;</div>\n<div class=\"qwen-markdown-paragraph\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">增强鲁棒性的实践建议</span></strong><br /><span class=\"qwen-markdown-text\">实际部署时建议结合重试机制提升容错能力：当模型首次输出不符合枚举要求时，自动触发2-3次重试并强化格式提示，成功率可达99%以上。对于金融、医疗等关键场景，可设置异常兜底策略——连续解析失败时自动转交人工审核，形成“自动约束+人工兜底”的完整质量保障闭环。</span></div>\n\n</div>\n\n</div>\n\n</div>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 10:56</span>&nbsp;\n<a href=\"https://www.cnblogs.com/yclh\">万笑佛</a>&nbsp;\n阅读(<span id=\"post_view_count\">135</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "LVM动态扩容完全指南｜小白也能上手，零停机扩展磁盘空间（5种方法）",
      "link": "https://www.cnblogs.com/liuziyi1/p/19577932",
      "published": "",
      "description": "<h2>\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/liuziyi1/p/19577932\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 10:42\">\n    <span>LVM动态扩容完全指南｜小白也能上手，零停机扩展磁盘空间（5种方法）</span>\n    \n\n</a>\n\n\t\t</h2>\n\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>做运维、搞服务器的小伙伴，肯定遇到过这种崩溃场景：</p>\n<p>半夜被告警惊醒，提示磁盘空间满了，MySQL、日志服务直接卡壳；想扩容，却发现传统磁盘分区一旦创建就固定大小，改起来又麻烦又容易丢数据……</p>\n<p>别慌！今天给大家带来 <strong>LVM动态扩容完全指南</strong>，从基础理论到实操步骤，全程通俗拆解，小白也能跟着一步步做，轻松实现「零停机扩容」，再也不用怕磁盘告警。</p>\n<p>先一句话搞懂LVM：它就像一个「磁盘魔术盒」，能把多块物理硬盘（或云盘）整合起来，变成一个统一的「存储水池」，后续扩容、缩容不用动底层硬盘，也不用停业务，灵活又安全。</p>\n<h1 id=\"一先搞懂lvm是什么为什么要用它\">一、先搞懂：LVM是什么？为什么要用它？</h1>\n<h2 id=\"11-核心背景小白必看\">1.1 核心背景（小白必看）</h2>\n<p>传统磁盘分区（比如/dev/sda1），创建时多大，后续就只能用多大，想扩容就得重新分区、格式化，不仅麻烦，还容易丢失数据，尤其生产环境根本不敢动。</p>\n<p>LVM（逻辑卷管理器）的出现，就是为了解决这个痛点——它在物理硬盘和文件系统之间加了一层「中间层」，把多块硬盘整合为一个「存储池」，空间可以自由分配、随时扩容，不用关心底层硬盘的边界。</p>\n<p>重点优势：<strong>零停机扩容</strong>，业务完全无感知；多硬盘整合，打破单盘容量限制；支持快照备份，数据更安全。</p>\n<h2 id=\"12-小白能懂的核心术语必记3个\">1.2 小白能懂的核心术语（必记3个）</h2>\n<p>LVM就像「水池系统」，记住这3个术语，后续操作不迷路：</p>\n<ul>\n<li>\n<p><strong>PV（物理卷）</strong>：最底层的「水源」，就是我们的物理硬盘（比如/dev/sdb）、云盘，是LVM管理的最小物理单元。</p>\n</li>\n<li>\n<p><strong>VG（卷组）</strong>：把多个PV整合起来的「大水池」，相当于把多块小硬盘合并成一个大存储池，统一管理空间。</p>\n</li>\n<li>\n<p><strong>LV（逻辑卷）</strong>：从VG「大水池」里分出来的「小水盆」，我们实际使用的磁盘空间（比如挂载到/var/lib/mysql的分区），就是LV。</p>\n</li>\n</ul>\n<p>简单类比：PV=多个小水桶，VG=把小水桶都倒进一个大水池，LV=从大水池里舀出的一碗水（我们实际用的）。扩容，就是给大水池加水（加PV），或者把碗变大（扩LV）。</p>\n<h2 id=\"13-适用场景看看你有没有需求\">1.3 适用场景（看看你有没有需求）</h2>\n<ul>\n<li>\n<p>数据库服务器（MySQL、PostgreSQL）：数据越存越多，需要在线扩容，不能停库。</p>\n</li>\n<li>\n<p>虚拟机/容器（KVM、K8s）：虚拟机磁盘不够用，容器持久化存储不足，需要灵活扩容。</p>\n</li>\n<li>\n<p>日志服务器（ELK）：日志量突增，磁盘很快满，需要快速扩容。</p>\n</li>\n<li>\n<p>普通服务器：多块小硬盘，想合并成一块大空间使用，后续方便扩容。</p>\n</li>\n</ul>\n<h2 id=\"14-环境要求提前准备避免踩坑\">1.4 环境要求（提前准备，避免踩坑）</h2>\n<p>不用记太复杂，满足这几个基础条件即可：</p>\n<ul>\n<li>\n<p>操作系统：CentOS 8+/Ubuntu 20.04+/Debian 11+（主流版本都支持）。</p>\n</li>\n<li>\n<p>安装LVM工具：系统一般默认安装，没有的话，下文有安装命令。</p>\n</li>\n<li>\n<p>磁盘：至少有1块未使用的硬盘/分区（或已扩容的云盘），用于扩容。</p>\n</li>\n</ul>\n<h1 id=\"二实操准备3步搞定前置工作必做\">二、实操准备：3步搞定前置工作（必做）</h1>\n<p>开始扩容前，先做好这3步，避免操作出错，小白也能轻松完成。</p>\n<h2 id=\"21-检查系统环境确认支持lvm\">2.1 检查系统环境（确认支持LVM）</h2>\n<p>复制下面的命令，依次执行，查看系统是否支持LVM（不用懂命令含义，执行就行）：</p>\n<pre><code class=\"language-bash\"># 检查操作系统版本\ncat /etc/os-release | grep -E \"^(NAME|VERSION)=\"\n\n# 检查LVM版本（有输出就是已安装）\nlvm version 2&gt;&amp;1 | head -5\n\n# 检查内核模块（确保LVM能正常工作）\nlsmod | grep -E \"^dm_\"\n\n# 若没有输出（模块未加载），执行下面2行命令加载\nmodprobe dm-mod\nmodprobe dm-snapshot\n</code></pre>\n<h2 id=\"22-安装lvm工具未安装则执行\">2.2 安装LVM工具（未安装则执行）</h2>\n<pre><code class=\"language-bash\"># CentOS/Rocky Linux（红帽系）\ndnf install -y lvm2 device-mapper-persistent-data\n\n# Ubuntu/Debian（debian系）\napt update &amp;&amp; apt install -y lvm2 thin-provisioning-tools\n\n# 验证安装（有输出就是安装成功）\npvs --version\n</code></pre>\n<h2 id=\"23-检查磁盘状态关键避免误操作\">2.3 检查磁盘状态（关键！避免误操作）</h2>\n<p>执行命令，查看当前磁盘情况，确认有未使用的磁盘（比如/dev/sdb，没有挂载点，没有文件系统）：</p>\n<pre><code class=\"language-bash\"># 列出所有磁盘，重点看「MOUNTPOINT」列（空的就是未使用）\nlsblk -o NAME,SIZE,TYPE,FSTYPE,MOUNTPOINT\n</code></pre>\n<p>输出示例（重点看sdb，未挂载、无文件系统，可用于扩容）：</p>\n<p>NAME SIZE TYPE FSTYPE MOUNTPOINT sda 50G disk ├─sda1 1G part xfs /boot └─sda2 49G part LVM2_member sdb 100G disk （未使用，可做PV）</p>\n<p>⚠️ 警告：一定要确认磁盘无数据、未挂载，否则操作会丢失数据！</p>\n<h1 id=\"三核心实操5种lvm扩容方法小白优先看前3种\">三、核心实操：5种LVM扩容方法（小白优先看前3种）</h1>\n<p>以下5种方法，覆盖不同场景，小白优先掌握「方法一、二、三」（最常用、最简单），方法四、五适合特定场景（虚拟化、备份存储）。</p>\n<p>所有命令都可以直接复制执行，重点看「步骤+注释」，不用死记命令含义。</p>\n<h2 id=\"方法一添加新磁盘最常用-新增硬盘云盘扩展vg再扩lv\">方法一：添加新磁盘（最常用）—— 新增硬盘/云盘，扩展VG再扩LV</h2>\n<p>场景：服务器新增物理硬盘，或云主机挂载新云盘（比如阿里云、腾讯云新增云盘），最通用的扩容方式。</p>\n<p>步骤（4步搞定，全程在线，不用停业务）：</p>\n<h3 id=\"步骤1创建pv把新磁盘变成水源\">步骤1：创建PV（把新磁盘变成「水源」）</h3>\n<pre><code class=\"language-bash\"># 把新磁盘（/dev/sdb）初始化为PV（替换成你的磁盘路径，比如/dev/sdc）\npvcreate /dev/sdb\n\n# 验证PV创建成功（有输出就是成功）\npvdisplay /dev/sdb\n</code></pre>\n<h3 id=\"步骤2扩展vg给大水池加水\">步骤2：扩展VG（给「大水池」加水）</h3>\n<pre><code class=\"language-bash\"># 把新创建的PV（/dev/sdb）加入现有的VG（替换成你的VG名称，比如vg_data）\n# 不知道VG名称？执行vgs命令查看\nvgextend vg_data /dev/sdb\n\n# 验证VG扩展成功（查看VG剩余空间，有新增空间就是成功）\nvgdisplay vg_data | grep -E \"(VG Size|Free)\"\n</code></pre>\n<h3 id=\"步骤3扩展lv把小水盆变大\">步骤3：扩展LV（把「小水盆」变大）</h3>\n<pre><code class=\"language-bash\"># 3种方式，选一种即可（替换成你的LV路径，比如/dev/vg_data/lv_mysql）\n# 方式1：扩展指定大小（比如新增50G）\nlvextend -L +50G /dev/vg_data/lv_mysql\n\n# 方式2：扩展到指定大小（比如扩展到150G）\nlvextend -L 150G /dev/vg_data/lv_mysql\n\n# 方式3：使用VG所有剩余空间（推荐，直接用满新增的空间）\nlvextend -l +100%FREE /dev/vg_data/lv_mysql\n\n# 验证LV扩展成功（查看LV大小，变大就是成功）\nlvdisplay /dev/vg_data/lv_mysql | grep \"LV Size\"\n</code></pre>\n<h3 id=\"步骤4扩展文件系统关键让系统识别新增空间\">步骤4：扩展文件系统（关键！让系统识别新增空间）</h3>\n<p>LV扩展后，系统还识别不到新增空间，需要执行下面的命令，根据你的文件系统类型选择（常见ext4、XFS）：</p>\n<pre><code class=\"language-bash\"># 情况1：文件系统是ext4（执行这个）\nresize2fs /dev/vg_data/lv_mysql\n\n# 情况2：文件系统是XFS（执行这个，替换成LV的挂载点，比如/data/mysql）\nxfs_growfs /data/mysql\n\n# 验证成功（查看挂载点空间，变大就是成功）\ndf -hT /data/mysql\n</code></pre>\n<p>✅ 完成：全程不用停业务，磁盘空间已经扩容成功！</p>\n<h2 id=\"方法二扩展现有磁盘云盘常用-云盘扩容后扩展现有pv\">方法二：扩展现有磁盘（云盘常用）—— 云盘扩容后，扩展现有PV</h2>\n<p>场景：云主机（阿里云、腾讯云）在控制台扩容了云盘（比如把50G云盘扩到100G），但系统里看不到新增空间，需要把新增空间纳入LVM管理。</p>\n<p>步骤（4步，不用加新磁盘，直接扩现有空间）：</p>\n<h3 id=\"步骤1确认云盘已扩容让系统识别新容量\">步骤1：确认云盘已扩容（让系统识别新容量）</h3>\n<pre><code class=\"language-bash\"># 重新扫描磁盘（云盘热扩容后必执行，让系统识别新容量）\necho 1 &gt; /sys/class/block/sdb/device/rescan\n\n# 确认磁盘新容量（查看磁盘大小，已经变成扩容后的大小就是成功）\nlsblk /dev/sdb\nfdisk -l /dev/sdb\n</code></pre>\n<h3 id=\"步骤2扩展磁盘分区让分区占用新增空间\">步骤2：扩展磁盘分区（让分区占用新增空间）</h3>\n<pre><code class=\"language-bash\"># 安装扩展工具（红帽系、debian系二选一）\ndnf install -y cloud-utils-growpart  # CentOS/Rocky Linux\napt install -y cloud-guest-utils     # Ubuntu/Debian\n\n# 扩展分区（替换成你的磁盘和分区，比如/dev/sdb的第1个分区）\ngrowpart /dev/sdb 1\n</code></pre>\n<h3 id=\"步骤3扩展pv让lvm识别分区的新增空间\">步骤3：扩展PV（让LVM识别分区的新增空间）</h3>\n<pre><code class=\"language-bash\"># 调整PV大小（替换成你的分区路径，比如/dev/sdb1）\npvresize /dev/sdb1\n\n# 验证成功（查看PV剩余空间，有新增空间就是成功）\npvdisplay /dev/sdb1 | grep -E \"(PV Size|Free PE)\"\n</code></pre>\n<h3 id=\"步骤4一键扩展lv和文件系统简化操作\">步骤4：一键扩展LV和文件系统（简化操作）</h3>\n<pre><code class=\"language-bash\"># 一条命令搞定：扩展LV，同时自动扩展文件系统（不用单独执行resize2fs/xfs_growfs）\nlvextend -r -l +100%FREE /dev/vg_data/lv_mysql\n</code></pre>\n<p>✅ 完成：云盘扩容后的空间，已经成功纳入LVM管理，业务无感知。</p>\n<h2 id=\"方法三一键扩容最简单-lvextend--r-一键搞定\">方法三：一键扩容（最简单）—— lvextend -r 一键搞定</h2>\n<p>场景：VG有剩余空间（比如之前加过PV，还有未分配空间），想快速扩容LV，不用分步执行。</p>\n<p>核心：利用LVM的「-r」参数，自动扩展LV+文件系统，一步到位。</p>\n<pre><code class=\"language-bash\"># 3种一键扩容方式，选一种即可（替换成你的LV路径）\n# 方式1：新增20G空间，自动扩展文件系统\nlvextend -r -L +20G /dev/vg_data/lv_mysql\n\n# 方式2：使用VG剩余空间的50%\nlvextend -r -l +50%FREE /dev/vg_data/lv_mysql\n\n# 方式3：使用VG所有剩余空间（推荐）\nlvextend -r -l +100%FREE /dev/vg_data/lv_mysql\n</code></pre>\n<p>✅ 完成：最简单的扩容方式，小白首选，避免分步操作出错。</p>\n<h2 id=\"方法四精简配置扩容虚拟化常用-thin-provisioning\">方法四：精简配置扩容（虚拟化常用）—— Thin Provisioning</h2>\n<p>场景：虚拟化环境（KVM、Proxmox）、容器存储，需要「超分配空间」（比如实际只有100G物理空间，却能创建200G的LV），按需分配实际空间，节省存储成本。</p>\n<p>步骤（简化版，小白了解即可，实操按需参考）：</p>\n<pre><code class=\"language-bash\"># 1. 创建精简池（thin pool，替换成你的VG名称）\nlvcreate -L 80G -T vg_data/thin_pool\n\n# 2. 创建精简卷（虚拟200G，实际按需分配）\nlvcreate -V 200G -T vg_data/thin_pool -n lv_vm_disk1\n\n# 3. 扩容精简池（当池空间不足时）\nlvextend -L +50G vg_data/thin_pool\n\n# 4. 扩容精简卷和文件系统\nlvextend -L +100G /dev/vg_data/lv_vm_disk1\nxfs_growfs /mnt/vm_disk1\n</code></pre>\n<h2 id=\"方法五vdo压缩扩容2026新特性-压缩去重节省空间\">方法五：VDO压缩扩容（2026新特性）—— 压缩去重，节省空间</h2>\n<p>场景：备份存储、虚拟机镜像（重复数据多的场景），利用LVM2 2.03.x新特性，实现数据压缩+去重，比如100G物理空间，可存储300G数据（3:1压缩比）。</p>\n<p>步骤（简化版，需LVM2 2.03.x以上版本）：</p>\n<pre><code class=\"language-bash\"># 1. 创建VDO池（100G物理空间，虚拟300G逻辑空间）\nlvcreate --type vdo -L 100G -V 300G -n vdo_pool vg_data\n\n# 2. 扩容VDO物理空间（新增50G）\nlvextend -L +50G vg_data/vdo_pool_vpool\n\n# 3. 扩容VDO逻辑空间（新增100G）\nlvextend -L +100G vg_data/vdo_pool\n\n# 4. 扩展文件系统\nxfs_growfs /mnt/vdo_storage\n</code></pre>\n<h1 id=\"四实战案例mysql数据库零停机扩容小白必看\">四、实战案例：MySQL数据库零停机扩容（小白必看）</h1>\n<p>结合方法一，实战演示「MySQL数据库在线扩容」，模拟生产环境最常见场景，跟着做就能搞定。</p>\n<h2 id=\"场景描述\">场景描述</h2>\n<p>生产环境MySQL服务器，数据目录/var/lib/mysql（挂载在/dev/vg_data/lv_mysql，100G，XFS文件系统），使用率达到85%，需要新增50G空间，不停止MySQL服务。</p>\n<p>环境：CentOS 9，MySQL 8.0，新增磁盘/dev/sdc（100G）。</p>\n<h2 id=\"实操步骤直接复制执行\">实操步骤（直接复制执行）</h2>\n<pre><code class=\"language-bash\"># 1. 确认当前磁盘使用率（查看是否真的满了）\ndf -hT /var/lib/mysql\n\n# 2. 确认MySQL服务正常运行（扩容不影响服务）\nsystemctl status mysqld\nmysql -e \"SHOW STATUS LIKE 'Uptime';\"\n\n# 3. 创建PV（新增磁盘/dev/sdc）\npvcreate /dev/sdc\n\n# 4. 扩展VG（加入现有vg_data）\nvgextend vg_data /dev/sdc\n\n# 5. 一键扩容LV+文件系统（新增50G）\nlvextend -r -L +50G /dev/vg_data/lv_mysql\n\n# 6. 验证扩容成功\ndf -hT /var/lib/mysql\n\n# 7. 确认MySQL服务正常（无中断）\nmysql -e \"SHOW STATUS LIKE 'Uptime';\"\n</code></pre>\n<p>✅ 结果：MySQL服务全程未中断，磁盘空间从100G扩展到150G，使用率从85%降到57%，完美解决磁盘告警。</p>\n<h1 id=\"五小白必看注意事项避坑指南重中之重\">五、小白必看：注意事项+避坑指南（重中之重）</h1>\n<p>LVM操作涉及底层存储，一步错可能丢数据，以下注意事项，一定要看完再操作！</p>\n<h2 id=\"51-核心警告必看\">5.1 核心警告（必看）</h2>\n<ul>\n<li>\n<p>⚠️ 所有操作前，<strong>务必备份数据</strong>！哪怕是在线扩容，也有极小概率出错，备份是最后保障（比如备份MySQL数据、重要文件）。</p>\n</li>\n<li>\n<p>⚠️ XFS文件系统「只能扩容，不能缩容」！规划LV大小时，尽量预留一定空间，避免后续想缩容却无法操作。</p>\n</li>\n<li>\n<p>⚠️ 不要误操作正在使用的磁盘！执行lsblk命令，确认磁盘未挂载、无数据，再进行PV创建。</p>\n</li>\n</ul>\n<h2 id=\"52-常见避坑点\">5.2 常见避坑点</h2>\n<ul>\n<li>\n<p>坑1：LV扩展后，df命令看不到新增空间 → 忘记扩展文件系统，执行resize2fs（ext4）或xfs_growfs（XFS）即可。</p>\n</li>\n<li>\n<p>坑2：云盘扩容后，系统看不到新容量 → 忘记执行磁盘扫描命令（echo 1 &gt; /sys/class/block/sdb/device/rescan）。</p>\n</li>\n<li>\n<p>坑3：vgextend命令失败 → 确认PV创建成功（pvdisplay查看），且VG名称正确（vgs命令查看）。</p>\n</li>\n</ul>\n<h2 id=\"53-最佳实践小白参考\">5.3 最佳实践（小白参考）</h2>\n<ul>\n<li>\n<p>创建VG时，指定PE大小为16MB或32MB（大容量存储更高效）：vgcreate -s 16M vg_data /dev/sdb。</p>\n</li>\n<li>\n<p>定期备份LVM元数据：vgcfgbackup vg_data（避免VG损坏，无法恢复）。</p>\n</li>\n<li>\n<p>监控VG剩余空间，当剩余空间低于10%时，提前扩容，避免磁盘满导致业务中断。</p>\n</li>\n</ul>\n<h1 id=\"六结尾总结命令速查表\">六、结尾总结+命令速查表</h1>\n<h2 id=\"61-核心总结\">6.1 核心总结</h2>\n<p>LVM动态扩容的核心逻辑：<strong>扩PV → 扩VG → 扩LV → 扩文件系统</strong>（一键扩容可跳过部分步骤）。</p>\n<p>小白重点掌握前3种方法（添加新磁盘、扩展现有磁盘、一键扩容），就能应对90%以上的扩容场景，全程零停机，业务无感知。</p>\n<h2 id=\"62-常用命令速查表小白收藏备用\">6.2 常用命令速查表（小白收藏，备用）</h2>\n<pre><code class=\"language-bash\"># PV操作（物理卷）\npvcreate /dev/sdb    # 创建PV\npvdisplay /dev/sdb   # 查看PV详情\npvs                  # 列出所有PV\n\n# VG操作（卷组）\nvgcreate vg_data /dev/sdb  # 创建VG\nvgextend vg_data /dev/sdc  # 扩展VG\nvgdisplay vg_data          # 查看VG详情\nvgs                        # 列出所有VG\n\n# LV操作（逻辑卷）\nlvcreate -L 50G -n lv_data vg_data  # 创建LV\nlvextend -r -L +20G /dev/vg_data/lv_data  # 一键扩容LV\nlvdisplay /dev/vg_data/lv_data            # 查看LV详情\nlvs                                       # 列出所有LV\n\n# 文件系统扩展\nresize2fs /dev/vg_data/lv_data  # ext4扩容\nxfs_growfs /data/mysql          # XFS扩容\n</code></pre>\n<p>💡 最后提醒：实操时，一定要替换成自己的磁盘路径（/dev/sdb）、VG名称（vg_data）、LV路径，避免复制错误！</p>\n<p>如果操作中遇到问题，评论区留言，我会一一解答，帮助大家避坑，轻松搞定LVM扩容～</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t<p class=\"postfoot\">\n\t\t\tposted on \n<span id=\"post-date\">2026-02-05 10:42</span>&nbsp;\n<a href=\"https://www.cnblogs.com/liuziyi1\">刘子毅</a>&nbsp;\n阅读(<span id=\"post_view_count\">159</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</p>"
    },
    {
      "title": "FastAPI生命周期管理实战：从启动到关闭，如何优雅地管好你的“资源家当”",
      "link": "https://www.cnblogs.com/ymtianyu/p/19577804",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ymtianyu/p/19577804\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 10:23\">\n    <span>FastAPI生命周期管理实战：从启动到关闭，如何优雅地管好你的“资源家当”</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        本文深入讲解了FastAPI的Lifespan生命周期管理机制，详细分析了其工作原理，并提供了使用@asynccontextmanager的完整实战代码示例。重点总结了资源初始化、优雅关闭的实现方法，以及在实际使用中常见的四个“坑点”与解决方案，旨在帮助开发者构建更稳定、专业的FastAPI应用。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<h2>FastAPI实战：你以为的启动与关闭，远不止开始和结束</h2>\n<p><strong style=\"color: rgba(186, 55, 42, 1);\">摘要：</strong>本文详细探讨了FastAPI的Lifespan（生命周期）管理，从为什么需要它、它的核心工作原理，到使用<span style=\"color: rgba(186, 55, 42, 1);\"><code>@asynccontextmanager</code></span>的最佳实践、常见“坑点”及解决方案。通过一个完整的实战示例，你将学会如何优雅地管理数据库连接池、配置加载等资源，确保应用稳定可控。</p>\n<hr />\n<p>有没有在部署FastAPI应用后，遇到过这些状况？👇</p>\n<p>→ 服务一重启，前几个请求总是失败，日志里飘着“数据库连接错误”。</p>\n<p>→ 想优雅地关闭，给下游服务发个通知，却发现请求直接被掐断，资源清理了个寂寞。</p>\n<p>→ 依赖项（比如配置、外部客户端）的初始化代码，散落在路由和中间件里，维护起来头大。</p>\n<p>这些都是用 <code style=\"color: rgba(186, 55, 42, 1);\">@app.on_event(\"startup\")</code> 和 <code style=\"color: rgba(186, 55, 42, 1);\">\"shutdown\"</code> 踩坑踩出来的经验（和眼泪）。今天，咱们就来聊聊它的正统接班人——<strong><span style=\"color: rgba(186, 55, 42, 1);\"><code>Lifespan</code></span></strong>，以及怎么用它写出更健壮、更专业的FastAPI应用。</p>\n<h2>🎯 第一部分：问题出在哪？从“餐厅”的混乱开业说起</h2>\n<p>想象一下你开了一家数字餐厅（你的FastAPI应用）。</p>\n<p>🙅 <strong>老做法（on_event）：</strong> 早上开业时间到了，厨师（数据库连接池）、服务员（配置）、采购员（HTTP客户端）才慌慌张张地各自开始准备。客人（请求）已经进门点菜了，厨师可能连锅都还没热。晚上打烊时，一声令下直接拉闸，采购员手里还在进行的订单、厨师没洗的锅，全都戛然而止。</p>\n<p>这，就是早期启动/关闭事件的异步问题，以及缺乏对“准备”和“清理”阶段的精细控制。</p>\n<p>FastAPI在较新的版本中，引入了<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>参数，它是一个异步上下文管理器。它的核心思想是：<strong style=\"color: rgba(186, 55, 42, 1);\">在应用正式处理请求前，把所有“家当”都准备好；在应用退出前，有条不紊地收拾好所有“家当”。</strong></p>\n<p>这确保了在应用生命周期内，资源状态是确定的。</p>\n<h2>🧠 第二部分：核心原理与步骤，一个 yield 搞定所有</h2>\n<p><code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code> 的原理，其实就是Python异步上下文管理器的经典模式：<code style=\"color: rgba(186, 55, 42, 1);\">__aenter__</code> 和 <code style=\"color: rgba(186, 55, 42, 1);\">__aexit__</code>。在FastAPI里，我们用一个<code style=\"color: rgba(186, 55, 42, 1);\">@asynccontextmanager</code>装饰器就能优雅实现。</p>\n<p>它的工作流超级清晰：</p>\n<div>\n<p> 1️⃣ <code style=\"color: rgba(186, 55, 42, 1);\">yield</code> 之前：启动逻辑。在这里连接数据库、加载配置、创建各种客户端。</p>\n<p> 2️⃣ <code style=\"color: rgba(186, 55, 42, 1);\">yield</code> 那一刻：应用进入“运行就绪”状态。此时所有资源都已初始化完毕，可以安全接收请求。</p>\n<p> 3️⃣ <code style=\"color: rgba(186, 55, 42, 1);\">yield</code> 之后：关闭逻辑。在这里关闭连接池、清理临时文件、发送关闭通知。</p>\n</div>\n<p><strong style=\"color: rgba(186, 55, 42, 1);\">重点：</strong> 所有在<code style=\"color: rgba(186, 55, 42, 1);\">yield</code>前初始化的对象，都可以通过<code style=\"color: rgba(186, 55, 42, 1);\">request.app.state</code>在整个应用范围内共享。这是依赖注入的“后勤总部”！</p>\n<h2>💻 第三部分：实战演示，手把手搭一个稳如老狗的应用</h2>\n<p>光说不练假把式，来看一个集成了Redis连接池和配置管理的实战例子。你完全可以直接复制，改改就能用。</p>\n<pre class=\"language-python highlighter-hljs\"><code>from contextlib import asynccontextmanager\nfrom fastapi import FastAPI, Depends, Request\nimport redis.asyncio as redis\n\n# 假装从环境或文件加载的配置\nAPP_CONFIG = {\"redis_url\": \"redis://localhost\", \"max_connections\": 10}\n\n# 这里是核心！定义 lifespan\n@asynccontextmanager\nasync def app_lifespan(app: FastAPI):\n    \"\"\"\n    应用生命周期管理\n    1. 启动：创建Redis连接池\n    2. 运行：保持\n    3. 关闭：清理连接池\n    \"\"\"\n    print(\"🚀 应用启动中...正在初始化资源\")\n    # --- 启动阶段 ---\n    # 初始化Redis连接池\n    try:\n        redis_pool = redis.ConnectionPool.from_url(\n            APP_CONFIG[\"redis_url\"],\n            max_connections=APP_CONFIG[\"max_connections\"],\n            decode_responses=True\n        )\n        app.state.redis_pool = redis_pool\n        app.state.config = APP_CONFIG  # 配置也存进去\n        print(\"✅ Redis连接池和配置初始化完成！\")\n    except Exception as e:\n        print(f\"❌ 资源初始化失败: {e}\")\n        # 这里一定要失败，不要让应用带着问题启动\n        raise\n\n    #  yield 标志着应用正式启动完成，开始接收请求\n    yield\n\n    # --- 关闭阶段 ---\n    print(\"\\n🛑 应用关闭中...正在清理资源\")\n    # 关闭Redis连接池\n    if hasattr(app.state, 'redis_pool'):\n        await app.state.redis_pool.disconnect()\n        print(\"✅ Redis连接池已关闭。\")\n    print(\"👋 资源清理完毕，应用安全退出。\")\n\n# 创建App，注入lifespan\napp = FastAPI(title=\"Lifespan Demo\", lifespan=app_lifespan)\n\n# 一个依赖项，用于在路由中方便地获取Redis连接\nasync def get_redis(request: Request):\n    # 直接从app.state获取连接池，并创建临时连接\n    async with redis.Redis(connection_pool=request.app.state.redis_pool) as client:\n        yield client\n\n# 示例路由\n@app.get(\"/cache/{key}\")\nasync def get_cache(key: str, redis = Depends(get_redis)):\n    value = await redis.get(key)\n    return {\"key\": key, \"value\": value}\n\n@app.post(\"/cache/{key}\")\nasync def set_cache(key: str, value: str, redis = Depends(get_redis)):\n    await redis.set(key, value, ex=60)  # 60秒过期\n    return {\"msg\": \"OK\"}\n\n@app.get(\"/config\")\nasync def show_config(request: Request):\n    # 直接访问全局配置\n    return request.app.state.config\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n    </code></pre>\n<p>跑起来之后，你会看到控制台先打印启动日志，Uvicorn工人就位后，应用才开始监听请求。用<code style=\"color: rgba(186, 55, 42, 1);\">Ctrl+C</code>关闭时，关闭逻辑也会被执行。</p>\n<h2>🚨 第四部分：常见坑点 &amp; 我的血泪经验</h2>\n<p>好了，代码跑通了，但下面这些才是保证你不上生产环境“演砸”的关键。</p>\n<div>\n<p><strong>🔥 坑点一：启动失败处理不当</strong></p>\n<p> - <strong>问题：</strong> 如果在<code style=\"color: rgba(186, 55, 42, 1);\">yield</code>之前初始化数据库失败了，你该怎么办？</p>\n<p> - <strong>我的教训：</strong> 千万别吞异常！就像上面代码里做的，必须<code style=\"color: rgba(186, 55, 42, 1);\">raise</code>出去，让应用启动失败。一个连不上数据库的应用，不如不启动。可以通过日志和监控系统及时告警。</p>\n</div>\n<div>\n<p><strong>🔥 坑点二：在lifespan里写“慢”逻辑</strong></p>\n<p> - <strong>问题：</strong> Lifespan的启动阶段会阻塞应用启动。如果你在这里跑一个耗时10分钟的数据同步任务，你的服务就10分钟不可用。</p>\n<p> - <strong>我的建议：</strong> 对于非核心、耗时的初始化（如预加载大数据模型），考虑在<code style=\"color: rgba(186, 55, 42, 1);\">yield</code>后，用后台任务异步执行。或者，设计成懒加载模式，在第一次请求时初始化。</p>\n</div>\n<div>\n<p><strong>🔥 坑点三：忽略异步上下文管理</strong></p>\n<p> - <strong>问题：</strong> 像数据库连接池、HTTP客户端这类资源，它们自己往往也需要<code style=\"color: rgba(186, 55, 42, 1);\">async with</code>来管理生命周期。</p>\n<p> - <strong>正确做法：</strong> 仔细阅读你用的客户端库文档。像上面redis的<code style=\"color: rgba(186, 55, 42, 1);\">ConnectionPool.disconnect()</code>，它就是异步的，必须<code style=\"color: rgba(186, 55, 42, 1);\">await</code>。同步客户端则通常在<code style=\"color: rgba(186, 55, 42, 1);\">.close()</code>。</p>\n</div>\n<div>\n<p><strong>🔥 坑点四：State滥用</strong></p>\n<p> - <strong>问题：</strong> <code style=\"color: rgba(186, 55, 42, 1);\">app.state</code>不是万能的储物间，它适合放<strong>全局、只读或线程/协程安全</strong>的对象。</p>\n<p> - <strong>警告：</strong> 别往里塞请求级别的数据，也别放频繁修改的全局变量，这会在多worker环境下让你怀疑人生。共享配置、连接池、初始化好的客户端实例，是它的最佳拍档。</p>\n</div>\n<h2>💎 最后啰嗦一句</h2>\n<p>使用<code style=\"color: rgba(186, 55, 42, 1);\">lifespan</code>，本质上是一种<strong style=\"color: rgba(186, 55, 42, 1);\">“契约编程”</strong>。你向框架承诺：“我会管好我带来的资源”；框架向你保证：“我会在正确的时间点给你执行管理的权利”。</p>\n<p>它让我们的应用从“能跑”进化到“跑得稳、关得优雅”。尤其是在云原生和容器化环境下，优雅启停是保证服务高可用的基础一环。</p>\n<p>希望这篇融合了我不少“坑”的文章，能帮你把FastAPI用得更加得心应手。如果你有更妙的用法或者也踩过有趣的坑，欢迎来聊聊！</p>\n<hr />\n<p style=\"text-align: center; color: rgba(127, 140, 141, 1); font-size: 0.9em;\">觉得有用就赶紧收藏吧，这种实战细节，下次配置新项目时翻出来看一眼，能省下不少查文档和Debug的时间。我是一名程序媛，我们下次见！👩💻</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 10:23</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ymtianyu\">一名程序媛呀</a>&nbsp;\n阅读(<span id=\"post_view_count\">129</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Redis 事务的“原子性”迷思：为什么我们最终选择了 Lua 脚本",
      "link": "https://www.cnblogs.com/xzqcsj/p/19577555",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xzqcsj/p/19577555\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 09:49\">\n    <span>Redis 事务的“原子性”迷思：为什么我们最终选择了 Lua 脚本</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"Redis 事务的“原子性”迷思：为什么我们最终选择了 Lua 脚本\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3703499/202602/3703499-20260205094823688-1305507315.png\" />\n        作为一个长期和关系型数据库（RDBMS）打交道的开发者，初次查阅 Redis 文档时，看到 MULTI、EXEC、DISCARD 这些指令，心中难免涌起一股由于熟悉而带来的安全感。\n我们的大脑会自动建立映射：MULTI 就是 BEGIN，EXEC 就是 COMMIT，DISCARD 就是 ROLLBACK。这套组合拳打下来，所有的业务逻辑似乎都应该具备了“不成功便成仁”的原子性保障。\n但这恰恰是 Redis 给我上的第一课：相似的命名背后，往往藏着截然不同的灵魂。 当你把 MySQL 的事务观生搬硬套到 Redis 身上时，错付就已经开始了。\n这篇文章将带你剥开 Redis 事务的外衣，从“原子性”的定义偏差说起，聊聊为什么在现代开发中，我们越来越倾向于用 Lua 脚本来替代它。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p><strong>写在前面的话</strong></p>\n<p>作为一个长期和关系型数据库（RDBMS）打交道的开发者，初次查阅 Redis 文档时，看到 <code>MULTI</code>、<code>EXEC</code>、<code>DISCARD</code> 这些指令，心中难免涌起一股由于熟悉而带来的安全感。</p>\n<p>我们的大脑会自动建立映射：<code>MULTI</code> 就是 <code>BEGIN</code>，<code>EXEC</code> 就是 <code>COMMIT</code>，<code>DISCARD</code> 就是 <code>ROLLBACK</code>。这套组合拳打下来，所有的业务逻辑似乎都应该具备了“不成功便成仁”的原子性保障。</p>\n<p>但这恰恰是 Redis 给我上的第一课：<strong>相似的命名背后，往往藏着截然不同的灵魂。</strong> 当你把 MySQL 的事务观生搬硬套到 Redis 身上时，错付就已经开始了。</p>\n<p>这篇文章将带你剥开 <strong>Redis 事务</strong>的外衣，从“原子性”的定义偏差说起，聊聊为什么在现代开发中，我们越来越倾向于<strong>用 Lua 脚本来替代它。</strong></p>\n</blockquote>\n<hr />\n<h2 id=\"一先把误会解开redis-事务不是-acid\">一、先把误会解开：Redis 事务不是 ACID</h2>\n<p><strong>在关系型数据库的世界里，“事务”二字重若千钧</strong>，它几乎等同于 <strong>ACID</strong>（原子性、一致性、隔离性、持久性）。我们习惯了“要么全有，要么全无”的安全感。</p>\n<p>而在 Redis 的世界里，<code>MULTI</code> 和 <code>EXEC</code> 更像是一个<strong>批处理信号</strong>：</p>\n<blockquote>\n<p><strong>把一堆命令先放进队列里排队，等到 <code>EXEC</code> 时，一次性、按顺序地执行它们。</strong></p>\n</blockquote>\n<p>这里有一个巨大的认知偏差。当我们谈论 Redis 的“原子性”时，Redis 指的其实是 <strong>隔离性（Isolation）</strong>，而不是 <strong>回滚（Rollback）</strong>。</p>\n<ul>\n<li><strong>它保证的是</strong>：<strong>我执行这段命令的时候，别人不能插队（独占执行）。</strong></li>\n<li><strong>它不保证的是</strong>：<strong>如果我执行到一半报错了，我会帮你把前面的操作撤销（失败回滚）。</strong></li>\n</ul>\n<p>为了更直观地理解，我们可以对比一下 Redis 事务和标准 ACID 事务的区别：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">特性</th>\n<th style=\"text-align: left;\">关系型数据库 (MySQL)</th>\n<th style=\"text-align: left;\">Redis 事务</th>\n<th style=\"text-align: left;\">差异解读</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>原子性 (Atomicity)</strong></td>\n<td style=\"text-align: left;\"><strong>All or Nothing</strong> <br /> 失败即回滚，如同未发生过</td>\n<td style=\"text-align: left;\"><strong>All or Partial</strong> <br /> 没得商量，错了就错了，剩下的接着干</td>\n<td style=\"text-align: left;\">Redis 不支持 Rollback，部分成功是常态</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>一致性 (Consistency)</strong></td>\n<td style=\"text-align: left;\">强一致性 <br /> 约束必须满足</td>\n<td style=\"text-align: left;\">弱一致性 <br /> 依赖业务代码保障</td>\n<td style=\"text-align: left;\">Redis 不会校验业务约束（如外键、非空等）</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>隔离性 (Isolation)</strong></td>\n<td style=\"text-align: left;\">有多种隔离级别 (RC/RR/Serializable)</td>\n<td style=\"text-align: left;\"><strong>串行化执行</strong> <br /> 执行期间不可被打断</td>\n<td style=\"text-align: left;\">得益于单线程模型，<code>EXEC</code> 期间天然隔离</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>持久性 (Durability)</strong></td>\n<td style=\"text-align: left;\">WAL 日志保障 <br /> 掉电不丢失</td>\n<td style=\"text-align: left;\">取决于 AOF/RDB 配置</td>\n<td style=\"text-align: left;\">默认配置下通常有数据丢失风险</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p><strong>一句话总结：</strong><br />\n<strong>Redis 事务是“命令队列 + 独占执行”，绝不是“失败回滚 + 强一致”。</strong></p>\n</blockquote>\n<hr />\n<h2 id=\"二残酷的真相它真的不包回滚\">二、残酷的真相：它真的不包回滚</h2>\n<p>为了把这个概念刻进 DNA，我们看两种真实的错误场景。</p>\n<h3 id=\"1-入队时的低级错误全员连坐\">1. 入队时的“低级错误”（全员连坐）</h3>\n<p>如果你在<strong>命令入队阶段</strong>就犯了语法错误（比如参数写少了），Redis 还是讲道理的，它会直接拒绝整个事务。</p>\n<pre><code class=\"language-bash\">127.0.0.1:6379&gt; MULTI\nOK\n127.0.0.1:6379&gt; SET key1 value1\nQUEUED\n127.0.0.1:6379&gt; SET key2      # &lt;--- 语法错误：少了参数\n(error) ERR wrong number of arguments for 'set' command\n127.0.0.1:6379&gt; EXEC\n(error) EXECABORT Transaction discarded because of previous errors.\n</code></pre>\n<p>这时候，<strong>所有命令都不会执行</strong>。这符合我们对“事务”的预期。</p>\n<h3 id=\"2-执行时的运行时错误虽死犹进\">2. 执行时的“运行时错误”（虽死犹进）</h3>\n<p>这才是真正的坑。假设语法没问题，但在<strong>执行期间</strong>，某条命令因为数据类型不匹配报错了：</p>\n<pre><code class=\"language-bash\">127.0.0.1:6379&gt; MULTI\nOK\n127.0.0.1:6379&gt; SET user:A:points 100\nQUEUED\n127.0.0.1:6379&gt; LPUSH user:A:points \"error_data\"  # &lt;--- 对 String 类型做 List 操作，注定运行报错\nQUEUED\n127.0.0.1:6379&gt; INCR user:A:points                 # &lt;--- 后续命令\nQUEUED\n\n127.0.0.1:6379&gt; EXEC\n1) OK\n2) (error) WRONGTYPE Operation against a key holding the wrong kind of value  &lt;--- 报错！\n3) (integer) 101                                                              &lt;--- 依然成功了！\n</code></pre>\n<p><strong>目瞪口呆了吗？</strong><br />\n第二条命令报错了，但第三条命令依然欢快地执行了。数据出现了中间态：即所谓的“不一致”。</p>\n<p>Redis 官方对此的解释非常“直男”：</p>\n<blockquote>\n<p>“只有语法错误才会被拦截，运行时错误属于程序员的逻辑 Bug（比如把 String 当 List 用）。<strong>数据库不应该为了程序员的 Bug 买单，去搞复杂的回滚机制。</strong>”</p>\n</blockquote>\n<hr />\n<h2 id=\"三进阶之路从原生批量到-lua-脚本\">三、进阶之路：从原生批量到 Lua 脚本</h2>\n<blockquote>\n<p><strong>💡 预备知识：RTT 是性能杀手</strong></p>\n<p>一个 Redis 命令的执行可以简化为 4 步：<strong>发送命令 → 命令排队 → 命令执行 → 返回结果</strong>。</p>\n<p>其中，第 1 步和第 4 步的时间之和称为 <strong>RTT (往返时间)</strong>。如果我有 100 个命令，一个个发就需要 100 次 RTT，大部分时间都浪费在网络传输上。<br />\n<strong>批量操作的核心意义，就是把 100 次 RTT 压缩成 1 次。</strong></p>\n</blockquote>\n<p>既然 <code>MULTI/EXEC</code> 这么“头铁”，那我们在实际开发中到底该怎么选？我们可以把 Redis 的批量操作能力分为几个段位。</p>\n<h3 id=\"lv1-原生批量命令-mset--mget\">Lv1. 原生批量命令 (MSET / MGET)</h3>\n<p>这是最简单、最快的方式。</p>\n<ul>\n<li><strong>特点</strong>：原生的原子性。<code>MSET key1 val1 key2 val2</code> 是一个原子操作，<strong>要么都成功，要么都失败</strong>（在 Redis 层面）。</li>\n<li><strong>示例</strong>：<pre><code class=\"language-bash\">MSET key1 \"Hello\" key2 \"World\"\n</code></pre>\n</li>\n<li><strong>局限</strong>：只能处理同一种命令，逻辑死板。</li>\n</ul>\n<h3 id=\"lv2-管道-pipeline\">Lv2. 管道 (Pipeline)</h3>\n<p>当你需要批量执行几十个不同的命令，且不需要它们之间有逻辑依赖时，Pipeline 是首选。</p>\n<ul>\n<li><strong>特点</strong>：<strong>唯快不破</strong>。它把几十个命令打包，一次网络请求（RTT）发给服务器，服务器执行完再一次性返回。</li>\n<li><strong>形象理解</strong>：<strong>下 100 个单 -&gt; 一次性收 100 个快递 (1 次 RTT)</strong>。</li>\n<li><strong>与事务的区别</strong>：\n<ul>\n<li><strong>非原子性</strong>：Pipeline 只是打包发送，Redis 可能会在处理 Pipeline 中间穿插执行其他客户端的命令（交错执行）。</li>\n<li><strong>效率更高</strong>：不需要像事务那样每个命令都发一次，只需要发送一次。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"lv3-事务-multi--exec\">Lv3. 事务 (MULTI / EXEC)</h3>\n<p>比 Pipeline 多了一层保障：<strong>独占执行</strong>。</p>\n<ul>\n<li><strong>特点</strong>：<strong>原子操作（隔离性）</strong>。\n<ul>\n<li><strong>两个不同的事务不会同时运行</strong>。在 <code>EXEC</code> 执行期间，Redis 会“以此为尊”，保证没有其他客户端能插队。</li>\n</ul>\n</li>\n<li><strong>缺点</strong>：\n<ul>\n<li><strong>RTT 开销大</strong>：事务中 <strong>每个命令都需要单独发送</strong> 到服务端入队，请求次数并没有减少。</li>\n<li><strong>不支持回滚</strong>，不支持在事务中间做逻辑判断。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"lv31-事务--watch-乐观锁\">Lv3.1 事务 + WATCH (乐观锁)</h3>\n<p>单纯的 <code>MULTI/EXEC</code> 往往比较鸡肋，因为它无法感知中间状态。<strong>但这套机制唯一的“王牌”组合是配合 <code>WATCH</code> 命令，实现乐观锁 (CAS)。</strong></p>\n<ul>\n<li>\n<p><strong>场景</strong>：秒杀扣减库存。</p>\n<ul>\n<li>在 <code>MULTI</code> 之前 <code>WATCH stock</code>。</li>\n<li>如果在 <code>EXEC</code> 执行前 <code>stock</code> 被别人改了，整个事务原地取消（返回 nil）。</li>\n</ul>\n</li>\n<li>\n<p><strong>代码示例</strong>：</p>\n<pre><code class=\"language-bash\">WATCH stock:001                # 1. 监视库存\nGET stock:001                  # 2. 读库存，发现是 10\nMULTI                          # 3. 开启事务 (开始排队)\nDECR stock:001                 # 4. 减库存\nEXEC                           # 5. 执行\n# 如果在步骤 1-5 之间，别人改了 stock:001，这里会返回 (nil)，事务回滚。\n</code></pre>\n</li>\n<li>\n<p><strong>致命弱点</strong>：<strong>高并发下性能极差</strong>。</p>\n<ul>\n<li>就像一群人抢一个麦克风，一个人抢到了，其他人的 <code>CAS</code> 全部失败，只能客户端重试（自旋）。</li>\n<li><strong>竞争越激烈，重试越频繁，CPU 空转越严重。</strong></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"lv4-最终兵器--lua-脚本\">Lv4. 最终兵器 —— Lua 脚本</h3>\n<p>从 Redis 2.6 开始，Lua 脚本成为了解决复杂原子性问题的核心方案，它完美替代了 <code>WATCH</code> 事务。</p>\n<p><strong>为什么它比事务强？</strong></p>\n<ol>\n<li><strong>逻辑原子性</strong>：一段 Lua 脚本被视作一条命令。Redis 保证脚本执行期间，<strong>不会有任何其他脚本或命令插入</strong>。</li>\n<li><strong>效率更高</strong>：不需要像 <code>WATCH</code> 那样反复重试。脚本在服务器端执行，只有一次 RTT。</li>\n</ol>\n<p><strong>示例：安全的“先查后改”</strong></p>\n<pre><code class=\"language-lua\">-- 判断 key 是否等于预期值，如果是则删除\nif redis.call(\"GET\", KEYS[1]) == ARGV[1] then\n    return redis.call(\"DEL\", KEYS[1])\nelse\n    return 0\nend\n</code></pre>\n<p><strong>⚠️ 必须警惕的缺陷：Lua 也不回滚！</strong><br />\n虽然 Lua 脚本被称为“原子操作”，但请注意：它的原子性依然指的是<strong>不被打扰</strong>，而不是<strong>失败回滚</strong>。</p>\n<blockquote>\n<p><strong>如果 Lua 脚本运行到中途出错（比如调用了不存在的命令，或显式报错退出），脚本会停止执行，但之前已经执行过的写操作，是不会被撤销的！</strong></p>\n</blockquote>\n<p>这意味着，即使是 Lua，也不能给你带来 RDBMS 那种“回滚一切”的安全感。你依然需要在代码层面保证逻辑的严密性。</p>\n<hr />\n<h2 id=\"四总结选型决策表\">四、总结：选型决策表</h2>\n<p>为了让你在实际业务中不再纠结，我整理了一份简单的决策表：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: left;\">需求场景</th>\n<th style=\"text-align: left;\">推荐方案</th>\n<th style=\"text-align: left;\">核心理由</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: left;\"><strong>简单批量读写 (KV)</strong></td>\n<td style=\"text-align: left;\"><strong>MSET / MGET</strong></td>\n<td style=\"text-align: left;\">原生命令，最快，最省心。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>大量离散命令 (无关联)</strong></td>\n<td style=\"text-align: left;\"><strong>Pipeline</strong></td>\n<td style=\"text-align: left;\">网络开销最低，吞吐量最高。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>需要 CAS (低并发)</strong></td>\n<td style=\"text-align: left;\"><strong>WATCH + MULTI</strong></td>\n<td style=\"text-align: left;\"><strong>事务唯一的用武之地。</strong> 适合低频竞争，实现简单。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>复杂逻辑 / 高并发</strong></td>\n<td style=\"text-align: left;\"><strong>Lua 脚本</strong></td>\n<td style=\"text-align: left;\"><strong>行业标准。</strong> 避免了 CAS 自旋的性能开销，原子性强。</td>\n</tr>\n<tr>\n<td style=\"text-align: left;\"><strong>即使报错也要回滚</strong></td>\n<td style=\"text-align: left;\"><strong>MySQL / RDBMS</strong></td>\n<td style=\"text-align: left;\"><strong>别为难 Redis。</strong> 它没有 Undo Log，做不到真正的回滚。</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"写在最后\">写在最后</h2>\n<p>回头看，<strong>Redis 事务这套机制，就像是一个“如果不仔细读说明书一定会用错”的半成品。</strong></p>\n<p>但正是这个“半成品”，折射出了 Redis 最底层的价值观：<strong>为了性能，可以牺牲一切“看起来很美”的抽象</strong>。它拒绝了沉重的 Undo Log，拒绝了复杂的隔离级别，只留下了一个最简单的“排队执行”逻辑。</p>\n<p>所以，当我们下次再写下 <code>MULTI</code> 的时候，心里要清楚：</p>\n<ul>\n<li>如果只是为了快，<strong>Pipeline</strong> 才是那个不讲武德的“加速器”。</li>\n<li>如果只是为了防插队，<strong>Transaction</strong> 够用了，但在高并发下，它脆弱得像个易碎品。</li>\n<li>如果要处理真正的复杂逻辑，请毫不犹豫地拥抱 <strong>Lua</strong> —— 虽然它也不会回滚，但至少在“执行原子性”上，它是我们手里最稳的那张牌。</li>\n</ul>\n<p>真正的技术成熟，不是背诵八股文里的 ACID 定义，而是懂得在<strong>由于物理限制而满是遗憾的真实世界里</strong>，做出那个最不坏的选择。</p>\n<hr />\n<blockquote>\n<p>文章的最后，想和你多聊两句。</p>\n<p>技术之路，常常是热闹与孤独并存。那些深夜的调试、灵光一闪的方案、还有踩坑爬起后的顿悟，如果能有人一起聊聊，该多好。</p>\n<p>为此，我建了一个小花园——我的微信公众号「<strong>[努力的小郑]</strong>」。</p>\n<p>这里没有高深莫测的理论堆砌，只有我对后端开发、系统设计和工程实践的持续思考与沉淀。它更像我的<strong>数字笔记本</strong>，记录着那些值得被记住的解决方案和思维火花。</p>\n<p>如果你觉得今天的文章还有一点启发，或者单纯想找一个同行者偶尔聊聊技术、谈谈思考，那么，欢迎你来坐坐。<br />\n<img alt=\"85f114bceb12e933bb817ec5fecdfef7\" class=\"lazyload\" /></p>\n<p>愿你前行路上，总有代码可写，有梦可追，也有灯火可亲。</p>\n</blockquote>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 09:49</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xzqcsj\">一旅人</a>&nbsp;\n阅读(<span id=\"post_view_count\">275</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "32岁程序员猝死：让我想起了我曾经的加班经历，庆幸自己还活着",
      "link": "https://www.cnblogs.com/StephenYoung/p/19539895",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/StephenYoung/p/19539895\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 20:41\">\n    <span>32岁程序员猝死：让我想起了我曾经的加班经历，庆幸自己还活着</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>最近，看到32岁程序员猝死的新闻刷爆全网。</p>\n<p>我瞬间想起了自己曾经的加班经历，心底只剩一句庆幸——还好，我还活着。</p>\n<p>曾经，我负责全球著名A客户的项目设备软件开发，那段时间常年辗转于国内各大代工厂，脚步从未停歇。最难忘的一次，是连续加班整整90天，这三个月里，我几乎连轴转、无一天休息，其中还有好几天熬了通宵。因为工厂是24小时的，设备是24小时的。新产品试产，任何一个环节的任何一个小问题，都是大问题。必须重视，必须解决。对于质量的把控，近乎变态，否则，这家A客户的产品也不可能独步全球了。</p>\n<p>印象最深的一次，头天早上7点准时起床，匆匆洗漱后就钻进工厂车间投入工作，直到第二天上午八九点，才拖着灌了铅似的身体走出厂区。本以为能短暂歇口气、眯上一会儿，可当天下午3点，一个紧急电话突然打来，我又不得不打起精神，风风火火地再次冲进车间，投入到设备调试中。</p>\n<p>车间里调试代码，必须站着操作电脑，我平均一天站立超过10小时，下班后双脚肿得连鞋子都难脱下。一段时间后，痔疮也犯了，彼时出差在陌生城市，我只能独自抽2个小时去附近医院拿药，即便顶着疼痛，第二天依旧要站着调试代码。当时我只有一个念头：忙完这阵，一定要换个环境，别真的没命挣钱、没命花。</p>\n<p>和我有同样感受的，还有我们在同一个项目组的合作厂商的一个H哥们儿，H哥和我同龄，只是人家正经985毕业的，而我不是。H哥也曾多次跟我抱怨：“真不想做A客户的项目了，太累了，吃不消。每天连饭都不能按时吃上，最近都饿得胃疼。跟客户开会是英文，我必须吃饱了才能听懂老外说话”。确实，可常常到下午七八点因为客户还在跟我们开会，或者check我们的项目进度，安排工作任务。经常都是没吃晚饭，那会儿脑子发懵，别说听英语了，我连汉语都听不懂了。我回答H哥说：哈哈，你这是电池没电了嘛，当然听不懂了。我有电也听不太懂多少。确实，现在我也是也饿得不行，这绝非长久之计呀。</p>\n<p>果然，项目的下一个阶段开始时，我没再见到他的身影，微信询问后才得知，他申请调到了其他项目，不用再饿肚子加班了，哈哈。偶尔的加班也能接受，语气里满是久违的朝气与平和。</p>\n<p>半年后，我也抓住一次偶然的机会离职，离开了原东家，彻底告别了A客户的项目，不用再忍受饿肚子、连轴转的加班日子。</p>\n<p>如今回想那段日子，依旧心有余悸。</p>\n<p>感叹：那些没日没夜的加班、身体承受的痛苦，都成了难忘的过往。工作是生活的一部分，挣钱是为了滋养生活，而非消耗生活。加班或许无法完全避免，挣钱也依然是我们前行的动力，但我们不能被这份执念裹挟，忽略了生活质量，透支了身体健康。</p>\n<p>真正的成熟，从来不是一味拼命加班、盲目挣钱，而是懂得平衡加班与生活，兼顾挣钱与健康。工作再重要，也不能替代三餐四季的安稳；薪资再诱人，也不及身体健康的珍贵。愿每一个为生活奔波、为挣钱努力的人，都能守住平衡，不辜负工作，也不辜负自己，不透支健康，也不浪费时光——毕竟，能好好工作、好好生活、好好活着，才是最难得的幸福。</p>\n<p>今年，2026年，我依然庆幸自己还活着。<br />\n<img alt=\"公众号动漫+微信官方码\" src=\"https://img2024.cnblogs.com/blog/2213992/202602/2213992-20260205204048587-607747837.png\" /></p>\n<p>更多精彩关注微信公众号：</p>\n<p>轻松做出漂亮的LabVIEW界面-FlateUI2.0(源码分享)</p>\n<p>15年技术人+3年创业失败者的感悟：工控圈里，技术的话语权为啥这么弱？</p>\n<p>LabVIEW记录XY轴运动轨迹并绘图的编程方法(源码附件)</p>\n<p>C#重难点2：事件</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 20:41</span>&nbsp;\n<a href=\"https://www.cnblogs.com/StephenYoung\">Stephen_Young</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}