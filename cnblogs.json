{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "基于 nano-vLLM 学习大模型推理关键功能",
      "link": "https://www.cnblogs.com/cswuyg/p/19471225",
      "published": "",
      "description": "<div class=\"postTitle\">\n\t\t<h1><a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/cswuyg/p/19471225\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 12:38\">\n    <span>基于 nano-vLLM 学习大模型推理关键功能</span>\n    \n\n</a>\n</h1>\n\t</div>\n\t<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<blockquote>注：本文已于2025.12.31 发表于知乎和公众号</blockquote>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>1. 背景</span></h1>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>如果要向一位完全不了解大模型推理技术的开发者介绍这个领域，我应该从哪里讲起？</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>大模型推理的最简流程可以概括为：输入一串文本 → 文本通过词典映射表转换成一串数字序号 → 序号再经过 embedding 层的计算，变成一组能代表语义的浮点数向量 → 这组向量送入推理系统，经过层层的矩阵乘法、加法和各类专用函数的运算，得到新的输出向量 → 对输出向量做概率筛选，选出概率最高的那个数值对应的序号 → 最后再通过词典映射表 “翻译” 回文字，得到最终输出的一个词。</span></div>\n</div>\n<div class=\"Image-captionContainer\">\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"204\" src=\"https://pic1.zhimg.com/80/v2-52738e1009b99b73843b74399250f825_1440w.png?source=ccfced1a\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"877\" />\n<div class=\"ImageEdit-button css-187js3w\" style=\"text-align: center;\">图 1&nbsp;</div>\n<div class=\"ImageEdit-button css-187js3w\"><span>这是对大模型推理最朴素的理解，上述流程看似简单，但背后的推理计算环节对普通开发者而言仍是一个 “黑盒”。如果想更进一步拆解推理引擎的底层加速原理，nano-vllm 会是一个极佳的入门切入点。</span></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>2. 简介</span></h1>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><a class=\"Link ztext-link\" href=\"https://github.com/GeeeekExplorer/nano-vllm\" rel=\"noopener nofollow\" target=\"_blank\">nano-vLLM</a><span> 代码量仅约 1200 行，却实现了生产级推理框架的核心技术原型，具体包括：</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>连续批处理（Continuous Batching）</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>KV 缓存（Prefix KV Cache / Paged KV Cache）</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>高性能编译与执行优化（Torch Compilation、Triton、CUDA Graph）</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>张量并行（Tensor Parallelism）</span></div>\n</li>\n</ul>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>该框架极具入门学习价值，本文将先介绍 nano-vLLM 的基本组成架构，再对部分核心技术要点展开深入解析。</span></div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>3. 系统架构</span></h1>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>nano-vLLM 的架构非常有层次感。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>3.1. 整体架构概览</span></h2>\n<div class=\"Image-captionContainer\">\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"403\" src=\"https://picx.zhimg.com/80/v2-41cd766bd06373d580df6302b2ba3203_1440w.png?source=ccfced1a\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"655\" />\n<div class=\"ImageEdit-button css-187js3w\" style=\"text-align: center;\">图 2, 来自：https://deepwiki.com/GeeeekExplorer/nano-vllm</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>三层结构</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>接口层：User Interface Layer</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>推理引擎中控层：Inference Engine Layer</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>显存管理和模型执行层：Memory Management &amp; Model Execution Layer</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>3.2. 类层面架构</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>从类设计层面观察 nano-vLLM 的架构。</span></div>\n</div>\n<div class=\"Image-captionContainer\">\n<div>\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div>\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"433\" src=\"https://pic1.zhimg.com/80/v2-8612f2dcf6e20d6dd1fccb4ebb494bd2_1440w.png?source=ccfced1a\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"409\" />\n<div class=\"ImageEdit-button css-187js3w\" style=\"text-align: center;\">&nbsp;</div>\n<div class=\"ImageDelete-button css-5sjb75\" style=\"text-align: center;\">&nbsp;图 3</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>上图中四种颜色代表系统的四个组成部分</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>浅蓝色，入口和推理引擎中控层</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>浅绿色，模型推理</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>浅红色，KV Cache 管理</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>浅紫色，权重加载和矩阵计算的封装</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>3.3. 码层面划分</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>源码规划上也较为简洁。目录结构如下：</span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:bash;gutter:true;\">nanovllm/\n├── engine\n├── layers\n├── models\n└── utils</pre>\n</div>\n</div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>engine，引擎的入口、中控，同时 KV Cache 比较简单，代码也放在这个目录下。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>layers，模型推理的通用组件，内部包括：linear、layernorm、rotary_embedding、attention、activation 等基础功能的封装，可以被不同模型使用。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>models，模型的实现，依赖 layers 的组件实现不同模型的推理。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>utils，不同层都可能会用到的工具函数。</span></div>\n</li>\n</ul>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>4. 连续批处理</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>4.1. 概念理解</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）定义</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>连续批处理 (Continuous Batching)：是一种迭代级（Iteration-level）的调度策略。它以“Token 生成步骤”为调度粒度。通过动态地在每一轮迭代中替换已完成的任务，消除了由于生成长度不一导致的 GPU 计算气泡，极大地提升了系统的吞吐量。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）朴素理解</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>一个请求需要执行多轮，不同请求需要执行的轮数不同，系统一轮最多只能同时执行一批 N 个请求，当一个批次里的请求参差不齐的完成时，每完成一个请求就将其用新请求替代掉。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>对比传统批处理和连续批处理：</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>传统批处理 (Static Batching)：必须等待 Batch 中生成序列最长的那个请求完成，整个 Batch 才会释放。在此期间，生成序列短请求完成后槽位会空转。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>连续批处理 (Continuous Batching)：请求完成即退出，新请求立即补位，槽位始终满载。</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>4.2. 最基础的连续批处理</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>最简单的连续批处理，不考虑 prefill 和 decode 的差异，示例代码：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> time\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> threading\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> queue\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> random\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 初始化线程安全的等待队列</span>\nwaiting_queue =<span style=\"color: rgba(0, 0, 0, 1);\"> queue.Queue()\nMAX_BATCH_SIZE </span>= 3\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 模拟用户请求线程 (生产者) ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> user_request_producer():\n    request_id </span>= 1\n    <span style=\"color: rgba(0, 0, 255, 1);\">while</span><span style=\"color: rgba(0, 0, 0, 1);\"> True:\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 模拟用户随机到达：每 1~2 秒来一个新请求</span>\n        time.sleep(random.uniform(1, 2<span style=\"color: rgba(0, 0, 0, 1);\">))\n        \n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 每个请求需要的 Token 长度随机（3到8之间）</span>\n        req = {<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">id</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">REQ-{request_id}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: random.randint(3, 8<span style=\"color: rgba(0, 0, 0, 1);\">)}\n        waiting_queue.put(req)\n        \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n[用户端] 送入新请求: {req['id']} (预计长度: {req['remain']})</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n        request_id </span>+= 1\n        <span style=\"color: rgba(0, 0, 255, 1);\">if</span> request_id &gt; 5<span style=\"color: rgba(0, 0, 0, 1);\">:\n          </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span>\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 核心推理循环 (消费者/执行器) ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> inference_loop():\n    running_batch </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []\n    \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">--- 推理引擎已启动 ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    iteration </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> 0\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">while</span><span style=\"color: rgba(0, 0, 0, 1);\"> True:\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> A. 补位逻辑：只要 Batch 没满且队列里有货，就拉进来</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">while</span> len(running_batch) &lt;<span style=\"color: rgba(0, 0, 0, 1);\"> MAX_BATCH_SIZE:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">try</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 使用 block=False，如果队列空了直接报错进 except，不阻塞推理逻辑</span>\n                new_req = waiting_queue.get(block=<span style=\"color: rgba(0, 0, 0, 1);\">False)\n                running_batch.append(new_req)\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">  &gt;&gt;&gt; [调度] {new_req['id']} 进入 Batch</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">except</span><span style=\"color: rgba(0, 0, 0, 1);\"> queue.Empty:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span>\n\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> B. 推理逻辑：如果当前 Batch 有任务，就执行一次 Step</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">if</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_batch:\n            iteration </span>+= 1\n            <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">=</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>*20 + f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{iteration=}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> + <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">=</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>*20<span style=\"color: rgba(0, 0, 0, 1);\">)\n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 模拟 GPU 推理耗时 (Step 耗时)</span>\n            time.sleep(1.2<span style=\"color: rgba(0, 0, 0, 1);\">) \n            \n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 当前 Batch 状态展示</span>\n            active_ids = [f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{r['id']}(剩{r['remain']-1})</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> <span style=\"color: rgba(0, 0, 255, 1);\">for</span> r <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_batch]\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[GPU推理] 处理中: {active_ids}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n            \n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 每一个请求的剩余长度减 1</span>\n            finished_this_step =<span style=\"color: rgba(0, 0, 0, 1);\"> []\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_batch:\n                req[</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>] -= 1\n                <span style=\"color: rgba(0, 0, 255, 1);\">if</span> req[<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>] &lt;=<span style=\"color: rgba(0, 0, 0, 1);\"> 0:\n                    finished_this_step.append(req)\n            \n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> C. 剔除逻辑：做完的立刻踢出，下一轮循环开头就会有新请求补进来</span>\n            <span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> finished_this_step:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">  &lt;&lt;&lt; [完成] {req['id']} 生成完毕，释放位置</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n                running_batch.remove(req)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 如果 Batch 和 队列都空了，稍微歇会，避免 CPU 空转</span>\n            time.sleep(0.5<span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 启动程序 ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(128, 0, 128, 1);\">__name__</span> == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">__main__</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 启动用户请求线程</span>\n    t = threading.Thread(target=user_request_producer, daemon=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    t.start()\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 主线程执行推理循环</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">try</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        inference_loop()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">except</span><span style=\"color: rgba(0, 0, 0, 1);\"> KeyboardInterrupt:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n服务已停止</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<p><span>核心逻辑：</span></p>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>存储结构：代码的核心有两个队列，waiting_queue 负责存储请求线程不断接收到的新请求，running_queue 负责存储已经运行但还没有结束的请求。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>迭代循环：生产者持续往 waiting_queue 写入新请求，迭代循环持续从 waiting_queue 获取新请求加入到 running_queue，同时清理 running_queue 里已经完成的请求。</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>4.3. prefill 优先的连续批处理</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>prefill 优先的批处理，需要区分 prefll 和 decode，优先处理新请求，示例代码：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> time\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> queue\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> random\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> threading\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 核心队列</span>\nwaiting_queue =<span style=\"color: rgba(0, 0, 0, 1);\"> queue.Queue()  \nrunning_queue </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []             \n\nMAX_BATCH_SIZE </span>= 4\n\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> user_request_producer():\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\n    修改点：模拟爆发式请求到达，以触发多请求 Prefill\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span>\n    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 第一波：爆发式到达 (3个请求同时进入队列)</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n[用户] --- 爆发式请求到达 (3个请求) ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> i <span style=\"color: rgba(0, 0, 255, 1);\">in</span> range(1, 4<span style=\"color: rgba(0, 0, 0, 1);\">):\n        req </span>= {<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">id</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">REQ-{i}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: random.randint(2, 5<span style=\"color: rgba(0, 0, 0, 1);\">)}\n        waiting_queue.put(req)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[用户] 请求 {req['id']} 进入等待队列</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 延迟一会儿，再来第二波单点请求</span>\n    time.sleep(5<span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n[用户] --- 延迟请求到达 (1个请求) ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    req </span>= {<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">id</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">REQ-4</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: 3<span style=\"color: rgba(0, 0, 0, 1);\">}\n    waiting_queue.put(req)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[用户] 请求 {req['id']} 进入等待队列</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> inference_loop():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">--- 连续批处理引擎：多请求 Prefill 模式 ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    iteration </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> 0\n    \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">while</span><span style=\"color: rgba(0, 0, 0, 1);\"> True:\n        current_batch </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []\n        is_prefill_stage </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> False\n        \n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 调度：构建当前批次</span>\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 只要 waiting_queue 非空，就尽可能填满 MAX_BATCH_SIZE</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(0, 0, 255, 1);\">not</span><span style=\"color: rgba(0, 0, 0, 1);\"> waiting_queue.empty():\n            is_prefill_stage </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> True\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">while</span> <span style=\"color: rgba(0, 0, 255, 1);\">not</span> waiting_queue.empty() <span style=\"color: rgba(0, 0, 255, 1);\">and</span> len(current_batch) &lt;<span style=\"color: rgba(0, 0, 0, 1);\"> MAX_BATCH_SIZE:\n                req </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> waiting_queue.get()\n                current_batch.append(req)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">elif</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_queue:\n            is_prefill_stage </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> False\n            current_batch </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> list(running_queue)\n        \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(0, 0, 255, 1);\">not</span><span style=\"color: rgba(0, 0, 0, 1);\"> current_batch:\n            time.sleep(</span>0.5<span style=\"color: rgba(0, 0, 0, 1);\">)\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">continue</span>\n\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 执行：模拟推理</span>\n        iteration += 1\n        <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n{'='*15} Iteration {iteration} {'='*15}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n        \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span><span style=\"color: rgba(0, 0, 0, 1);\"> is_prefill_stage:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[PREFILL] 批量生成中: {[r['id'] for r in current_batch]}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n            time.sleep(</span>1.5<span style=\"color: rgba(0, 0, 0, 1);\">) \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[DECODE ] 批量生成中: {[f'{r['id']}(剩{r['remain']})' for r in current_batch]}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n            time.sleep(</span>0.4<span style=\"color: rgba(0, 0, 0, 1);\">) \n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 统一状态更新</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> current_batch:\n            req[</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span>] -= 1\n\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 统一判断生命周期</span>\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 注意：为了避免在遍历列表时删除元素，我们先收集要删除的对象</span>\n        to_remove_from_running =<span style=\"color: rgba(0, 0, 0, 1);\"> []\n        \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> current_batch:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> req[<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span>] &lt;=<span style=\"color: rgba(0, 0, 0, 1);\"> 0:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">  &lt;&lt;&lt; [完成] {req['id']} 退出系统</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_queue:\n                    to_remove_from_running.append(req)\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span><span style=\"color: rgba(0, 0, 0, 1);\"> is_prefill_stage:\n                    running_queue.append(req)\n                    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">  -&gt; {req['id']} Prefill 完成，转入 running_queue</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                    </span><span style=\"color: rgba(0, 0, 255, 1);\">pass</span>\n        \n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 真正的从 running_queue 移除</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> to_remove_from_running:\n            running_queue.remove(req)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(128, 0, 128, 1);\">__name__</span> == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">__main__</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    t </span>= threading.Thread(target=user_request_producer, daemon=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    t.start()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">try</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        inference_loop()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">except</span><span style=\"color: rgba(0, 0, 0, 1);\"> KeyboardInterrupt:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">pass</span></pre>\n</div>\n<p><span>叠加上 prefill 优先之后的连续批处理代码也较为简单，主要是维护三个变量：waiting_queue、running_queue、current_batch。</span></p>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5. KV Cache</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.1. 概念理解</span></h2>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.1.1. KV Cache 的用途</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>KV Cache 有两层用途。一是用在同一个请求的 Decode 阶段，复用之前已经计算过的 KV 结果以避免重复计算；二是用在不同请求之间，使具有相同前缀的请求可以共享一部分 KV 数据，这就是 Prefix KV Cache。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.1.2. PagedAttention 技术</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在 Cache 的存储层面，PagedAttention 实现了显存的按需申请。由于 KV Cache 空间不再一次性预分配，请求序列对应的物理地址是离散的。PagedAttention 的核心在于，它能够直接读取这些物理离散的块来完成注意力计算，这背后实现了一层从“逻辑连续地址”到“物理离散地址”的映射。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>对于没有接触过非 PagedAttention 实现的读者来说，这种设计似乎理所当然：按需申请、分页管理、地址映射、局部性原理——这些都是计算机科学中非常常规的思维，甚至很难想到不这么写的理由。那么，为什么 PagedAttention 会被认为是一项里程碑式的先进技术呢？</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>首先，在 PagedAttention 出现之前，业界普遍认为 KV Cache 在显存中必须物理连续，否则会因访存不连续导致性能大幅下降。其次，当时的注意力算子（如标准的 FlashAttention）并不支持二次寻址映射。PagedAttention 证明了即便物理存储不连续，性能依然可以保持极高。其代码实现最关键的点在于重构了 CUDA 内核，使其原生支持 KV Cache 二次寻址。一个序列的 KV Cache 不需要物理连续，也正是不同序列间能够灵活复用 Prefix KV Cache 的技术前提。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>总的来说，虽然分页虚拟内存在 CPU 领域是常识，但在 GPU 算子领域其发展相对缓慢。实现一套既能分页管理、又不损失算力利用率的 Attention Kernel 是 PagedAttention 的核心所在。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.2. Prefix KV Cache 的实现</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Cache 的管理较为简单，只有 BlockManager 类，负责维护显存池各个 block 的的状态。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.2.1. 功能细节</span></h3>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>使用 hash 来识别是否有可复用前缀，以 block 为基本单元</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>链式 hash，每个 block 的 hash 计算输入为前序 block 的 hash 值加上本 block 的 token id</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>每一个 block 有对应的 meta 信息对象，记录 block 被复用的引用计数，确保复用时不会被释放</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>为避免 hash 碰撞出现错误，block meta 信息还需要记录原始的 token id</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在获取 KV Cache 空间时需要考虑是否跨 block</span></div>\n</li>\n</ul>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.2.2. 内存池</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在进程启动时，一次性申请内存池的空间：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre>kv_cache =<span style=\"color: rgba(0, 0, 0, 1);\"> torch.empty(\n    </span>2,                          <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> K 和 V</span>\n    num_layers,                 <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 层数</span>\n    num_blocks,                 <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 总块数</span>\n    block_size,                 <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 每块 token 数</span>\n    num_kv_heads // tp_size,    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> KV head 数（考虑张量并行）</span>\n    head_dim                    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> head 维度</span>\n)</pre>\n</div>\n<p><span>上述申请显存代码中的 num_blocks 是根据可用于 KV Cache 的显存算出来的：</span></p>\n<div class=\"cnblogs_code\">\n<pre>block_bytes = 2 * hf_config.num_hidden_layers * self.block_size * num_kv_heads * head_dim *<span style=\"color: rgba(0, 0, 0, 1);\"> hf_config.torch_dtype.itemsize\nconfig.num_kvcache_blocks </span>= int(total * config.gpu_memory_utilization - used - peak + current) // block_bytes</pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>上述代码中的 block_bytes 并不是指 block 的大小，而是把计算 blocks 数的所有除数乘到了一起，除数包括：block 大小、k 和 v、模型层数。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><span>total * config.gpu_memory_utilization - used - peak + current<span> 这部分则是根据最高的显存利用率算出来可用显存，减去当前模型加载完后使用了的部分，再减去模型预热时使用的激活显存：<span><span>peak - current<span>。</span></span></span></span></span></span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>申请到内存池后，按层共享视图给各个层的 Attention 对象，代码看起来比较 tricky，但在 python 里倒比较常见：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">for</span> module <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> self.model.modules():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> hasattr(module, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">k_cache</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>) <span style=\"color: rgba(0, 0, 255, 1);\">and</span> hasattr(module, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">v_cache</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n        module.k_cache </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> self.kv_cache[0, layer_id]\n        module.v_cache </span>= self.kv_cache[1<span style=\"color: rgba(0, 0, 0, 1);\">, layer_id]\n        layer_id </span>+= 1</pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>遍历模型中的所有 nn.Module 子模块，通过检查是否存在 k_cache 和 v_cache 属性来识别 Attention 层。对于每个 Attention 层，将其 k_cache 和 v_cache 属性替换为指向全局 KV Cache 显存池的张量视图，这样所有层共享同一块连续的显存空间，但每层只能访问自己对应的切片。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.2.3. KV Cache 写入</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在 attention 子层的 forward 前做 KV Cache 的写入，使用的 store_kvcache_kernel 函数是 triton.jit 实现的，代码也比较简洁：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">@triton.jit\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> store_kvcache_kernel(\n    key_ptr,\n    key_stride,\n    value_ptr,\n    value_stride,\n    k_cache_ptr,\n    v_cache_ptr,\n    slot_mapping_ptr,\n    D: tl.constexpr,\n):\n    idx </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> tl.program_id(0)\n    slot </span>= tl.load(slot_mapping_ptr +<span style=\"color: rgba(0, 0, 0, 1);\"> idx)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> slot == -1: <span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\">\n    key_offsets </span>= idx * key_stride +<span style=\"color: rgba(0, 0, 0, 1);\"> tl.arange(0, D)\n    value_offsets </span>= idx * value_stride +<span style=\"color: rgba(0, 0, 0, 1);\"> tl.arange(0, D)\n    key </span>= tl.load(key_ptr +<span style=\"color: rgba(0, 0, 0, 1);\"> key_offsets)\n    value </span>= tl.load(value_ptr +<span style=\"color: rgba(0, 0, 0, 1);\"> value_offsets)\n    cache_offsets </span>= slot * D +<span style=\"color: rgba(0, 0, 0, 1);\"> tl.arange(0, D)\n    tl.store(k_cache_ptr </span>+<span style=\"color: rgba(0, 0, 0, 1);\"> cache_offsets, key)\n    tl.store(v_cache_ptr </span>+<span style=\"color: rgba(0, 0, 0, 1);\"> cache_offsets, value)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> store_kvcache(key: torch.Tensor, value: torch.Tensor, k_cache: torch.Tensor, v_cache: torch.Tensor, slot_mapping: torch.Tensor):\n    N, num_heads, head_dim </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> key.shape\n    D </span>= num_heads *<span style=\"color: rgba(0, 0, 0, 1);\"> head_dim\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">assert</span> key.stride(-1) == 1 <span style=\"color: rgba(0, 0, 255, 1);\">and</span> value.stride(-1) == 1\n    <span style=\"color: rgba(0, 0, 255, 1);\">assert</span> key.stride(1) == head_dim <span style=\"color: rgba(0, 0, 255, 1);\">and</span> value.stride(1) ==<span style=\"color: rgba(0, 0, 0, 1);\"> head_dim\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">assert</span> k_cache.stride(1) == D <span style=\"color: rgba(0, 0, 255, 1);\">and</span> v_cache.stride(1) ==<span style=\"color: rgba(0, 0, 0, 1);\"> D\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">assert</span> slot_mapping.numel() ==<span style=\"color: rgba(0, 0, 0, 1);\"> N\n    store_kvcache_kernel[(N,)](key, key.stride(0), value, value.stride(0), k_cache, v_cache, slot_mapping, D)</span></pre>\n</div>\n<p><span>使用 stride 函数来确认显存是否连续，因为在 store_kvcache_kernel 的实现里会按照显存连续来读取指定位置的值。</span></p>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>显存连续和不连续的例子：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n\nkey </span>= torch.randn(2, 3, 4<span style=\"color: rgba(0, 0, 0, 1);\">)\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span><span style=\"color: rgba(0, 0, 0, 1);\">(key.stride())  \nkey_t </span>= key.transpose(1, 2<span style=\"color: rgba(0, 0, 0, 1);\">)\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">转置后（非连续）:</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">, key_t.stride())  \nkey_t </span>= key_t.contiguous()  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 重新分配，使其连续</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">contiguous 后:</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">, key_t.stride())\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 输出：</span><span style=\"color: rgba(0, 128, 0, 1);\">\n#</span><span style=\"color: rgba(0, 128, 0, 1);\"> (12, 4, 1)</span><span style=\"color: rgba(0, 128, 0, 1);\">\n#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 转置后（非连续）: (12, 1, 4)</span><span style=\"color: rgba(0, 128, 0, 1);\">\n#</span><span style=\"color: rgba(0, 128, 0, 1);\"> contiguous后: (12, 3, 1)</span></pre>\n</div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6. cuda graph</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6.1. 概念理解</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>CUDA Graph 是一种将一系列 CUDA 操作录制成图的技术，在重复执行的固定操作序列场景下可以显著提升推理性能，主要基于这几方面：</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>减少 CPU 与 GPU 之间的频繁同步和指令下发开销，降低传统独立操作带来的控制流交互损耗；</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>减少执行过程中的 CPU 干预，GPU 自主批量执行图内操作，最大化 GPU 利用率，降低延迟、提升吞吐。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>规避多次独立 CUDA Kernel 的启动固定开销，多个 Kernel 打包后仅需一次调度触发，大幅提升小 Kernel 密集场景的执行效率；</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>可配合显存池实现显存资源复用，减少 “少量多次” 显存申请 / 释放的开销，同时驱动会基于图内显存访问模式优化带宽利用率；</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>CUDA 驱动可获取操作序列的全局视图，基于完整的依赖关系进行全局优化（如 Kernel 顺序调整、资源合并等）；</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6.2. 功能细节</span></h2>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>录制时使用的张量内存地址，在重放时必须保持不变，也就是后面多次 replay 都会使用捕获时申请的变量空间</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>捕获后的 graph 对象记录在成员变量里，供下次推理时选择</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>重放时选择比请求 batch size 大的最小 graph batch size</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>捕获时不同的 batch size 共享相同的静态显存空间，并让多个批次共享显存池，使得虽然有多个 batch size，但只会使用 Max Batch Size 的显存空间</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6.3. 示例代码</span></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch.nn as nn\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 基础配置</span>\ndevice = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\nD </span>= 512                 <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 维度</span>\ngraph_bs = [1, 8, 32]   <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 预定义的桶（分桶尺寸）</span>\nNUM_LAYERS = 100        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 极深模型，增加 Kernel 数量以放大 Graph 优势</span>\niters = 50              <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 性能测试迭代次数</span>\nmax_bs =<span style=\"color: rgba(0, 0, 0, 1);\"> max(graph_bs)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 定义深层模型 (产生约 400 个 Kernel)</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> UltraDeepModel(nn.Module):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> <span style=\"color: rgba(128, 0, 128, 1);\">__init__</span><span style=\"color: rgba(0, 0, 0, 1);\">(self):\n        super().</span><span style=\"color: rgba(128, 0, 128, 1);\">__init__</span><span style=\"color: rgba(0, 0, 0, 1);\">()\n        self.blocks </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> nn.ModuleList()\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(NUM_LAYERS):\n            block </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> nn.ModuleDict({\n                </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">ln</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">: nn.LayerNorm(D).to(device),\n                </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">linear</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">: nn.Linear(D, D).to(device),\n                </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">act</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">: nn.ReLU()\n            })\n            self.blocks.append(block)\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> forward(self, x):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> block <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> self.blocks:\n            identity </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> x\n            x </span>= block[<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">ln</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">](x)\n            x </span>= block[<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">linear</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">](x)\n            x </span>= block[<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">act</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">](x)\n            x </span>= x +<span style=\"color: rgba(0, 0, 0, 1);\"> identity \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> x\n\nmodel </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> UltraDeepModel().eval()\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 静态缓冲区准备</span>\nstatic_input = torch.empty(max_bs, D, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\nstatic_output </span>= torch.empty(max_bs, D, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\n\ngraphs </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> {}\ngraph_pool </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> None\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 录制阶段 (从大到小，共享内存池)</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">--- 开始录制分桶 CUDA Graphs ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n</span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> bs <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> reversed(sorted(graph_bs)):\n    current_input </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> static_input[:bs]\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> Warmup</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span> range(5<span style=\"color: rgba(0, 0, 0, 1);\">):\n        _ </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> model(current_input)\n    \n    g </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g, pool</span>=<span style=\"color: rgba(0, 0, 0, 1);\">graph_pool):\n        static_output[:bs] </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> model(current_input)\n    \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> graph_pool <span style=\"color: rgba(0, 0, 255, 1);\">is</span><span style=\"color: rgba(0, 0, 0, 1);\"> None:\n        graph_pool </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> g.pool()\n    graphs[bs] </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> g\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">✅ 已录制桶 BS={bs}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 5. 辅助函数：根据实际 BS 匹配最近的桶</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> get_bucket_bs(actual_bs):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> b <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> sorted(graph_bs):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> actual_bs &lt;=<span style=\"color: rgba(0, 0, 0, 1);\"> b:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> b\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> None\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 6. 性能对比测试 (包含 Padding 逻辑)</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span> benchmark(actual_test_bs=7<span style=\"color: rgba(0, 0, 0, 1);\">):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n--- 性能测试开始: 实际请求 BS={actual_test_bs} ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 生成测试数据</span>\n    test_data = torch.randn(actual_test_bs, D, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\n    \n    start_event </span>= torch.cuda.Event(enable_timing=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    end_event </span>= torch.cuda.Event(enable_timing=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 段落 A: Standard Eager Mode (直接跑 7 个) ---</span>\n    torch.cuda.nvtx.range_push(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Eager_Mode</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    start_event.record()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(iters):\n        _ </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> model(test_data)\n    end_event.record()\n    torch.cuda.synchronize()\n    eager_time </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> start_event.elapsed_time(end_event)\n    torch.cuda.nvtx.range_pop()\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 段落 B: CUDA Graph Mode (Padding 对齐到 8) ---</span>\n    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 路由逻辑</span>\n    bucket_bs =<span style=\"color: rgba(0, 0, 0, 1);\"> get_bucket_bs(actual_test_bs)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> bucket_bs <span style=\"color: rgba(0, 0, 255, 1);\">is</span><span style=\"color: rgba(0, 0, 0, 1);\"> None:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">❌ 错误: 实际 BS={actual_test_bs} 超过了最大分桶 {max_bs}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\">\n\n    torch.cuda.nvtx.range_push(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Graph_Mode_Bucket_{bucket_bs}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 数据对齐 (Padding): 将 7 条数据拷入 8 的静态区域</span>\n    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> static_input 的前 7 行被覆盖，第 8 行保持不变（即 Padding 位）</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">    static_input[:actual_test_bs].copy_(test_data)\n    \n    start_event.record()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(iters):\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 重放分桶 8 的图</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">        graphs[bucket_bs].replay()\n    end_event.record()\n    torch.cuda.synchronize()\n    graph_time </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> start_event.elapsed_time(end_event)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 结果截断 (Slicing): 从静态区拿回前 7 条</span>\n    final_res =<span style=\"color: rgba(0, 0, 0, 1);\"> static_output[:actual_test_bs]\n    torch.cuda.nvtx.range_pop()\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 打印结果</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">匹配到的桶: {bucket_bs} (Padding 浪费率: {(bucket_bs-actual_test_bs)/bucket_bs*100:.1f}%)</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{'Mode':&lt;20} | {'Avg Time (ms)':&lt;15}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> * 40<span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{'Eager Mode':&lt;20} | {eager_time/iters:&gt;15.4f}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{'Graph Mode':&lt;20} | {graph_time/iters:&gt;15.4f}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> * 40<span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">🚀 加速比: {eager_time/graph_time:.2f}x</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">最终输出形状: {final_res.shape}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(128, 0, 128, 1);\">__name__</span> == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">__main__</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 测试不同的输入 BS</span>\n    benchmark(actual_test_bs=7)  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 触发对齐到 8</span>\n    benchmark(actual_test_bs=1)  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 精确匹配到 1</span></pre>\n</div>\n<p><span>执行：nsys profile --trace=cuda,osrt,nvtx python3 cu2.py</span></p>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>输出：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">Collecting data...\n</span>--- 开始录制分桶 CUDA Graphs ---<span style=\"color: rgba(0, 0, 0, 1);\">\n✅ 已录制桶 BS</span>=32<span style=\"color: rgba(0, 0, 0, 1);\">\n✅ 已录制桶 BS</span>=8<span style=\"color: rgba(0, 0, 0, 1);\">\n✅ 已录制桶 BS</span>=1\n\n--- 性能测试开始: 实际请求 BS=7 ---<span style=\"color: rgba(0, 0, 0, 1);\">\n匹配到的桶: </span>8 (Padding 浪费率: 12.5%<span style=\"color: rgba(0, 0, 0, 1);\">)\nMode                 </span>|<span style=\"color: rgba(0, 0, 0, 1);\"> Avg Time (ms)  \n</span>----------------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\nEager Mode           </span>|          7.9331<span style=\"color: rgba(0, 0, 0, 1);\">\nGraph Mode           </span>|          1.0136\n----------------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\n🚀 加速比: </span>7<span style=\"color: rgba(0, 0, 0, 1);\">.83x\n最终输出形状: torch.Size([</span>7, 512<span style=\"color: rgba(0, 0, 0, 1);\">])\n\n</span>--- 性能测试开始: 实际请求 BS=1 ---<span style=\"color: rgba(0, 0, 0, 1);\">\n匹配到的桶: </span>1 (Padding 浪费率: 0.0%<span style=\"color: rgba(0, 0, 0, 1);\">)\nMode                 </span>|<span style=\"color: rgba(0, 0, 0, 1);\"> Avg Time (ms)  \n</span>----------------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\nEager Mode           </span>|          8.1803<span style=\"color: rgba(0, 0, 0, 1);\">\nGraph Mode           </span>|          0.8011\n----------------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\n🚀 加速比: </span>10<span style=\"color: rgba(0, 0, 0, 1);\">.21x\n最终输出形状: torch.Size([</span>1, 512])</pre>\n</div>\n<p><span>查看 nsys：</span></p>\n</div>\n<div class=\"Image-captionContainer\">\n<div>\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div>\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\" style=\"text-align: center;\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"394\" src=\"https://picx.zhimg.com/80/v2-fa4eefbebda0472fa1f21d8bd01e4c17_1440w.png?source=ccfced1a\" width=\"753\" /></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div style=\"text-align: center;\">图 4</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>可以看到，在 cuda graph 的时候，SM 使用更充分。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6.4. Q&amp;A</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）为什么推理时 cuda graph 的选择要采用向上对齐的分桶策略，即：批次相等或稍大的，而不是选择批次最大的？</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>虽然在建图（Capture）阶段，系统会按照最大批次（Max Batch Size）预先申请并锁定静态显存空间，此时即便选择最大批次执行也不会产生额外的显存容量浪费，但会引入以下两个维度的性能损耗：</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>显存带宽的无效占用，大模型推理（尤其是 Decoding 阶段）属于典型的访存密集型任务，其瓶颈在于模型权重从显存到计算单元的搬运速度。即便大部分批次位置是 Padding（空数据），CUDA Graph 依然会严格执行录制时的内存寻址定义，搬运完整批次的数据。使用过大的批次会导致 GPU 浪费极其宝贵的带宽去搬运“无效数据”，从而增加单次推理的耗时，推高推理延迟（Latency）。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>计算资源的无效占用，GPU 调度器会根据图定义的规模预分配硬件资源（如 SM 核心、寄存器、共享显存等）。虽然 Padding 部分的计算逻辑极快，但这些资源在整个 CUDA Graph 执行完成前无法被释放。这会导致 GPU 硬件处于“虚假繁忙”状态，阻塞了其他潜在任务（如多流并行等）获取硬件资源，削弱了系统整体的并发吞吐能力（Throughput）。</span></div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7. Torch Compilation</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7.1. 概念理解</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>torch.compile 能够将 PyTorch 张量计算相关的 Python 逻辑，转化为更高效的中间表示（在 CUDA 设备上，通常是 Triton 内核代码，也支持原生 CUDA 内核）。相较于传统的即时执行（Eager Mode），这种方式通过优化计算内核本身带来显著的运行效率提升；此外，当输入张量形状、数据类型固定时，torch.compile 还会自动启用 CUDA Graph 优化，进一步放大性能收益。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在 torch.compile 问世之前，PyTorch 开发者若想追求高性能，仅有两种核心选择：一是使用 Eager Mode 接受其原生性能上限，二是手动编写 Triton 或 CUDA 底层内核代码（该方式开发门槛高、周期长、维护成本高）。而有了 torch.compile 后，开发者只需编写简洁易懂的 PyTorch Python 业务逻辑，无需关注底层硬件适配与内核实现，即可获得接近手写 Triton/CUDA 的优异性能，大幅平衡了开发效率与运行性能。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7.2. 使用方法</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>应用 torch.compile 非常简单，核心有两类使用方式：</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>装饰器方式：在 PyTorch 函数上直接添加 @torch.compile 装饰器，定义时即完成编译声明；</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>显式调用方式：通过 compiled_obj = torch.compile(target) 显式编译目标对象，后续调用 compiled_obj 即可使用优化后的逻辑；</span></div>\n</li>\n</ul>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>另外，可以对模型实例的直接编译：对于 PyTorch 模型（nn.Module 子类实例），可直接传入 torch.compile 完成整体编译，无需单独修饰 forward 方法。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>示例代码：</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>方式 1：装饰器方式（适用于函数 / 模型方法）</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch.nn as nn\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 对普通PyTorch函数使用装饰器</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">@torch.compile\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> my_tensor_func(x, y):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> torch.matmul(x, y) +<span style=\"color: rgba(0, 0, 0, 1);\"> torch.relu(y)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 对模型的forward方法使用装饰器</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> MyModel(nn.Module):\n    @torch.compile  </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 修饰forward方法，自动编译模型推理逻辑</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> forward(self, x):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> nn.Linear(10, 20)(x)</pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>方式 2：显式调用方式（适用于函数 / 模型，灵活性更高）</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch.nn as nn\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 显式编译普通函数</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> my_tensor_func(x, y):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> torch.matmul(x, y) +<span style=\"color: rgba(0, 0, 0, 1);\"> torch.relu(y)\ncompiled_func </span>= torch.compile(my_tensor_func)  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 生成编译后的函数</span>\noutput = compiled_func(torch.randn(32, 10), torch.randn(10, 20))  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 调用编译后的函数</span>\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 显式编译模型（与方式3本质一致，更强调“先编译后使用”的显式流程）</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> MyModel(nn.Module):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> forward(self, x):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> nn.Linear(10, 20<span style=\"color: rgba(0, 0, 0, 1);\">)(x)\nmodel </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> MyModel()\ncompiled_model </span>= torch.compile(model)  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 直接编译整个模型实例</span>\noutput = compiled_model(torch.randn(32, 10))  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 调用编译后的模型</span></pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>方式 3：直接编译模型实例（深度学习中最常用，简化写法）</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch.nn as nn\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> MyModel(nn.Module):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> forward(self, x):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> nn.Linear(10, 20<span style=\"color: rgba(0, 0, 0, 1);\">)(x)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 直接编译模型实例，一步到位（无需装饰器，最简洁常用）</span>\nmodel =<span style=\"color: rgba(0, 0, 0, 1);\"> torch.compile(MyModel())\noutput </span>= model(torch.randn(32, 10))</pre>\n</div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7.3. 性能对比</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>下面以一个简单的例子对比 Eager Mode 和 Compiled Mode：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> time\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 确保使用的是 GPU</span>\ndevice = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> <span style=\"color: rgba(0, 0, 255, 1);\">if</span> torch.cuda.is_available() <span style=\"color: rgba(0, 0, 255, 1);\">else</span> <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cpu</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">if</span> device == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cpu</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">警告：CUDA 不可用，将使用 CPU 运行（torch.compile 的优势在 GPU 上最明显）。</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> complex_operation_eager(x, y):\n    z </span>= x *<span style=\"color: rgba(0, 0, 0, 1);\"> y\n    z </span>= z +<span style=\"color: rgba(0, 0, 0, 1);\"> x\n    z </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> torch.relu(z)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> z.sum()\n\n@torch.compile\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> complex_operation_graph(x, y):\n    z </span>= x *<span style=\"color: rgba(0, 0, 0, 1);\"> y\n    z </span>= z +<span style=\"color: rgba(0, 0, 0, 1);\"> x\n    z </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> torch.relu(z)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> z.sum()\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 准备数据</span>\nx = torch.randn(10000, 10000, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\ny </span>= torch.randn(10000, 10000, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 热身 (Warm up)</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">正在编译并进行多次热身以稳定 GPU 状态...</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 增加热身循环</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">for</span> i <span style=\"color: rgba(0, 0, 255, 1);\">in</span> range(3<span style=\"color: rgba(0, 0, 0, 1);\">): \n    complex_operation_graph(x, y)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> i ==<span style=\"color: rgba(0, 0, 0, 1);\"> 0:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-&gt; 首次编译完成，正在进行后续预热...</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\ntorch.cuda.synchronize()\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">预热完毕，开始正式测试。</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> benchmark(func, x, y, label, iterations=100<span style=\"color: rgba(0, 0, 0, 1);\">):\n    start_event </span>= torch.cuda.Event(enable_timing=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    end_event </span>= torch.cuda.Event(enable_timing=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    \n    start_event.record()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(iterations):\n        func(x, y)\n    end_event.record()\n    \n    torch.cuda.synchronize()\n    \n    elapsed_time_ms </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> start_event.elapsed_time(end_event)\n    avg_time_s </span>= (elapsed_time_ms / 1000) /<span style=\"color: rgba(0, 0, 0, 1);\"> iterations\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{label} 平均耗时: {avg_time_s:.6f} 秒</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> avg_time_s\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 执行测试并计算加速比</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">with torch.no_grad():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> * 30<span style=\"color: rgba(0, 0, 0, 1);\">)\n    eager_time </span>= benchmark(complex_operation_eager, x, y, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Eager Mode   </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    compile_time </span>= benchmark(complex_operation_graph, x, y, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Compiled Mode</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> * 30<span style=\"color: rgba(0, 0, 0, 1);\">)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 计算加速比逻辑</span>\n    speedup = eager_time /<span style=\"color: rgba(0, 0, 0, 1);\"> compile_time\n    improvement </span>= (speedup - 1) * 100\n    \n    <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">性能提升结果:</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">加速比 (Speedup): {speedup:.2f}x</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">运行速度提升了: {improvement:.1f}%</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<p><span>运行输出：</span></p>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">正在编译并进行多次热身以稳定 GPU 状态...\n</span>-&gt;<span style=\"color: rgba(0, 0, 0, 1);\"> 首次编译完成，正在进行后续预热...\n预热完毕，开始正式测试。\n</span>------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\nEager Mode    平均耗时: </span>0.005690<span style=\"color: rgba(0, 0, 0, 1);\"> 秒\nCompiled Mode 平均耗时: </span>0.001528<span style=\"color: rgba(0, 0, 0, 1);\"> 秒\n</span>------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\n性能提升结果:\n加速比 (Speedup): </span>3<span style=\"color: rgba(0, 0, 0, 1);\">.72x\n运行速度提升了: </span>272.3%</pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>性能有数倍的提升。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7.4. Q&amp;A</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）既然 torch.compile 有这么大的好处，为什么不能给所有的张量操作函数都加上 @torch.compile？</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>有几方面的原因：首先，存在编译开销，会导致首次运行显著变慢；然后，编译器生成的 Triton 内核（或其他后端代码）通常是针对特定张量形状、数据类型和设备配置优化的，如果输入张量的这些属性频繁变化，会反复触发重新编译（即 “编译缓存失效”），反而抵消性能收益；第三，torch.compile 自身会带来额外的显存开销（用于存储编译后的中间表示、内核缓存等），过多无差别使用可能导致显存不足（OOM）；最后，并非所有代码都能被成功图化优化，如果张量操作中调用了非 PyTorch 原生的第三方库（或纯 Python 原生逻辑），会导致计算图中断，此时编译器无法继续优化后续逻辑，还需要将控制权交回给 Python 解释器，产生不必要的上下文切换开销，可能导致负优化。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）torch.compile 支持生成原生 CUDA 内核代码，但通常来说，编译器自动生成的通用原生 CUDA 代码优化粒度不够精细；而 Triton 内置了极强的 Autotuning（自动调优）能力，针对深度学习张量计算场景做了深度适配，因此在绝大多数深度学习任务中，Triton 内核的性能通常优于自动生成的原生 CUDA 内核。</span></div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>8. Torch Compilation、Trition、CUDA Graph 三者的区别和联系</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>8.1. 核心区别</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）Torch Compilation：PyTorch 高层一站式性能优化入口（用户态抽象接口）</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>作为面向开发者的顶级优化封装，torch.compile 无需开发者关注底层硬件细节与优化实现，其核心定位是对 PyTorch 张量计算逻辑（函数 /nn.Module 模型）进行端到端自动优化，屏蔽了底层内核生成与执行优化的复杂性，是绝大多数 PyTorch 开发者的首选性能优化工具。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）Triton：高性能 GPU 内核专用 DSL </span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Triton 既是开发者手动编写高性能内核的领域专用语言（DSL），也是 torch.compile 自动化生成代码的核心目标后端。其中，triton.jit 是 Triton 框架提供的即时编译装饰器，定位为高性能跨平台 GPU 内核的手动开发入口，抽象层级低于 torch.compile、高于原生 CUDA C++。它允许开发者以 Python 风格语法编写 GPU 内核逻辑，无需手动处理线程调度、寄存器分配等底层细节，最终编译为高效 GPU 机器码，用于满足定制化算子的高性能需求。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）CUDA Graph：GPU 底层静态任务流执行优化技术</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>CUDA Graph 是一种静态任务流调度技术，旨在消除主机端（Host）与设备端（Device）之间的交互延迟。它并非 “内核生成工具”，也非 “用户态编程接口”，而是针对 CPU-GPU 交互瓶颈的底层执行优化技术，抽象层级最低。其核心作用是固化连续的 CUDA 内核调用序列与内存配置，通过 “录制 - 重放” 模式消除重复内核启动、CPU-GPU 频繁通信的开销，仅优化执行流程，不改变内核本身的计算性能。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>8.2. 核心联系</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）torch.compile 依赖 triton.jit 实现高性能内核生成</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>torch.compile 的默认底层编译器（Inductor）在 CUDA 设备上，会自动将 PyTorch 计算逻辑转化为 Triton 内核代码，并隐式调用 triton.jit 完成编译，生成高性能 GPU 内核（开发者无需手动编写 Triton 代码，也无需感知 triton.jit 的存在）。此外，torch.compile 也支持生成原生 CUDA 内核，作为 Triton 内核的可选补充方案。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）torch.compile 集成 CUDA Graph 实现执行层二次优化</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>当输入张量的形状、数据类型等属性固定时，torch.compile 会自动启用 CUDA Graph 优化，将编译生成的 Triton/CUDA 内核调用序列录制为 CUDA 图。后续重复执行该逻辑时，直接在 GPU 上重放该图，进一步放大性能收益，实现 “内核计算优化” 与 “执行流程优化” 的协同增效。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）triton.jit 自定义内核可与 CUDA Graph 手动协同</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>开发者手动通过 triton.jit 编写并编译的自定义内核，在批量重复执行（输入形状固定）的场景下，可手动集成 CUDA Graph 完成 “录制 - 重放” 流程，消除 CPU 对 GPU 的调度开销，实现内核计算性能与执行效率的双重极致优化。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（4）三者协同构建极致性能计算链路</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>典型极致性能链路：手动编写 triton.jit 定制内核 → 嵌入 PyTorch 模型 / 函数 → 通过 torch.compile 进行上层计算图优化（算子融合、内存复用等） → torch.compile 自动启用 CUDA Graph 优化执行流程 → 实现 GPU 计算性能最大化。</span></div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9. TP 模式</span></h1>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>TP 模式将矩阵计算按行、列拆分到多颗 GPU 上执行，涉及两个关键点：权重参数怎么加载、多核计算之间如何协同，下面做介绍。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.1. 加载权重参数</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>权重参数与矩阵计算强相关，因此权重参数的加载逻辑通常与矩阵计算逻辑一同封装在同一个类中，实现功能的内聚性。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.1.1. 关键技术点</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）参数文件中，权重矩阵以 Key-Value 键值对形式存储，读取时同样采用 Key-Value 方式解析。其中 key 对应权重矩阵在模型中的归属位置，例如：模型第 0 层 MLP 子层的 down proj 权重对应的 key 为 model.layers.0.mlp.down_proj.weight。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）参数文件由训练流程写入、推理流程读取，训练与推理两侧必须严格对齐 key 的命名规则。模型参数加载时，会根据参数文件中的 key 名称，在 nn.Module 对象中匹配并调用对应的 weight_loader 方法完成加载。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）模型结构包含多个层级，每一层内部又包含多个子模块，不同子模块对应各自专属的参数加载方法。PyTorch 的 nn.Module 通过特殊方法 __setattr__，将模型结构中的各个子模块构建为树形结构；树形结构中每个叶子节点的路径，与参数文件中的 key 一一映射，通过该路径找到叶子节点后，即可获取对应的参数对象 nn.Parameter，而该参数对象绑定了其所属子模块的 weight_loader 方法。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（4）矩阵乘法 A * B 遵循「A 的行 × B 的列」计算规则，在模型推理中，B 为权重矩阵，实际访问时以列维度为主。为提升读取效率、避免缓存（Cache）频繁失效，权重矩阵 B 通常以转置形式存储。TP（Tensor Parallel）worker 加载权重时，需适配该转置存储特性 —— 即权重矩阵第 0 维对应原始矩阵的列数据，第 1 维对应原始矩阵的行数据。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>从上述技术点可得出核心对应关系：参数文件中的模型结构以一个个 key 表示，这些 key 按层级关系可构建为一棵路径树；代码中的模型结构以有包含关系的类对象表示，这些类对象同样构成一棵与参数文件路径树完全对应的树。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.1.2. 实操举例（FFN 层 up proj 权重加载）</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）假设 TP size=2，up proj 权重矩阵的原始形状为 [1024, 3072]，下面介绍一个 TP worker 如何加载权重。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）首先，构造模型对象时，会初始化 ColumnParallelLinear 对象，并设定核心参数：input_size=1024，output_size=3072/2=1536（按 TP 尺寸做均分）。这两个参数最终用于初始化 nn.Parameter 对象，对应代码为 <span><span>self.weight = nn.Parameter(torch.empty(output_size, input_size))<span>，需注意此处初始化的张量以 output_size 为行维度、input_size 为列维度。</span></span></span></span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）随后，启动模型权重加载流程：先从参数文件中读取所有 key-value 键值对，再通过 key 在 nn.Module 树形结构中查找对应的 nn.Parameter 对象，匹配到后调用其绑定的 weight_loader 函数，执行具体的参数加载操作。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（4）参数加载阶段，针对列并行模式，需要对权重张量的第 0 维度进行拆分，再根据当前进程的 tp_rank（TP 进程编号），确定本进程需要加载的权重区间，完成分片权重的加载。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>注：代码实现中，会将 gate 矩阵与 up 矩阵进行合并加载到显存中，因此实际加载流程会在此基础上增加几步额外步骤。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.1.3. 构造树形结构示例代码</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>下面 demo 代码展示多个有层级的对象如何通过特殊方法 __setattr__ 构造树形结构.</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> MiniModule:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> <span style=\"color: rgba(128, 0, 128, 1);\">__init__</span>(self, name=<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">root</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n        self._name </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> name\n        self._modules </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> {}\n        self._parameters </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> {}\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> <span style=\"color: rgba(128, 0, 128, 1);\">__setattr__</span><span style=\"color: rgba(0, 0, 0, 1);\">(self, name, value):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span><span style=\"color: rgba(0, 0, 0, 1);\"> isinstance(value, MiniModule):\n            self._modules[name] </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> value\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">elif</span> name.endswith(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n            self._parameters[name] </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> value\n\n        super().</span><span style=\"color: rgba(128, 0, 128, 1);\">__setattr__</span><span style=\"color: rgba(0, 0, 0, 1);\">(name, value)\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> get_all_paths(self, prefix=<span style=\"color: rgba(128, 0, 0, 1);\">\"\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n        </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span><span style=\"color: rgba(128, 0, 0, 1);\">递归遍历并收集所有参数的完整路径</span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\n        paths </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []\n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 先收集当前层级的参数路径</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">for</span> p_name <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> self._parameters:\n            full_path </span>= f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{prefix}.{p_name}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> <span style=\"color: rgba(0, 0, 255, 1);\">if</span> prefix <span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\"> p_name\n            paths.append(full_path)\n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 递归进入子模块，传递更新后的前缀</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">for</span> m_name, m_obj <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> self._modules.items():\n            new_prefix </span>= f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{prefix}.{m_name}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> <span style=\"color: rgba(0, 0, 255, 1);\">if</span> prefix <span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\"> m_name\n            paths.extend(m_obj.get_all_paths(new_prefix))\n\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> paths\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> q_weight_loader():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">this is q_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> down_weight_loader():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">this is down_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 构造树形结构 ---</span>\nmodel = MiniModule(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Qwen3</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nmodel.layers </span>= MiniModule(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Layers</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nmodel.layers.attention </span>= MiniModule(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Attention</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nmodel.layers.attention.q_weight_loader </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> q_weight_loader\nmodel.layers.mlp </span>= MiniModule(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">MLP</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nmodel.layers.mlp.down_weight_loader </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> down_weight_loader\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 打印所有路径 ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">遍历模型的所有参数路径：</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nall_paths </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> model.get_all_paths()\n</span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> path <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> all_paths:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">路径: {path}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 模拟 nano-vllm 的访问逻辑 ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> mock_get_parameter(root, path):\n    parts </span>= path.split(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">.</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    curr </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> root\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> part <span style=\"color: rgba(0, 0, 255, 1);\">in</span> parts[:-1<span style=\"color: rgba(0, 0, 0, 1);\">]:\n        curr </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> curr._modules[part]\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> curr._parameters[parts[-1<span style=\"color: rgba(0, 0, 0, 1);\">]]\n\ntarget </span>= <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">layers.attention.q_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n模拟查找路径 '{target}':</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nloader </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> mock_get_parameter(model, target)\nloader()</span></pre>\n</div>\n<p><span>输出结果：</span></p>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">遍历模型的所有参数路径：\n路径: layers.attention.q_weight_loader\n路径: layers.mlp.down_weight_loader\n\n模拟查找路径 </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">layers.attention.q_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">:\nthis is q_weight_loader</span></pre>\n</div>\n<h2><span>9.2. 多 GPU 之间的计算协同</span></h2>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.2.1. 功能细节</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>nano-vllm 只考虑单机内的多 GPU 协同，协同过程如下：</span></div>\n</div>\n<div class=\"Image-captionContainer\">\n<div>\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div>\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\" style=\"text-align: center;\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"351\" src=\"https://pic1.zhimg.com/80/v2-7f054d91f4badab37cab087059b847da_1440w.png?source=ccfced1a\" width=\"500\" /></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div style=\"text-align: center;\">图 5</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）进程隔离与独立加载： 采用多进程模式，一个 GPU 对应一个独立进程。各进程并发读取权重文件，并根据自己的 tp_rank 按照预设的切分策略（如 ColumnParallel 的行切分或 RowParallel 的列切分），将属于自己的那部分数据从 safetensors 加载到显存中。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）控制面协同（Control Plane）： Rank 0 负责全局调度，通过共享内存将推理请求（Tokens、Sampling Params 等）同步给其他 Rank。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）数据面通信（Data Plane）： 在矩阵计算的关键节点，利用通信原语（如 all-reduce 处理分块求和、all-gather 处理序列拼接）完成张量并行的结果汇总，使分布在不同显卡上的计算结果在数学上等价于单卡计算。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.2.2. 示例代码</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>下面写一个简单的数据通信例子：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">import torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport </span><span style=\"color: rgba(0, 0, 255, 1);\">time</span><span style=\"color: rgba(0, 0, 0, 1);\">\n\ndef setup(rank, world_size):\n    dist.init_process_group(\n        </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">nccl</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">tcp://127.0.0.1:2333</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, world_size=world_size, rank=<span style=\"color: rgba(0, 0, 0, 1);\">rank\n    )\n    torch.cuda.set_device(rank)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef test_all_reduce(rank, world_size):\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"\n</span>    All-<span style=\"color: rgba(0, 0, 0, 1);\">Reduce 子函数：将所有 GPU 的数据进行归约操作（求和），结果同步到所有 GPU\n\n    使用场景：\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> RowParallelLinear 的输出聚合\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> 需要所有 GPU 都得到相同的聚合结果\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"\n</span>    print(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] ===== All-Reduce 示例 =====</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> rank == <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        data </span>= torch.tensor([<span style=\"color: rgba(128, 0, 128, 1);\">1.0</span>, <span style=\"color: rgba(128, 0, 128, 1);\">2.0</span>], device=f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda:{rank}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        data </span>= torch.tensor([<span style=\"color: rgba(128, 0, 128, 1);\">3.0</span>, <span style=\"color: rgba(128, 0, 128, 1);\">4.0</span>], device=f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda:{rank}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Reduce before: {data}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    dist.all_reduce(data, op</span>=<span style=\"color: rgba(0, 0, 0, 1);\">dist.ReduceOp.SUM)\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Reduce after:  {data}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\ndef test_all_gather(rank, world_size):\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"\n</span>    All-<span style=\"color: rgba(0, 0, 0, 1);\">Gather 子函数：收集所有 GPU 的数据到每个 GPU 上\n\n    使用场景：\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> VocabParallelEmbedding 的输出收集\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> ParallelLMHead 的 logits 收集\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> 需要每个 GPU 都获得所有 GPU 的完整数据\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"\n</span>    print(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] ===== All-Gather 示例 =====</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    # Rank </span><span style=\"color: rgba(128, 0, 128, 1);\">0</span> 产生 [<span style=\"color: rgba(128, 0, 128, 1);\">10</span>, <span style=\"color: rgba(128, 0, 128, 1);\">20</span>], Rank <span style=\"color: rgba(128, 0, 128, 1);\">1</span> 产生 [<span style=\"color: rgba(128, 0, 128, 1);\">30</span>, <span style=\"color: rgba(128, 0, 128, 1);\">40</span><span style=\"color: rgba(0, 0, 0, 1);\">]\n    local_data </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> torch.tensor(\n        [</span><span style=\"color: rgba(128, 0, 128, 1);\">10.0</span> + rank * <span style=\"color: rgba(128, 0, 128, 1);\">20</span>, <span style=\"color: rgba(128, 0, 128, 1);\">20.0</span> + rank * <span style=\"color: rgba(128, 0, 128, 1);\">20</span>], device=f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda:{rank}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\n    )\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Gather local data: {local_data}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    gathered_list </span>= [torch.zeros_like(local_data) <span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(world_size)]\n    dist.all_gather(gathered_list, local_data)\n\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Gather result: {gathered_list}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    gathered_tensor </span>= torch.<span style=\"color: rgba(0, 0, 255, 1);\">cat</span>(gathered_list, dim=<span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Gather concatenated: {gathered_tensor}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\ndef tp_demo(rank, world_size):\n    setup(rank, world_size)\n\n    test_all_reduce(rank, world_size)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">time</span>.<span style=\"color: rgba(0, 0, 255, 1);\">sleep</span>(<span style=\"color: rgba(128, 0, 128, 1);\">5</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    test_all_gather(rank, world_size)\n\n    cleanup()\n\ndef run_demo():\n    world_size </span>= <span style=\"color: rgba(128, 0, 128, 1);\">2</span><span style=\"color: rgba(0, 0, 0, 1);\">\n    mp.spawn(tp_demo, args</span>=(world_size,), nprocs=world_size, <span style=\"color: rgba(0, 0, 255, 1);\">join</span>=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> __name__ == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">__main__</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> torch.cuda.device_count() &gt;= <span style=\"color: rgba(128, 0, 128, 1);\">2</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        run_demo()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">需要至少 2 张 GPU 来运行此 TP 示例</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<h1><span>10. 其他</span></h1>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>本文基于 nano-vllm 项目，聚焦讲解大模型推理加速领域中最基础的若干核心技术点。需要说明的是，大模型推理加速的技术体系十分丰富，本项目并未覆盖全部内容，例如：计算通信重叠（Overlap）、多 token 预测（MTP，Multi‑Token Prediction）、多流、多进程服务（MPS，Multi-Process Service）、数据并行（DP）、流水线并行（PP）、上下文并行（CP）、专家并行（EP）以及 PD 分离等进阶技术方向，可作为后续深入学习的拓展内容。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>&nbsp;</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>注1：在本文的总结过程中，除了查看源代码，也借助 deepwiki 以及其他 AI 工具辅助。&nbsp;<br /><br /></span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>本文:&nbsp;<a href=\"https://www.cnblogs.com/cswuyg/p/19471225\" target=\"_blank\">https://www.cnblogs.com/cswuyg/p/19471225</a></span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>知乎:&nbsp;<a href=\"https://zhuanlan.zhihu.com/p/1989806890381746916\" rel=\"noopener nofollow\" target=\"_blank\">https://zhuanlan.zhihu.com/p/1989806890381746916</a></span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>公众号:&nbsp;<a href=\"https://mp.weixin.qq.com/s/6mAZ49iP1SCKt5ZdWf6ErQ\" rel=\"noopener nofollow\" target=\"_blank\">https://mp.weixin.qq.com/s/6mAZ49iP1SCKt5ZdWf6ErQ</a></span></div>\n\n\n</div>\n</div>\n<div class=\"clear\"></div>\n\n\t<div class=\"postDesc\">posted on \n<span id=\"post-date\">2026-01-12 12:38</span>&nbsp;\n<a href=\"https://www.cnblogs.com/cswuyg\">-银光-</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "上周热点回顾（1.5-1.11）",
      "link": "https://www.cnblogs.com/cmt/p/19469697",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/cmt/p/19469697\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 08:38\">\n    <span>上周热点回顾（1.5-1.11）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>热点随笔：</p>\n<p> · <a href=\"https://www.cnblogs.com/knqiufan/archive/2026/01/07/19449849.html\" target=\"_blank\">Claude Code 完全指南：使用方式、技巧与最佳实践</a> (<a href=\"https://www.cnblogs.com/knqiufan/\" target=\"_blank\">knqiufan</a>) <br />\n · <a href=\"https://www.cnblogs.com/shanyou/archive/2026/01/05/19441004.html\" target=\"_blank\">从 TIOBE 2025 年度语言到 2026 年 C# 智能体生态的全面崛起</a>\n(<a href=\"https://www.cnblogs.com/shanyou/\" target=\"_blank\">张善友</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/thisiswhy/archive/2026/01/08/19458678.html\" target=\"_blank\">可怕，看到一个冷血的算法。</a>\n(<a href=\"https://www.cnblogs.com/thisiswhy/\" target=\"_blank\">why技术</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/xueweihan/archive/2026/01/06/19445015.html\" target=\"_blank\">嫌 AI 写的界面太丑？装上这个开源插件，秒变资深设计师</a>\n(<a href=\"https://www.cnblogs.com/xueweihan/\" target=\"_blank\">削微寒</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/shanyou/archive/2026/01/07/19452660.html\" target=\"_blank\">XAML Studio 已正式开源</a>\n(<a href=\"https://www.cnblogs.com/shanyou/\" target=\"_blank\">张善友</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/yupi/archive/2026/01/09/19460549.html\" target=\"_blank\">干掉 Claude Code，这个开源 AI 编程工具杀疯了？</a>\n(<a href=\"https://www.cnblogs.com/yupi/\" target=\"_blank\">程序员鱼皮</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/jinjiangongzuoshi/archive/2026/01/05/19440715.html\" target=\"_blank\">最近很火爆的Claude Skills到底是个啥？解决什么问题？怎么用！</a>\n(<a href=\"https://www.cnblogs.com/jinjiangongzuoshi/\" target=\"_blank\">狂师</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/kklldog/archive/2026/01/07/19449864.html\" target=\"_blank\">为什么说 IO 操作异步才有意义</a>\n(<a href=\"https://www.cnblogs.com/kklldog/\" target=\"_blank\">Agile.Zhou</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/12lisu/archive/2026/01/08/19456855.html\" target=\"_blank\">强烈推荐 | 阿里开源的这10个神级项目</a>\n(<a href=\"https://www.cnblogs.com/12lisu/\" target=\"_blank\">苏三说技术</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/12lisu/archive/2026/01/06/19447172.html\" target=\"_blank\">用雪花算法就不会产生重复的ID?</a>\n(<a href=\"https://www.cnblogs.com/12lisu/\" target=\"_blank\">苏三说技术</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/ymtianyu/archive/2026/01/05/19442584.html\" target=\"_blank\">从安装到上线：一份 Nginx 实战指南，让你的 Web 应用稳建安全</a>\n(<a href=\"https://www.cnblogs.com/ymtianyu/\" target=\"_blank\">一名程序媛呀</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/tianqing/archive/2026/01/05/19439916.html\" target=\"_blank\">.NET 10 New feature 新增功能介绍-WebSocket功能增强</a>\n(<a href=\"https://www.cnblogs.com/tianqing/\" target=\"_blank\">Eric zhou</a>)                    <br />\n            </p>\n<p>热点新闻：</p>\n<p>\n · <a href=\"https://news.cnblogs.com/n/812350/\" target=\"_blank\">Stack Overflow 彻底凉了，比18年前上线首月问题数量还少</a><br />\n · <a href=\"https://news.cnblogs.com/n/812432/\" target=\"_blank\">终于有实锤了：缺少实体按键的车就是更危险</a><br />\n · <a href=\"https://news.cnblogs.com/n/812455/\" target=\"_blank\">“所有人请注意用脑卫生”：打工人最应该戒掉的6件事</a><br />\n · <a href=\"https://news.cnblogs.com/n/812551/\" target=\"_blank\">毫无征兆！DeepSeek R1爆更86页论文，这才是真正的Open</a><br />\n · <a href=\"https://news.cnblogs.com/n/812321/\" target=\"_blank\">Redis宣布闭源后，中国技术人的“上游时刻”</a><br />\n · <a href=\"https://news.cnblogs.com/n/812667/\" target=\"_blank\">Stack Overflow已死？CEO带队狂赚1.15亿刀，6个月原地反杀</a><br />\n · <a href=\"https://news.cnblogs.com/n/812425/\" target=\"_blank\">8天暴涨400万粉丝后塌房，蛋神是保质期最短的网红嘛？</a><br />\n · <a href=\"https://news.cnblogs.com/n/812328/\" target=\"_blank\">2025，这些互联网巨头赢麻了</a><br />\n · <a href=\"https://news.cnblogs.com/n/812333/\" target=\"_blank\">Claude Code、Cursor 都过时了？！硅谷顶流大牛炸场暴论：AI 编程要练满 2000 小时才算“会用”，一年不用世界级大神也会沦为实习生水平</a><br />\n · <a href=\"https://news.cnblogs.com/n/812428/\" target=\"_blank\">中产「自律三件套」，它第一个塌房？</a><br />\n · <a href=\"https://news.cnblogs.com/n/812541/\" target=\"_blank\">马斯克最新播客长叹：中国听懂了我的话，2026年将在算力上碾压世界</a><br />\n · <a href=\"https://news.cnblogs.com/n/812677/\" target=\"_blank\">因为AI编程，Tailwind CSS差点死了</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-12 08:38</span>&nbsp;\n<a href=\"https://www.cnblogs.com/cmt\">博客园团队</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "万字长文讲解：团队落地 AI 辅助编程和 AI Specs 实战",
      "link": "https://www.cnblogs.com/whuanle/p/19469026",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/whuanle/p/19469026\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 08:34\">\n    <span>万字长文讲解：团队落地 AI 辅助编程和 AI Specs 实战</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        建设使用 AI 辅助编程的团队，使用 AI Specs 规范化团队协作流程和编码规范，让 AI 落地、实现业务价值。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"ai-编程团队协作\">AI 编程团队协作</h2>\n<h3 id=\"历史背景\">历史背景</h3>\n<p>AI 发展的速度实在太快了，在 GPT-3 横空出世的阶段，那个时候只能使用对话框一问一答，到现在各种 RAG、AI Workflow、AI Agent 等各类技术，使得 AI 可以做更多的事情，实现更加强大的功能。在 cursor、trae 等 AI IDE 出现后，很多人便彻底迷上了这项足以颠覆传统工作模式的技术，尤其在编程领域，这场变革的浪潮来得尤为迅猛 —— 最初，当我们面对复杂的业务功能、晦涩的语法实现或是陌生的框架调用时，无需再耗费大量时间翻阅文档、调试代码，只需通过自然语言向 AI 描述需求，就能借助它的问答式回复获取可用的代码片段，这不仅大大降低了编程的门槛，更让开发者从重复的基础编码工作中得到了初步解放，也为后续对话式代码生成、Vibe Code 乃至 Spec Code 等更先进的编程范式，埋下了充满无限可能的种子。</p>\n<br />\n<p>目前，主流的 AI 辅助编程主要是 AI Agent，在 AI IDE 中输入要做的功能，AI 会自动读取项目文件，经过思考后编写对应的代码，并且会自动修正代码错误，确保代码可以编译通过。这便是大语言模型催生的对话式 AI 代码生成，开启了自然语言交互生成代码的时代，开发者通过多轮对话让 AI 产出代码片段并手动整合，AI 仅作为辅助工具；随后演进的 Vibe Code（氛围编程）范式，让开发者只需描述高阶意图即可驱动 AI 完成全量代码的编写与迭代，人类角色从编码者转向需求引导者与测试者，聚焦快速原型与创意验证。</p>\n<p><img alt=\"image-20260109185650106\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce1adf62.png\" /></p>\n<br />\n<p>但是，AI Agent 模式下的 AI 辅助编程，也存在一些常见的问题，所以在 IT 圈流行着这图：</p>\n<p><img alt=\"896b42e496e1d780c1b2ba5685065dc5\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce1d759f.\" /></p>\n<blockquote>\n<p>感谢热心网友【迪迦】提供的图片。</p>\n</blockquote>\n<br />\n<p>虽然上图有搞笑成分，这里我们将问题分为大语言模型和 AI IDE 两部分，下面介绍常见的 AI 辅助编程问题。</p>\n<p>大语言模型：</p>\n<ul>\n<li><strong>目标漂移</strong>：在多步骤、长流程的编程任务中（如复杂功能开发、项目重构），AI 执行到中后阶段易偏离初始目标，例如原本需求是优化数据库查询性能，后续却无端重写 UI 组件，本质是缺乏有效的目标锚定机制。</li>\n<li><strong>重复犯错</strong>：对已出现过的错误缺乏长期记忆，相同问题会反复出现（如未正确处理异步函数的 <code>await</code> 关键字、文件路径引用错误），无法自动沉淀历史解决方案。</li>\n<li><strong>上下文爆炸</strong>：为让 AI 掌握完整项目信息，需将大量代码、需求文档、历史对话塞进上下文窗口，导致模型处理效率下降、响应延迟，且易忽略关键信息。</li>\n<li><strong>进度丢失</strong>：依赖对话窗口存储任务状态，一旦对话重置、刷新或中断，之前的开发进度、中间决策、已积累的项目认知会全部消失，无法无缝续工。</li>\n<li><strong>幻觉生成</strong>：在缺乏明确参考依据时，可能编造不存在的 API 方法、语法规则或项目配置，生成看似合理但实际无法运行的代码，增加调试成本。</li>\n</ul>\n<p><br />AI IDE：</p>\n<ul>\n<li><strong>上下文管理能力不足</strong>：多数 AI IDE 未建立独立的外部记忆机制，仍依赖模型自带的上下文窗口，无法高效关联项目文件、历史操作记录，难以支撑复杂任务的连贯开发。</li>\n<li><strong>任务追踪与可视化缺失</strong>：缺乏对编程任务的结构化拆解与进度可视化功能，无法像 <code>task_plan.md</code> 那样清晰呈现阶段划分、完成状态、决策依据，开发者难以掌控 AI 的工作轨迹。</li>\n<li><strong>错误记录与复用薄弱</strong>：未集成错误追踪体系，AI 遇到的报错、修复方案无法自动归档，既不便于开发者回溯问题根源，也无法让 AI 后续快速复用已验证的解决方案。</li>\n<li><strong>文件协同与持久化不足</strong>：对 AI 生成的中间产物（如调研笔记、技术选型文档）缺乏统一的存储与管理机制，文件分散且易丢失，无法形成可追溯、可复用的项目知识库。</li>\n<li><strong>人机协作衔接不畅</strong>：开发者难以直接干预 AI 的任务执行流程，例如无法通过编辑结构化计划文件调整开发方向，也缺乏便捷的方式审查 AI 的决策逻辑，导致人机协同效率低下。</li>\n<li><strong>性能与成本失衡</strong>：处理大规模项目时，频繁加载全量上下文会导致 IDE 运行卡顿，且重复处理相同静态内容（如项目框架定义、工具配置）会增加 Token 消耗，提升使用成本。</li>\n</ul>\n<br />\n<p>所以，在目前的技术局限下，要玩 AI 编程，其实还不能为所欲为，同时使用 AI 编程时也会碰到很多阻碍问题，降低了编程的体验和项目代码质量。</p>\n<h3 id=\"ai-编程下的团队协作痛点与核心诉求\">AI 编程下的团队协作痛点与核心诉求</h3>\n<p>前面介绍了 AI 编程本身容易出现的问题和技术局限，回到以团队为单位进行 AI 编程协作时，当团队缺乏标准化协作机制时，AI 编程易陷入 “单兵作战” 的困境，会出现更加多的头疼的问题。</p>\n<p>因为笔者在公司带一个项目组，已经有很多成员使用 AI 写代码，在经过一段时间的观察和思考后，发现了一些问题，所以才想到写这篇文章的，相信这些问题在大家的公司里面也会存在。</p>\n<br />\n<p><strong>代码碎片化</strong></p>\n<blockquote>\n<p>很多同事发现用 AI 写代码，可以早点下班，于是大家都用 AI 写代码，你写你的我写我的，导致成员各自使用 AI 生成代码，风格、逻辑差异显著，模块衔接困难，后期维护成本激增。反正同事用 AI 写的代码，我是压根不想维护。</p>\n</blockquote>\n<br />\n<p><strong>规范失控</strong></p>\n<blockquote>\n<p>大家应该都有感受，在不同提示词、编写不同功能时，AI 编写的代码风格各异，没有统一的规范和风格，AI 生成代码可能偏离团队代码规范、安全标准，埋下质量隐患。</p>\n</blockquote>\n<br />\n<p><strong>知识孤岛</strong></p>\n<blockquote>\n<p>个人使用 AI 积累的经验无法共享，团队整体效率难以提升。你用 cursor，我用 kiro，各写各的代码，没法为团队沉淀知识经验，提示词、功能历史记录、背景上下文等完全没法在团队内共享，每个人都得在本地使用 AI 阅读代码生成上下文记忆。</p>\n</blockquote>\n<br />\n<p><strong>协作低效</strong></p>\n<blockquote>\n<p>AI 写的代码实际上会出现一个简单的功能写一堆代码的情况，也有可能 A 同事写 A 功能时生成了 C，B 同事写 B 功能时又生成了一个类似的 C，项目各种代码错综复杂。导致难以统一任务分配、进行代码评审流程，导致信息传递滞后，冲突频发。</p>\n</blockquote>\n<br />\n<p>除了以上问题在团队编程中需要解决，很多公司都有代码质量规范要求、安全要求、单元测试覆盖率要求，所以基于这些问题和常见公司编程要求，笔者认为团队 AI 编程的核心诉求应聚焦：</p>\n<ul>\n<li>维持一致代码质量</li>\n<li>防止安全漏洞</li>\n<li>减少重复工作量</li>\n<li>标准化协作流程</li>\n<li>加快开发周期</li>\n<li>实现从 “快速原型” 到 “工程化落地” 的跨越。</li>\n</ul>\n<br />\n<p>很多领导对 AI 编程的想法很难评，觉得可以 ”降本增效“ ，一个项目平时需要一个月做完，用 AI 一天就行，代码还能写得比开发还好，这样可以开除很多开发人员，所以号召大家在公司使用 AI 编程。</p>\n<blockquote>\n<p>后面会提到 AI 辅助编程大约可以提高多少速度，但不可能是一天写完一个月的代码。</p>\n</blockquote>\n<p><br />而实际上，除非原因开盲盒，产品做得怎么样取决于 AI 怎么写，前期速度确实很快。但是到了中后期，AI 写的代码难以维护，反而会因为要 AI 写代码，花费大量时间编写提示词，为了实现一个小需求，可能要跟 AI 一直对骂，最终陷入混乱。</p>\n<p>其实，正如笔者前面所说的，大语言模型和 AI IDE 在目前有一些技术局限，而且在团队使用 AI 编程时会带来很多协作问题，如果没有应对这些问题的解决方法，那么使用 AI 编写的代码最终可能是一片混乱，而且生成代码人类可能无法阅读，鬼才知道 AI 写的是啥。</p>\n<p>公司落地 AI 编程，确实可以提高开发速度，缩短开发周期，但是不应该荒谬到认为 ”一个月工作量使用 AI 一天就行“ 。笔者认为，除了提速，应当更多聚焦实现前面提到的 AI 编程的核心诉求。</p>\n<br />\n<p>很多公司天天鼓吹单元测试、代码规范、高性能高质量代码，给开发指定单元测试覆盖率和代码审查等指标，反复折腾开发人员，但是公司存在各种各样的开发管理和技术问题，盲目定制高要求的开发指标，完全解决不了当前的问题，又会使得开发身心疲倦。</p>\n<p>笔者之所以要说这些，是因为代码审查、代码规范、单元测试等，做起来没有那么简单，有多少公司折腾这些，又能最终真正做到？</p>\n<blockquote>\n<p>能不能做好这些，其实跟公司基因有关系，跟落地难度也有关系。</p>\n</blockquote>\n<p><br />其实，只有足够简单，规范才能真正落地。</p>\n<p>所以，笔者一直在思考，如何解决 AI 编程下的团队协作痛点与满足核心诉求，并且真正落地单元测试、代码审查等技术要求。</p>\n<p>这就是本文的重点，AI Spec，到底要怎么做，接下来本文将以架构师的角度，去思考，去研究，我们是架构师，那么应该怎么把 AI 编程落地到团队协作中。</p>\n<h2 id=\"规范开发spec-development\">规范开发(Spec development)</h2>\n<p>前面提到，AI 辅助编程逐渐成为主流，大部分开发人员已经在日常工作中使用 AI 编写代码，但是也会引入很多问题，导致生成很多不可预测的、质量参差不齐的代码。那么为了解决这些问题，出现了 SDD(Spec-Driven Development) 这种概念，强调在使用 AI 编写代码之前，先有 Specification，以便约束 AI 生成高质量的、符合业务需求和技术要求的代码。</p>\n<p>目前社区中主要存在三类 SDD 工具：OpenSpec、Kiro、Spec-Kit。</p>\n<p><br />为了实现 Spec development，选择 SDD 工具时，需要考虑：</p>\n<ul>\n<li>\n<p><strong>工具与流程适配</strong>：选择 Kiro 这类支持规范定制、多场景协作的 AI 工具，避免工具功能与团队流程脱节；</p>\n</li>\n<li>\n<p><strong>重视规范落地</strong>：将团队规则转化为可执行的钩子、Spec 模板，借助 AI 强制落地，而非仅停留在文档层面；</p>\n</li>\n<li>\n<p><strong>构建协作文化</strong>：鼓励成员主动共享 AI 使用经验、参与流程优化，避免 “单兵作战” 思维；</p>\n</li>\n<li>\n<p><strong>平衡自动化与人工</strong>：AI 负责标准化、重复性工作（如测试、规范检查），人类聚焦核心架构与创意决策，实现人机协同增效。</p>\n</li>\n</ul>\n<br />\n<p>在本文，笔者要讲解的是 Kiro 落地的内容。</p>\n<p><br />在搜索资料的过程中，笔者发现几篇写得不错的文章，这里供读者参考，本文就不单独讲解这些 SDD 工具了。</p>\n<blockquote>\n<p>AI 规范驱动开发“三剑客”深度对比：Spec-Kit、Kiro 与 OpenSpec 实战指南 - 技术栈：</p>\n<p><a href=\"https://jishuzhan.net/article/1988226029513670657\" rel=\"noopener nofollow\" target=\"_blank\">https://jishuzhan.net/article/1988226029513670657</a></p>\n<p>Transforming Dev Practices with Kiro’s Spec-Driven Tools | AI Native Dev</p>\n<p><a href=\"https://ainativedev.io/transforming-dev-practices-with-kiros-spec-driven-tools\" rel=\"noopener nofollow\" target=\"_blank\">https://ainativedev.io/transforming-dev-practices-with-kiros-spec-driven-tools</a></p>\n</blockquote>\n<br />\n<p>这里简单介绍一下 Kiro。</p>\n<p>Kiro 是亚马逊云科技推出的 AI IDE，以 “规范驱动开发” 为核心，完美适配团队协作需求。</p>\n<blockquote>\n<p>对国内用户友好，无需特殊网络配置。</p>\n</blockquote>\n<p><img alt=\"img\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce20c847.png\" /></p>\n<h3 id=\"团队对齐ai-编程核心概念解析\">团队对齐：AI 编程核心概念解析</h3>\n<p>在深入团队协作实践前，需先明确支撑 AI 编程的关键技术概念，无论是产品还是测试，也必须了解 LLM、MCP、Agent 等概念，需要团队对一些知识都过关，确保整个团队交流协作不存在知识障碍。</p>\n<blockquote>\n<p>这些概念的知识，笔者就不赘述了。</p>\n</blockquote>\n<br />\n<p>主流大语言模型用于编程的评分，作为架构师需要清晰了解不同模型的特长和优缺点，在编程领域哪个模型最优秀最适合用于编程。</p>\n<blockquote>\n<p>笔者发现几个网站很不错，有各类模型的介绍和评分，地址：</p>\n<p><a href=\"https://apxml.com/zh/leaderboards/coding-llms\" rel=\"noopener nofollow\" target=\"_blank\">Best LLMs for Coding | LLM Leaderboards</a></p>\n<p><a href=\"https://llm-stats.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://llm-stats.com/</a></p>\n<p><a href=\"https://www.aibase.com/zh\" rel=\"noopener nofollow\" target=\"_blank\">https://www.aibase.com/zh</a></p>\n</blockquote>\n<p><img alt=\"image-20260109204244573\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce239a78.png\" /></p>\n<br />\n<p>主流企业使用 AI 辅助编程提升的效率。</p>\n<p>要是领导觉得你一个月写的代码不如 AI 一天写的，那就尴尬了。作为技术人员，必须合理评估使用 AI 编程后，整个团队到底提升了多少效率，编码速度提高了多少。</p>\n<p>比如，根据这篇研究报告，完成任务的编码速度大概提升了 <code>21%</code>，报告地址 <a href=\"https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/\" rel=\"noopener nofollow\" target=\"_blank\">https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/</a></p>\n<p>而根据豆包联网搜索总结的内容来看，提升的开发速度并没有那么夸张。</p>\n<p><img alt=\"image-20260109205849267\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce35ec28.png\" /></p>\n<br />\n<p>总而言之，打造 AI 辅助编程团队，要求团队人员对 AI 有足够的认识，了解基础知识，并且争取在 AI 编程上的认知一致，否则可能会闹出各种笑话，被人引为笑谈。<strong>知识和认知都很重要</strong>，否则团队分分钟散架。</p>\n<h3 id=\"项目模板\">项目模板</h3>\n<p>为什么要使用开发框架，这个事情不用多说，C# 开发应该知道 ABP 框架，Go 开发应该知道 gin、beego，使用框架后大大简化了开发负担。在使用开发框架的情况下，其实团队还需要定制项目模板，以便适配公司技术栈，以及约束公司内的一些技术要求，例如审计属性、微服务通讯方式和鉴权等，好的项目模板可以统一开发模式和习惯。</p>\n<br />\n<p>笔者在写个人开源项目过程中，结合 DDD 、清洁架构、CQRS 的一些概念，辅以 AI 编程，形成了个人开发习惯，所以为了写这篇文章整理了一个模板项目：</p>\n<p><a href=\"https://github.com/whuanle/aispec\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/whuanle/aispec</a></p>\n<br />\n<p>如果以我的开发习惯来定，我会使用模块化 + CQRS，每个业务领域遵循三层模块化架构，无论开发还是做单元测试，都比较简单。</p>\n<pre><code>src/{domain}/\n├── MoAI.{Domain}.Shared/     # 共享层 - DTO、Command、Query 定义\n├── MoAI.{Domain}.Core/       # 核心层 - Handler 实现、业务逻辑\n└── MoAI.{Domain}.Api/        # API 层 - Controller/Endpoint 暴露\n</code></pre>\n<p>依赖关系：<code>Api → Core → Shared</code>，具体参考：</p>\n<p>代码编写约束参考：<a href=\"https://github.com/whuanle/aispec/blob/master/.kiro/steering/cqrs-conventions.md\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/whuanle/aispec/blob/master/.kiro/steering/cqrs-conventions.md</a></p>\n<p><img alt=\"image-20260109211559319\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce3883d6.png\" /></p>\n<p><img alt=\"image-20260109211703315\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce3ac8c4.png\" /></p>\n<br />\n<p>具体设计思路和各类细节就不讲解，本节其实目的是说，无论你使用何种开发框架，是否使用 DDD，还是传统三层结构，团队应该要有一个统一的项目模板以及开发习惯约束，AI 会参考项目已有代码和约束文件，编写符合要求的代码，避免代码天马行空。</p>\n<p>关于模板项目就不展开说了，读者可以在这里找到这个本身用于实战演示的模板项目：<a href=\"https://github.com/whuanle/aispec\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/whuanle/aispec</a></p>\n<h3 id=\"准备定制团队规则与项目索引\">准备定制团队规则与项目索引</h3>\n<p>在使用 AI 编程时，会有一些常见问题：</p>\n<p>1，开发跟 AI 容易陷入盲目对话，AI 始终没有给出想要的代码，导致代码可用率和确定性低下。</p>\n<p>2，开发难以清晰准确向 AI IDE表达目标方案和代码</p>\n<p>3，前置需要大量精力对项目的 Rules、Docs 进行编写和积累</p>\n<p>4，啥都让 AI 写，但是 AI 写出来的方案总是不满意</p>\n<br />\n<p>解决这些问题本身不难，但是要落到团队协作中，作为架构师就要思考更多了，需要保证代码质量和开发效率，避免反复处理这些问题。</p>\n<br />\n<h4 id=\"什么是-steering\">什么是 Steering</h4>\n<p>这里的 steering 其实是 Kiro 里面的概念，目的是让 AI 编程过程中，始终遵循团队已建立的 patterns、libraries、standards。</p>\n<blockquote>\n<p><a href=\"https://kiro.dev/docs/steering/\" rel=\"noopener nofollow\" target=\"_blank\">https://kiro.dev/docs/steering/</a></p>\n</blockquote>\n<br />\n<p>也就是说，基于团队的项目模板，我们要指定一些跟业务无关的技术约束，例如审计属性怎么定，这个开发框架/项目模板怎么使用，不同功能的代码文件怎么命名等，都可以写到 Kiro steering 里面。AI 在写代码时，会一直围绕这些 steering 去写，如果写出的代码不符合 steering 要求，则 AI 会逐步修正代码。</p>\n<br />\n<p>那么，团队要落地代码规范，是不是变简单了？前面提到的 AI 编程的问题和核心诉求，是不是可以解决了？</p>\n<p>是的，所以，要重视项目模板和 steering，在团队开发实现需求之前，就应该定制团队乃至公司研发部门的 steering。</p>\n<h4 id=\"常见-steering-文件策略\">常见 Steering 文件策略</h4>\n<p>后面会提到 Kiro 默认会有的 steering 规则模板，但是从团队的角度考虑，可能还需要这些 steering ：</p>\n<p><strong>API 标准</strong> (<code>api-standards.md</code>) - 定义 REST 约定、错误响应格式、认证流程和版本策略。包括端点命名模式、HTTP 状态码使用和请求/响应示例。</p>\n<p><strong>测试方法</strong> (<code>testing-standards.md</code>) - 建立单元测试模式、集成测试策略、模拟方法和覆盖率期望。记录首选测试库、断言样式和测试文件组织。</p>\n<p><strong>代码风格</strong> (<code>code-conventions.md</code>) - 指定命名模式、文件组织、导入排序和架构决策。包括首选代码结构、组件模式和要避免的反模式示例。</p>\n<p><strong>安全指南</strong> (<code>security-policies.md</code>) - 记录认证要求、数据验证规则、输入清理标准和漏洞预防措施。包括特定于您应用程序的安全编码实践。</p>\n<p><strong>部署流程</strong> (<code>deployment-workflow.md</code>) - 概述构建程序、环境配置、部署步骤和回滚策略。包括 CI/CD 管道详细信息和环境特定要求。</p>\n<br />\n<p>当然，这些 steering 不需要都首选，可以先做一个项目模板，然后让 Kiro 阅读代码并生成即可。</p>\n<p>不同公司团队可能有不同要求，只要看着写就行，可以在摸索的过程中，逐渐积累团队经验，逐渐完善 steering。</p>\n<h3 id=\"核心执行ai-驱动的协作流程设计\">核心执行：AI 驱动的协作流程设计</h3>\n<p>团队协作 AI 编程，把要做的功能丢在对话框，让 AI 写代码就行？</p>\n<p>No！大错特错！</p>\n<br />\n<p>让我们回到 Kiro 的介绍，<code>Agentic AI development from prototype to production</code>，翻译过来是从原型到产品的 Agentic AI 开发。</p>\n<p>注意，要玩 AI 编程，我们是要做从原型到产品整个周期的东西，而不单单是用 AI 写完代码早点下班。</p>\n<p>所以要考虑，团队的角色有哪些，大家应该怎么参与协作，<strong>更重要的是怎么设计新的团队研发流程</strong>。</p>\n<br />\n<p>不同公司的团队组成不一样，所以笔者按照常规团队（产品、UI设计师、前后端开发、测试）组成来讲解后面的内容。</p>\n<blockquote>\n<p>感谢这位作者的文章，给了我很多想法：<a href=\"https://aicodingtools.blog/zh/kiro/kiro-spec-guide\" rel=\"noopener nofollow\" target=\"_blank\">https://aicodingtools.blog/zh/kiro/kiro-spec-guide</a></p>\n</blockquote>\n<br />\n<p>使用 AI 编程后，团队中各个角色要负责的内容会发生变化，要求工作输出的内容能够被 AI 识别并引用，并且在团队内流通。</p>\n<blockquote>\n<p>如果产品输出的需求文档，AI 都不会分析总结，你给开发看？逻辑狗屁不通，AI 怎么写代码？</p>\n<p>产品的需求文档，可以经过开发整理后，作为让 AI 编写代码的提示词和需求约束，大大减轻开发的负担。</p>\n</blockquote>\n<br />\n<p>开发和测试将会大大依赖需求文档（或其它形式的文档），因为需求文档会作为开发、测试和验收依据，在 AI 编程阶段就要让 AI 知道写代码还要考虑怎么测试和验收，不能等让 AI 写完代码，再叫 AI 写单元测试、检查验收能不能过。</p>\n<p><br />另外，前期准备阶段，也依赖架构师或 leader 的架构设计和功能实现设计，而不是说 ”实现一个短链接服务，将长链接缩短为短链接“，技术负责人需要思考和设计，使用何种算法缩短地址、还原地址，怎么存储到数据库，高并发环境下怎么提高并发量和减少对数据库的压力。</p>\n<br />\n<p>所以，要思考，使用 AI 编程落地后，</p>\n<ul>\n<li>\n<p>团队需要哪些角色？</p>\n</li>\n<li>\n<p>每个角色要负责哪些内容？</p>\n</li>\n<li>\n<p>整个研发流程应该分哪些阶段？</p>\n</li>\n</ul>\n<br />\n<p>每个公司情况都是不一样的，所以针对这些问题，笔者没有什么说法和观点，不过后面实战部分会提到 Kiro Specs 是怎么划分研发流程阶段。</p>\n<br />\n<p>笔者设想，未来围绕 AI 辅助编程开发，团队的角色和参与内容可能发生重大改变。</p>\n<p>一是团队开发流程发生改变，不再像以往从产品原型设计、需求会、开发、提测的模式，而是围绕 AI 编程更加靠敏捷开发模式接近。</p>\n<p>二是团队角色负责的内容发生改变，更多围绕产生的资料能够被 AI 吸收，知识可以沉淀，减少信息流通的难度，以及降低产品、设计师、开发、测试之间的知识和职责边界。</p>\n<p>三是可能会出现以 AI 编程为中心的产品研发一体化平台，无论是产品、设计师、开发、测试，都可以围绕这个平台进行符合自己角色的参与，例如产品编写需求文档和验收文档时，可以借助 AI 平台检测需求是否合理、调整设计、细化需求，转化为开发人员便于阅读的文档。产品、设计师通过需求文档借助 AI 平台快速实现产品原型设计。开发人员则可以将需求导入 AI，创建开发任务，逐步与 AI 协作编写代码，还可以借助 AI 快速生成单元测试、集成测试等，最后根据验收文档验证检测最终输出。</p>\n<br />\n<p>而关于团队的研发流程，则会在 <a href=\"#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B\" rel=\"noopener nofollow\">工作流程</a> 一节详细讲解。</p>\n<h3 id=\"总结\">总结</h3>\n<p>作为文章的第二部分，主要是思考怎么建设使用 AI 辅助编程的团队，以及进行团队协作。</p>\n<p>第三部分将会讲解实战，从原型到产品的整个环节，每个步骤应该做什么。</p>\n<h2 id=\"实战\">实战</h2>\n<p>作为文章的第三部分内容，将会以短链接服务为案例，去讲解如何落地实战团队协作和 AI 编程，去设计一个以 AI 辅助开发为核心的项目研发流程。</p>\n<h3 id=\"思考整体架构\">思考整体架构</h3>\n<p>AI 时代，还需要我去设计项目？</p>\n<p>是的，开发人员要自行对技术方案进行调研、上手尝试，然后让 AI 根据你的方案和设计做出来，不要都让 AI 帮你做解决方案。</p>\n<br />\n<p>只需要一个 idea，AI 就可以写代码，可是 AI 写出来的东西可能跟专业人员需要的东西相去甚远，并且还有大量细节满足不了需求，开发人员可能会陷入跟 AI 的对骂中，不断调整提示词，不断等待 AI 生成，最终生成了 90% 的内容，剩下的 10%，开发人员只能对着键盘一点点敲完。</p>\n<p><br />虽然是 AI 编程，但是编程的重点是人的想法和需求，而不是 AI 的天马行空，虽然 AI 实现的项目可能有很多很好的创意和想法，质量可能很好，但是对于专业领域和公司业务项目来说，需求的中心是公司对应的业务，而不是一个 idea。</p>\n<p>而且 AI 有上下文限制，有幻觉等，你只需要一个简单的一个登录功能，可能 AI 把单点登录、OAuth2 等一堆东西给你加上去了（目标偏移）。</p>\n<br />\n<p>所以即使在 AI 时代，我们也要构思项目设计和功能实现的算法和逻辑，让 AI 在这个边界内实现代码和测试。</p>\n<blockquote>\n<p>当然，我们可以利用 AI 去帮助我们验证想法和构思设计，然后将这些内容生成为架构设计和技术方案。</p>\n</blockquote>\n<h4 id=\"短链接服务的架构\">短链接服务的架构</h4>\n<p>由于本文的重点不是怎么实现一个短链接服务，所以笔者这里只简单讲解这个服务的一些算法和实现思路，以便后续使用 AI 写出符合需求的代码。</p>\n<br />\n<p>核心问题1：短链接生成和还原</p>\n<p>核心问题2：短链接存储和查找</p>\n<br />\n<p>笔者的设计是，短链接跟长链接无直接映射关系，也就是不能通过算法转换直接将长链接生成短链接。</p>\n<p>对于每个长链接，创建记录存到数据库时会使用 int64 雪花做主键，存到数据库。</p>\n<p>例如新增一个长链接 <code>https://whuanle.cn</code>，当前雪花id 是 2009194627277520896，经过检查数据库没有重复数据后存储到数据库。</p>\n<p>接着，将雪花 id 使用 base62 生成短链接编码。之所以使用 base62 做缩短编码，是因为<code>[0-9]</code>、<code>[a-z]</code>、<code>[A-Z]</code> 刚刚好是 62 个字符，能够在不使用特殊符号的情况下，使用数字和大小写字母表达值，也就是相当于 62 进制。将 2009194627277520896 使用 base62 编码是得出字符串是 <code>00E3uWKkzx</code>，而存数据只需要一个 int64 就行。</p>\n<p>满足能够将长链接生成短链接，存储空间小，不容易被人碰撞规则的需求。</p>\n<p><img alt=\"image-20260108174726106\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce3d2b7d.png\" /></p>\n<br />\n<p>有了短链接生成和还原算法后，我们继续聊一下数据查找。</p>\n<p>用户访问 <code>/00E3uWKkzx</code> 时，还原得到 2009194627277520896，然后通过这个 id 从数据库查找数据得到  <code>https://whuanle.cn</code>，然后让用户跳转到这个地址即可。</p>\n<p>每次都要查数据库，数据库压力大，而且万一被人攻击，机器人随机拼接的字符串，也要到数据库检查数据在不在，数据库迟早被打崩。</p>\n<br />\n<p>所以第一层是使用 redis 的布隆过滤器，先判断数据是否存在。</p>\n<p><img alt=\"image-20260108180240039\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce4030ea.png\" /></p>\n<br />\n<p>第二层，数据分片，将数据存储到 redis，避免频繁从数据库查找。还可以做本地离线缓存，避免高频度从 redis 查找数据。</p>\n<p>也就是最终是三层缓存。</p>\n<br />\n<p>其它就不多说了，前面提到的算法处理逻辑，将会作为后续 AI 编写功能的依据。</p>\n<h3 id=\"模板项目\">模板项目</h3>\n<p>首选是安装笔者提供的项目模板。</p>\n<pre><code class=\"language-bash\">dotnet new install Maomi.AiSpec.Templates\n</code></pre>\n<p><img alt=\"image-20260109092342372\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce430090.png\" /></p>\n<br />\n<p>执行命令从模板创建新的项目，将 <code>MyShortUri</code> 替换为你需要的项目名称即可。</p>\n<pre><code class=\"language-bash\">dotnet new aispec -n MyShortUri\n</code></pre>\n<p><img alt=\"image-20260109092658516\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce458366.png\" /></p>\n<h3 id=\"后端开发原则和规范\">后端开发原则和规范</h3>\n<p>使用 Kiro 打开 <code>MyShortUri</code> 项目， 在 AGENT STEERING 菜单可以看到已经模板自带四个 steering 文件。</p>\n<p><img alt=\"image-20260109224727421\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce47b764.png\" /></p>\n<br />\n<p>Kiro 通过 <code>.kiro/steering/</code> 目录中的 markdown 文件设置项目约束规定，Kiro 只规定了 product.md、tech.md、structure.md 三个文件，这些基础文件默认包含在每次交互中，形成 Kiro 项目理解的基线。</p>\n<p>cqrs-conventions.md 则是笔者的模板项目的开发规则，约定 AI 生成的代码文件如何存放以及代码格式约束，AI 会生成符合项目要求的代码。</p>\n<p>读者也可以创建一些其它文件，例如 <code>rest-api.md</code> 要求生成的 api 接口需要符合 restapi 格式，将公司研发规范等写入到  <code>.kiro/steering/</code> 目录。</p>\n<h4 id=\"后端代码规范约束\">后端代码规范约束</h4>\n<p><code>cqrs-conventions.md</code> 是笔者自定义的 steering 文件，用来约束 AI 生成的代码必须分为 <code>Command/Query</code>、<code>Api</code>、<code>Handler</code> 三层的 CQRS 结构。</p>\n<p>定义了很多约束规范，这里就不讲解了，自己研究一下。</p>\n<p><img alt=\"image-20260109094451145\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce4a8557.png\" /></p>\n<h4 id=\"我要做什么样的产品\">我要做什么样的产品</h4>\n<p>拉取项目模板后，第一步是修改 <code>product.md</code>，告诉 AI 这是一个短链接项目。</p>\n<blockquote>\n<p><strong>产品概述</strong> (<code>product.md</code>) - 定义产品的目的、目标用户、关键功能和业务目标。</p>\n</blockquote>\n<br />\n<p>一般来说，<strong>产品概述</strong> (<code>product.md</code>)  是产品经理的工作任务，开发只需要将产品经理写的文档拷贝到代码项目即可，不过既然现在没有产品经理，我们可以一句话描述核心需求，让 Kiro 帮助我们生成具体的产品说明，等 AI 生成后，根据实际情况，调整好 <code>product.md</code> 文件。</p>\n<p><img alt=\"image-20260109094959587\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce63e3a9.png\" /></p>\n<br />\n<p>生成效果：</p>\n<p><img alt=\"image-20260109094351449\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce66b451.png\" /></p>\n<h4 id=\"项目结构\">项目结构</h4>\n<p><strong>项目结构</strong> (<code>structure.md</code>) - 概述文件组织、命名约定、导入模式和架构决策，这确保生成的代码无缝融入现有的代码结构里面，这样 AI 不会乱创建文件以及随便找个地方塞代码。</p>\n<p><code>structure.md</code> 要常常随着项目的进展而更新，不要一直不变，每次新增模块后，都要重新生成 <code>structure.md</code> 。</p>\n<p>现在我们点击 Kiro 的 <code>Refine</code> ，重新生成 <code>structure.md</code> 。</p>\n<p><img alt=\"image-20260109095652954\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce6eb390.png\" /></p>\n<h4 id=\"技术约定\">技术约定</h4>\n<p><strong>技术栈</strong> (<code>tech.md</code>) - 记录这个项目选择的框架、库、开发工具和技术约束，当 Kiro 建议实现方案时，它会优先选择已建立的技术栈而非替代方案。</p>\n<p>突然想起来一个事情，某同事写 Go 语言程序需要解决一个打印功能，结果用 AI 写的代码引入了 python，后面被我发现了，我要求用 Go 重写，然后某同事又用 AI 劈里啪啦写了一天，总算用 Go 写出来了。</p>\n<p>所以 <code>tech.md</code> 可以避免这种情况，不然 AI 为了实现一个功能，到处查资料，然后引入一堆依赖，用一种意想不到的方式去实现功能。</p>\n<br />\n<p>现在我们点击 Kiro 的 <code>Refine</code> ，重新生成 <code>tech.md</code> ，例如这个项目使用的技术栈如下：</p>\n<pre><code>| Category | Technology |\n|----------|------------|\n| Framework | ASP.NET Core 9 |\n| Language | C# 12 (nullable reference types enabled) |\n| ORM | Entity Framework Core 9 (MySQL) |\n| CQRS | MediatR |\n| Validation | FluentValidation |\n| Authentication | JWT Bearer tokens |\n| Logging | Serilog |\n| API Docs | OpenAPI/Swagger with Scalar UI |\n| Caching | Redis (StackExchange.Redis) |\n| Module System | Maomi.Core |\n| Code Analysis | StyleCop.Analyzers |\n</code></pre>\n<br />\n<p>作者还可以加上接口设计约定等文件，这里不再赘述。</p>\n<h3 id=\"数据库设计\">数据库设计</h3>\n<p>现在出现了一个使用 AI 做数据库设计方案的方向：Text2SQL。</p>\n<p>Text2SQL 指将自然语言转换为 SQL 的技术，当一个项目从零开始时，我们可以借助 AI 构思、设计项目架构，最后总结输出 SQL，这一步其实比较简单。</p>\n<p>但是后续项目迭代后，需要 AI 去了解整个数据库表结构，需要 AI 根据业务情况设计新的索引、字段约束等，这就要求 AI 需要挖掘数据价值，应对复杂的分析任务，才能给出合理的数据库变更建议。</p>\n<p>可以借助专业的 AI DB 客户端去做，例如 Chat2DB、也可以在 Kiro 装上 MCP 工具读取数据库，由于不是本文重点，这里只讲解思路，就不再详细讲述。</p>\n<br />\n<p>数据库创建表，以便后续实战让 AI 写代码。</p>\n<pre><code class=\"language-csharp\">CREATE TABLE `short_url` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '唯一ID（对应短链接编码）',\n  `long_url` varchar(2048) NOT NULL COMMENT '原始长链接',\n  `hash` varbinary(32) NOT NULL COMMENT '网址哈希值，方便对比',\n  `create_user_id` int(11) NOT NULL DEFAULT 0 COMMENT '创建人',\n  `create_time` datetime NOT NULL DEFAULT current_timestamp() COMMENT '创建时间',\n  `update_user_id` int(11) NOT NULL DEFAULT 0 COMMENT '更新人',\n  `update_time` datetime NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp() COMMENT '更新时间',\n  `is_deleted` bigint(20) NOT NULL DEFAULT 0 COMMENT '软删除',\n  PRIMARY KEY (`id`),\n  KEY `short_url_hash_index` (`hash`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='短链接'\n\nCREATE TABLE `user` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '用户ID',\n  `user_name` varchar(50) NOT NULL COMMENT '用户名',\n  `email` longtext NOT NULL COMMENT '邮箱',\n  `password` varchar(255) NOT NULL COMMENT '密码',\n  `nick_name` varchar(50) NOT NULL COMMENT '昵称',\n  `password_salt` varchar(255) NOT NULL COMMENT '计算密码值的salt',\n  `is_deleted` bigint(20) NOT NULL COMMENT '软删除',\n  `create_user_id` int(11) NOT NULL COMMENT '创建人',\n  `create_time` datetime NOT NULL DEFAULT utc_timestamp() COMMENT '创建时间',\n  `update_user_id` int(11) NOT NULL COMMENT '最后修改人',\n  `update_time` datetime NOT NULL DEFAULT utc_timestamp() COMMENT '最后更新时间',\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `users_user_name_is_deleted_uindex` (`user_name`,`is_deleted`)\n) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT='用户'\n\n</code></pre>\n<p><img alt=\"image-20260109110209561\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce719e9b.png\" /></p>\n<br />\n<p>修改数据库链接，启动 MysqlScaffold 还原数据库生成代码，将 <code>Database</code> ，目录的文件放到对应位置接口。</p>\n<blockquote>\n<p>您看，有个项目模板多重要，很多时候省下大量的时间，例如笔者这个模板，先设计数据库，然后定制数据库生成代码的步骤，使得生成的实体结构和 数据库上下文类能够符合业务需求，大大提高开发效率。</p>\n</blockquote>\n<p><img alt=\"6f39e423-8590-4a90-bee3-cd0f34a55caa\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce862a66.png\" /></p>\n<h3 id=\"ui-原型设计\">UI 原型设计</h3>\n<p>UI 原型设计就是 UI 设计师根据产品原型设计页面的过程，随着专业的原型设计工具支持 AI 后，产品经理跟 UI 设计师更好地协作，有时候一句话就可以生成一个不错的界面。产品经理可以借助这些 AI 工具快速实现原型页面。</p>\n<p><img alt=\"image-20260109103159002\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce998d33.png\" /></p>\n<br />\n<p>由于墨刀的 AI 需要付费使用，太贵了，本来想演示 AI 根据设计稿生成页面代码的，最后放弃了，用这钱买了几盒凤爪。</p>\n<p>本节意思读者理解就行，就是说目前市面上有很多 AI 生成设计稿的产品，产品可以不用化那么多时间画原型了，可以专心思考产品设计和流程逻辑，最大程度输出文档，把画原型这些事情留给 AI，然后还可以进一步转换为 UI，设计师也可以借助 AI 快速实现初版界面。</p>\n<h3 id=\"后端开发实战\">后端开发实战</h3>\n<p>基于项目模板和 steering，其实一句话需求，就可以让 Kiro 给我们编写一个模块的后端代码。</p>\n<p>但是 Kiro 提出了 Specs，让团队协作和开发人员早点下班的神器。</p>\n<br />\n<p>Specs 弥合了概念产品需求和技术实施细节之间的差距，确保了一致性并减少了开发迭代。Specs 提供了一种系统化的方法，将需求和想法转化为详细的实施计划，生成验收标准、技术实现和代码生成及测试验收计划。</p>\n<br />\n<p>接下来，我们将会使用 Kiro Specs 生成某个功能的工作流程，最后根据方案生成代码并通过测试验收代码。</p>\n<h4 id=\"实现创建短链接的接口\">实现创建短链接的接口</h4>\n<p>要做短链接服务，第一步是实现一个长链接转短链接的功能，Kiro Specs 要求编码之前需要按照三阶段工作流程进行：需求 → 设计 → 实施。</p>\n<p>在 Kiro 面板中，点击 <strong>Specs</strong> 下的 <code>+</code> 按钮，或者从聊天面板中选择 <strong>Spec</strong>，在对话框内输入要做的功能。</p>\n<pre><code>Create Spec: 实现创建短链接的接口，创建的数据存储到 ShortUrlEntity，使用雪花id赋值id，将长地址使用 SHA-256 生成 32 字节存储到 hash 字段。插入数据时要判断数据库是否存在对应的数据。\n</code></pre>\n<p><img alt=\"a91e33a4-6ae2-48cc-af68-25cffb5a7898\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ceaa6b59.png\" /></p>\n<p><img alt=\"382cf906-3618-4d01-abc1-5b3f6bb39e51\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cebdc604.png\" /></p>\n<br />\n<p>按照引导操作后，最后一步会显示 “保留可选任务（更快完成 MVP）”、“全部设为必需（完整测试覆盖）”。</p>\n<p>如果选择了 “全部设为必需（完整测试覆盖）”，AI 在任务列表加上基于属性的测试要求和执行步骤，Kiro 从提出的需求中提取 <code>属性</code> 并生成测试用例，以便确保 AI 生成的代码符合开发者的意图。</p>\n<blockquote>\n<p>Kiro 文档里面解释 <code>属性</code>：对于任何一组具有特定前提条件的输入，某些预期行为是成立的。Kiro 从格式化的需求中提取属性 (例如，“THE System SHALL 允许经过认证的用户查看活动车辆列表”)，确定哪些属性可以进行逻辑测试，然后在你选择运行它们时生成数百或数千个随机测试用例。</p>\n</blockquote>\n<br />\n<p><img alt=\"image-20260109115335208\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cec13d1c.png\" /></p>\n<p><br />结果一顿操作后，需求已经下发到 <code>.kiro/specs/create-short-url</code>，Kiro 会生成三个关键文件，这三个文件构成一个 spec。</p>\n<ul>\n<li><strong>requirements.md</strong>：使用结构化的 EARS 记号法捕获用户故事和验收标准</li>\n<li><strong>design.md</strong>：记录技术架构、序列图和实施考虑因素</li>\n<li><strong>tasks.md</strong>：提供详细的实施计划，包含离散的、可跟踪的任务</li>\n</ul>\n<p><img alt=\"image-20260109115648904\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ced335de.png\" /></p>\n<br />\n<p>我们可以先编辑 requirements.md、design.md，确保 AI 生成的具体业务要求和测试说明、技术栈和实现思路符合我们的要求，最后打开 tasks.md 文件，点击任务旁边的 <code>Start task</code> 按钮开始实现代码。</p>\n<p><img alt=\"0dce46d5-0884-4a84-ac8d-792c18df1988\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ced5de13.png\" /></p>\n<br />\n<p>由于创建短链接存储代码时，使用的算法跟我的设想相差甚远，所以这里也可以重新生成代码，不过建议好好写  requirements.md、design.md，不要在对话框调整代码逻辑，只有沉淀的文档才是最重要的。</p>\n<p><img alt=\"image-20260109130137218\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ceeab3a2.png\" /></p>\n<h3 id=\"工作流程\">工作流程</h3>\n<p>前面利用做 ”实现创建短链接的接口“ 这个需求，上手了 Kiro specs，各位应该大概了解怎么玩了，但是回到团队协作下，我们要好好构思，研发团队应该怎么做 specs。</p>\n<p>所以，在这一节中，讲解 Kiro specs 的一些概念，以便回答 [核心执行：AI 驱动的协作流程设计](#核心执行：AI 驱动的协作流程设计) 中提到的问题，怎么指定新的团队研发流程。</p>\n<h4 id=\"需求阶段\">需求阶段</h4>\n<p>Kiro specs 提出在需求阶段用结构化 EARS 表示法定义用户故事和验收标准，也就是 requirements.md 应该撰写的内容。requirements.md 文件以用户故事的形式写成，其中包含 EARS 表示法中的验收标准。</p>\n<p>核心工作如下：</p>\n<ul>\n<li>定义用户故事</li>\n<li>编写验收标准</li>\n<li>采用 EARS 符号进行需求规范</li>\n</ul>\n <br />\n<p>与传统需求编写方法的比较，EARS 符号表示需求有很大优势，尤其在使用 AI 编程时。</p>\n<table>\n<thead>\n<tr>\n<th><strong>方面</strong></th>\n<th><strong>传统要求</strong></th>\n<th><strong>EARS 符号</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>明晰</strong></td>\n<td>通常含糊不清或冗长</td>\n<td>简洁明了</td>\n</tr>\n<tr>\n<td><strong>标准化</strong></td>\n<td>不同团队之间存在很大差异</td>\n<td>所有需求的统一语法</td>\n</tr>\n<tr>\n<td><strong>易于理解</strong></td>\n<td>对于非技术利益相关者来说很困难</td>\n<td>所有利益相关者都能轻松理解</td>\n</tr>\n<tr>\n<td><strong>可追溯分析仪</strong></td>\n<td>维持起来很困难</td>\n<td>通过结构化语法增强可追溯性</td>\n</tr>\n</tbody>\n</table>\n <br />\n<p>回想我们公司，产品经理使用飞书文档写产品文档，逻辑混乱，阅读起来非常头疼，常常在编码开发过程中反复调整，在提测和验收环节存在各种各样的问题，上线后一堆 bug。研发流程和上线后的软件，存在各种各样大大小小的问题，研发怪产品经理文档写得烂，产品经理怪研发没有理解需求，测试人员喷研发代码写得烂。</p>\n<p>而领导们忙着定制各种 ”规范“，要求产品经理写产品文档要按照某些排版格式去做，做了各种研发流程要求，试图通过指定一系列所谓的规范去彻底解决研发团队内的问题。</p>\n <br />\n<p>EARS 表示法，做了深入了解后，发现这个真的很适合团队落地，既可以解决产品经理到编码、提测后的一些常见问题，又能便于 AI 理解需求。AI 可以很容易基于 EARS 需求表示法，生成对应的编码需求和测试用例，确保代码的逻辑跟需求一致，并且生成对应的验收文档，作为最终代码验收依据。</p>\n<p>requirements.md 文档分为多个部分，其中 <strong>Requirements</strong> 部分应当使用 EARS 表示法编写，每个 requirement 都遵循以下模式。</p>\n<pre><code>当[条件/事件]发生时\n系统应[预期行为]\n</code></pre>\n<p><img alt=\"image-20260110103653398\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ceed27e6.png\" /></p>\n <br />\n<h4 id=\"设计阶段\">设计阶段</h4>\n<p>design.md 文件是记录技术架构、顺序图和实现注意事项的地方，通过 design.md 可以全面了解系统的工作方式，包括组件及其交互。Kiri 的规范为设计文档提供了一种结构化的方法，使得理解和协作复杂系统变得更加容易。</p>\n<p>设计阶段可以参考前面 AI 生成的 <code>design.md</code>，包括了下图中的内容。</p>\n<p><img alt=\"image-20260110104444972\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cef01f19.png\" /></p>\n<h4 id=\"实施阶段\">实施阶段</h4>\n<p>tasks.md 文件提供了一个详细的实施计划，其中包含离散的、可跟踪的任务和子任务。每个任务都有明确的定义，包括清晰的描述、预期结果以及任何必要的资源或依赖关系。每个步骤都可以点击，AI 执行任务后会实时刷新到 task.md 里面。</p>\n<p>AI 生成的 task.md 如下，基本也是分三步：分解任务、定义任务输出结果、设置任务上下依赖关系，当然最后可能还有验收实施的说明</p>\n<p><img alt=\"image-20260109131506945\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cef30fb1.png\" /></p>\n<h4 id=\"执行阶段\">执行阶段</h4>\n<p>其实就是我们在 task.md 手动点击各个 task 的过程，我们应该一直跟踪任务进度，随时调整更新补充 specs 三个文档，完善内容，让 AI 输出的效果更加好。</p>\n<h3 id=\"使用-hook-自动构建单元测试\">使用 Hook 自动构建单元测试</h3>\n<p>当在工作空间中创建、保存或删除与特定全局模式匹配的文件时，会触发文件钩子。这些钩子需要一个模式数组来指定要监视的文件。</p>\n <br />\n<p>这里笔者使用单元测试来做 Hook，让读者了解 Kiro 的 Hook 怎么玩。</p>\n<p>单元测试是项目最重要的一部分，由于笔者这个模板项目采用整洁架构，所以做测试也是相当简单。</p>\n<p>主要分为三部分考虑：</p>\n<p>对于无业务相关的框架、工具、算法代码，单独设计验证的单元测试即可。</p>\n<p>对于模型类，要验证 api 请求时参数限制，要识别字段长度范围等规则。</p>\n<p>对于 Api、Handler 可以一起测，编写集成测试，直接使用 TestWebHost 模拟请求。</p>\n <br />\n<p>现在我们编写对应的提示词，让 Kiro 生成 Hook。</p>\n<pre><code>给 Api 编写单元测试，使用 EFCore内存数据库模拟，redis 使用mock替代对于无业务相关的框架、工具、算法代码，单独设计验证的单元测试即可。对于模型类，要验证 api 请求时参数限制，要识别字段长度范围等规则。对于 Api、Handler 可以一起测，编写集成测试，直接使用 TestWebHost 模拟请求。\n</code></pre>\n<p><img alt=\"image-20260109125656799\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf060a20.png\" /></p>\n<p><img alt=\"image-20260109130251045\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf08c52b.png\" /></p>\n<p><img alt=\"image-20260109125731788\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf0b69fe.png\" /></p>\n <br />\n<p>现在使用 spec 加入新的功能：</p>\n<pre><code>Create Spec: 用户访问短链接后，先将短链接使用 base62 还原为雪花id，先从redis布隆过滤器里面过滤，如果查找不到则直接404。然后从 key `short_url:{雪花id}` 里面找，找不到就查数据库然后重新塞到 redis（超时时间30分钟）。\n</code></pre>\n <br />\n<p>创建完成后，执行 Task list。</p>\n <br />\n<p>可以等待 Hook 自动触发，或在对话界面告诉 AI 手动执行 <code>api-unit-test-gen.kiro.hook</code>。</p>\n<p>最终生成单元测试如下：</p>\n<p><img alt=\"image-20260109134821100\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf234a04.png\" /></p>\n<p><img alt=\"image-20260109135655490\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf25f739.png\" /></p>\n<h3 id=\"最后\">最后</h3>\n<p>希望可以通过本文帮助您在公司内建设一个 AI 辅助编程团队。</p>\n\n</div>\n<div id=\"MySignature\">\n    痴者工良(https://whuanle.cn)\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-12 08:34</span>&nbsp;\n<a href=\"https://www.cnblogs.com/whuanle\">痴者工良</a>&nbsp;\n阅读(<span id=\"post_view_count\">304</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Java中线程安全问题的原因和解决方案",
      "link": "https://www.cnblogs.com/xi-yongqi/p/19468853",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xi-yongqi/p/19468853\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 23:41\">\n    <span>Java中线程安全问题的原因和解决方案</span>\n    \n\n</a>\n\n\t\t</h1>\n\t\t<div class=\"clear\"></div>\n\t\t<div class=\"postBody\">\n\t\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h3 id=\"线程安全问题的核心原因\">线程安全问题的核心原因</h3>\n<ul>\n<li>线程安全问题本质是多个线程并发访问共享且可变的资源时，操作的原子性、可见性或有序性被破坏，导致程序执行结果不符合预期。</li>\n</ul>\n<ol>\n<li>根本原因：共享可变资源</li>\n</ol>\n<ul>\n<li>共享资源：多个线程都能访问到的资源（如成员变量、静态变量、共享内存区域）；</li>\n<li>可变资源：资源的状态（值）可以被修改（如int计数器、HashMap的元素）；</li>\n<li>经典的i++ 操作。它在底层分为“读取-修改-写入”三个步骤。如果两个线程同时读取 i=1，各自加1后写回，结果是2而不是3。</li>\n</ul>\n<ol start=\"2\">\n<li>直接原因：三大特性被破坏</li>\n</ol>\n<ul>\n<li>Java内存模型（JMM）定义的多线程并发三大核心特性，任何一个被破坏都会引发线程安全问题：\n<ul>\n<li>原子性：一个操作（如count++）包含“读 - 改 - 写”三步，非原子操作会被多线程交错执行；</li>\n<li>可见性：线程修改共享变量后，不会立即同步到主内存，其他线程读取的仍是旧值；</li>\n<li>有序性：JVM的指令重排序优化，会导致多线程下执行顺序混乱（如未加volatile的双重检查锁单例）。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"线程安全问题的解决方案\">线程安全问题的解决方案</h3>\n<ul>\n<li>核心思路：要么避免共享可变资源（从根源消除问题），要么控制并发访问规则（保证三大特性）。</li>\n</ul>\n<h5 id=\"方案1避免共享可变资源优先推荐\">方案1：避免共享可变资源（优先推荐）</h5>\n<ul>\n<li>栈封闭（局部变量）：局部变量存储在线程私有栈中，每个线程有独立副本，天然线程安全。</li>\n</ul>\n<pre><code class=\"language-java\">public class StackClosedDemo {\n    // 每个线程调用该方法时，都会创建独立的count副本\n    public void calculate() {\n        int count = 0;\n        count++; // 无线程安全问题\n        System.out.println(Thread.currentThread().getName() + \": \" + count);\n    }\n\n    public static void main(String[] args) {\n        StackClosedDemo demo = new StackClosedDemo();\n        // 10个线程各自操作自己的局部变量\n        for (int i = 0; i &lt; 10; i++) {\n            new Thread(demo::calculate, \"Thread-\" + i).start();\n        }\n    }\n}\n</code></pre>\n<ul>\n<li>不可变对象:对象创建后状态不可修改（如String、Integer），即使共享也无法修改值。</li>\n</ul>\n<pre><code class=\"language-java\">// 自定义不可变类（final类+final成员变量+无setter）\npublic final class ImmutableUser {\n    private final String name;\n    private final int age;\n\n    public ImmutableUser(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    // 仅提供getter，无setter\n    public String getName() { return name; }\n    public int getAge() { return age; }\n}\n</code></pre>\n<ul>\n<li>ThreadLocal（线程本地存储）:为每个线程提供独立的变量副本，线程操作自身副本，互不干扰。</li>\n</ul>\n<pre><code class=\"language-java\">public class ThreadLocalDemo {\n    // 每个线程有独立的Integer副本，初始值为0\n    private static ThreadLocal&lt;Integer&gt; threadLocal = ThreadLocal.withInitial(() -&gt; 0);\n\n    public void increment() {\n        threadLocal.set(threadLocal.get() + 1);\n        System.out.println(Thread.currentThread().getName() + \": \" + threadLocal.get());\n    }\n\n    public static void main(String[] args) {\n        ThreadLocalDemo demo = new ThreadLocalDemo();\n        // 3个线程各自操作自己的副本\n        for (int i = 0; i &lt; 3; i++) {\n            new Thread(() -&gt; {\n                for (int j = 0; j &lt; 2; j++) {\n                    demo.increment();\n                }\n            }, \"Thread-\" + i).start();\n        }\n    }\n}\n// 输出（顺序可能不同）：\n// Thread-0: 1、Thread-0: 2\n// Thread-1: 1、Thread-1: 2\n// Thread-2: 1、Thread-2: 2\n</code></pre>\n<h4 id=\"方案2同步加锁控制并发访问\">方案2：同步/加锁（控制并发访问）</h4>\n<h5 id=\"互斥同步阻塞同步这是最常见的方案通过加锁来保证同一时刻只有一个线程操作资源\">互斥同步（阻塞同步）:这是最常见的方案，通过加锁来保证同一时刻只有一个线程操作资源。</h5>\n<ul>\n<li>synchronized 关键字：Java 原生支持，使用简单。可修饰方法或代码块。属于不可中断的锁。</li>\n</ul>\n<pre><code class=\"language-java\">public class SynchronizedDemo {\n    private int count = 0;\n\n    // 同步实例方法，锁是this对象\n    public synchronized void increment() {\n        count++;\n    }\n\n    public static void main(String[] args) throws InterruptedException {\n        SynchronizedDemo demo = new SynchronizedDemo();\n        // 1000个线程执行increment\n        for (int i = 0; i &lt; 1000; i++) {\n            new Thread(demo::increment).start();\n        }\n        Thread.sleep(1000);\n        System.out.println(\"最终count：\" + demo.count); // 输出1000\n    }\n}\n</code></pre>\n<h5 id=\"reentrantlock显式锁比synchronized灵活可中断可超时公平锁需手动释放锁必须在finally中\">ReentrantLock（显式锁）:比synchronized灵活（可中断、可超时、公平锁），需手动释放锁（必须在finally中）。</h5>\n<pre><code class=\"language-java\">import java.util.concurrent.locks.Lock;\nimport java.util.concurrent.locks.ReentrantLock;\n\npublic class ReentrantLockDemo {\n    private int count = 0;\n    private Lock lock = new ReentrantLock(); // 默认非公平锁\n\n    public void increment() {\n        lock.lock(); // 加锁\n        try {\n            count++;\n        } finally {\n            lock.unlock(); // 释放锁，避免死锁\n        }\n    }\n\n    public static void main(String[] args) throws InterruptedException {\n        ReentrantLockDemo demo = new ReentrantLockDemo();\n        for (int i = 0; i &lt; 1000; i++) {\n            new Thread(demo::increment).start();\n        }\n        Thread.sleep(1000);\n        System.out.println(\"最终count：\" + demo.count); // 输出1000\n    }\n}\n</code></pre>\n<h4 id=\"方案3volatile关键字保证可见性有序性\">方案3：volatile关键字（保证可见性/有序性）</h4>\n<ul>\n<li>保证可见性：强制失效工作内存，直接读写主内存。</li>\n<li>保证有序性：禁止指令重排序。</li>\n<li>注意：它不保证原子性（不能解决i++问题）。</li>\n</ul>\n<pre><code class=\"language-java\">public class VolatileDemo {\n    private volatile boolean stop = false; // 保证可见性和有序性\n\n    public void runThread() {\n        new Thread(() -&gt; {\n            int i = 0;\n            while (!stop) { // 能立即感知stop的修改\n                i++;\n            }\n            System.out.println(\"线程停止，i=\" + i);\n        }).start();\n    }\n\n    public static void main(String[] args) throws InterruptedException {\n        VolatileDemo demo = new VolatileDemo();\n        demo.runThread();\n        Thread.sleep(3000);\n        demo.stop = true; // 修改后，线程立即停止\n    }\n}\n</code></pre>\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-11 23:41</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xi-yongqi\">我会替风去</a>&nbsp;\n阅读(<span id=\"post_view_count\">42</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "飞书 .NET SDK 事件处理的幂等性与去重机制",
      "link": "https://www.cnblogs.com/mudtools/p/19469184",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/mudtools/p/19469184\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 23:00\">\n    <span>飞书 .NET SDK 事件处理的幂等性与去重机制</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>飞书事件处理过程中如何让你的应用不再\"重复劳动\"，如何用三层防护筑起安全墙，结合内存与 Redis 双重保障，让你的飞书应用稳如磐石——不再重复处理，告别混乱状态。</p>\n</blockquote>\n<hr />\n<h2 id=\"为什么需要去重\">为什么需要\"去重\"？</h2>\n<p>想象一下这样的场景：</p>\n<p>你在飞书里收到一条消息，应用收到通知后创建了待办事项。但因为网络不稳定，飞书以为你没收到，又发了一遍同样的通知——结果呢？你的应用又创建了一次待办，同一个任务出现了两次。</p>\n<p>这就是我们所说的\"重复处理\"问题。</p>\n<hr />\n<h3 id=\"什么时候会出现这种情况\">什么时候会出现这种情况？</h3>\n<p>在飞书事件驱动的世界里，以下情况都可能导致<strong>同一事件被多次送达</strong>：</p>\n<ul>\n<li>📡 <strong>网络波动</strong>：飞书服务器没收到你的确认，于是重发</li>\n<li>🔄 <strong>服务重启</strong>：内存清空，之前的事件又来了</li>\n<li>👥 <strong>多实例运行</strong>：多个实例同时收到同一事件</li>\n<li>🔌 <strong>断线重连</strong>：WebSocket 重连后可能重复消息</li>\n</ul>\n<h3 id=\"真实案例一分钟内的混乱\">真实案例：一分钟内的混乱</h3>\n<pre><code>时间线：\n─────────────────────────────────────────────────────────────\n09:00:00  飞书推送：收到一条新消息\n09:00:01  实例A 接收并处理 → 创建待办 ✅\n09:00:05  飞书没收到确认，再次推送\n09:00:06  实例B 接收并处理 → 又创建待办 ❌\n─────────────────────────────────────────────────────────────\n</code></pre>\n<p><strong>后果有多严重？</strong></p>\n<table>\n<thead>\n<tr>\n<th>问题</th>\n<th>实际影响</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>📉 数据重复</td>\n<td>用户收到两条相同的待办</td>\n</tr>\n<tr>\n<td>💰 资金损失</td>\n<td>订单重复扣款，退钱都退不完</td>\n</tr>\n<tr>\n<td>📧 骚扰用户</td>\n<td>同一条通知发十次</td>\n</tr>\n<tr>\n<td>🔄 状态混乱</td>\n<td>数据库里说\"已处理\"，实际只做了一半</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"三层防护像保安一样层层把关\">三层防护：像保安一样层层把关</h2>\n<p>Mud.Feishu SDK 设计了一套<strong>三层递进式防护机制</strong>，就像小区的三道岗哨，从外到内层层把关，确保不会有\"坏人\"（重复事件）混进来。</p>\n<div class=\"mermaid\">graph TB\n    subgraph AppLayer[\"应用层去重（业务键）\"]\n        AppHandler[\"IdempotentFeishuEventHandler&lt;T&gt;\"]\n        AppDesc[\"基于业务主键（消息ID、订单ID等）去重\"]\n    end\n\n    subgraph DispatchLayer[\"分发层去重（EventId）\"]\n        EventDedup[\"IFeishuEventDeduplicator / IFeishuEventDistributedDeduplicator\"]\n        EventDesc[\"基于飞书事件ID去重，24小时窗口期\"]\n    end\n\n    subgraph ProtocolLayer[\"协议层去重（SeqID/Nonce）\"]\n        WS_Dedup[\"WebSocket: IFeishuSeqIDDeduplicator\"]\n        Webhook_Dedup[\"Webhook: IFeishuNonceDistributedDeduplicator\"]\n        ProtoDesc[\"基于消息序列号去重，过滤重复消息\"]\n    end\n\n    AppHandler --&gt;|处理器内部业务逻辑| EventDedup\n    EventDedup --&gt;|事件路由与分发| WS_Dedup\n    EventDedup --&gt;|事件路由与分发| Webhook_Dedup\n\n    style AppLayer fill:#e1f5ff\n    style DispatchLayer fill:#fff4e1\n    style ProtocolLayer fill:#ffe1e1\n</div><h3 id=\"这三层分别负责什么\">这三层分别负责什么？</h3>\n<p>可以把这三层想象成工厂流水线上的三个质检员：</p>\n<table>\n<thead>\n<tr>\n<th>质检员</th>\n<th>检查什么？</th>\n<th>在哪检查？</th>\n<th>过滤范围</th>\n<th>适合什么时候用？</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>协议层</strong></td>\n<td>消息编号（SeqID）</td>\n<td>消息刚到达时</td>\n<td>每条消息</td>\n<td>过滤网络重复，最外层防护</td>\n</tr>\n<tr>\n<td><strong>分发层</strong></td>\n<td>事件ID（EventId）</td>\n<td>事件分发前</td>\n<td>每个事件</td>\n<td>过滤飞书重发，中间层防护</td>\n</tr>\n<tr>\n<td><strong>应用层</strong></td>\n<td>业务键（TaskId）</td>\n<td>业务处理时</td>\n<td>每次业务操作</td>\n<td>防止逻辑重复，最后一道防线</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"分发层事件的身份证检查\">分发层：事件的\"身份证检查\"</h2>\n<p>这是最核心的一层，就像门口的保安，每个进来的事件都要先出示身份证（EventId），保安查过记录后才能放行。</p>\n<pre><code class=\"language-csharp\">public interface IFeishuEventDeduplicator\n{\n    /// &lt;summary&gt;\n    /// 尝试标记事件为处理中\n    /// &lt;/summary&gt;\n    /// &lt;returns&gt;\n    /// true: 已处理或正在处理中（跳过）\n    /// false: 新事件（继续处理）\n    /// &lt;/returns&gt;\n    bool TryMarkAsProcessing(string eventId);\n\n    /// &lt;summary&gt;\n    /// 标记事件为已完成\n    /// &lt;/summary&gt;\n    void MarkAsCompleted(string eventId);\n\n    /// &lt;summary&gt;\n    /// 回滚处理中状态（异常时调用）\n    /// &lt;/summary&gt;\n    void RollbackProcessing(string eventId);\n\n    /// &lt;summary&gt;\n    /// 检查事件是否已处理\n    /// &lt;/summary&gt;\n    bool IsProcessed(string eventId);\n}\n</code></pre>\n<h3 id=\"事件的一生三种状态流转\">事件的一生：三种状态流转</h3>\n<p>每个事件在去重系统里都像过安检一样，经历三种状态的变化：</p>\n<div class=\"mermaid\">stateDiagram-v2\n    [*] --&gt; Pending: 收到新事件\n    Pending --&gt; Processing: TryMarkAsProcessing()\n    Processing --&gt; Completed: 处理成功\n    Processing --&gt; Pending: 超时/异常（Rollback）\n    Completed --&gt; Pending: 缓存过期（24小时后）\n\n    note right of Processing\n        处理中超时（默认5分钟）\n        允许重新处理\n    end note\n\n    note right of Completed\n        24小时后自动清理\n        支持TTL配置\n    end note\n</div><h3 id=\"真实场景websocket-是怎么去重的\">真实场景：WebSocket 是怎么去重的？</h3>\n<p>来看看 WebSocket 收到事件后做了什么（<code>FeishuEventMessageHandler.cs#104-147</code>）：</p>\n<pre><code class=\"language-csharp\">// 1. 去重检查\nbool isProcessing = false;\n\nif (_options.EnableDistributedDeduplication &amp;&amp; _distributedDeduplicator != null)\n{\n    // 优先使用分布式去重（Redis）\n    isProcessing = await _distributedDeduplicator.TryMarkAsProcessedAsync(\n        eventData.EventId, \n        cancellationToken: cancellationToken);\n}\nelse if (_options.EnableEventDeduplication &amp;&amp; _deduplicator != null)\n{\n    // 使用内存去重\n    isProcessing = _deduplicator.TryMarkAsProcessing(eventData.EventId);\n}\n\n// 2. 跳过已处理事件\nif (isProcessing)\n{\n    _logger.LogDebug(\"事件 {EventId} 已在处理中或已处理，跳过\", eventData.EventId);\n    return;\n}\n\n// 3. 处理事件\ntry\n{\n    await _eventHandlerFactory.HandleEventParallelAsync(\n        eventData.EventType, \n        eventData, \n        cancellationToken);\n\n    // 4. 处理成功，标记为已完成\n    if (_options.EnableEventDeduplication &amp;&amp; _deduplicator != null)\n    {\n        _deduplicator.MarkAsCompleted(eventData.EventId);\n    }\n}\ncatch (Exception ex)\n{\n    // 5. 处理失败，回滚状态\n    if (_options.EnableEventDeduplication &amp;&amp; _deduplicator != null)\n    {\n        _deduplicator.RollbackProcessing(eventData.EventId);\n    }\n    throw;\n}\n</code></pre>\n<h3 id=\"方案一内存去重适合开发和小规模应用\">方案一：内存去重（适合开发和小规模应用）</h3>\n<p><strong>就像在大脑里记笔记</strong>：把每个事件ID记在内存里，收到新事件时先查查笔记，如果有就跳过。</p>\n<p><strong>特点</strong>：</p>\n<ul>\n<li>🧠 <strong>全靠脑子记</strong>：所有数据存在内存里</li>\n<li>⏰ <strong>记24小时</strong>：超过时间就自动忘掉</li>\n<li>🧹 <strong>定期打扫</strong>：每5分钟清理一次过期的记录</li>\n<li>⏱️ <strong>超时保护</strong>：处理超过5分钟还没完成，就允许重试</li>\n</ul>\n<p><strong>核心代码</strong>（<code>FeishuEventDeduplicator.cs#86-135</code>）：</p>\n<pre><code class=\"language-csharp\">public bool TryMarkAsProcessing(string eventId)\n{\n    lock (_lock)\n    {\n        // 检查是否已存在\n        if (_eventCache.TryGetValue(eventId, out var entry))\n        {\n            // 如果已处理，返回 true（跳过）\n            if (entry.Status == DeduplicationStatus.Completed)\n                return true;\n\n            // 如果已在处理中，检查是否超时\n            if (entry.Status == DeduplicationStatus.Processing)\n            {\n                if (DateTimeOffset.UtcNow - entry.ProcessedAt &gt; _processingTimeout)\n                {\n                    // 处理中超时，允许重新处理\n                    _logger?.LogWarning(\"事件 {EventId} 处理中超时，允许重新处理\", eventId);\n                    _eventCache.Remove(eventId);\n                    // 继续处理\n                }\n                else\n                {\n                    // 仍在处理中，跳过\n                    return true;\n                }\n            }\n        }\n\n        // 标记为处理中\n        _eventCache[eventId] = new EventCacheEntry\n        {\n            ProcessedAt = DateTimeOffset.UtcNow,\n            EventId = eventId,\n            Status = DeduplicationStatus.Processing\n        };\n\n        return false; // 未处理，新事件\n    }\n}\n</code></pre>\n<h3 id=\"方案二redis-分布式去重生产环境首选\">方案二：Redis 分布式去重（生产环境首选）</h3>\n<p><strong>就像共享笔记本</strong>：用 Redis 把去重记录记在外部，所有实例都能查到。就算重启服务，笔记本还在，不会忘记。</p>\n<p><strong>特点</strong>：</p>\n<ul>\n<li>📒 <strong>写在共享笔记本上</strong>：所有实例一起看</li>\n<li>🔄 <strong>自动翻页</strong>：到期自动清理，不用管</li>\n<li>🤝 <strong>大家一起用</strong>：多实例部署没问题</li>\n<li>⚡ <strong>原子操作</strong>：SETNX + EXPIRE 确保不会写错</li>\n</ul>\n<p><strong>核心代码</strong>（<code>RedisFeishuEventDistributedDeduplicator.cs#53-89</code>）：</p>\n<pre><code class=\"language-csharp\">public async Task&lt;bool&gt; TryMarkAsProcessedAsync(\n    string eventId, \n    TimeSpan? ttl = null, \n    CancellationToken cancellationToken = default)\n{\n    var actualTtl = ttl ?? _defaultCacheExpiration;\n    var redisKey = $\"{_keyPrefix}{eventId}\";\n\n    // 使用 SETNX + EXPIRE 实现原子性去重\n    // 仅当键不存在时设置，并设置过期时间\n    var setResult = await _database.StringSetAsync(\n        redisKey,\n        \"1\",\n        actualTtl,\n        When.NotExists);\n\n    if (!setResult)\n    {\n        _logger?.LogDebug(\"事件 {EventId} 已处理过，跳过\", eventId);\n        return true; // 已处理\n    }\n\n    _logger?.LogDebug(\"事件 {EventId} 标记为已处理，TTL: {Ttl}\", eventId, actualTtl);\n    return false; // 未处理，新事件\n}\n</code></pre>\n<h3 id=\"两种方案怎么选\">两种方案怎么选？</h3>\n<table>\n<thead>\n<tr>\n<th>对比项</th>\n<th>内存去重（自己记）</th>\n<th>Redis 去重（共享笔记本）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>速度</strong></td>\n<td>⚡ 像闪电一样快（直接读内存）</td>\n<td>🚀 很快（但需要网络）</td>\n</tr>\n<tr>\n<td><strong>可靠性</strong></td>\n<td>⚠️ 重启就忘光</td>\n<td>✅ 永远记着，重启也还在</td>\n</tr>\n<tr>\n<td><strong>多实例</strong></td>\n<td>❌ 每个人各记各的</td>\n<td>✅ 大家一起看同一本笔记</td>\n</tr>\n<tr>\n<td><strong>麻烦程度</strong></td>\n<td>🟢 零依赖，开箱即用</td>\n<td>🟡 需要部署 Redis</td>\n</tr>\n<tr>\n<td><strong>适合谁</strong></td>\n<td>🏠 开发环境、单机运行</td>\n<td>🏢 生产环境、多实例部署</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"协议层消息的排队号检查\">协议层：消息的\"排队号检查\"</h2>\n<h3 id=\"websocket-的-seqid-是什么\">WebSocket 的 SeqID 是什么？</h3>\n<p>飞书 WebSocket 用的是 ProtoBuf 二进制协议，每条消息都带着一个<strong>递增的序号</strong>，就像去银行办事拿的排队号。通过记住已经处理过的排队号，就能在消息层面就把重复的挡在外面。</p>\n<h4 id=\"seqid-去重怎么工作\">SeqID 去重怎么工作？</h4>\n<pre><code class=\"language-csharp\">public interface IFeishuSeqIDDeduplicator\n{\n    /// &lt;summary&gt;\n    /// 尝试标记 SeqID 为已处理\n    /// &lt;/summary&gt;\n    /// &lt;returns&gt;\n    /// true: 已处理过（跳过）\n    /// false: 新消息（继续处理）\n    /// &lt;/returns&gt;\n    bool TryMarkAsProcessed(ulong seqId);\n\n    /// &lt;summary&gt;\n    /// 检查 SeqID 是否已处理\n    /// &lt;/summary&gt;\n    bool IsProcessed(ulong seqId);\n\n    /// &lt;summary&gt;\n    /// 异步检查 SeqID 是否已处理\n    /// &lt;/summary&gt;\n    Task&lt;bool&gt; IsProcessedAsync(ulong seqId);\n\n    /// &lt;summary&gt;\n    /// 清空缓存\n    /// &lt;/summary&gt;\n    void ClearCache();\n\n    /// &lt;summary&gt;\n    /// 获取缓存中的 SeqID 数量\n    /// &lt;/summary&gt;\n    int GetCacheCount();\n\n    /// &lt;summary&gt;\n    /// 获取已处理的最大 SeqID\n    /// &lt;/summary&gt;\n    ulong GetMaxProcessedSeqId();\n}\n</code></pre>\n<h4 id=\"实现机制\">实现机制</h4>\n<div class=\"mermaid\">graph LR\n    A[收到ProtoBuf消息] --&gt; B{SeqID检查}\n    B --&gt;|已处理| C[跳过消息]\n    B --&gt;|未处理| D[记录SeqID]\n    D --&gt; E[处理消息]\n    E --&gt; F[发送ACK确认]\n    D --&gt; G[更新最大SeqID]\n    G --&gt; H[定时清理过期SeqID]\n</div><h4 id=\"核心实现\">核心实现</h4>\n<p><strong>内存实现</strong>（<code>FeishuSeqIDDeduplicator.cs</code>）：</p>\n<pre><code class=\"language-csharp\">public bool TryMarkAsProcessed(ulong seqId)\n{\n    lock (_lock)\n    {\n        // 检查是否已存在\n        if (_processedSeqIds.Contains(seqId))\n        {\n            _logger?.LogDebug(\"SeqID {SeqId} 已处理过，跳过\", seqId);\n            return true; // 已处理\n        }\n\n        // 记录新 SeqID\n        _processedSeqIds.Add(seqId);\n        _seqIdTimestamps[seqId] = DateTimeOffset.UtcNow;\n\n        // 更新最大 SeqID\n        if (seqId &gt; _maxProcessedSeqId)\n        {\n            _maxProcessedSeqId = seqId;\n        }\n\n        return false; // 未处理，新消息\n    }\n}\n</code></pre>\n<p><strong>使用位置</strong>（<code>BinaryMessageProcessor.cs#186-194</code>）：</p>\n<pre><code class=\"language-csharp\">// SeqID 去重检查\nif (_seqIdDeduplicator != null &amp;&amp; _seqIdDeduplicator.TryMarkAsProcessed(frame.SeqID))\n{\n    _logger.LogDebug(\"SeqID {SeqID} 已处理过，跳过\", frame.SeqID);\n    eventArgs.SkipReason = $\"SeqID {frame.SeqID} 已处理过\";\n    BinaryMessageReceived?.Invoke(this, eventArgs);\n    \n    // 仍然发送ACK确认\n    await SendAckMessageAsync(frame, true, cancellationToken);\n    return;\n}\n</code></pre>\n<h4 id=\"特性总结\">特性总结</h4>\n<ul>\n<li>✅ <strong>基于 HashSet 高效查找</strong>：O(1) 查询复杂度</li>\n<li>✅ <strong>自动跟踪最大 SeqID</strong>：支持顺序性验证</li>\n<li>✅ <strong>24小时过期机制</strong>：定期清理历史数据</li>\n<li>✅ <strong>内存友好</strong>：仅存储 SeqID，不存储完整消息</li>\n</ul>\n<h3 id=\"webhook-的-nonce防重放攻击的秘密武器\">Webhook 的 Nonce：防重放攻击的秘密武器</h3>\n<p>飞书 Webhook 的请求里带有一个 <code>nonce</code>（随机数），这就像一次性密码——用过的密码就不能再用了，这样就能防止\"重放攻击\"（坏人拿同一个请求重复发送）。</p>\n<h4 id=\"防重放攻击流程\">防重放攻击流程</h4>\n<pre><code>攻击者尝试重放请求：\n─────────────────────────────────────────────────────────────\n原始请求：Nonce=\"abc123\", Timestamp=\"1234567890\"\n✅ 正常处理（首次接收）\n\n重放请求：Nonce=\"abc123\", Timestamp=\"1234567890\"\n❌ 拒绝处理（Nonce 已记录）\n─────────────────────────────────────────────────────────────\n</code></pre>\n<h4 id=\"redis-实现\">Redis 实现</h4>\n<pre><code class=\"language-csharp\">public class RedisFeishuNonceDistributedDeduplicator : IFeishuNonceDistributedDeduplicator\n{\n    private readonly IDatabase _database;\n    private readonly TimeSpan _defaultTtl = TimeSpan.FromMinutes(5); // 5分钟过期\n\n    public async Task&lt;bool&gt; TryMarkAsProcessedAsync(string nonce, CancellationToken cancellationToken = default)\n    {\n        var redisKey = $\"{_keyPrefix}{nonce}\";\n\n        // SETNX + EXPIRE：5分钟内重复 nonce 会被拒绝\n        var setResult = await _database.StringSetAsync(\n            redisKey,\n            \"1\",\n            _defaultTtl,\n            When.NotExists);\n\n        if (!setResult)\n        {\n            _logger?.LogWarning(\"检测到重复的 Nonce: {Nonce}，可能为重放攻击\", nonce);\n            return true; // 已处理，拒绝\n        }\n\n        return false; // 首次处理\n    }\n}\n</code></pre>\n<hr />\n<h2 id=\"应用层给每个业务操作发身份证\">应用层：给每个业务操作发\"身份证\"</h2>\n<h3 id=\"为什么还需要这一层\">为什么还需要这一层？</h3>\n<p>就算前面两层都挡住了，业务逻辑还是有可能重复执行，比如：</p>\n<ul>\n<li>🔀 同一事件触发了多个处理器（并行处理）</li>\n<li>🔄 处理器内部多次访问同一个资源</li>\n<li>📡 第三方接口重试导致重复调用</li>\n</ul>\n<h3 id=\"实际的幂等性实现方式\">实际的幂等性实现方式</h3>\n<p>在实际的 Mud.Feishu SDK 中，业务层幂等性主要通过以下两种方式实现：</p>\n<p><strong>方式1：使用 <code>DefaultFeishuObjectEventHandler&lt;T&gt;</code>（推荐）</strong><br />\nSDK 提供了 <code>DefaultFeishuObjectEventHandler&lt;T&gt;</code> 基类，专门用于处理对象类型的事件。你只需要继承这个基类，并在业务逻辑中检查数据是否已存在即可。</p>\n<p><strong>方式2：使用 <code>IdempotentFeishuEventHandler&lt;T&gt;</code></strong><br />\nSDK 还提供了一个 <code>IdempotentFeishuEventHandler&lt;T&gt;</code> 基类，提供了基于业务键的自动去重能力。你需要重写 <code>GetBusinessKey()</code> 方法和 <code>HandleEventInternalAsync()</code> 方法，基类会自动处理业务去重。</p>\n<hr />\n<h3 id=\"手把手教你用三个实际案例\">手把手教你用：三个实际案例</h3>\n<h4 id=\"案例-1消息处理防止重复创建待办\">案例 1：消息处理——防止重复创建待办</h4>\n<pre><code class=\"language-csharp\">public class MessageEventHandler : DefaultFeishuObjectEventHandler&lt;MessageReceiveResult&gt;\n{\n    private readonly IMessageService _messageService;\n\n    public MessageEventHandler(\n        ILogger&lt;MessageEventHandler&gt; logger,\n        IMessageService messageService)\n        : base(logger)\n    {\n        _messageService = messageService;\n    }\n\n    protected override async Task ProcessBusinessLogicAsync(\n        EventData eventData,\n        ObjectEventResult&lt;MessageReceiveResult&gt;? messageData,\n        CancellationToken cancellationToken = default)\n    {\n        if (eventData == null)\n            throw new ArgumentNullException(nameof(eventData));\n\n        // 检查消息是否已处理（业务层幂等性）\n        var messageId = messageData?.Object.MessageId;\n        if (string.IsNullOrEmpty(messageId))\n        {\n            _logger.LogWarning(\"消息ID为空，跳过处理\");\n            return;\n        }\n\n        var alreadyProcessed = await _messageService.IsMessageProcessedAsync(messageId, cancellationToken);\n        if (alreadyProcessed)\n        {\n            _logger.LogInformation(\"消息 {MessageId} 已处理，跳过\", messageId);\n            return;\n        }\n\n        // 处理消息\n        await _messageService.ProcessMessageAsync(messageData!.Object, cancellationToken);\n    }\n}\n</code></pre>\n<h4 id=\"案例-2部门处理避免重复创建\">案例 2：部门处理——避免重复创建</h4>\n<pre><code class=\"language-csharp\">public class DepartmentCreatedEventHandler : DefaultFeishuObjectEventHandler&lt;DepartmentCreatedResult&gt;\n{\n    private readonly IDepartmentService _departmentService;\n\n    public DepartmentCreatedEventHandler(\n        ILogger&lt;DepartmentCreatedEventHandler&gt; logger,\n        IDepartmentService departmentService)\n        : base(logger)\n    {\n        _departmentService = departmentService;\n    }\n\n    protected override async Task ProcessBusinessLogicAsync(\n        EventData eventData,\n        ObjectEventResult&lt;DepartmentCreatedResult&gt;? departmentData,\n        CancellationToken cancellationToken = default)\n    {\n        if (eventData == null)\n            throw new ArgumentNullException(nameof(eventData));\n\n        if (departmentData?.Object == null)\n        {\n            _logger.LogWarning(\"部门数据为空，跳过处理\");\n            return;\n        }\n\n        // 检查部门是否已存在（业务层幂等性）\n        var departmentId = departmentData.Object.DepartmentId;\n        var exists = await _departmentService.ExistsAsync(departmentId, cancellationToken);\n\n        if (exists)\n        {\n            _logger.LogInformation(\"部门 {DepartmentId} 已存在，跳过创建\", departmentId);\n            return;\n        }\n\n        // 创建部门\n        await _departmentService.CreateAsync(departmentData.Object, cancellationToken);\n    }\n}\n</code></pre>\n<h4 id=\"案例-3用户创建避免重复注册\">案例 3：用户创建——避免重复注册</h4>\n<pre><code class=\"language-csharp\">public class UserCreatedEventHandler : DefaultFeishuObjectEventHandler&lt;UserCreatedResult&gt;\n{\n    private readonly IUserService _userService;\n\n    public UserCreatedEventHandler(\n        ILogger&lt;UserCreatedEventHandler&gt; logger,\n        IUserService userService)\n        : base(logger)\n    {\n        _userService = userService;\n    }\n\n    protected override async Task ProcessBusinessLogicAsync(\n        EventData eventData,\n        ObjectEventResult&lt;UserCreatedResult&gt;? userData,\n        CancellationToken cancellationToken = default)\n    {\n        if (eventData == null)\n            throw new ArgumentNullException(nameof(eventData));\n\n        if (userData?.Object == null)\n        {\n            _logger.LogWarning(\"用户数据为空，跳过处理\");\n            return;\n        }\n\n        // 检查用户是否已存在（业务层幂等性）\n        var userId = userData.Object.UserId;\n        var exists = await _userService.ExistsAsync(userId, cancellationToken);\n\n        if (exists)\n        {\n            _logger.LogInformation(\"用户 {UserId} 已存在，跳过创建\", userId);\n            return;\n        }\n\n        // 创建用户\n        await _userService.CreateAsync(userData.Object, cancellationToken);\n    }\n}\n</code></pre>\n<h3 id=\"怎么设计业务键有套路吗\">怎么设计业务键？有套路吗？</h3>\n<table>\n<thead>\n<tr>\n<th>业务场景</th>\n<th>业务键长什么样？</th>\n<th>为什么这么设计？</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>收到消息</td>\n<td><code>im.message.receive_v1:om_xxxxxxxxxx</code></td>\n<td>事件类型 + 消息ID，一眼看出是哪条消息</td>\n</tr>\n<tr>\n<td>创建部门</td>\n<td><code>contact.department.created_v3:od_xxxxxxxxxx</code></td>\n<td>事件类型 + 部门ID，避免和其他ID冲突</td>\n</tr>\n<tr>\n<td>创建用户</td>\n<td><code>contact.user.created_v3:ou_xxxxxxxxxx</code></td>\n<td>事件类型 + 用户ID，唯一标识用户</td>\n</tr>\n<tr>\n<td>删除部门</td>\n<td><code>contact.department.deleted_v3:od_xxxxxxxxxx</code></td>\n<td>事件类型 + 部门ID，处理删除事件</td>\n</tr>\n<tr>\n<td>消息已读</td>\n<td><code>im.message.message_read_v1:ou_xxxxxxxxxx:om_xxxxxxxxxx</code></td>\n<td>事件类型 + 用户ID + 消息ID</td>\n</tr>\n</tbody>\n</table>\n<p><strong>四条黄金法则</strong>：</p>\n<ol>\n<li>🔑 <strong>唯一性</strong>：一个操作对应一个键，不能有两个操作撞车</li>\n<li>👀 <strong>可读性</strong>：看日志时能快速知道这是什么</li>\n<li>🧱 <strong>稳定性</strong>：别用时间戳这种会变的字段做键</li>\n<li>✂️ <strong>简洁性</strong>：别写太长，省内存、好查询</li>\n</ol>\n<hr />\n<h2 id=\"配置实战开发-vs-生产环境\">配置实战：开发 vs 生产环境</h2>\n<h3 id=\"websocket-怎么配置\">WebSocket 怎么配置？</h3>\n<pre><code class=\"language-csharp\">// 使用建造者模式配置 WebSocket\nbuilder.Services.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        // 事件去重配置\n        options.EnableEventDeduplication = true;           // 启用事件去重\n        options.EnableDistributedDeduplication = false;    // 单机场景使用内存去重\n        options.EventDeduplicationCacheExpirationMs = 86400000;  // 24小时过期\n        options.EventDeduplicationCleanupIntervalMs = 300000;    // 5分钟清理间隔\n    });\n</code></pre>\n<h4 id=\"生产环境必备配置一定要看\">生产环境必备配置（一定要看！）</h4>\n<pre><code class=\"language-csharp\">// 1. 配置 Redis（在 appsettings.json 中）\n// {\n//   \"Feishu\": {\n//     \"Redis\": {\n//       \"ConnectionString\": \"your-redis-server\"\n//     }\n//   }\n// }\n\n// 2. 注册 Redis 服务和去重服务\nbuilder.Services\n    .AddFeishuRedis()\n    .AddFeishuRedisDeduplicators();\n\n// 3. 启用分布式去重\nbuilder.Services.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        options.EnableDistributedDeduplication = true;   // 启用分布式去重\n        options.EventDeduplicationCacheExpirationMs = 86400000;  // 24小时\n    });\n</code></pre>\n<h3 id=\"webhook-配置\">Webhook 配置</h3>\n<pre><code class=\"language-csharp\">// 注册 Webhook 服务\nbuilder.Services.AddFeishuWebhookServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        options.VerificationToken = \"your_verification_token\";\n        options.EncryptKey = \"your_encrypt_key\";\n        options.EventHandlingTimeoutMs = 30000;          // 30秒超时\n        options.MaxConcurrentEvents = 10;                // 最大并发数\n    });\n\n// 注册 Redis 去重\nbuilder.Services\n    .AddFeishuRedis()\n    .AddFeishuRedisEventDeduplicator();\n</code></pre>\n<h3 id=\"一张表看懂开发与生产的区别\">一张表看懂开发与生产的区别</h3>\n<table>\n<thead>\n<tr>\n<th>配置项</th>\n<th>开发环境</th>\n<th>生产环境</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>EnableEventDeduplication</code></td>\n<td><code>true</code></td>\n<td><code>true</code></td>\n<td>始终启用内存去重</td>\n</tr>\n<tr>\n<td><code>EnableDistributedDeduplication</code></td>\n<td><code>false</code></td>\n<td><code>true</code></td>\n<td>生产环境启用Redis去重</td>\n</tr>\n<tr>\n<td><code>EventDeduplicationCacheExpirationMs</code></td>\n<td><code>3600000</code> (1小时)</td>\n<td><code>86400000</code> (24小时)</td>\n<td>生产环境延长窗口期</td>\n</tr>\n<tr>\n<td><code>EventDeduplicationCleanupIntervalMs</code></td>\n<td><code>60000</code> (1分钟)</td>\n<td><code>300000</code> (5分钟)</td>\n<td>生产环境降低清理频率</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"完整配置从零到上线一次搞定\">完整配置：从零到上线一次搞定</h3>\n<pre><code class=\"language-csharp\">public void ConfigureServices(IServiceCollection services)\n{\n    // 配置 Redis（appsettings.json 中配置）\n    services\n        .AddFeishuRedis()\n        .AddFeishuRedisDeduplicators();\n\n    // 注册 WebSocket 服务\n    services.AddFeishuWebSocketServiceBuilder(configuration)\n        .ConfigureOptions(wsOptions =&gt;\n        {\n            wsOptions.EnableEventDeduplication = true;\n            wsOptions.EnableDistributedDeduplication = true;\n            wsOptions.EventDeduplicationCacheExpirationMs = 86400000;\n            wsOptions.AutoReconnect = true;\n            wsOptions.MaxReconnectAttempts = 5;\n            wsOptions.HeartbeatIntervalMs = 30000;\n        })\n        .AddHandler&lt;MessageEventHandler&gt;()\n        .AddHandler&lt;DepartmentCreatedEventHandler&gt;()\n        .Build();\n\n    // 注册 Webhook 服务\n    services.AddFeishuWebhookServiceBuilder(configuration)\n        .ConfigureOptions(webhookOptions =&gt;\n        {\n            webhookOptions.VerificationToken = \"your_token\";\n            webhookOptions.EncryptKey = \"your_key\";\n            webhookOptions.EventHandlingTimeoutMs = 30000;\n            webhookOptions.MaxConcurrentEvents = 20;\n        })\n        .AddHandler&lt;UserCreatedEventHandler&gt;()\n        .Build();\n}\n</code></pre>\n<h3 id=\"还有个贴心功能配置错误自动提醒\">还有个贴心功能：配置错误自动提醒</h3>\n<p>SDK 会自动检查配置，如果你把去重功能全关了，它会直接报错提醒你：</p>\n<pre><code class=\"language-csharp\">// FeishuWebSocketOptions.Validate() 自动执行\n// SDK 会在服务启动时检查配置，如果去重功能全关闭会抛出警告\n</code></pre>\n<hr />\n<h2 id=\"踩坑指南五个常见问题和解决方案\">踩坑指南：五个常见问题和解决方案</h2>\n<h3 id=\"问题-1服务重启重复事件又来了\">问题 1：服务重启，重复事件又来了</h3>\n<p><strong>现场是这样的</strong>：</p>\n<p><strong>现象</strong>：</p>\n<pre><code>09:00 服务接收 EventId=\"evt_123\" ✅ 处理成功\n09:05 服务重启（内存缓存清空）\n09:06 飞书重发 EventId=\"evt_123\" ❌ 重复处理\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>🧠 内存去重就像短期记忆，重启就忘光了</li>\n<li>📡 飞书没收到你的确认，以为你没收到，继续发</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>方案1：启用 Redis 分布式去重（推荐）</strong></p>\n<pre><code class=\"language-csharp\">// 配置 Redis（appsettings.json 中配置连接信息）\nservices\n    .AddFeishuRedis()\n    .AddFeishuRedisDeduplicators();\n\nservices.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        options.EnableDistributedDeduplication = true; // 使用Redis\n        options.EnableEventDeduplication = false;    // 禁用内存去重\n    });\n</code></pre>\n<p>✅ <strong>方案2：实现去重状态持久化</strong></p>\n<pre><code class=\"language-csharp\">public class PersistentEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly IDatabase _database;\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        // 尝试写入数据库\n        var exists = _database.EventExists(eventId);\n        if (!exists)\n        {\n            _database.MarkProcessing(eventId);\n            return false;\n        }\n        return true;\n    }\n}\n</code></pre>\n<hr />\n<h3 id=\"问题-2多实例一起跑重复处理挡不住\">问题 2：多实例一起跑，重复处理挡不住</h3>\n<p><strong>现场是这样的</strong>：</p>\n<pre><code>实例A 和 实例B 同时启动\n飞书推送 EventId=\"evt_123\"\n实例A 收到 → 处理 ✅\n实例B 收到 → 处理 ❌ (重复！)\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>🧠 每个实例都有自己独立的\"小本本\"</li>\n<li>🚫 实例之间互相看不到对方的记录</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>必须使用 Redis 分布式去重</strong></p>\n<pre><code class=\"language-csharp\">// 所有实例连接到同一个 Redis（在配置文件中配置）\nservices\n    .AddFeishuRedis()\n    .AddFeishuRedisDeduplicators();\n\nservices.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        options.EnableDistributedDeduplication = true;\n    });\n</code></pre>\n<p><strong>一张图看懂区别</strong>：</p>\n<div class=\"mermaid\">graph TB\n    subgraph Wrong[\"❌ 错误架构（内存去重）\"]\n        A1[\"实例A&lt;br/&gt;内存缓存A&lt;br/&gt;evt_123 ✅\"]\n        A2[\"实例B&lt;br/&gt;内存缓存B&lt;br/&gt;evt_123 ❌\"]\n    end\n\n    subgraph Right[\"✅ 正确架构（Redis去重）\"]\n        B1[\"实例A\"]\n        B2[\"实例B\"]\n        Redis[\"Redis 去重&lt;br/&gt;evt_123 ✅\"]\n    end\n\n    B1 --&gt; Redis\n    B2 --&gt; Redis\n\n    style Wrong fill:#ffebee\n    style Right fill:#e8f5e9\n    style Redis fill:#e3f2fd\n</div><hr />\n<h3 id=\"问题-3处理超时重复事件趁虚而入\">问题 3：处理超时，重复事件趁虚而入</h3>\n<p><strong>现场是这样的</strong>：</p>\n<pre><code>09:00 开始处理 EventId=\"evt_123\"\n09:01 处理超时（超时30秒）\n09:01 回滚 Processing 状态\n09:02 飞书重发 EventId=\"evt_123\"\n09:02 重复处理 ❌ (实际已成功！)\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>⏱️ 超时后系统以为没完成，把\"处理中\"标记撤回了</li>\n<li>✅ 但业务可能已经做完了，只是响应慢了一点点</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>方案1：增加最终一致性检查</strong></p>\n<pre><code class=\"language-csharp\">public class DepartmentCreatedEventHandler : DefaultFeishuObjectEventHandler&lt;DepartmentCreatedResult&gt;\n{\n    private readonly IDepartmentService _departmentService;\n\n    protected override async Task ProcessBusinessLogicAsync(\n        EventData eventData,\n        ObjectEventResult&lt;DepartmentCreatedResult&gt;? departmentData,\n        CancellationToken ct)\n    {\n        if (departmentData?.Object == null)\n            return;\n\n        var departmentId = departmentData.Object.DepartmentId;\n\n        // 先检查部门是否已存在（幂等性）\n        var existingDepartment = await _departmentService.GetAsync(departmentId, ct);\n        if (existingDepartment != null)\n        {\n            _logger.LogInformation(\"部门 {DepartmentId} 已存在，跳过创建\", departmentId);\n            return;\n        }\n\n        // 创建部门\n        await _departmentService.CreateAsync(departmentData.Object, ct);\n    }\n}\n</code></pre>\n<p>✅ <strong>方案2：实现处理续期机制</strong></p>\n<pre><code class=\"language-csharp\">public class LongRunningTaskHandler : DefaultFeishuEventHandler\n{\n    private readonly IFeishuEventDeduplicator _deduplicator;\n\n    protected override async Task ProcessBusinessLogicAsync(EventData eventData, CancellationToken ct)\n    {\n        var eventId = eventData.EventId;\n\n        // 定期续期处理时间（每30秒）\n        var renewalTask = Task.Run(async () =&gt;\n        {\n            while (!ct.IsCancellationRequested)\n            {\n                await Task.Delay(30000, ct);\n                _deduplicator.RenewProcessing(eventId);\n            }\n        }, ct);\n\n        try\n        {\n            await ProcessLongRunningTask(eventData, ct);\n        }\n        finally\n        {\n            await renewalTask;\n        }\n    }\n}\n</code></pre>\n<hr />\n<h3 id=\"问题-4redis-宕机去重防护全失效\">问题 4：Redis 宕机，去重防护全失效</h3>\n<p><strong>现场是这样的</strong>：</p>\n<pre><code>Redis 宕机\n服务无法调用 TryMarkAsProcessedAsync()\n返回 false（允许处理）\n多个实例重复处理同一事件 ❌\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>🚫 Redis 挂了，代码为了不阻塞选择\"允许处理\"</li>\n<li>❌ 结果重复事件全涌进来</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>方案1：实现降级到内存去重</strong></p>\n<pre><code class=\"language-csharp\">public class HybridEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly IFeishuEventDistributedDeduplicator _redis;\n    private readonly IFeishuEventDeduplicator _memory;\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        try\n        {\n            // 优先使用 Redis\n            return _redis.TryMarkAsProcessedAsync(eventId).GetAwaiter().GetResult();\n        }\n        catch (Exception ex)\n        {\n            _logger.LogError(ex, \"Redis去重失败，降级到内存去重\");\n            // 降级到内存去重\n            return _memory.TryMarkAsProcessing(eventId);\n        }\n    }\n}\n</code></pre>\n<p>✅ <strong>方案2：配置熔断机制</strong></p>\n<pre><code class=\"language-csharp\">public class CircuitBreakerEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly CircuitBreaker _circuitBreaker;\n    private int _failureCount;\n    private DateTime _lastFailureTime;\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        if (_circuitBreaker.IsOpen())\n        {\n            _logger.LogWarning(\"Redis去重熔断，拒绝处理\");\n            return true; // 拒绝处理，保护下游\n        }\n\n        try\n        {\n            var result = _redis.TryMarkAsProcessedAsync(eventId).GetAwaiter().GetResult();\n            _circuitBreaker.RecordSuccess();\n            return result;\n        }\n        catch (Exception ex)\n        {\n            _circuitBreaker.RecordFailure();\n            throw;\n        }\n    }\n}\n</code></pre>\n<hr />\n<h3 id=\"问题-5缓存越积越多内存快爆了\">问题 5：缓存越积越多，内存快爆了</h3>\n<p><strong>现场是这样的</strong>：</p>\n<pre><code>服务运行24小时\n去重缓存条目：100万+\n内存占用：超过500MB\nGC压力增大，性能下降\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>📈 高频事件（比如接收消息）一天能产生几十万条记录</li>\n<li>🕐 清理间隔太长，旧记录堆积如山</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>方案1：缩短缓存过期时间</strong></p>\n<pre><code class=\"language-csharp\">services.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        // 根据实际业务调整\n        options.EventDeduplicationCacheExpirationMs = 3600000;  // 缩短为1小时\n        options.EventDeduplicationCleanupIntervalMs = 60000;    // 提高清理频率为1分钟\n    });\n</code></pre>\n<p>✅ <strong>方案2：使用 LRU 缓存</strong></p>\n<pre><code class=\"language-csharp\">public class LRUEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly LRUCache&lt;string, EventCacheEntry&gt; _cache;\n    private const int MaxCacheSize = 10000; // 最多保留1万条\n\n    public LRUEventDeduplicator()\n    {\n        _cache = new LRUCache&lt;string, EventCacheEntry&gt;(MaxCacheSize);\n    }\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        if (_cache.TryGetValue(eventId, out var entry))\n            return true;\n\n        _cache.Add(eventId, new EventCacheEntry { /* ... */ });\n        return false;\n    }\n}\n</code></pre>\n<p>✅ <strong>方案3：业务层优化</strong></p>\n<pre><code class=\"language-csharp\">// 某些高频事件不需要去重\npublic class SystemEventHandler : DefaultFeishuEventHandler\n{\n    // 不继承 IdempotentFeishuEventHandler\n    // 直接处理，减少业务层去重压力\n\n    protected override Task ProcessBusinessLogicAsync(EventData eventData, CancellationToken ct)\n    {\n        // 仅记录日志，不进行业务操作\n        _logger.LogInformation(\"系统事件：{EventType}\", eventData.EventType);\n        return Task.CompletedTask;\n    }\n}\n</code></pre>\n<hr />\n<h2 id=\"给系统做体检监控与调试\">给系统做体检：监控与调试</h2>\n<h3 id=\"采集什么数据最有效\">采集什么数据最有效？</h3>\n<pre><code class=\"language-csharp\">public class DeduplicatorMetrics\n{\n    public long TotalProcessed { get; set; }\n    public long DuplicateSkipped { get; set; }\n    public long ProcessingTimeout { get; set; }\n    public long CacheHitCount { get; set; }\n    public long CacheMissCount { get; set; }\n\n    public double DuplicateRate =&gt; \n        TotalProcessed &gt; 0 ? (double)DuplicateSkipped / TotalProcessed : 0;\n\n    public double CacheHitRate =&gt;\n        (CacheHitCount + CacheMissCount) &gt; 0 \n            ? (double)CacheHitCount / (CacheHitCount + CacheMissCount) \n            : 0;\n}\n</code></pre>\n<h3 id=\"暴露一个接口随时查看健康状态\">暴露一个接口，随时查看健康状态</h3>\n<pre><code class=\"language-csharp\">[ApiController]\n[Route(\"api/[controller]\")]\npublic class DeduplicatorController : ControllerBase\n{\n    private readonly IFeishuEventDeduplicator _deduplicator;\n\n    [HttpGet(\"stats\")]\n    public ActionResult&lt;DeduplicatorStats&gt; GetStats()\n    {\n        return Ok(new DeduplicatorStats\n        {\n            CacheCount = _deduplicator.GetCacheCount(),\n            MaxProcessedSeqId = _seqIdDeduplicator?.GetMaxProcessedSeqId(),\n            DuplicateRate = _metrics.DuplicateRate,\n            CacheHitRate = _metrics.CacheHitRate\n        });\n    }\n\n    [HttpGet(\"check/{eventId}\")]\n    public ActionResult&lt;bool&gt; CheckEventId(string eventId)\n    {\n        var isProcessed = _deduplicator.IsProcessed(eventId);\n        return Ok(new { eventId, isProcessed });\n    }\n}\n</code></pre>\n<h3 id=\"日志怎么写才容易排查问题\">日志怎么写才容易排查问题？</h3>\n<pre><code class=\"language-csharp\">public class EnhancedEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly ILogger&lt;EnhancedEventDeduplicator&gt; _logger;\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        lock (_lock)\n        {\n            if (_eventCache.TryGetValue(eventId, out var entry))\n            {\n                // 详细记录去重命中原因\n                _logger.LogInformation(\n                    \"[Deduplication] EventId={EventId} 已处理，\" +\n                    \"Status={Status}, ProcessedAt={ProcessedAt}\",\n                    eventId,\n                    entry.Status,\n                    entry.ProcessedAt);\n\n                return true;\n            }\n\n            _logger.LogDebug(\"[Deduplication] EventId={EventId} 新事件\", eventId);\n            // ...\n            return false;\n        }\n    }\n}\n</code></pre>\n<hr />\n<h2 id=\"快速回顾与行动清单\">快速回顾与行动清单</h2>\n<h3 id=\"三层防护一表览\">三层防护一表览</h3>\n<table>\n<thead>\n<tr>\n<th>去重层级</th>\n<th>去重依据</th>\n<th>实现方式</th>\n<th>推荐配置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>协议层</strong></td>\n<td>SeqID / Nonce</td>\n<td><code>IFeishuSeqIDDeduplicator</code> / <code>IFeishuNonceDistributedDeduplicator</code></td>\n<td>始终启用</td>\n</tr>\n<tr>\n<td><strong>分发层</strong></td>\n<td>EventId</td>\n<td><code>IFeishuEventDeduplicator</code> / <code>IFeishuEventDistributedDeduplicator</code></td>\n<td>生产环境启用 Redis</td>\n</tr>\n<tr>\n<td><strong>应用层</strong></td>\n<td>数据存在性检查</td>\n<td><code>DefaultFeishuObjectEventHandler&lt;T&gt;</code> 中的业务逻辑</td>\n<td>按需实现</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"还想了解更多\">还想了解更多？</h3>\n<ul>\n<li>📖 <a href=\"https://open.feishu.cn/document/server-docs/event-subscription-guide/overview\" rel=\"noopener nofollow\" target=\"_blank\">飞书官方文档：事件订阅</a></li>\n<li>💻 <a href=\"https://github.com/mudtools/MudFeishu\" rel=\"noopener nofollow\" target=\"_blank\">MudFeishu GitHub 仓库</a></li>\n<li>💻 <a href=\"https://gitee.com/mudtools/MudFeishu\" rel=\"noopener nofollow\" target=\"_blank\">MudFeishu Gitee 仓库</a></li>\n<li>🔒 <a href=\"https://redis.io/topics/distlock\" rel=\"noopener nofollow\" target=\"_blank\">Redis 分布式锁最佳实践</a></li>\n</ul>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-11 23:00</span>&nbsp;\n<a href=\"https://www.cnblogs.com/mudtools\">玩泥巴的|mudtools.cn</a>&nbsp;\n阅读(<span id=\"post_view_count\">125</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "迭代器 iteration、iter 与 多线程 concurrent 交叉实践（详细）",
      "link": "https://www.cnblogs.com/io-T-T/p/19469168",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/io-T-T/p/19469168\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 22:52\">\n    <span>迭代器 iteration、iter 与 多线程 concurrent 交叉实践（详细）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        迭代器 多线程 concurrent iter python\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"迭代器iterationiter-与-多线程-concurrent-交叉实践详细\">迭代器<code>iteration</code>、<code>iter</code> 与 多线程 <code>concurrent</code> 交叉实践（详细）</h2>\n<h3 id=\"实践及简介说明\">实践及简介说明</h3>\n<p>​\t由于在实际运用重，迭代器（或生成器）经常与多线程一并使用。本实践旨在对迭代器（及生成器）、多线程库（主要为<code>concurrent</code>）进行交叉实践说明，用来使读者更加理解迭代器和多线程在实际的应用。因此本篇轻教程，重实践，当然也会简要说明相关知识。</p>\n<h3 id=\"基础知识相关介绍简要\">基础知识相关介绍（简要）</h3>\n<blockquote>\n<p>详细介绍建议观看<a href=\"https://space.bilibili.com/1318868/?spm_id_from=333.788.upinfo.detail.click\" rel=\"noopener nofollow\" target=\"_blank\">Hucci写代码</a>博主教的基础知识，详细易懂</p>\n<p><a href=\"https://www.bilibili.com/video/BV1jt421c7yN\" rel=\"noopener nofollow\" target=\"_blank\">【Python】从迭代器到生成器：小内存也能处理大数据</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1tVsyzUEtX/?\" rel=\"noopener nofollow\" target=\"_blank\">在Python中用多线程之前，需要先搞懂这些</a></p>\n</blockquote>\n<h3 id=\"你为什么需要学习迭代器多线程\">你为什么需要学习迭代器、多线程</h3>\n<p>学这个教程/实践之前，你需要先看看自己有没有这个需求，当然你感兴趣也可以学。</p>\n<h4 id=\"迭代器生成器的优劣\">迭代器（生成器）的优劣</h4>\n<p><strong>优势：</strong></p>\n<ul>\n<li>使用时生成数据，缓解爆缓存问题（RAM、GRAM）【主要】</li>\n<li>统一标准接口，无需关系数据的数据结构调用问题，使用 <code>for _iter in iteration</code> 即可调用</li>\n</ul>\n<p><strong>劣势：</strong></p>\n<ul>\n<li>单次使用</li>\n<li>不支持索引</li>\n<li>单向遍历（向前）</li>\n</ul>\n<h4 id=\"多线程的优劣python限定\">多线程的优劣（Python限定）</h4>\n<p><strong>优势</strong>：</p>\n<ul>\n<li>并行执行，提高CPU的使用率</li>\n<li>充分利用I/O的中断时间，从而提高代码的执行速度</li>\n</ul>\n<p><strong>劣势：</strong></p>\n<ul>\n<li>调试难度大</li>\n<li>老版本（好像是3.12之前），只有一个GIL锁，导致进程会出现抢占现象。</li>\n<li>少数据量带来的提升可能较少</li>\n</ul>\n<h3 id=\"01-环境来源数据\">01-环境来源数据</h3>\n<p>我用的是<code>kaggle</code>的开源数据，由于很多都是<code>csv</code>类型，因此我也以简单的<code>csv</code>数据集多线程读取进行说明，你们也可以用其他数据集，最好文本行长度&gt;5k, 才能展现多线程、迭代器的优势，数量级低还是用单线程、串行吧，容易理解好维护。</p>\n<blockquote>\n<p><a href=\"https://www.kaggle.com/datasets/dansbecker/powerlifting-database/data\" rel=\"noopener nofollow\" target=\"_blank\">powerlifting-database</a></p>\n</blockquote>\n<h3 id=\"02-任务介绍及编码思路\">02-任务介绍及编码思路</h3>\n<p>空有编码能力，但是没有编码思路是大忌吧。因此先概述任务及编码思路。</p>\n<h4 id=\"21-任务\">2.1 任务</h4>\n<p>本次实践，任务有以下几点：</p>\n<ol>\n<li>了解并构造迭代器类</li>\n<li>使用迭代器类对数据集（<strong>csv数据</strong>）进行数据划分</li>\n<li>使用多线程库（<code>concurrent</code>）来模拟多线程处理数据。</li>\n</ol>\n<h4 id=\"22-编码思路\">2.2 编码思路</h4>\n<p>想要达成多线程与迭代器的结合，首先需要构造合适的数据，因此需要：</p>\n<ol>\n<li>构建迭代器类（框架）</li>\n</ol>\n<p>然后需要将数据集导入进来，完善迭代器类，还需要对输入的数据做一下分块，方便后续使用：</p>\n<ol start=\"2\">\n<li>数据导入+分块+完善迭代器类</li>\n</ol>\n<p>数据来源解决了，也分块了，接下来是将数据与多线程联动了</p>\n<ol start=\"3\">\n<li>引入多线程库，处理切分的数据。</li>\n</ol>\n<h3 id=\"03-构建迭代器类\">03 构建迭代器类</h3>\n<h4 id=\"31迭代器的最小框架\">3.1迭代器的最小框架</h4>\n<blockquote>\n<p>可参考 <a href=\"https://www.cnblogs.com/Ravenna/p/15676404.html\" target=\"_blank\">python迭代器简单理解 __iter__和__next__方法</a></p>\n</blockquote>\n<p>​\t成为一个最小迭代器类通常需要包含以下几个类接口：</p>\n<img alt=\"image-20260111171210573\" class=\"lazyload\" />\n<ol>\n<li><code>__init__</code>：类初始化接口</li>\n<li><code>__iter__</code>: 获取迭代对象的接口</li>\n<li><code>__next__</code>：数据迭代接口。</li>\n</ol>\n<h4 id=\"32--构建迭代器类\">3.2  构建迭代器类</h4>\n<p>为了照顾更多的读者，我这里使用传统接口对<code>csv</code>进行读写，有<code>panda</code>能力的读者可以使用<code>pd</code>快捷读取并构造。</p>\n<pre><code class=\"language-python\">class read_file_batch:\n    def __init__(self,file_path:str, batch_size:int):\n        self.file_fp = open(file_path, encoding='utf-8', mode='r+')\n        self.batch_size = batch_size\n    def __iter__(self):\n        return self\t\t\t\t\t#返回这个迭代器本身，我们是以类作为迭代对象，因此返回他自己吧\n    def __next__(self):\t\t\t\t#数据划分\n        batch_res = []\n        for i in range(self.batch_size):\t\n            line = self.file_fp.readline()\t#最底层获取行数据的交互接口\n            if line:                        #如果行非空\n                batch_res.append(line)\n            else:                           #读到最后就算空的了\n                self.file_fp.close()\n                raise StopIteration\n\n        return batch_res\n</code></pre>\n<ul>\n<li>\n<p>调用尝试：</p>\n<p>可以调试一下，感受一下分批的迭代及数据的输入输出。</p>\n<pre><code class=\"language-python\">if __name__ == '__main__':\n    your_file_path:str = r\"E:\\powerlifting-database\\openpowerlifting.csv\"\n    iteration = read_file_batch(your_file_path, batch_size=1000)\n    count = 0\n    for lines in iteration:\n        print('-'*50+f\"count:{count}\"+'-'*50)\n        count += 1\n        print(lines)\n</code></pre>\n</li>\n</ul>\n<h3 id=\"04-引入多线程处理批量数据\">04 引入多线程处理批量数据</h3>\n<h4 id=\"41-多线程写法\">4.1 多线程写法</h4>\n<p>其实蛮简单的，只要把接口函数写好、数据处理一下，就行了。</p>\n<ol>\n<li>\n<p>导入库：</p>\n<pre><code class=\"language-python\">from concurrent.futures import ThreadPoolExecutor as Excecutor\n</code></pre>\n</li>\n<li>\n<p>使用上下文管理工具管理线程（确保安全及释放）：</p>\n<pre><code class=\"language-python\">with Excecutor(max_workers=8) as executor:\n</code></pre>\n</li>\n<li>\n<p>使用map接口进行多线程指令执行：</p>\n<pre><code class=\"language-python\">executor.map(\n    fn= function, iter_element1, iter_element2, iter_element3\n)#function是你对应的函数名， iter_element1-3是函数依赖的输入数据\n</code></pre>\n</li>\n<li>\n<p>汇总：</p>\n<pre><code class=\"language-python\">from concurrent.futures import ThreadPoolExecutor as Excecutor\nwith Excecutor(max_workers=8) as executor:\n\texecutor.map(\n    fn= function, iter_element1, iter_element2, iter_element3\n)#function是你对应的函数名， iter_element1-3是函数依赖的输入数据\n</code></pre>\n</li>\n</ol>\n<h4 id=\"42-引入多线程处理数据\">4.2 引入多线程处理数据</h4>\n<ol>\n<li>\n<p>首先，模拟一个处理行数据的函数用户处理行信息：</p>\n<pre><code class=\"language-python\">def process_dealing(line:str):\n    '''\n    模拟处理某一行的数据\n    :param line:处理每一行\n    '''\n    print(f'line top 20 info is :{line[:20]}')\n    pass\n</code></pre>\n</li>\n<li>\n<p>在迭代器的迭代过程中对数据进行处理。</p>\n<pre><code class=\"language-python\">if __name__ == '__main__':\n    your_file_path:str = r\"E:\\powerlifting-database\\openpowerlifting.csv\"\n    iteration = read_file_batch(your_file_path, batch_size=1000)\n    count = 0\n    with Excecutor(max_workers=8) as executor:\n\n        for lines in iteration:\n            print('-'*50+f\"count:{count}\"+'-'*50)\n            count += 1\n            print(lines)\n            executor.map(process_dealing, lines)\n            #第一个参数是处理的函数，后面是*iteration 就是你函数依赖的参数的可迭代对象，这里就是放入list[str]\n</code></pre>\n</li>\n<li>\n<p>结束了，根据你的需要可以对数据进行处理了，其实最难的步骤在于如何构造迭代器（生成器），多线程其实俩步就走完了。</p>\n</li>\n</ol>\n<h3 id=\"05-非必须使用迭代器多线程前后对比\">05 （非必须）使用迭代器、多线程前后对比</h3>\n<pre><code class=\"language-python\">from concurrent.futures import ThreadPoolExecutor as Excecutor\nimport time\nimport tracemalloc  #跟踪内存接口，内置\n\ndef process_dealing(line:str):\n    '''\n    模拟处理某一行的数据\n    :param line:处理每一行\n    '''\n    # print(f'line top 20 info is :{line[:20]}')\n    pass\n\nclass read_file_batch:\n    def __init__(self,file_path:str, batch_size:int):\n        self.file_fp = open(file_path, encoding='utf-8', mode='r+')\n        self.batch_size = batch_size\n    def __iter__(self):\n        return self\n    def __next__(self):                     #数据划分,对迭代器进行迭代，实际上是获取他的next，因此在这里划分数据\n        batch_res = []\n        for i in range(self.batch_size):    #取batch个\n            line = self.file_fp.readline()\n            if line:                        #如果行非空\n                batch_res.append(line)\n            else:                           #读到最后就算空的了\n                self.file_fp.close()\n                raise StopIteration\n\n        return batch_res                    #返回迭代数据\n\ndef main_concurrent(file_path,batch_size:int):\n    start_malloc = tracemalloc.start()\n    start_time = time.time()\n    your_file_path:str = file_path\n    iteration = read_file_batch(your_file_path, batch_size=batch_size)\n    count = 0\n    with Excecutor(max_workers=8) as executor:\n\n        for lines in iteration:                         #返回是batch行,每一行是一个大的str\n            print('-'*50+f\"count:{count}\"+'-'*50+f\"RAM cost:{round(tracemalloc.get_tracemalloc_memory() / 1024**2,4)} M\")\n            count += 1\n            print(lines[:50])                           #节约时间，只取前50字符\n            executor.map(process_dealing, lines)\n            #第一个参数是处理的函数，后面是*iteration 就是你函数依赖的参数的可迭代对象，这里就是放入list[str]\n    print(f'spend time:{round(time.time()-start_time,4)}')\n\ndef main_not_concurrent(file_path,batch_size:int=1000):\n\n    start_time = time.time()\n    start_trace = tracemalloc.start()\n\n    with open(file=file_path,encoding='utf-8',mode='r+') as fp:\n        data = fp.readlines()\n        for idx,line in enumerate(data):\n            if idx % batch_size == 0:\n                print('-'*50 +str(idx)+'-'*50+f\"RAM cost:{round(tracemalloc.get_tracemalloc_memory() / 1024**2,4)} M\")\n                print(line[:50])\n                process_dealing(line)\n    print(f'spend time:{round(time.time()-start_time,4)}')\n\n\nif __name__ == '__main__':\n    file_path = r'E:\\openpowerlifting.csv'\n    batch_size = 1000\n    main_concurrent(file_path,batch_size)\n    # main_not_concurrent(file_path,batch_size)\n    # 其实能看出来这种数据量还是直接缓存在内存比较好，但使用多线程+迭代器确实减少了很多内存\n</code></pre>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-11 22:52</span>&nbsp;\n<a href=\"https://www.cnblogs.com/io-T-T\">io_T_T</a>&nbsp;\n阅读(<span id=\"post_view_count\">37</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "TensorRtSharp：在 C# 世界中释放 GPU 推理的极致性能",
      "link": "https://www.cnblogs.com/guojin-blogs/p/19468745",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/guojin-blogs/p/19468745\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 18:47\">\n    <span>TensorRtSharp：在 C# 世界中释放 GPU 推理的极致性能</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"TensorRtSharp：在 C# 世界中释放 GPU 推理的极致性能\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/2933426/202601/2933426-20260111184433092-325501944.png\" />\n        TensorRtSharp 3.0 是一个为 C# 开发者打造的 TensorRT 封装库，通过 NuGet 一键安装，提供完整的 GPU 推理加速功能。该库基于 TensorRT 10.x 开发，支持 CUDA 11/12，具备类型安全、自动资源管理等特性，显著提升 .NET 环境下的深度学习推理性能（速度提升 2-10 倍，显存降低 50%+）。安装简单，只需添加两个 NuGet 包，并通过环境变量配置原生库路径即可使用。推荐配置为 CUDA 11.6 + TensorRT 10.13.0.35，支持\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"tensorrtsharp在-c-世界中释放-gpu-推理的极致性能\">TensorRtSharp：在 C# 世界中释放 GPU 推理的极致性能</h1>\n<h2 id=\"目录\">目录</h2>\n<ul>\n<li><a href=\"#%E4%B8%80%E5%89%8D%E8%A8%80\" rel=\"noopener nofollow\">一、前言</a></li>\n<li><a href=\"#%E4%BA%8C%E4%BB%80%E4%B9%88%E6%98%AF-tensortrtsharp\" rel=\"noopener nofollow\">二、什么是 TensorRtSharp</a></li>\n<li><a href=\"#%E4%B8%89%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE\" rel=\"noopener nofollow\">三、安装与配置</a></li>\n<li><a href=\"#%E5%9B%9B%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1\" rel=\"noopener nofollow\">四、核心架构设计</a></li>\n<li><a href=\"#%E4%BA%94%E6%A0%B8%E5%BF%83%E7%B1%BB%E4%B8%8E-api\" rel=\"noopener nofollow\">五、核心类与 API</a></li>\n<li><a href=\"#%E5%85%AD%E5%AE%8C%E6%95%B4%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B\" rel=\"noopener nofollow\">六、完整使用示例</a></li>\n<li><a href=\"#%E4%B8%83%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86\" rel=\"noopener nofollow\">七、异常处理</a></li>\n<li><a href=\"#%E5%85%AB%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F\" rel=\"noopener nofollow\">八、日志系统</a></li>\n<li><a href=\"#%E4%B9%9D%E4%B8%8E%E5%85%B6%E4%BB%96%E5%BA%93%E7%9A%84%E5%AF%B9%E6%AF%94\" rel=\"noopener nofollow\">九、与其他库的对比</a></li>\n<li><a href=\"#%E5%8D%81%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98\" rel=\"noopener nofollow\">十、常见问题</a></li>\n<li><a href=\"#%E5%8D%81%E4%B8%80%E6%80%BB%E7%BB%93\" rel=\"noopener nofollow\">十一、总结</a></li>\n</ul>\n<hr />\n<h2 id=\"一前言\">一、前言</h2>\n<h3 id=\"11-为什么需要-tensorrtsharp\">1.1 为什么需要 TensorRtSharp？</h3>\n<p>在深度学习模型部署领域，NVIDIA TensorRT 凭借其卓越的推理性能已成为 GPU 加速的事实标准。根据 NVIDIA 官方数据，使用 TensorRT 进行模型优化和推理加速，通常可以获得：</p>\n<ul>\n<li>📈 <strong>推理速度提升 2-10 倍</strong>（相比原生框架）</li>\n<li>💾 <strong>显存占用降低 50% 以上</strong>（通过精度优化和层融合）</li>\n<li>⚡ <strong>延迟降低至毫秒级</strong>（满足实时应用需求）</li>\n</ul>\n<p>然而，TensorRT 官方仅提供 C++ 和 Python API，这让广大 .NET 开发者面临一个两难的选择：</p>\n<ul>\n<li><strong>放弃熟悉的 C# 生态</strong>，转向 C++ 或 Python</li>\n<li><strong>通过复杂的互操作层</strong>进行调用，开发效率低下</li>\n</ul>\n<p><strong>TensorRtSharp</strong> 应运而生 —— 这是一个纯 C# 编写的 TensorRT 完整封装库，为 .NET 开发者提供了：</p>\n<ul>\n<li>✅ <strong>类型安全的 API 接口</strong> - 强类型系统，编译时错误检查</li>\n<li>✅ <strong>易于使用且性能卓越</strong> - 直观的 API 设计，零性能损失</li>\n<li>✅ <strong>完整的 TensorRT 功能覆盖</strong> - 支持所有核心功能</li>\n<li>✅ <strong>自动资源管理</strong> - 基于 RAII 和 Dispose 模式，无需担心内存泄漏</li>\n<li>✅ <strong>开箱即用</strong> - NuGet 一键安装，无需复杂配置</li>\n<li>✅ <strong>完善的文档和示例</strong> - 丰富的代码示例和详细的使用说明</li>\n</ul>\n<h3 id=\"12-tensorrtsharp-的核心优势\">1.2 TensorRtSharp 的核心优势</h3>\n<p><strong>1. 原生 C# 体验</strong></p>\n<pre><code class=\"language-csharp\">// 简洁直观的 API 设计\nusing Runtime runtime = new Runtime();\nusing CudaEngine engine = runtime.deserializeCudaEngineByBlob(data, size);\nusing ExecutionContext context = engine.createExecutionContext();\ncontext.executeV3(stream);\n</code></pre>\n<p><strong>2. 完整功能覆盖</strong></p>\n<ul>\n<li>✅ 模型构建（ONNX → Engine）</li>\n<li>✅ 推理执行（同步/异步）</li>\n<li>✅ 动态形状支持</li>\n<li>✅ 多精度推理（FP32/FP16/INT8）</li>\n<li>✅ 多 GPU 并行推理</li>\n</ul>\n<h3 id=\"13-tensorrtsharp-30-的重大改进\">1.3 TensorRtSharp 3.0 的重大改进</h3>\n<p>在前期开发的 TensorRtSharp 1.0 和 2.0 中，使用者需要下载源码编译才能使用，过程繁琐且容易出错。</p>\n<p><strong>在最新的 3.0 版本中，我们进行了重大改进</strong>：</p>\n<p>✅ <strong>一键安装</strong> - 直接将编译好的原生库与托管代码打包至 NuGet 包中<br />\n✅ <strong>开箱即用</strong> - 无需配置复杂的构建环境<br />\n✅ <strong>版本一致</strong> - 降低因环境差异导致的潜在错误</p>\n<p>开发者仅需通过 Visual Studio 的 NuGet 包管理器安装即可直接使用，显著提升了开发效率与部署便捷性！</p>\n<p>本文将全面介绍 TensorRtSharp 的设计理念、核心功能和使用方法，助力大家快速上手使用。</p>\n<hr />\n<h2 id=\"二什么是-tensorrtsharp\">二、什么是 TensorRtSharp</h2>\n<h3 id=\"21-项目简介\">2.1 项目简介</h3>\n<p><strong>TensorRtSharp 3.0</strong> 是作者对 NVIDIA TensorRT 官方库的完整 C# 接口封装。通过 P/Invoke 技术，它将 TensorRT 的原生 C++ API 映射为符合 .NET 设计规范的托管代码，让 C# 开发者能够无缝使用 TensorRT 的全部功能。</p>\n<h3 id=\"22-核心特性\">2.2 核心特性</h3>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>完整的 API 覆盖</strong></td>\n<td>支持 TensorRT 核心功能，包括模型构建、推理执行、动态形状等</td>\n</tr>\n<tr>\n<td><strong>类型安全</strong></td>\n<td>强类型系统，编译时错误检查，避免运行时类型错误</td>\n</tr>\n<tr>\n<td><strong>自动资源管理</strong></td>\n<td>基于 RAII 和 Dispose 模式的资源管理，防止内存泄漏</td>\n</tr>\n<tr>\n<td><strong>跨平台支持</strong></td>\n<td>支持 Windows、Linux，兼容 .NET 5.0-10.0、.NET Core 3.1、.NET Framework 4.7.1-4.8.1</td>\n</tr>\n<tr>\n<td><strong>高性能异步执行</strong></td>\n<td>支持 CUDA Stream、多执行上下文并行推理</td>\n</tr>\n<tr>\n<td><strong>开箱即用</strong></td>\n<td>NuGet 包含所有依赖，无需复杂配置</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"23-项目信息\">2.3 项目信息</h3>\n<table>\n<thead>\n<tr>\n<th>项目</th>\n<th>信息</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>版本</strong></td>\n<td>目前最新 NuGet 版本为 0.0.5（持续更新中，建议使用最新版本）</td>\n</tr>\n<tr>\n<td><strong>GitHub</strong></td>\n<td><code>https://github.com/guojin-yan/TensorRT-CSharp-API</code></td>\n</tr>\n<tr>\n<td><strong>接口 NuGet</strong></td>\n<td><code>JYPPX.TensorRT.CSharp.API</code></td>\n</tr>\n<tr>\n<td><strong>Runtime NuGet</strong></td>\n<td><code>JYPPX.TensorRT.CSharp.API.runtime.win-x64.cuda12</code> 或 <code>JYPPX.TensorRT.CSharp.API.runtime.win-x64.cuda11</code></td>\n</tr>\n<tr>\n<td><strong>编程语言</strong></td>\n<td>C# 10</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"三安装与配置\">三、安装与配置</h2>\n<h3 id=\"31-通过-nuget-安装\">3.1 通过 NuGet 安装</h3>\n<p>安装 TensorRtSharp 非常简单，只需安装两个 NuGet 包：</p>\n<pre><code class=\"language-bash\"># 安装接口包\ndotnet add package JYPPX.TensorRT.CSharp.API\n\n# 安装运行时包（根据您的 CUDA 版本选择）\n# CUDA 12.x 版本\ndotnet add package JYPPX.TensorRT.CSharp.API.runtime.win-x64.cuda12\n\n# 或 CUDA 11.x 版本\ndotnet add package JYPPX.TensorRT.CSharp.API.runtime.win-x64.cuda11\n</code></pre>\n<blockquote>\n<p><strong>💡 小贴士</strong>：Runtime 包与 CUDA 版本相关，请根据您设备上安装的 CUDA 版本选择对应的包。</p>\n</blockquote>\n<p><img alt=\"image-20260109233203782\" class=\"lazyload\" /></p>\n<h3 id=\"32-系统要求\">3.2 系统要求</h3>\n<table>\n<thead>\n<tr>\n<th>要求</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>操作系统</strong></td>\n<td>Windows 10+、Linux（Ubuntu 18.04+）、macOS 10.15+</td>\n</tr>\n<tr>\n<td><strong>.NET 版本</strong></td>\n<td>.NET 5.0-10.0、.NET Core 3.1、.NET Framework 4.7.1+</td>\n</tr>\n<tr>\n<td><strong>GPU</strong></td>\n<td>NVIDIA GPU（支持 CUDA 11.x 或 12.x）</td>\n</tr>\n<tr>\n<td><strong>依赖</strong></td>\n<td>NVIDIA TensorRT 10.x、CUDA Runtime</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"33-重要版本说明\">3.3 重要版本说明</h3>\n<blockquote>\n<p><strong>⚠️ 重要提醒：NVIDIA TensorRT 必须是 10.x 系列！！</strong></p>\n</blockquote>\n<p>TensorRtSharp 3.0 基于 TensorRT 10.x 开发，不支持 TensorRT 8.x 或 9.x 版本。</p>\n<p>为了防止出现兼容性问题，建议使用与博主相同的配置：</p>\n<p><strong>配置 1（推荐）：</strong></p>\n<ul>\n<li>CUDA 11.6</li>\n<li>cuDNN 9.2.0</li>\n<li>TensorRT 10.13.0.35</li>\n</ul>\n<p><strong>配置 2：</strong></p>\n<ul>\n<li>CUDA 12.3</li>\n<li>cuDNN 9.2.0</li>\n<li>TensorRT 10.11.0.33</li>\n</ul>\n<h3 id=\"34-配置原生库\">3.4 配置原生库</h3>\n<p>TensorRtSharp 依赖 TensorRT 的原生库（<code>nvinfer.dll</code>）和 CUDA 的原生库（<code>cudart64_*.dll</code> 等）。有两种配置方式：</p>\n<h4 id=\"方式一拷贝-dll-到应用程序目录不推荐\">方式一：拷贝 DLL 到应用程序目录（不推荐）</h4>\n<p>将 TensorRT 和 CUDA 的所有 DLL 文件拷贝到程序可执行目录下。</p>\n<p><strong>缺点</strong>：</p>\n<ul>\n<li>会导致程序目录文件庞大</li>\n<li>不方便管理与部署</li>\n<li><strong>不推荐使用此方式</strong></li>\n</ul>\n<h4 id=\"方式二设置系统-path推荐\">方式二：设置系统 PATH（推荐）</h4>\n<p>将 TensorRT 的 lib 目录和 CUDA 的 bin 目录路径添加到系统 PATH 环境变量中。</p>\n<p><strong>优点</strong>：</p>\n<ul>\n<li>无需复制大量文件</li>\n<li>保持应用目录整洁</li>\n<li>便于版本管理和部署维护</li>\n</ul>\n<p><strong>配置步骤</strong>：</p>\n<ol>\n<li><strong>设置 CUDA_PATH 环境变量</strong></li>\n</ol>\n<p><img alt=\"CUDA_PATH 配置\" class=\"lazyload\" /></p>\n<ol start=\"2\">\n<li><strong>设置 PATH 环境变量</strong></li>\n</ol>\n<p>将以下路径添加到 PATH：</p>\n<ul>\n<li>CUDA 的 bin 目录（如 <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin</code>）</li>\n<li>TensorRT 的 lib 目录（如 <code>C:\\TensorRT-10.13.0.35\\lib</code>）</li>\n</ul>\n<p><img alt=\"PATH 配置\" class=\"lazyload\" /></p>\n<blockquote>\n<p><strong>💡 建议</strong>：优先使用环境变量方式配置，避免因文件冗余导致部署复杂。同时注意不同 CUDA 版本间的兼容性问题。</p>\n</blockquote>\n<hr />\n<h2 id=\"四核心架构设计\">四、核心架构设计</h2>\n<h3 id=\"41-三层架构\">4.1 三层架构</h3>\n<p>TensorRtSharp 采用清晰的三层架构设计：</p>\n<pre><code>┌─────────────────────────────────────────────────────────┐\n│           业务 API 层 (High-Level API)                   │\n│  Runtime, Builder, CudaEngine, ExecutionContext         │\n└─────────────────────────────────────────────────────────┘\n                           ▲\n                           │\n┌─────────────────────────────────────────────────────────┐\n│         资源管理层 (Resource Management)                 │\n│  DisposableTrtObject, DisposableObject, IOvPtrHolder    │\n└─────────────────────────────────────────────────────────┘\n                           ▲\n                           │\n┌─────────────────────────────────────────────────────────┐\n│         P/Invoke 层 (Native Interop)                    │\n│  NativeMethodsTensorRt*, NativeMethodsCuda*             │\n└─────────────────────────────────────────────────────────┘\n</code></pre>\n<h3 id=\"42-自动资源管理\">4.2 自动资源管理</h3>\n<p>TensorRtSharp 实现了完善的资源管理机制，所有 TensorRT 对象都继承自 <code>DisposableTrtObject</code>：</p>\n<pre><code class=\"language-csharp\">// 所有 TensorRT 对象继承自 DisposableTrtObject\npublic abstract class DisposableTrtObject : DisposableObject\n{\n    protected IntPtr ptr;                      // 原生对象指针\n    public bool IsDisposed { get; protected set; }\n\n    // 安全访问原生指针（自动检查释放状态）\n    public IntPtr TrtPtr\n    {\n        get\n        {\n            ThrowIfDisposed();\n            return ptr;\n        }\n    }\n\n    // 释放非托管资源\n    protected override void DisposeUnmanaged()\n    {\n        if (ptr != IntPtr.Zero)\n        {\n            // 调用原生释放函数\n            NativeDestroy(ptr);\n            ptr = IntPtr.Zero;\n        }\n    }\n}\n\n// 使用 using 语句自动释放资源\nusing Runtime runtime = new Runtime();\nusing CudaEngine engine = runtime.deserializeCudaEngineByBlob(data, size);\n// 离开作用域时自动释放\n</code></pre>\n<p><strong>设计亮点</strong>：</p>\n<ul>\n<li>✅ 采用标准 Dispose 模式，确保资源正确释放</li>\n<li>✅ 线程安全的资源释放机制（使用 <code>Interlocked.Exchange</code>）</li>\n<li>✅ 自动内存压力通知（<code>GC.AddMemoryPressure</code>）</li>\n<li>✅ 指针安全访问（<code>ThrowIfDisposed</code> 检查）</li>\n</ul>\n<hr />\n<h2 id=\"五核心类与-api\">五、核心类与 API</h2>\n<h3 id=\"51-命名空间\">5.1 命名空间</h3>\n<p>在使用 TensorRtSharp 之前，首先引入必要的命名空间：</p>\n<pre><code class=\"language-csharp\">using JYPPX.TensorRtSharp.Cuda;      // CUDA 接口的程序集命名空间\nusing JYPPX.TensorRtSharp.Nvinfer;   // TensorRT 接口的程序集命名空间\n</code></pre>\n<h3 id=\"52-runtime推理运行时\">5.2 Runtime（推理运行时）</h3>\n<p>Runtime 是 TensorRT 推理的入口点，负责从序列化的引擎文件创建推理引擎。</p>\n<pre><code class=\"language-csharp\">// 创建 Runtime 实例\nRuntime runtime = new Runtime();\nstring filePath = \"yolov8s-obb.engine\";\n\n// 从字节数组反序列化引擎\nbyte[] data = File.ReadAllBytes(filePath);\nusing CudaEngine cudaEngine = runtime.deserializeCudaEngineByBlob(data, (ulong)data.Length);\n\n// 从文件流反序列化\nusing var reader = new FileStreamReader();\nreader.open(filePath);\nusing CudaEngine cudaEngine = runtime.deserializeCudaEngineByFileStreamReader(reader);\n\n// 配置 DLA（深度学习加速器）\nruntime.setDLACore(0);  // 使用 DLA 核心 0\nint dlaCores = runtime.getNbDLACores();\n\n// 设置最大线程数\nruntime.setMaxThreads(4);\n</code></pre>\n<p><strong>主要用途</strong>：</p>\n<ul>\n<li>反序列化 TensorRT 引擎文件</li>\n<li>配置 DLA 加速器</li>\n<li>加载插件库</li>\n</ul>\n<h3 id=\"53-builder模型构建器\">5.3 Builder（模型构建器）</h3>\n<p>Builder 用于从 ONNX 模型构建 TensorRT 引擎。</p>\n<pre><code class=\"language-csharp\">using Builder builder = new Builder();\n\n// 查询平台能力\nbool hasFP16 = builder.platformHasFastFp16();  // 是否支持 FP16\nbool hasINT8 = builder.platformHasFastInt8();  // 是否支持 INT8\nint maxDLABatch = builder.maxDLABatchSize();   // DLA 最大批大小\n\n// 创建网络定义（显式批处理模式）\nusing NetworkDefinition network = builder.createNetworkV2(\n    TrtNetworkDefinitionCreationFlag.kEXPLICIT_BATCH);\n\n// 创建构建器配置\nusing BuilderConfig config = builder.createBuilderConfig();\n\n// 创建优化配置文件（用于动态形状）\nusing OptimizationProfile profile = builder.createOptimizationProfile();\n\n// 构建序列化网络\nusing HostMemory serialized = builder.buildSerializedNetwork(network, config);\n\n// 保存引擎文件\nusing (FileStream fs = new FileStream(\"model.engine\", FileMode.Create, FileAccess.Write))\n{\n    fs.Write(serialized.getByteData(), 0, (int)serialized.Size);\n}\n</code></pre>\n<p><strong>主要用途</strong>：</p>\n<ul>\n<li>创建网络定义和构建配置</li>\n<li>查询硬件能力（FP16、INT8、DLA）</li>\n<li>构建 TensorRT 引擎</li>\n<li>注册自定义插件</li>\n</ul>\n<h3 id=\"54-cudaengine推理引擎\">5.4 CudaEngine（推理引擎）</h3>\n<p>CudaEngine 是推理的核心对象，包含优化后的模型计算图。</p>\n<pre><code class=\"language-csharp\">// 获取张量信息\nint numTensors = engine.getNbIOTensors();\nstring inputName = engine.getIOTensorName(0);    // 输入张量名称\nstring outputName = engine.getIOTensorName(1);   // 输出张量名称\n\nDims inputShape = engine.getTensorShape(inputName);\nTrtDataType inputType = engine.getTensorDataType(inputName);\n\n// 创建执行上下文\nusing ExecutionContext context = engine.createExecutionContext();\nusing ExecutionContext contextStatic = engine.createExecutionContext(\n    TrtExecutionContextAllocationStrategy.kSTATIC);\n\n// 序列化引擎\nusing HostMemory memory = engine.serialize();\n\n// 查询引擎属性\nint numLayers = engine.getNbLayers();\nstring name = engine.getName();\nlong deviceMemory = engine.getDeviceMemorySize();\n</code></pre>\n<p><strong>主要用途</strong>：</p>\n<ul>\n<li>查询模型输入输出信息</li>\n<li>创建执行上下文</li>\n<li>序列化引擎</li>\n<li>性能分析</li>\n</ul>\n<h3 id=\"55-executioncontext执行上下文\">5.5 ExecutionContext（执行上下文）</h3>\n<p>ExecutionContext 管理单次推理的执行环境，支持异步推理和动态形状。</p>\n<pre><code class=\"language-csharp\">// 绑定张量地址\nCuda1DMemory&lt;float&gt; input = new Cuda1DMemory&lt;float&gt;(3 * 1024 * 1024);\nCuda1DMemory&lt;float&gt; output = new Cuda1DMemory&lt;float&gt;(1 * 20 * 21504);\ncontext.setInputTensorAddress(\"images\", input.get());\ncontext.setOutputTensorAddress(\"output0\", output.get());\n\n// 设置动态形状\ncontext.setinputShape(\"images\", new Dims(1, 3, 1024, 1024));\nDims shape = context.getTensorShape(\"images\");\n\n// 执行推理（异步，使用 CUDA Stream）\nusing CudaStream stream = new CudaStream();\ncontext.executeV3(stream);\nstream.Synchronize();  // 等待完成\n\n// 设置优化配置文件（动态形状）\ncontext.setOptimizationProfileAsync(0, stream);\n\n// 调试功能\ncontext.setDebugSync(true);\n</code></pre>\n<p><strong>主要用途</strong>：</p>\n<ul>\n<li>绑定输入输出张量</li>\n<li>设置动态形状</li>\n<li>执行推理（异步）</li>\n<li>性能分析和调试</li>\n</ul>\n<h3 id=\"56-onnxparseronnx-解析器\">5.6 OnnxParser（ONNX 解析器）</h3>\n<p>OnnxParser 将 ONNX 模型转换为 TensorRT 网络定义。</p>\n<pre><code class=\"language-csharp\">// 解析 ONNX 文件\nusing NetworkDefinition network = build.createNetworkV2(TrtNetworkDefinitionCreationFlag.kEXPLICIT_BATCH);\nusing OnnxParser parser = new OnnxParser(network);\nbool success = parser.parseFromFile(\"yolov8s-obb.onnx\", verbosity: 2);\n\n// 检查算子支持\nbool supportsConv = parser.supportsOperator(\"Conv\");\n\n// 子图支持\nlong numSubgraphs = parser.getNbSubgraphs();\nbool supported = parser.isSubgraphSupported(0);\nlong[] nodes = parser.getSubgraphNodes(0);\n\n// 设置解析器标志\nparser.setFlag(TrtOnnxParserFlag.kNATIVE_INSTANCENORM);\n</code></pre>\n<p><strong>主要用途</strong>：</p>\n<ul>\n<li>解析 ONNX 模型</li>\n<li>检查算子支持</li>\n<li>处理子图</li>\n</ul>\n<h3 id=\"57-cuda-内存管理\">5.7 CUDA 内存管理</h3>\n<h4 id=\"1设备内存cuda1dmemory\">（1）设备内存（Cuda1DMemory）</h4>\n<pre><code class=\"language-csharp\">// 创建设备内存\nusing Cuda1DMemory&lt;float&gt; input = new Cuda1DMemory&lt;float&gt;(1000);\nulong numElements = input.SizeElements;\nulong numBytes = input.SizeBytes;\nIntPtr ptr = input.DevicePointer;\n\n// 同步数据传输\nfloat[] hostData = new float[1000];\ninput.copyFromHost(hostData);   // 主机 → 设备\ninput.copyToHost(hostData);      // 设备 → 主机\n\n// 异步数据传输\nusing CudaStream stream = new CudaStream();\ninput.copyFromHostAsync(hostData, stream);\ninput.copyToHostAsync(hostData, stream);\n\n// 内存操作\ninput.memset(0);                 // 填充为 0\ninput.memsetAsync(0, stream);    // 异步填充\n</code></pre>\n<h4 id=\"2cuda-流cudastream\">（2）CUDA 流（CudaStream）</h4>\n<pre><code class=\"language-csharp\">// 创建流（带优先级）\nusing CudaStream stream = new CudaStream();\nusing CudaStream streamHigh = new CudaStream(0, -1);  // 高优先级\n\n// 同步操作\nstream.Synchronize();  // 等待流完成\nbool isComplete = stream.Query();  // 查询是否完成\n\n// 事件依赖\nusing CudaEvent cudaEvent = new CudaEvent();\nstream.WaitEvent(cudaEvent);  // 等待事件\n\n// 添加回调\nstream.AddCallback((streamPtr, statue, userData) =&gt;\n{\n    Console.WriteLine(\"Stream callback executed\");\n}, IntPtr.Zero, 0);\n\n// CUDA Graph 捕获\nstream.BeginCapture(CudaStreamCaptureMode.Global);\n// ... 执行操作 ...\nCudaGraph_t graph = stream.EndCapture();\n</code></pre>\n<h4 id=\"3cuda-设备cudadevice\">（3）CUDA 设备（CudaDevice）</h4>\n<pre><code class=\"language-csharp\">// 获取系统中启用的 CUDA 兼容设备的数量\nint nbDevices = CudaDevice.GetDeviceCount();\n\n// 获取指定设备的属性\nCudaDeviceProp properties = CudaDevice.GetDeviceProperties(deviceIdx);\n\n// 设置执行设备\nCudaDevice.SetDevice(device);\n\n// 获取有关设备的请求信息\nint clockRate = CudaDevice.GetAttribute(CudaDeviceAttr.ClockRate, device);\n</code></pre>\n<hr />\n<h2 id=\"六完整使用示例\">六、完整使用示例</h2>\n<h3 id=\"示例-1获取和设置设备信息\">示例 1：获取和设置设备信息</h3>\n<p>下面的代码可以获取当前设备的相关信息，同时可以设置推理设备。</p>\n<pre><code class=\"language-csharp\">using JYPPX.TensorRtSharp.Cuda;\nusing JYPPX.TensorRtSharp.Nvinfer;\n\nnamespace TestDemo\n{\n    internal class Program\n    {\n        static void Main(string[] args)\n        {\n            // 指定默认使用的 GPU 设备索引\n            // 在多 GPU 环境下，可以通过修改此变量来选择特定的显卡\n            int device = 0;\n\n            // 记录日志，标记设备信息查询的开始\n            Logger.Instance.INFO(\"=== Device Information ===\");\n\n            // 获取当前系统中可见的 NVIDIA GPU 数量\n            int nbDevices = CudaDevice.GetDeviceCount();\n\n            // 检查系统中是否存在可用的 GPU 设备\n            if (nbDevices &lt;= 0)\n            {\n                Logger.Instance.ERROR(\"Cannot find any available devices (GPUs)!\");\n                Environment.Exit(0);\n            }\n\n            // 打印所有可用设备的列表\n            Logger.Instance.INFO(\"Available Devices: \");\n\n            // 遍历系统中的每一个 GPU\n            for (int deviceIdx = 0; deviceIdx &lt; nbDevices; ++deviceIdx)\n            {\n                // 获取索引为 deviceIdx 的 GPU 的详细属性\n                CudaDeviceProp tempProperties = CudaDevice.GetDeviceProperties(deviceIdx);\n\n                // 打印设备 ID、设备名称以及 UUID (唯一标识符)\n                Logger.Instance.INFO($\"  Device {deviceIdx}: \\\"{tempProperties.Name}\\\" UUID: {GetUuidString(tempProperties.Uuid)}\");\n\n                // 如果当前遍历到的设备 ID 是我们想要使用的目标设备\n                // 则将该设备的属性保存下来，供后续使用\n                if (deviceIdx == device)\n                {\n                    properties = tempProperties;\n                }\n            }\n\n            // 安全检查：确保请求的目标设备 ID 在有效范围内\n            if (device &lt; 0 || device &gt;= nbDevices)\n            {\n                Logger.Instance.ERROR($\"Cannot find device ID {device}!\");\n                Environment.Exit(0);\n            }\n\n            // 将 CUDA 上下文设置到指定的 GPU 设备上\n            CudaDevice.SetDevice(device);\n\n            // 打印选定设备的详细信息\n            Logger.Instance.INFO($\"Selected Device: {properties.Name}\");\n            Logger.Instance.INFO($\"Selected Device ID: {device}\");\n            Logger.Instance.INFO($\"Selected Device UUID: {GetUuidString(properties.Uuid)}\");\n            Logger.Instance.INFO($\"Compute Capability: {properties.Major}.{properties.Minor}\");\n            Logger.Instance.INFO($\"SMs: {properties.MultiProcessorCount}\");\n            Logger.Instance.INFO($\"Device Global Memory: {(properties.TotalGlobalMem + 20)} MiB\");\n            Logger.Instance.INFO($\"Shared Memory per SM: {(properties.SharedMemPerMultiprocessor &gt;&gt; 10)} KiB\");\n            Logger.Instance.INFO($\"Memory Bus Width: {properties.MemoryBusWidth} bits (ECC {(properties.ECCEnabled != 0 ? \"enabled\" : \"disabled\")})\");\n\n            // 获取并打印 GPU 核心时钟频率和显存时钟频率\n            int clockRate = CudaDevice.GetAttribute(CudaDeviceAttr.ClockRate, device);\n            int memoryClockRate = CudaDevice.GetAttribute(CudaDeviceAttr.MemoryClockRate, device);\n\n            Logger.Instance.INFO($\"Application Compute Clock Rate: {clockRate / 1000000.0F} GHz\");\n            Logger.Instance.INFO($\"Application Memory Clock Rate: {memoryClockRate / 1000000.0F} GHz\");\n        }\n\n        /// &lt;summary&gt;\n        /// 辅助方法：将 CudaUUID 结构体转换为格式化的 GPU UUID 字符串\n        /// 格式通常为：GPU-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n        /// &lt;/summary&gt;\n        public static string GetUuidString(CudaUUID uuid)\n        {\n            int kUUID_SIZE = uuid.Bytes.Length;\n            var ss = new System.Text.StringBuilder();\n\n            // 定义 UUID 的分段点，用于插入连字符 \"-\"\n            int[] splits = { 0, 4, 6, 8, 10, kUUID_SIZE };\n\n            // 添加固定的 \"GPU\" 前缀\n            ss.Append(\"GPU\");\n\n            // 遍历分段定义，格式化每一部分的字节\n            for (int splitIdx = 0; splitIdx &lt; splits.Length - 1; ++splitIdx)\n            {\n                ss.Append(\"-\");\n                for (int byteIdx = splits[splitIdx]; byteIdx &lt; splits[splitIdx + 1]; ++byteIdx)\n                {\n                    ss.AppendFormat(\"{0:x2}\", uuid.Bytes[byteIdx]);\n                }\n            }\n\n            return ss.ToString();\n        }\n    }\n}\n</code></pre>\n<p><strong>程序运行结果：</strong></p>\n<p><img alt=\"设备信息输出\" class=\"lazyload\" /></p>\n<blockquote>\n<p><strong>💡 注意</strong>：不同的设备输出会有不同，以具体设备输出为准。</p>\n<p>🔗<strong>程序路径链接</strong>：完整程序已经上传到GitHub，请自行下载，链接为：</p>\n<pre><code>https://github.com/guojin-yan/TensorRT-CSharp-API/tree/TensorRtSharp3.0/samples/SetCudaDeviceInfo\n</code></pre>\n</blockquote>\n<hr />\n<h3 id=\"示例-2onnx-转-engine-模型\">示例 2：ONNX 转 Engine 模型</h3>\n<p>下面是按照官方模型转换代码编写的一个简单的转换代码：</p>\n<pre><code class=\"language-csharp\">using JYPPX.TensorRtSharp.Cuda;\nusing JYPPX.TensorRtSharp.Nvinfer;\n\nnamespace OnnxToEngine\n{\n    internal class Program\n    {\n        static void Main(string[] args)\n        {\n            // === 配置 TensorRT 日志回调 ===\n            // 定义一个委托，用于处理 TensorRT 内部产生的日志消息\n            LogCallbackFunction _callbackDelegate = (message) =&gt;\n            {\n                Console.WriteLine(message);\n            };\n\n            // 将自定义的回调函数注册给 TensorRT 的全局 Logger 实例\n            Logger.Instance.SetCallback(_callbackDelegate);\n\n            // 设置日志的严重性级别阈值\n            // LoggerSeverity.kINFO: 打印信息、警告和错误\n            Logger.Instance.SetThreshold(LoggerSeverity.kINFO);\n\n            // 1. 创建 TensorRT Builder (构建器)\n            Builder build = new Builder();\n\n            // 2. 创建网络定义 (Network Definition)\n            // 显式批处理 标志表示网络定义中显式包含批处理维度\n            NetworkDefinition networkDefinition = build.createNetworkV2(TrtNetworkDefinitionCreationFlag.kEXPLICIT_BATCH);\n\n            // 3. 创建构建器配置\n            BuilderConfig builderConfig = build.createBuilderConfig();\n\n            // 4. 创建 ONNX 解析器\n            OnnxParser onnxParser = new OnnxParser(networkDefinition);\n\n            // 指定待转换的 ONNX 模型文件路径\n            string modelpath = \"yolo11s-obb.onnx\";\n\n            // 5. 解析 ONNX 模型文件\n            // 参数 2: 日志级别 (1=ERROR, 2=WARNING, 3=INFO, 4=VERBOSE)\n            if (onnxParser.parseFromFile(modelpath, 2) == false)\n            {\n                Console.WriteLine($\"parse onnx model failed\");\n                return;\n            }\n\n            // 6. 设置构建精度标志\n            // kFP16: 启用半精度 (FP16) 推理模式\n            builderConfig.setFlag(TrtBuilderFlag.kFP16);\n\n            // 7. 创建 CUDA 流\n            CudaStream cudaStream = new CudaStream();\n\n            // 8. 设置优化配置文件的流\n            builderConfig.setProfileStream(cudaStream);\n\n            // 9. 构建并序列化网络\n            // 这是一个耗时较长的过程，因为 TensorRT 会进行内核自动调优、层融合等优化\n            HostMemory hostMemory = build.buildSerializedNetwork(networkDefinition, builderConfig);\n\n            // 10. 保存 Engine 到磁盘\n            string filePath = \"yolo11s-obb.engine\";\n            using (FileStream fs = new FileStream(filePath, FileMode.Create, FileAccess.Write))\n            {\n                fs.Write(hostMemory.getByteData(), 0, (int)hostMemory.Size);\n            }\n\n            Console.WriteLine(\"Engine saved successfully!\");\n        }\n    }\n}\n</code></pre>\n<p><strong>程序运行结果：</strong></p>\n<p><img alt=\"ONNX 转换结果\" class=\"lazyload\" /></p>\n<blockquote>\n<p>🔗<strong>程序路径链接</strong>：完整程序已经上传到GitHub，请自行下载，链接为：</p>\n<pre><code>https://github.com/guojin-yan/TensorRT-CSharp-API/tree/TensorRtSharp3.0/samples/OnnxToEngine\n</code></pre>\n</blockquote>\n<hr />\n<h3 id=\"-使用-trtexec-工具转换模型推荐\">💡 使用 trtexec 工具转换模型（推荐）</h3>\n<p>当前 ONNX 转 Engine 代码由于没有进行优化，转换速度会较慢。<strong>建议使用 TensorRT SDK 自带的 <code>trtexec.exe</code> 工具转换模型</strong>。</p>\n<h4 id=\"trtexec-使用方式\">trtexec 使用方式</h4>\n<p><strong>（1）使用 CMD 切换到工具目录</strong></p>\n<p>该工具存放在下载的 TensorRT 库中：</p>\n<p><img alt=\"trtexec 位置\" class=\"lazyload\" /></p>\n<p>打开 CMD 并切换到该路径：</p>\n<p><img alt=\"CMD 切换路径\" class=\"lazyload\" /></p>\n<p><strong>（2）固定形状模型转换指令</strong></p>\n<p>对于形状固定的模型，直接输入常规指令转换即可：</p>\n<pre><code class=\"language-bash\">trtexec.exe --onnx=yolov8s-obb.onnx --saveEngine=yolov8s-obb.engine --fp16 --workspace=1024\n</code></pre>\n<p><strong>参数说明：</strong></p>\n<ul>\n<li><code>--onnx=yolov8s-obb.onnx</code>：指定输入的 ONNX 模型文件路径</li>\n<li><code>--saveEngine=yolov8s-obb.engine</code>：指定输出的 Engine 文件保存路径</li>\n<li><code>--fp16</code>：启用 FP16 精度（可选）</li>\n<li><code>--workspace=1024</code>：指定最大工作空间，单位 MB（可选）</li>\n</ul>\n<p><img alt=\"trtexec 转换中\" class=\"lazyload\" /></p>\n<p><strong>（3）动态形状模型转换指令</strong></p>\n<p>对于输入形状是动态的情况，转换时要设置形状参数：</p>\n<pre><code class=\"language-bash\">trtexec.exe --onnx=yolov8s-obb_b.onnx --saveEngine=yolov8s-obb_b.engine --fp16 --minShapes=images:1x3x1024x1024 --optShapes=images:8x3x1024x1024 --maxShapes=images:24x3x1024x1024\n</code></pre>\n<p><strong>参数说明：</strong></p>\n<ul>\n<li><code>--minShapes=images:1x3x1024x1024</code>：最小输入形状</li>\n<li><code>--optShapes=images:8x3x1024x1024</code>：最优输入形状（Engine 会为此形状优化）</li>\n<li><code>--maxShapes=images:24x3x1024x1024</code>：最大输入形状</li>\n</ul>\n<p><img alt=\"动态形状转换\" class=\"lazyload\" /></p>\n<p><strong>多输入模型转换指令：</strong></p>\n<pre><code class=\"language-bash\">trtexec --onnx=model.onnx --minShapes=input1:1x3x224x224,input2:1x256 --optShapes=input1:4x3x224x224,input2:4x256 --maxShapes=input1:8x3x224x224,input2:8x256\n</code></pre>\n<hr />\n<h3 id=\"示例-3yolo-目标检测\">示例 3：YOLO 目标检测</h3>\n<p>下面是一个完整的 YOLO 目标检测示例，展示从模型构建到推理的全流程。</p>\n<blockquote>\n<p><strong>⚠️ 由于代码较长，此处仅展示核心思路。完整代码请参考项目示例。</strong></p>\n</blockquote>\n<pre><code class=\"language-csharp\">using JYPPX.TensorRtSharp.Cuda;\nusing JYPPX.TensorRtSharp.Nvinfer;\nusing OpenCvSharp;\nusing OpenCvSharp.Dnn;\nusing System.Diagnostics;\nusing System.Runtime.InteropServices;\n\nnamespace YoloDetInfer\n{\n    internal class Program\n    {\n        // ================= 配置参数 =================\n        // 模型输入尺寸 (宽=高)\n        private const int InputSize = 640;\n\n\n        // 建议根据实际模型动态获取或使用 Netron 查看\n        private const int OutputSize = 8400;\n\n        // 模型类别数 (根据您的具体数据集修改，此处假设为15类)\n        private const int CategoryNum = 80;\n\n        // 置信度阈值\n        private const float ConfThreshold = 0.25f;\n\n        // NMS IOU 阈值\n        private const float NmsThreshold = 0.3f;\n\n        static void Main(string[] args)\n        {\n            //  ============= 配置 TensorRT 日志回调 =============\n            // 定义一个委托，用于处理 TensorRT 内部产生的日志消息。\n            // 这允许我们将 C++ 层面的日志输出到 C# 的控制台。\n            LogCallbackFunction _callbackDelegate = (message) =&gt;\n            {\n                Console.WriteLine(message);\n            };\n\n            // 将自定义的回调函数注册给 TensorRT 的全局 Logger 实例。\n            Logger.Instance.SetCallback(_callbackDelegate);\n\n            // 设置日志的严重性级别阈值。\n            // LoggerSeverity.kINFO: 打印信息、警告和错误。\n            // 开发调试阶段通常设为 kINFO 或 kVERBOSE；生产环境可设为 kWARNING 或 kERROR 以减少输出。\n            Logger.Instance.SetThreshold(LoggerSeverity.kINFO);\n\n            string enginePath = \"yolov8s.engine\";\n            string imagePath = \"bus.jpg\";\n\n\n            // ================= 1. 加载 TensorRT Engine =================\n            // 使用 using 语句确保文件流正确关闭\n            byte[] engineData;\n            using (FileStream fs = new FileStream(enginePath, FileMode.Open, FileAccess.Read))\n            using (BinaryReader br = new BinaryReader(fs))\n            {\n                engineData = br.ReadBytes((int)fs.Length);\n            }\n\n            // 反序列化 Engine\n            // Runtime 必须在 Engine 生命周期内保持存活，通常建议设为全局或静态，或者确保它最后释放\n            Runtime runtime = new Runtime();\n\n            // 创建 CudaEngine (此处使用 using 确保推理完成后引擎被销毁)\n            using (CudaEngine cudaEngine = runtime.deserializeCudaEngineByBlob(engineData, (ulong)engineData.Length))\n            {\n                // ================= 2. 初始化推理上下文与显存 =================\n                // 创建执行上下文\n                using (JYPPX.TensorRtSharp.Nvinfer.ExecutionContext executionContext = cudaEngine.createExecutionContext(TrtExecutionContextAllocationStrategy.kSTATIC))\n                using (CudaStream cudaStream = new CudaStream()) // 创建 CUDA 流用于异步执行\n                {\n                    // 获取输入维度信息 (用于校验)\n                    Dims inputDims = executionContext.getTensorShape(\"images\");\n                    Logger.Instance.INFO($\"Input Shape: {inputDims.d[0]}x{inputDims.d[1]}x{inputDims.d[2]}x{inputDims.d[3]}\");\n\n                    // 计算所需显存大小\n                    // 输入: Batch=1, Channel=3, Height=640, Width=640\n                    ulong inputSizeInBytes = 1 * 3 * InputSize * InputSize;\n                    // 输出: Batch=1, Channels=CategoryNum+4(box)+1(angle), Num=8400\n                    int outputChannels = CategoryNum + 4; // 4坐标 + N类别\n                    ulong outputSizeInBytes = (ulong)(1 * outputChannels * OutputSize);\n\n                    Stopwatch sw = new Stopwatch();\n                    // 分配 GPU 显存\n                    using (Cuda1DMemory&lt;float&gt; inputGpuMemory = new Cuda1DMemory&lt;float&gt;(inputSizeInBytes))\n                    using (Cuda1DMemory&lt;float&gt; outputGpuMemory = new Cuda1DMemory&lt;float&gt;(outputSizeInBytes))\n                    {\n                        // 绑定显存地址到 TensorRT 上下文\n                        executionContext.setInputTensorAddress(\"images\", inputGpuMemory.get());\n                        executionContext.setOutputTensorAddress(\"output0\", outputGpuMemory.get());\n                        // 预热推理 (可选，但推荐，尤其是首次推理时)\n                        executionContext.executeV3(cudaStream);\n                        cudaStream.Synchronize();\n                        // ================= 3. 图像预处理 =================\n                        Mat img = Cv2.ImRead(imagePath);\n                        if (img.Empty())\n                        {\n                            Logger.Instance.INFO(\"Image not found!\");\n                            return;\n                        }\n\n                        sw.Start();\n                        float[] inputData = PreProcess(img, out float scale, out int xOffset, out int yOffset);\n                        sw.Stop();\n                        Logger.Instance.INFO($\"Pre-processing time: {sw.ElapsedMilliseconds} ms\");\n                        // ================= 4. 推理 =================\n                        // 准备主机内存接收结果\n                        float[] outputData = new float[outputChannels * OutputSize];\n\n\n                        sw.Restart();\n                        // 将数据从主机 拷贝到设备\n                        inputGpuMemory.copyFromHostAsync(inputData, cudaStream);\n\n                        // 执行推理 (enqueueV3 是异步的)\n                        executionContext.executeV3(cudaStream);\n                        // 等待推理完成\n                        cudaStream.Synchronize();\n\n           \n\n                        // 将结果从设备 拷贝回主机\n                        // 这里的拷贝是同步的，会等待 GPU 计算完成\n                        outputGpuMemory.copyToHostAsync(outputData, cudaStream);\n                        sw.Stop();\n                        Logger.Instance.INFO($\"Inference time: {sw.ElapsedMilliseconds} ms\");\n                        // ================= 5. 后处理 =================\n\n                        sw.Restart();\n                        List&lt;DetData&gt; results = PostProcess(outputData, scale, xOffset, yOffset);\n                        sw.Stop();\n                        Logger.Instance.INFO($\"Post-processing time: {sw.ElapsedMilliseconds} ms\");\n\n                        // ================= 6. 结果可视化 =================\n                        Mat resultImg = DrawDetResult(results, img);\n                        Cv2.ImShow(\"YOLO11-DET Result\", resultImg);\n                        Cv2.WaitKey(0);\n                    }\n                }\n            }\n        }\n\n        /// &lt;summary&gt;\n        /// 图像预处理：Letterbox 缩放、归一化、HWC 转 CHW\n        /// &lt;/summary&gt;\n        private static float[] PreProcess(Mat img, out float scale, out int xOffset, out int yOffset)\n        {\n            // 转换颜色空间 BGR -&gt; RGB\n            Mat rgbImg = new Mat();\n            Cv2.CvtColor(img, rgbImg, ColorConversionCodes.BGR2RGB);\n\n            // 计算 Letterbox 缩放比例\n            int maxDim = Math.Max(rgbImg.Width, rgbImg.Height);\n            scale = (float)maxDim / InputSize;\n\n            // 计算缩放后的尺寸\n            int newWidth = (int)(rgbImg.Width / scale);\n            int newHeight = (int)(rgbImg.Height / scale);\n\n            // Resize 图像\n            Mat resizedImg = new Mat();\n            Cv2.Resize(rgbImg, resizedImg, new Size(newWidth, newHeight));\n\n            // 创建黑色背景 Canvas (InputSize x InputSize)\n            Mat paddedImg = Mat.Zeros(InputSize, InputSize, MatType.CV_8UC3);\n\n            // 计算粘贴位置 (居中)\n            xOffset = (InputSize - newWidth) / 2;\n            yOffset = (InputSize - newHeight) / 2;\n\n            // 将图像拷贝到 Canvas 中央\n            Rect roi = new Rect(xOffset, yOffset, newWidth, newHeight);\n            resizedImg.CopyTo(new Mat(paddedImg, roi));\n\n            // 归一化 (0-255 -&gt; 0-1) 并转为 float 类型\n            Mat floatImg = new Mat();\n            paddedImg.ConvertTo(floatImg, MatType.CV_32FC3, 1.0 / 255.0);\n\n            // HWC 转 CHW 并展平为一维数组\n            Mat[] channels = Cv2.Split(floatImg);\n            float[] chwData = new float[3 * InputSize * InputSize];\n\n            // 拷贝数据：R通道 -&gt; C通道 -&gt; B通道 (OpenCV Split 出来顺序是 B, G, R，对应索引 0, 1, 2)\n            int channelSize = InputSize * InputSize;\n            // 将 R, G, B 依次拷入数组\n            Marshal.Copy(channels[0].Data, chwData, 0, channelSize); // R\n            Marshal.Copy(channels[1].Data, chwData, channelSize, channelSize); // G\n            Marshal.Copy(channels[2].Data, chwData, channelSize * 2, channelSize); // B\n\n            // 释放临时 Mat\n            rgbImg.Dispose();\n            resizedImg.Dispose();\n            paddedImg.Dispose();\n            floatImg.Dispose();\n            foreach (var c in channels) c.Dispose();\n\n            return chwData;\n        }\n\n        /// &lt;summary&gt;\n        /// 后处理：解析 TensorRT 输出、NMS 过滤\n        /// &lt;/summary&gt;\n        private static List&lt;DetData&gt; PostProcess(float[] result, float scale, int xOffset, int yOffset)\n        {\n            List&lt;Rect&gt; boxes = new List&lt;Rect&gt;();\n            List&lt;float&gt; confidences = new List&lt;float&gt;();\n            List&lt;int&gt; classIds = new List&lt;int&gt;();\n\n            // 遍历所有预测框 (OutputSize)\n            // 数据布局: [4(box) + 80(classes)] * OutputSize\n            // 展平数组中，同一属性的数据是连续存储的，例如所有 cx 在一起，所有 cy 在在一起...\n            int stride = OutputSize; // 步长，不同属性在数组中的偏移量\n\n            for (int i = 0; i &lt; OutputSize; i++)\n            {\n                // 查找最大类别概率及其索引\n                float maxConf = 0;\n                int maxClassId = -1;\n\n                // 遍历类别\n                for (int c = 0; c &lt; CategoryNum; c++)\n                {\n                    // 数组索引：(坐标/角度偏移量 + 类别偏移) * 框索引\n                    // 注意：原始代码中 result[outputSize * j + i] 这种访问方式基于 Transposed 数据布局\n                    float conf = result[(4 + c) * stride + i];\n                    if (conf &gt; maxConf)\n                    {\n                        maxConf = conf;\n                        maxClassId = c;\n                    }\n                }\n\n                // 置信度过滤\n                if (maxConf &gt; ConfThreshold)\n                {\n                    // 提取坐标 (cx, cy, w, h)\n                    float cx = result[0 * stride + i];\n                    float cy = result[1 * stride + i];\n                    float w = result[2 * stride + i];\n                    float h = result[3 * stride + i];\n                    // 还原坐标到原图尺寸\n                    int rx = (int)((cx - xOffset - 0.5 * w) * scale);\n                    int ry = (int)((cy - yOffset - 0.5 * h) * scale);\n                    int rw = (int)(w * scale);\n                    int rh = (int)(h * scale);\n\n                    boxes.Add(new Rect(rx, ry, rw, rh));\n                    confidences.Add(maxConf);\n                    classIds.Add(maxClassId);\n                }\n            }\n\n            // 执行 NMS (旋转框 NMS)\n            // OpenCV 的 NMSBoxes 支持 RotatedRect\n            int[] indices;\n            CvDnn.NMSBoxes(boxes, confidences, ConfThreshold, NmsThreshold, out indices);\n\n            List&lt;DetData&gt; finalResults = new List&lt;DetData&gt;();\n            foreach (int idx in indices)\n            {\n                finalResults.Add(new DetData\n                {\n                    index = classIds[idx],\n                    score = confidences[idx],\n                    box = boxes[idx]\n                });\n            }\n\n            return finalResults;\n        }\n\n        /// &lt;summary&gt;\n        /// 绘制检测结果（水平矩形框）\n        /// &lt;/summary&gt;\n        /// &lt;param name=\"results\"&gt;检测结果列表&lt;/param&gt;\n        /// &lt;param name=\"image\"&gt;原始图像&lt;/param&gt;\n        /// &lt;returns&gt;绘制后的图像&lt;/returns&gt;\n        public static Mat DrawDetResult(List&lt;DetData&gt; results, Mat image)\n        {\n            // 克隆图像以免修改原图\n            Mat mat = image.Clone();\n\n            foreach (var item in results)\n            {\n                // 1. 绘制矩形框\n                // Rect 结构包含 X, Y, Width, Height\n                Cv2.Rectangle(mat, item.box, new Scalar(0, 255, 0), thickness: 2);\n                // 2. 准备标签文本 (类别ID - 置信度)\n                string label = $\"{item.index} - {item.score:F2}\";\n                // 3. 计算文本的尺寸，用于绘制背景\n                int baseLine = 1;\n                Size textSize = Cv2.GetTextSize(label, HersheyFonts.HersheySimplex, 0.6, 1, out baseLine);\n                // 4. 绘制标签背景（半透明黑色矩形），防止文字与背景混淆\n                // 位置：矩形左上角略微上移，或者直接贴着左上角\n                Point labelPosition = new Point(item.box.X, item.box.Y - (int)textSize.Height - 5);\n\n                // 确保标签不画出图像边界\n                if (labelPosition.Y &lt; 0) labelPosition.Y = item.box.Y + (int)textSize.Height + 5;\n                Rect labelBgRect = new Rect(labelPosition.X,\n                                            labelPosition.Y - (int)textSize.Height, // OpenCV GetTextSize 返回的高度是基线到底部的距离，需调整\n                                            (int)textSize.Width,\n                                            (int)textSize.Height + (int)baseLine);\n                // 如果背景框也在图像范围内，则绘制\n                // 注意：这里简化处理，直接画在框上方\n                Cv2.Rectangle(mat,\n                               new Point(item.box.X, item.box.Y - textSize.Height - 5),\n                               new Point(item.box.X + textSize.Width, item.box.Y),\n                               new Scalar(0, 255, 0),\n                               thickness: -1); // -1 表示填充\n                // 5. 绘制文本（白色文字）\n                Cv2.PutText(mat,\n                            label,\n                            new Point(item.box.X, item.box.Y - 5),\n                            HersheyFonts.HersheySimplex,\n                            0.6,\n                            new Scalar(0, 0, 0),\n                            1);\n            }\n            return mat;\n        }\n\n        public class DetData\n        {\n            public int index;\n            public float score;\n            public Rect box;\n        }\n    }\n}\n\n\n</code></pre>\n<p><strong>程序运行结果：</strong></p>\n<p><img alt=\"推理结果\" class=\"lazyload\" /></p>\n<p><img alt=\"检测结果\" class=\"lazyload\" /></p>\n<p><strong>性能测试结果：</strong></p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">Batch Size</th>\n<th style=\"text-align: center;\">1</th>\n<th style=\"text-align: center;\">2</th>\n<th style=\"text-align: center;\">4</th>\n<th style=\"text-align: center;\">6</th>\n<th style=\"text-align: center;\">8</th>\n<th style=\"text-align: center;\">10</th>\n<th style=\"text-align: center;\">12</th>\n<th style=\"text-align: center;\">14</th>\n<th style=\"text-align: center;\">16</th>\n<th style=\"text-align: center;\">18</th>\n<th style=\"text-align: center;\">20</th>\n<th style=\"text-align: center;\">22</th>\n<th style=\"text-align: center;\">24</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">前处理 (ms)</td>\n<td style=\"text-align: center;\">9</td>\n<td style=\"text-align: center;\">13</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">38</td>\n<td style=\"text-align: center;\">56</td>\n<td style=\"text-align: center;\">59</td>\n<td style=\"text-align: center;\">63</td>\n<td style=\"text-align: center;\">83</td>\n<td style=\"text-align: center;\">96</td>\n<td style=\"text-align: center;\">105</td>\n<td style=\"text-align: center;\">118</td>\n<td style=\"text-align: center;\">130</td>\n<td style=\"text-align: center;\">144</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">模型推理 (ms)</td>\n<td style=\"text-align: center;\">7</td>\n<td style=\"text-align: center;\">15</td>\n<td style=\"text-align: center;\">24</td>\n<td style=\"text-align: center;\">36</td>\n<td style=\"text-align: center;\">48</td>\n<td style=\"text-align: center;\">60</td>\n<td style=\"text-align: center;\">96</td>\n<td style=\"text-align: center;\">84</td>\n<td style=\"text-align: center;\">93</td>\n<td style=\"text-align: center;\">153</td>\n<td style=\"text-align: center;\">120</td>\n<td style=\"text-align: center;\">133</td>\n<td style=\"text-align: center;\">203</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">后处理 (ms)</td>\n<td style=\"text-align: center;\">25</td>\n<td style=\"text-align: center;\">26</td>\n<td style=\"text-align: center;\">26</td>\n<td style=\"text-align: center;\">26</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">31</td>\n<td style=\"text-align: center;\">29</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>🔗<strong>程序路径链接</strong>：完整程序已经上传到GitHub，请自行下载，链接为：</p>\n<pre><code>https://github.com/guojin-yan/TensorRT-CSharp-API/tree/TensorRtSharp3.0/samples/YoloDetInfer\n</code></pre>\n<p>同时也提供了<strong>YoloOBB</strong>模型的推理程序，请自行下载，链接为：</p>\n<pre><code>https://github.com/guojin-yan/TensorRT-CSharp-API/tree/TensorRtSharp3.0/samples/YoloObbInfer\n</code></pre>\n</blockquote>\n<hr />\n<h3 id=\"示例-4动态形状推理\">示例 4：动态形状推理</h3>\n<p>对于输入尺寸可变的模型，需要根据输入的数据配置动态形状。</p>\n<p><strong>核心代码：</strong></p>\n<pre><code class=\"language-csharp\">using JYPPX.TensorRtSharp.Cuda;\nusing JYPPX.TensorRtSharp.Nvinfer;\nusing OpenCvSharp;\nusing OpenCvSharp.Dnn;\nusing System.Diagnostics;\nusing System.Runtime.InteropServices;\n\nnamespace YoloObbBatchInfer\n{\n    internal class Program\n    {\n        // ================= 配置参数 =================\n        // 模型输入尺寸 (宽=高)\n        private const int InputSize = 1024;\n        // 建议根据实际模型动态获取或使用 Netron 查看\n        private const int OutputSize = 21504;\n        // 模型类别数 (根据您的具体数据集修改，此处假设为15类)\n        private const int CategoryNum = 15;\n        // 置信度阈值\n        private const float ConfThreshold = 0.25f;\n\n        // NMS IOU 阈值\n        private const float NmsThreshold = 0.3f;\n        private const int MaxBatchSize = 24;\n        static void Main(string[] args)\n        {\n            //  ============= 配置 TensorRT 日志回调 =============\n            // 定义一个委托，用于处理 TensorRT 内部产生的日志消息。\n            // 这允许我们将 C++ 层面的日志输出到 C# 的控制台。\n            LogCallbackFunction _callbackDelegate = (message) =&gt;\n            {\n                Console.WriteLine(message);\n            };\n            // 将自定义的回调函数注册给 TensorRT 的全局 Logger 实例。\n            Logger.Instance.SetCallback(_callbackDelegate);\n\n            // 设置日志的严重性级别阈值。\n            // LoggerSeverity.kINFO: 打印信息、警告和错误。\n            // 开发调试阶段通常设为 kINFO 或 kVERBOSE；生产环境可设为 kWARNING 或 kERROR 以减少输出。\n            Logger.Instance.SetThreshold(LoggerSeverity.kINFO);\n\n            string enginePath = \"yolov8s-obb_b.engine\";\n            string[] imagePaths = { \n                \"P0006.png\" , \"P0016.png\", \"P0456.png\", \"P0813.png\"};\n            // ================= 1. 加载 TensorRT Engine =================\n            // 使用 using 语句确保文件流正确关闭\n            byte[] engineData;\n            using (FileStream fs = new FileStream(enginePath, FileMode.Open, FileAccess.Read))\n            using (BinaryReader br = new BinaryReader(fs))\n            {\n                engineData = br.ReadBytes((int)fs.Length);\n            }\n\n            // 反序列化 Engine\n            // Runtime 必须在 Engine 生命周期内保持存活，通常建议设为全局或静态，或者确保它最后释放\n            Runtime runtime = new Runtime();\n            runtime.setMaxThreads(10);\n            // 创建 CudaEngine (此处使用 using 确保推理完成后引擎被销毁)\n            using (CudaEngine cudaEngine = runtime.deserializeCudaEngineByBlob(engineData, (ulong)engineData.Length))\n            {\n                // ================= 2. 初始化推理上下文与显存 =================\n                // 创建执行上下文\n                using (JYPPX.TensorRtSharp.Nvinfer.ExecutionContext executionContext = cudaEngine.createExecutionContext(TrtExecutionContextAllocationStrategy.kSTATIC))\n                using (CudaStream cudaStream = new CudaStream()) // 创建 CUDA 流用于异步执行\n                {\n                    // 获取输入维度信息 (用于校验)\n                    Dims inputDims = executionContext.getTensorShape(\"images\");\n                    Logger.Instance.INFO($\"Input Shape: {inputDims.d[0]}x{inputDims.d[1]}x{inputDims.d[2]}x{inputDims.d[3]}\");\n\n                    // 计算所需显存大小\n                    // 输入: Batch=1, Channel=3, Height=1024, Width=1024\n                    ulong inputSizeInBytes = MaxBatchSize * 3 * InputSize * InputSize;\n                    // 输出: Batch=1, Channels=CategoryNum+4(box)+1(angle), Num=8400\n                    int outputChannels = CategoryNum + 5; // 4坐标 + 1角度 + N类别\n                    ulong outputSizeInBytes = (ulong)(MaxBatchSize * outputChannels * OutputSize);\n\n                    Stopwatch sw = new Stopwatch();\n                    // 分配 GPU 显存\n                    using (Cuda1DMemory&lt;float&gt; inputGpuMemory = new Cuda1DMemory&lt;float&gt;(inputSizeInBytes))\n                    using (Cuda1DMemory&lt;float&gt; outputGpuMemory = new Cuda1DMemory&lt;float&gt;(outputSizeInBytes))\n                    {\n                        // 绑定显存地址到 TensorRT 上下文\n                        executionContext.setInputTensorAddress(\"images\", inputGpuMemory.get());\n                        executionContext.setOutputTensorAddress(\"output0\", outputGpuMemory.get());\n\n                        // 关键一步，修改本次推理的形状\n                        executionContext.setinputShape(\"images\", new Dims(imagePaths.Count(), 3, 1024, 1024));\n                        // 预热推理 (可选，但推荐，尤其是首次推理时)\n                        executionContext.executeV3(cudaStream);\n                        cudaStream.Synchronize();\n\n                        // ================= 3. 图像预处理 =================\n                        List&lt;Mat&gt; images = new List&lt;Mat&gt;();\n                        foreach (var path in imagePaths) \n                        {\n                            Mat img = Cv2.ImRead(path);\n                            if (img.Empty())\n                            {\n                                Logger.Instance.INFO(\"Image not found!\");\n                                return;\n                            }\n                            images.Add(img);\n                        }\n\n                        (float[] inputData1, float[] scales1, int[] xOffsets1, int[] yOffsets1) = PreProcessBatch(images);\n                        sw.Start();\n                        (float[] inputData, float[] scales, int[] xOffsets, int[] yOffsets) = PreProcessBatch(images);\n                        sw.Stop();\n                        Logger.Instance.INFO($\"Pre-processing time: {sw.ElapsedMilliseconds} ms\");\n                        // ================= 4. 推理 =================\n\n                        // 准备主机内存接收结果\n                        float[] outputData1 = new float[imagePaths.Count() * outputChannels * OutputSize];\n                        // 将数据从主机 拷贝到设备\n                        inputGpuMemory.copyFromHostAsync(inputData, cudaStream);\n\n                        // 执行推理 (enqueueV3 是异步的)\n                        executionContext.executeV3(cudaStream);\n                        // 等待推理完成\n                        cudaStream.Synchronize();\n                        // 将结果从设备 拷贝回主机\n                        // 这里的拷贝是同步的，会等待 GPU 计算完成\n                        outputGpuMemory.copyToHostAsync(outputData1, cudaStream);\n\n                        sw.Restart();\n                        // 准备主机内存接收结果\n                        float[] outputData = new float[imagePaths.Count() * outputChannels * OutputSize];\n                        // 将数据从主机 拷贝到设备\n                        inputGpuMemory.copyFromHostAsync(inputData, cudaStream);\n\n                        // 执行推理 (enqueueV3 是异步的)\n                        executionContext.executeV3(cudaStream);\n                        // 等待推理完成\n                        cudaStream.Synchronize();\n                        // 将结果从设备 拷贝回主机\n                        // 这里的拷贝是同步的，会等待 GPU 计算完成\n                        outputGpuMemory.copyToHostAsync(outputData, cudaStream);\n\n                        sw.Stop();\n                        Logger.Instance.INFO($\"Inference time: {sw.ElapsedMilliseconds} ms\");\n                        // ================= 5. 后处理 =================\n                        List&lt;List&lt;ObbData&gt;&gt; results1 = PostProcessBatch(outputData, scales, xOffsets, yOffsets);\n                        sw.Restart();\n                        List&lt;List&lt;ObbData&gt;&gt; results = PostProcessBatch(outputData, scales, xOffsets, yOffsets);\n                        sw.Stop();\n                        Logger.Instance.INFO($\"Post-processing time: {sw.ElapsedMilliseconds} ms\");\n\n                        // ================= 6. 结果可视化 =================\n                        List&lt;Mat&gt; resultMats = new List&lt;Mat&gt;();\n                        for(int i = 0; i &lt; results.Count; ++i)\n                        {\n                            resultMats.Add(DrawObbResult(results[i], images[i]));\n                        }\n                        Mat putResultImgs = StitchHorizontalWithPadding(resultMats);\n                        Cv2.ImWrite(\"YOLO11-OBB Result.png\", putResultImgs);\n                        Cv2.ImShow(\"YOLO11-OBB Result\", putResultImgs);\n                        Cv2.WaitKey(0);\n                    }\n                }\n            }\n        }\n\n        /// &lt;summary&gt;\n        /// 图像预处理：Letterbox 缩放、归一化、HWC 转 CHW\n        /// &lt;/summary&gt;\n        private static (float[], float[] ,  int[] , int[] ) PreProcessBatch(List&lt;Mat&gt; imgs)\n        {\n            int dataLen = 3 * InputSize * InputSize;\n            float[] chwData = new float[imgs.Count * dataLen];\n            float[] scales = new float[imgs.Count];\n            int[] xOffsets = new int[imgs.Count];\n            int[]  yOffsets = new int[imgs.Count];\n            Parallel.For(0, imgs.Count, i =&gt;\n            {\n                Mat img = imgs[i];\n                // 转换颜色空间 BGR -&gt; RGB\n                Mat rgbImg = new Mat();\n                Cv2.CvtColor(img, rgbImg, ColorConversionCodes.BGR2RGB);\n\n                // 计算 Letterbox 缩放比例\n                int maxDim = Math.Max(rgbImg.Width, rgbImg.Height);\n                scales[i] = (float)maxDim / InputSize;\n\n                // 计算缩放后的尺寸\n                int newWidth = (int)(rgbImg.Width / scales[i]);\n                int newHeight = (int)(rgbImg.Height / scales[i]);\n\n                // Resize 图像\n                Mat resizedImg = new Mat();\n                Cv2.Resize(rgbImg, resizedImg, new Size(newWidth, newHeight));\n\n                // 创建黑色背景 Canvas (InputSize x InputSize)\n                Mat paddedImg = Mat.Zeros(InputSize, InputSize, MatType.CV_8UC3);\n\n                // 计算粘贴位置 (居中)\n                xOffsets[i] = (InputSize - newWidth) / 2;\n                yOffsets[i] = (InputSize - newHeight) / 2;\n\n                // 将图像拷贝到 Canvas 中央\n                Rect roi = new Rect(xOffsets[i], yOffsets[i], newWidth, newHeight);\n                resizedImg.CopyTo(new Mat(paddedImg, roi));\n\n                // 归一化 (0-255 -&gt; 0-1) 并转为 float 类型\n                Mat floatImg = new Mat();\n                paddedImg.ConvertTo(floatImg, MatType.CV_32FC3, 1.0 / 255.0);\n\n                // HWC 转 CHW 并展平为一维数组\n                Mat[] channels = Cv2.Split(floatImg);\n\n\n                // 拷贝数据：R通道 -&gt; C通道 -&gt; B通道 (OpenCV Split 出来顺序是 B, G, R，对应索引 0, 1, 2)\n                int channelSize = InputSize * InputSize;\n                // 将 R, G, B 依次拷入数组\n                Marshal.Copy(channels[0].Data, chwData, dataLen * i, channelSize); // R\n                Marshal.Copy(channels[1].Data, chwData, dataLen * i + channelSize, channelSize); // G\n                Marshal.Copy(channels[2].Data, chwData, dataLen * i + channelSize * 2, channelSize); // B\n\n                // 释放临时 Mat\n                rgbImg.Dispose();\n                resizedImg.Dispose();\n                paddedImg.Dispose();\n                floatImg.Dispose();\n                foreach (var c in channels) c.Dispose();\n            });\n\n\n\n            return (chwData, scales, xOffsets, yOffsets);\n        }\n\n        /// &lt;summary&gt;\n        /// 后处理：解析 TensorRT 输出、NMS 过滤\n        /// &lt;/summary&gt;\n        private static List&lt;List&lt;ObbData&gt;&gt; PostProcessBatch(float[] result, float[] scales, int[] xOffsets, int[] yOffsets)\n        {\n            List&lt;ObbData&gt;[] obbDatas = new List&lt;ObbData&gt;[scales.Length];\n\n            Parallel.For(0, scales.Length, b =&gt;\n            {\n                List&lt;RotatedRect&gt; boxes = new List&lt;RotatedRect&gt;();\n                List&lt;float&gt; confidences = new List&lt;float&gt;();\n                List&lt;int&gt; classIds = new List&lt;int&gt;();\n\n                // 遍历所有预测框 (OutputSize)\n                // 数据布局: [4(box) + 15(classes) + 1(angle)] * OutputSize\n                // 展平数组中，同一属性的数据是连续存储的，例如所有 cx 在一起，所有 cy 在在一起...\n                int stride = OutputSize; // 步长，不同属性在数组中的偏移量\n\n                int resultDataOffset = OutputSize * (CategoryNum + 5) * b;\n\n                for (int i = 0; i &lt; OutputSize; i++)\n                {\n                    // 查找最大类别概率及其索引\n                    float maxConf = 0;\n                    int maxClassId = -1;\n\n                    // 遍历类别 \n                    for (int c = 0; c &lt; CategoryNum; c++)\n                    {\n                        // 数组索引：(坐标/角度偏移量 + 类别偏移) * 框索引\n                        // 注意：原始代码中 result[outputSize * j + i] 这种访问方式基于 Transposed 数据布局\n                        float conf = result[(4 + c) * stride + i + resultDataOffset];\n                        if (conf &gt; maxConf)\n                        {\n                            maxConf = conf;\n                            maxClassId = c;\n                        }\n                    }\n\n                    // 置信度过滤\n                    if (maxConf &gt; ConfThreshold)\n                    {\n                        // 提取坐标 (cx, cy, w, h)\n                        float cx = result[0 * stride + i + resultDataOffset];\n                        float cy = result[1 * stride + i + resultDataOffset];\n                        float w = result[2 * stride + i + resultDataOffset];\n                        float h = result[3 * stride + i + resultDataOffset];\n\n                        // 提取角度 (通常在第 5 个位置，即类别之前)\n                        float angleRad = result[(CategoryNum + 4) * stride + i + resultDataOffset];\n\n                        // 还原坐标到原图尺寸\n                        float rx = (cx - xOffsets[b]) * scales[b];\n                        float ry = (cy - yOffsets[b]) * scales[b];\n                        float rw = w * scales[b];\n                        float rh = h * scales[b];\n\n                        // 将弧度转换为角度\n                        // Normalize angle to [-π/2, π/2] range\n                        // 将角度归一化到[-π/2, π/2]范围\n                        if (angleRad &gt;= Math.PI &amp;&amp; angleRad &lt;= 0.75 * Math.PI)\n                        {\n                            angleRad -= (float)Math.PI;\n                        }\n                        float angleDeg = angleRad * (float)(180f / Math.PI);  // Convert to degrees/转换为角度制\n\n                        boxes.Add(new RotatedRect(new Point2f(rx, ry), new Size2f(rw, rh), angleDeg));\n                        confidences.Add(maxConf);\n                        classIds.Add(maxClassId);\n                    }\n                }\n\n                // 执行 NMS (旋转框 NMS)\n                // OpenCV 的 NMSBoxes 支持 RotatedRect\n                int[] indices;\n                CvDnn.NMSBoxes(boxes, confidences, ConfThreshold, NmsThreshold, out indices);\n\n                List&lt;ObbData&gt; finalResults = new List&lt;ObbData&gt;();\n                foreach (int idx in indices)\n                {\n                    finalResults.Add(new ObbData\n                    {\n                        index = classIds[idx],\n                        score = confidences[idx],\n                        box = boxes[idx]\n                    });\n                }\n                obbDatas[b] = finalResults;\n            });\n\n           \n\n            return obbDatas.Select(x =&gt; x?.ToList() ?? new List&lt;ObbData&gt;()).ToList();\n        }\n\n        /// &lt;summary&gt;\n        /// 绘制旋转检测结果\n        /// &lt;/summary&gt;\n        public static Mat DrawObbResult(List&lt;ObbData&gt; results, Mat image)\n        {\n            // 克隆图像以免修改原图\n            Mat mat = image.Clone();\n\n            foreach (var item in results)\n            {\n                // 获取旋转矩形的四个顶点\n                Point2f[] points = item.box.Points();\n\n                // 绘制多边形框\n                for (int j = 0; j &lt; 4; j++)\n                {\n                    Cv2.Line(mat, (Point)points[j], (Point)points[(j + 1) % 4],\n                            new Scalar(0, 255, 0), 2);\n                }\n\n                // 绘制标签 (类别 - 置信度)\n                string label = $\"{item.index} - {item.score:F2}\";\n                Point2f textPos = points[0]; // 左上角\n\n                Cv2.PutText(mat, label, (Point)textPos, HersheyFonts.HersheySimplex, 0.8,\n                            new Scalar(255, 0, 0), 2);\n            }\n\n            return mat;\n        }\n\n        public class ObbData\n        {\n            public int index;\n            public float score;\n            public RotatedRect box;\n        }\n\n\n        /// &lt;summary&gt;\n        /// 智能水平拼接：自动处理高度不一致的图片\n        /// &lt;/summary&gt;\n        /// &lt;param name=\"images\"&gt;图片列表&lt;/param&gt;\n        /// &lt;param name=\"backgroundColor\"&gt;填充背景颜色，默认为黑色&lt;/param&gt;\n        /// &lt;returns&gt;拼接后的 Mat&lt;/returns&gt;\n        public static Mat StitchHorizontalWithPadding(List&lt;Mat&gt; images, Scalar? backgroundColor = null)\n        {\n            if (images == null || images.Count == 0)\n                return new Mat();\n            // 1. 找到所有图片中的最大高度\n            int maxHeight = images.Max(img =&gt; img.Rows);\n            // 计算总宽度\n            int totalWidth = images.Sum(img =&gt; img.Cols);\n            // 2. 准备结果画布\n            Mat result = new Mat(maxHeight, totalWidth, images[0].Type(), backgroundColor ?? Scalar.Black);\n            // 3. 将每一张图片复制到画布的对应位置\n            int currentX = 0; // 当前 X 轴偏移量\n            foreach (var img in images)\n            {\n                if (img.Empty()) continue;\n                // 计算当前图片需要垂直偏移多少（底部对齐逻辑）\n                // 如果想顶部对齐，yOffset = 0\n                // 如果想居中，yOffset = (maxHeight - img.Rows) / 2\n                int yOffset = maxHeight - img.Rows;\n                // 定义 ROI (感兴趣区域)\n                Rect roi = new Rect(currentX, yOffset, img.Cols, img.Rows);\n\n                // 将原图片拷贝到结果图的 ROI 区域\n                img.CopyTo(new Mat(result, roi));\n                // 移动 X 轴指针\n                currentX += img.Cols;\n            }\n            return result;\n        }\n    }\n}\n\n\n</code></pre>\n<p>下图为上述程序运行后的输出，模型输入形状为 -1x3x1024x1024，其中Batch Size为动态输入；项目示例使用了四张图片进行同时推理，开启并行处理后，四张图像预处理时间仅用21ms，推理时间为25ms，后处理时间为26ms，累计时间为72ms.</p>\n<p><img alt=\"image-20260109225451392\" class=\"lazyload\" /></p>\n<p>下图为推理结果展示：</p>\n<p><img alt=\"YOLO11-OBB Result\" class=\"lazyload\" /></p>\n<p><strong>性能测试（不同 Batch Size）：</strong></p>\n<p>为了探究不同Batch Size推理时间差异，此处对不同Batch Size进行了测试，测试结果如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">Batch Size</th>\n<th style=\"text-align: center;\">1</th>\n<th style=\"text-align: center;\">2</th>\n<th style=\"text-align: center;\">4</th>\n<th style=\"text-align: center;\">6</th>\n<th style=\"text-align: center;\">8</th>\n<th style=\"text-align: center;\">10</th>\n<th style=\"text-align: center;\">12</th>\n<th style=\"text-align: center;\">14</th>\n<th style=\"text-align: center;\">16</th>\n<th style=\"text-align: center;\">18</th>\n<th style=\"text-align: center;\">20</th>\n<th style=\"text-align: center;\">22</th>\n<th style=\"text-align: center;\">24</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">前处理 (ms )</td>\n<td style=\"text-align: center;\">9</td>\n<td style=\"text-align: center;\">13</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">38</td>\n<td style=\"text-align: center;\">56</td>\n<td style=\"text-align: center;\">59</td>\n<td style=\"text-align: center;\">63</td>\n<td style=\"text-align: center;\">83</td>\n<td style=\"text-align: center;\">96</td>\n<td style=\"text-align: center;\">105</td>\n<td style=\"text-align: center;\">118</td>\n<td style=\"text-align: center;\">130</td>\n<td style=\"text-align: center;\">144</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">模型推理 (ms)</td>\n<td style=\"text-align: center;\">7</td>\n<td style=\"text-align: center;\">15</td>\n<td style=\"text-align: center;\">24</td>\n<td style=\"text-align: center;\">36</td>\n<td style=\"text-align: center;\">48</td>\n<td style=\"text-align: center;\">60</td>\n<td style=\"text-align: center;\">96</td>\n<td style=\"text-align: center;\">84</td>\n<td style=\"text-align: center;\">93</td>\n<td style=\"text-align: center;\">153</td>\n<td style=\"text-align: center;\">120</td>\n<td style=\"text-align: center;\">133</td>\n<td style=\"text-align: center;\">203</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">后处理 (ms)</td>\n<td style=\"text-align: center;\">25</td>\n<td style=\"text-align: center;\">26</td>\n<td style=\"text-align: center;\">26</td>\n<td style=\"text-align: center;\">26</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">28</td>\n<td style=\"text-align: center;\">27</td>\n<td style=\"text-align: center;\">31</td>\n<td style=\"text-align: center;\">29</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>🔗<strong>程序路径链接</strong>：完整程序已经上传到GitHub，请自行下载，链接为：</p>\n<pre><code>https://github.com/guojin-yan/TensorRT-CSharp-API/tree/TensorRtSharp3.0/samples/YoloObbBatchInfer\n</code></pre>\n</blockquote>\n<hr />\n<h3 id=\"示例-5并行推理\">示例 5：并行推理</h3>\n<p>使用一个 Runtime 创建多执行上下文，实现多并行推理。</p>\n<p><strong>核心代码：</strong></p>\n<pre><code class=\"language-csharp\">using JYPPX.TensorRtSharp.Cuda;\nusing JYPPX.TensorRtSharp.Nvinfer;\nusing OpenCvSharp;\nusing OpenCvSharp.Dnn;\nusing System.Diagnostics;\nusing System.Runtime.InteropServices;\nusing static OpenCvSharp.FileStorage;\n\nnamespace YoloDetParallelInfer\n{\n    internal class Program\n    {\n        // ================= 配置参数 =================\n        // 模型输入尺寸 (宽=高)\n        private const int InputSize = 640;\n\n\n        // 建议根据实际模型动态获取或使用 Netron 查看\n        private const int OutputSize = 8400;\n\n        // 模型类别数 (根据您的具体数据集修改，此处假设为15类)\n        private const int CategoryNum = 80;\n\n        // 置信度阈值\n        private const float ConfThreshold = 0.25f;\n\n        // NMS IOU 阈值\n        private const float NmsThreshold = 0.3f;\n\n        static void Main(string[] args)\n        {\n            //  ============= 配置 TensorRT 日志回调 =============\n            // 定义一个委托，用于处理 TensorRT 内部产生的日志消息。\n            // 这允许我们将 C++ 层面的日志输出到 C# 的控制台。\n            LogCallbackFunction _callbackDelegate = (message) =&gt;\n            {\n                Console.WriteLine(message);\n            };\n\n            // 将自定义的回调函数注册给 TensorRT 的全局 Logger 实例。\n            Logger.Instance.SetCallback(_callbackDelegate);\n\n            // 设置日志的严重性级别阈值。\n            // LoggerSeverity.kINFO: 打印信息、警告和错误。\n            // 开发调试阶段通常设为 kINFO 或 kVERBOSE；生产环境可设为 kWARNING 或 kERROR 以减少输出。\n            Logger.Instance.SetThreshold(LoggerSeverity.kINFO);\n\n            string enginePath = \"yolov8s.engine\";\n            string imagePath = \"bus.jpg\";\n\n                           Mat img = Cv2.ImRead(imagePath);\n                            if (img.Empty())\n                            {\n                                Logger.Instance.INFO(\"Image not found!\");\n                                return;\n                            }\n            // ================= 1. 加载 TensorRT Engine =================\n            // 使用 using 语句确保文件流正确关闭\n            byte[] engineData;\n            using (FileStream fs = new FileStream(enginePath, FileMode.Open, FileAccess.Read))\n            using (BinaryReader br = new BinaryReader(fs))\n            {\n                engineData = br.ReadBytes((int)fs.Length);\n            }\n\n            // 反序列化 Engine\n            // Runtime 必须在 Engine 生命周期内保持存活，通常建议设为全局或静态，或者确保它最后释放\n            Runtime runtime = new Runtime();\n            runtime.setMaxThreads(6);\n            // 创建 CudaEngine (此处使用 using 确保推理完成后引擎被销毁)\n            using (CudaEngine cudaEngine = runtime.deserializeCudaEngineByBlob(engineData, (ulong)engineData.Length))\n            {\n                // ================= 2. 初始化推理上下文与显存 =================\n                Stopwatch totalSw = new Stopwatch();\n                totalSw.Start();\n                Parallel.For(0, 24, b =&gt;\n                {           \n                    \n                    // 创建执行上下文\n                    using (JYPPX.TensorRtSharp.Nvinfer.ExecutionContext executionContext = cudaEngine.createExecutionContext(TrtExecutionContextAllocationStrategy.kSTATIC))\n                    using (CudaStream cudaStream = new CudaStream()) // 创建 CUDA 流用于异步执行\n                    {\n                        // 获取输入维度信息 (用于校验)\n                        Dims inputDims = executionContext.getTensorShape(\"images\");\n                        Logger.Instance.INFO($\"Input Shape: {inputDims.d[0]}x{inputDims.d[1]}x{inputDims.d[2]}x{inputDims.d[3]}\");\n\n                        // 计算所需显存大小\n                        // 输入: Batch=1, Channel=3, Height=640, Width=640\n                        ulong inputSizeInBytes = 1 * 3 * InputSize * InputSize;\n                        // 输出: Batch=1, Channels=CategoryNum+4(box)+1(angle), Num=8400\n                        int outputChannels = CategoryNum + 4; // 4坐标 + N类别\n                        ulong outputSizeInBytes = (ulong)(1 * outputChannels * OutputSize);\n\n                        Stopwatch sw = new Stopwatch();\n                        // 分配 GPU 显存\n                        using (Cuda1DMemory&lt;float&gt; inputGpuMemory = new Cuda1DMemory&lt;float&gt;(inputSizeInBytes))\n                        using (Cuda1DMemory&lt;float&gt; outputGpuMemory = new Cuda1DMemory&lt;float&gt;(outputSizeInBytes))\n                        {\n                            // 绑定显存地址到 TensorRT 上下文\n                            executionContext.setInputTensorAddress(\"images\", inputGpuMemory.get());\n                            executionContext.setOutputTensorAddress(\"output0\", outputGpuMemory.get());\n                            // 预热推理 (可选，但推荐，尤其是首次推理时)\n                            executionContext.executeV3(cudaStream);\n                            cudaStream.Synchronize();\n                            // ================= 3. 图像预处理 =================\n             \n\n                            sw.Start();\n                            float[] inputData = PreProcess(img, out float scale, out int xOffset, out int yOffset);\n                            sw.Stop();\n                            Logger.Instance.INFO($\"Channel {b}: Pre-processing time: {sw.ElapsedMilliseconds} ms\");\n                            // ================= 4. 推理 =================\n                            // 准备主机内存接收结果\n                            float[] outputData = new float[outputChannels * OutputSize];\n\n\n                            sw.Restart();\n                            // 将数据从主机 拷贝到设备\n                            inputGpuMemory.copyFromHostAsync(inputData, cudaStream);\n\n                            // 执行推理 (enqueueV3 是异步的)\n                            executionContext.executeV3(cudaStream);\n                            // 等待推理完成\n                            cudaStream.Synchronize();\n\n\n\n                            // 将结果从设备 拷贝回主机\n                            // 这里的拷贝是同步的，会等待 GPU 计算完成\n                            outputGpuMemory.copyToHostAsync(outputData, cudaStream);\n                            sw.Stop();\n                            Logger.Instance.INFO($\"Channel {b}: Inference time: {sw.ElapsedMilliseconds} ms\");\n                            // ================= 5. 后处理 =================\n\n                            sw.Restart();\n                            List&lt;DetData&gt; results = PostProcess(outputData, scale, xOffset, yOffset);\n                            sw.Stop();\n                            Logger.Instance.INFO($\"Channel {b}: Post-processing time: {sw.ElapsedMilliseconds} ms\");\n\n                            // ================= 6. 结果可视化 =================\n                            //Mat resultImg = DrawDetResult(results, img);\n                            //Cv2.ImShow(\"YOLO11-DET Result\", resultImg);\n                            //Cv2.WaitKey(0);\n                        }\n                    }\n                });\n\n                totalSw.Stop();\n                Logger.Instance.INFO($\"Total time for 8 inferences: {totalSw.ElapsedMilliseconds} ms\");\n\n\n            }\n        }\n\n        /// &lt;summary&gt;\n        /// 图像预处理：Letterbox 缩放、归一化、HWC 转 CHW\n        /// &lt;/summary&gt;\n        private static float[] PreProcess(Mat img, out float scale, out int xOffset, out int yOffset)\n        {\n            // 转换颜色空间 BGR -&gt; RGB\n            Mat rgbImg = new Mat();\n            Cv2.CvtColor(img, rgbImg, ColorConversionCodes.BGR2RGB);\n\n            // 计算 Letterbox 缩放比例\n            int maxDim = Math.Max(rgbImg.Width, rgbImg.Height);\n            scale = (float)maxDim / InputSize;\n\n            // 计算缩放后的尺寸\n            int newWidth = (int)(rgbImg.Width / scale);\n            int newHeight = (int)(rgbImg.Height / scale);\n\n            // Resize 图像\n            Mat resizedImg = new Mat();\n            Cv2.Resize(rgbImg, resizedImg, new Size(newWidth, newHeight));\n\n            // 创建黑色背景 Canvas (InputSize x InputSize)\n            Mat paddedImg = Mat.Zeros(InputSize, InputSize, MatType.CV_8UC3);\n\n            // 计算粘贴位置 (居中)\n            xOffset = (InputSize - newWidth) / 2;\n            yOffset = (InputSize - newHeight) / 2;\n\n            // 将图像拷贝到 Canvas 中央\n            Rect roi = new Rect(xOffset, yOffset, newWidth, newHeight);\n            resizedImg.CopyTo(new Mat(paddedImg, roi));\n\n            // 归一化 (0-255 -&gt; 0-1) 并转为 float 类型\n            Mat floatImg = new Mat();\n            paddedImg.ConvertTo(floatImg, MatType.CV_32FC3, 1.0 / 255.0);\n\n            // HWC 转 CHW 并展平为一维数组\n            Mat[] channels = Cv2.Split(floatImg);\n            float[] chwData = new float[3 * InputSize * InputSize];\n\n            // 拷贝数据：R通道 -&gt; C通道 -&gt; B通道 (OpenCV Split 出来顺序是 B, G, R，对应索引 0, 1, 2)\n            int channelSize = InputSize * InputSize;\n            // 将 R, G, B 依次拷入数组\n            Marshal.Copy(channels[0].Data, chwData, 0, channelSize); // R\n            Marshal.Copy(channels[1].Data, chwData, channelSize, channelSize); // G\n            Marshal.Copy(channels[2].Data, chwData, channelSize * 2, channelSize); // B\n\n            // 释放临时 Mat\n            rgbImg.Dispose();\n            resizedImg.Dispose();\n            paddedImg.Dispose();\n            floatImg.Dispose();\n            foreach (var c in channels) c.Dispose();\n\n            return chwData;\n        }\n\n        /// &lt;summary&gt;\n        /// 后处理：解析 TensorRT 输出、NMS 过滤\n        /// &lt;/summary&gt;\n        private static List&lt;DetData&gt; PostProcess(float[] result, float scale, int xOffset, int yOffset)\n        {\n            List&lt;Rect&gt; boxes = new List&lt;Rect&gt;();\n            List&lt;float&gt; confidences = new List&lt;float&gt;();\n            List&lt;int&gt; classIds = new List&lt;int&gt;();\n\n            // 遍历所有预测框 (OutputSize)\n            // 数据布局: [4(box) + 80(classes)] * OutputSize\n            // 展平数组中，同一属性的数据是连续存储的，例如所有 cx 在一起，所有 cy 在在一起...\n            int stride = OutputSize; // 步长，不同属性在数组中的偏移量\n\n            for (int i = 0; i &lt; OutputSize; i++)\n            {\n                // 查找最大类别概率及其索引\n                float maxConf = 0;\n                int maxClassId = -1;\n\n                // 遍历类别\n                for (int c = 0; c &lt; CategoryNum; c++)\n                {\n                    // 数组索引：(坐标/角度偏移量 + 类别偏移) * 框索引\n                    // 注意：原始代码中 result[outputSize * j + i] 这种访问方式基于 Transposed 数据布局\n                    float conf = result[(4 + c) * stride + i];\n                    if (conf &gt; maxConf)\n                    {\n                        maxConf = conf;\n                        maxClassId = c;\n                    }\n                }\n\n                // 置信度过滤\n                if (maxConf &gt; ConfThreshold)\n                {\n                    // 提取坐标 (cx, cy, w, h)\n                    float cx = result[0 * stride + i];\n                    float cy = result[1 * stride + i];\n                    float w = result[2 * stride + i];\n                    float h = result[3 * stride + i];\n                    // 还原坐标到原图尺寸\n                    int rx = (int)((cx - xOffset - 0.5 * w) * scale);\n                    int ry = (int)((cy - yOffset - 0.5 * h) * scale);\n                    int rw = (int)(w * scale);\n                    int rh = (int)(h * scale);\n\n                    boxes.Add(new Rect(rx, ry, rw, rh));\n                    confidences.Add(maxConf);\n                    classIds.Add(maxClassId);\n                }\n            }\n\n            // 执行 NMS (旋转框 NMS)\n            // OpenCV 的 NMSBoxes 支持 RotatedRect\n            int[] indices;\n            CvDnn.NMSBoxes(boxes, confidences, ConfThreshold, NmsThreshold, out indices);\n\n            List&lt;DetData&gt; finalResults = new List&lt;DetData&gt;();\n            foreach (int idx in indices)\n            {\n                finalResults.Add(new DetData\n                {\n                    index = classIds[idx],\n                    score = confidences[idx],\n                    box = boxes[idx]\n                });\n            }\n\n            return finalResults;\n        }\n\n        /// &lt;summary&gt;\n        /// 绘制检测结果（水平矩形框）\n        /// &lt;/summary&gt;\n        /// &lt;param name=\"results\"&gt;检测结果列表&lt;/param&gt;\n        /// &lt;param name=\"image\"&gt;原始图像&lt;/param&gt;\n        /// &lt;returns&gt;绘制后的图像&lt;/returns&gt;\n        public static Mat DrawDetResult(List&lt;DetData&gt; results, Mat image)\n        {\n            // 克隆图像以免修改原图\n            Mat mat = image.Clone();\n\n            foreach (var item in results)\n            {\n                // 1. 绘制矩形框\n                // Rect 结构包含 X, Y, Width, Height\n                Cv2.Rectangle(mat, item.box, new Scalar(0, 255, 0), thickness: 2);\n                // 2. 准备标签文本 (类别ID - 置信度)\n                string label = $\"{item.index} - {item.score:F2}\";\n                // 3. 计算文本的尺寸，用于绘制背景\n                int baseLine = 1;\n                Size textSize = Cv2.GetTextSize(label, HersheyFonts.HersheySimplex, 0.6, 1, out baseLine);\n                // 4. 绘制标签背景（半透明黑色矩形），防止文字与背景混淆\n                // 位置：矩形左上角略微上移，或者直接贴着左上角\n                Point labelPosition = new Point(item.box.X, item.box.Y - (int)textSize.Height - 5);\n\n                // 确保标签不画出图像边界\n                if (labelPosition.Y &lt; 0) labelPosition.Y = item.box.Y + (int)textSize.Height + 5;\n                Rect labelBgRect = new Rect(labelPosition.X,\n                                            labelPosition.Y - (int)textSize.Height, // OpenCV GetTextSize 返回的高度是基线到底部的距离，需调整\n                                            (int)textSize.Width,\n                                            (int)textSize.Height + (int)baseLine);\n                // 如果背景框也在图像范围内，则绘制\n                // 注意：这里简化处理，直接画在框上方\n                Cv2.Rectangle(mat,\n                               new Point(item.box.X, item.box.Y - textSize.Height - 5),\n                               new Point(item.box.X + textSize.Width, item.box.Y),\n                               new Scalar(0, 255, 0),\n                               thickness: -1); // -1 表示填充\n                // 5. 绘制文本（白色文字）\n                Cv2.PutText(mat,\n                            label,\n                            new Point(item.box.X, item.box.Y - 5),\n                            HersheyFonts.HersheySimplex,\n                            0.6,\n                            new Scalar(0, 0, 0),\n                            1);\n            }\n            return mat;\n        }\n\n        public class DetData\n        {\n            public int index;\n            public float score;\n            public Rect box;\n        }\n    }\n}\n\n\n</code></pre>\n<p>为了方便编写代码，上述并行处理即使时间包括了推理上下文的创建、推理预热等步骤，所以实际时间会偏长，上述程序运行后输出如下所示：</p>\n<p><img alt=\"image-20260109230808256\" class=\"lazyload\" /></p>\n<p><strong>并行测试结果：</strong></p>\n<p>同时为了比较不同并行数，测试了从1到24不同并行数的情况，推理总时间如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">并行数</th>\n<th style=\"text-align: center;\">1</th>\n<th style=\"text-align: center;\">2</th>\n<th style=\"text-align: center;\">4</th>\n<th style=\"text-align: center;\">6</th>\n<th style=\"text-align: center;\">8</th>\n<th style=\"text-align: center;\">10</th>\n<th style=\"text-align: center;\">12</th>\n<th style=\"text-align: center;\">14</th>\n<th style=\"text-align: center;\">16</th>\n<th style=\"text-align: center;\">18</th>\n<th style=\"text-align: center;\">20</th>\n<th style=\"text-align: center;\">22</th>\n<th style=\"text-align: center;\">24</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\">推理总时间 (ms)</td>\n<td style=\"text-align: center;\">80</td>\n<td style=\"text-align: center;\">85</td>\n<td style=\"text-align: center;\">95</td>\n<td style=\"text-align: center;\">115</td>\n<td style=\"text-align: center;\">130</td>\n<td style=\"text-align: center;\">155</td>\n<td style=\"text-align: center;\">180</td>\n<td style=\"text-align: center;\">210</td>\n<td style=\"text-align: center;\">230</td>\n<td style=\"text-align: center;\">255</td>\n<td style=\"text-align: center;\">270</td>\n<td style=\"text-align: center;\">285</td>\n<td style=\"text-align: center;\">310</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>🔗<strong>程序路径链接</strong>：完整程序已经上传到GitHub，请自行下载，链接为：</p>\n<pre><code>https://github.com/guojin-yan/TensorRT-CSharp-API/tree/TensorRtSharp3.0/samples/YoloObbBatchInfer\n</code></pre>\n</blockquote>\n<hr />\n<h2 id=\"七异常处理\">七、异常处理</h2>\n<p>TensorRtSharp 提供了完善的异常处理机制。</p>\n<pre><code class=\"language-csharp\">try\n{\n    Runtime runtime = new Runtime();\n    byte[] data = File.ReadAllBytes(\"model.engine\");\n    using CudaEngine engine = runtime.deserializeCudaEngineByBlob(data, (ulong)data.Length);\n}\ncatch (TrtException ex)\n{\n    // TensorRT 特定错误\n    Console.WriteLine($\"TensorRT Error: {ex.ErrMsg}\");\n    Console.WriteLine($\"Status: {ex.Status}\");\n}\ncatch (CudaException ex)\n{\n    // CUDA 运行时错误\n    Console.WriteLine($\"CUDA Error: {ex.Message}\");\n    Console.WriteLine($\"Status: {ex.Status}\");\n}\ncatch (InitException ex)\n{\n    // 初始化错误\n    Console.WriteLine($\"Initialization Failed: {ex.Message}\");\n    Console.WriteLine($\"Status: {ex.Status}\");\n}\n</code></pre>\n<p><strong>异常类型说明：</strong></p>\n<table>\n<thead>\n<tr>\n<th>异常类型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>TrtException</code></td>\n<td>TensorRT API 错误（20+ 错误码）</td>\n</tr>\n<tr>\n<td><code>CudaException</code></td>\n<td>CUDA 运行时错误（40+ 错误码）</td>\n</tr>\n<tr>\n<td><code>InitException</code></td>\n<td>库初始化错误</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"八日志系统\">八、日志系统</h2>\n<p>TensorRtSharp 提供了单例日志系统。</p>\n<pre><code class=\"language-csharp\">// 获取日志实例\nLogger logger = Logger.Instance;\n\n// 设置日志级别\nlogger.SetThreshold(LoggerSeverity.kINFO);  // INFO、WARNING、ERROR\n\n// 设置自定义回调\nlogger.SetCallback((message) =&gt;\n{\n    Console.WriteLine($\"[TensorRT] {message}\");\n});\n\n// 记录日志\nlogger.INFO(\"Engine building started...\");\nlogger.WARNING(\"FP16 not supported, falling back to FP32\");\nlogger.ERROR(\"Failed to parse ONNX model\");\n\n// 静默模式\nlogger.SetThreshold(LoggerSeverity.kINTERNAL_ERROR);  // 仅严重错误\n</code></pre>\n<hr />\n<h2 id=\"九与其他库的对比\">九、与其他库的对比</h2>\n<table>\n<thead>\n<tr>\n<th>特性</th>\n<th>TensorRtSharp</th>\n<th>ML.NET</th>\n<th>ONNX Runtime</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>编程语言</strong></td>\n<td>C#</td>\n<td>C#</td>\n<td>C++/Python</td>\n</tr>\n<tr>\n<td><strong>API 类型</strong></td>\n<td>托管封装</td>\n<td>托管库</td>\n<td>原生绑定</td>\n</tr>\n<tr>\n<td><strong>性能</strong></td>\n<td>原生速度</td>\n<td>中等</td>\n<td>原生速度</td>\n</tr>\n<tr>\n<td><strong>易用性</strong></td>\n<td>高</td>\n<td>高</td>\n<td>中等</td>\n</tr>\n<tr>\n<td><strong>TensorRT 支持</strong></td>\n<td>完整</td>\n<td>无</td>\n<td>有限</td>\n</tr>\n<tr>\n<td><strong>自定义算子</strong></td>\n<td>支持</td>\n<td>困难</td>\n<td>支持</td>\n</tr>\n<tr>\n<td><strong>动态形状</strong></td>\n<td>支持</td>\n<td>有限</td>\n<td>支持</td>\n</tr>\n<tr>\n<td><strong>多 GPU</strong></td>\n<td>支持</td>\n<td>有限</td>\n<td>支持</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"十常见问题\">十、常见问题</h2>\n<h3 id=\"问题一找不到-dll-模块\">问题一：找不到 DLL 模块</h3>\n<p><strong>错误信息：</strong></p>\n<pre><code>Unable to load DLL 'TensorRT-C-API' or one of its dependencies: 找不到指定的模块。\n</code></pre>\n<p><strong>解决方案：</strong></p>\n<ol>\n<li>检查是否安装了对应版本的 Runtime NuGet 包</li>\n<li>确认系统 PATH 环境变量中包含 TensorRT 的 lib 目录和 CUDA 的 bin 目录</li>\n<li>确认 TensorRT 版本为 10.x 系列</li>\n</ol>\n<p><strong>错误截图：</strong></p>\n<p><img alt=\"错误1\" class=\"lazyload\" /><br />\n<img alt=\"错误2\" class=\"lazyload\" /></p>\n<hr />\n<h3 id=\"问题二sehexception-异常\">问题二：SEHException 异常</h3>\n<p><strong>错误信息：</strong></p>\n<pre><code>System.Runtime.InteropServices.SEHException: \"External component has thrown an exception.\"\n</code></pre>\n<p><strong>可能原因：</strong></p>\n<ul>\n<li>TensorRT 版本不匹配（必须使用 10.x）</li>\n<li>CUDA 版本不兼容</li>\n<li>模型文件损坏</li>\n</ul>\n<p><strong>解决方案：</strong></p>\n<ol>\n<li>确认 TensorRT 版本为 10.x</li>\n<li>检查 CUDA 版本是否匹配</li>\n<li>重新生成 Engine 文件</li>\n</ol>\n<p><strong>错误截图：</strong></p>\n<p><img alt=\"SEHException\" class=\"lazyload\" /></p>\n<h3 id=\"问题三systemexecutionengineexception-异常\">问题三：System.ExecutionEngineException 异常</h3>\n<p><strong>错误信息：</strong></p>\n<pre><code>System.ExecutionEngineException \n</code></pre>\n<p><strong>可能原因：</strong></p>\n<ul>\n<li>模型文件与设备不匹配</li>\n</ul>\n<p><strong>解决方案：</strong></p>\n<ol>\n<li>在当前设备上重新生成模型文件</li>\n</ol>\n<p><strong>错误截图：</strong></p>\n<p><img alt=\"image-20260110002045090\" class=\"lazyload\" /></p>\n<p><img alt=\"image-20260110002045090\" class=\"lazyload\" /></p>\n<hr />\n<h2 id=\"十一总结\">十一、总结</h2>\n<p>TensorRtSharp 是一个功能完整、设计精良的 TensorRT C# 封装库，它填补了 .NET 生态在高性能深度学习推理方面的空白。通过提供类型安全的 API、自动资源管理和完善的异常处理，TensorRtSharp 让 C# 开发者能够充分发挥 GPU 的计算能力，而无需面对复杂的原生代码。</p>\n<h3 id=\"核心优势\">核心优势</h3>\n<p>✅ <strong>完整的 API 覆盖</strong>：支持 TensorRT 核心功能<br />\n✅ <strong>类型安全</strong>：强类型系统，编译时错误检查<br />\n✅ <strong>自动资源管理</strong>：RAII + Dispose 模式<br />\n✅ <strong>高性能</strong>：异步执行、多流并行<br />\n✅ <strong>易用性</strong>：直观的 API、详细注释<br />\n✅ <strong>跨平台</strong>：支持 Windows/Linux<br />\n✅ <strong>开箱即用</strong>：NuGet 包含所有依赖</p>\n<h3 id=\"适用场景\">适用场景</h3>\n<p>无论您是构建以下类型的应用，TensorRtSharp 都是您的理想选择：</p>\n<ul>\n<li>🎯 <strong>实时视觉应用</strong>：目标检测、图像分割、姿态估计</li>\n<li>🎤 <strong>语音处理</strong>：语音识别、语音合成</li>\n<li>🚀 <strong>边缘计算</strong>：嵌入式设备推理</li>\n</ul>\n<h3 id=\"立即开始\">立即开始</h3>\n<p><strong>安装命令：</strong></p>\n<pre><code class=\"language-bash\">dotnet add package JYPPX.TensorRT.CSharp.API\ndotnet add package JYPPX.TensorRT.CSharp.API.runtime.win-x64.cuda12\n</code></pre>\n<p><strong>GitHub 仓库：</strong></p>\n<pre><code>https://github.com/guojin-yan/TensorRT-CSharp-API\n</code></pre>\n<p>立即安装并体验 C# 世界中的 GPU 推理极致性能吧！</p>\n<hr />\n<h2 id=\"技术支持\">技术支持</h2>\n<p>如有问题或建议，欢迎通过以下方式交流：</p>\n<ul>\n<li>📧 <strong>GitHub Issues</strong>：在项目仓库提 Issue 或 Pull Request</li>\n<li>💬 <strong>QQ 交流群</strong>：加入 <strong>945057948</strong>，回复更方便更快哦</li>\n</ul>\n<p><img alt=\"QQ群二维码\" class=\"lazyload\" /></p>\n<hr />\n<p><em>作者：Guojin Yan</em><br />\n<em>版本：0.0.5</em><br />\n<em>最后更新：2026年1月</em></p>\n<hr />\n<p><strong>【文章声明】</strong></p>\n<p>本文主要内容基于作者的研究与实践，部分表述借助AI工具进行了辅助优化。由于技术局限性，文中可能存在错误或疏漏之处，恳请各位读者批评指正。如果内容无意中侵犯了您的权益，请及时通过公众号后台与我们联系，我们将第一时间核实并妥善处理。感谢您的理解与支持！</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-11 18:47</span>&nbsp;\n<a href=\"https://www.cnblogs.com/guojin-blogs\">椒颜皮皮虾</a>&nbsp;\n阅读(<span id=\"post_view_count\">294</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "TheIsle恐龙岛读取游戏基址做插件（C#语言）",
      "link": "https://www.cnblogs.com/TheIsle/p/19468703",
      "published": "",
      "description": "<h1 class=\"postTitle\"><a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/TheIsle/p/19468703\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 18:04\">\n    <span>TheIsle恐龙岛读取游戏基址做插件（C#语言）</span>\n    \n\n</a>\n</h1>\n\t<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<span class=\"js_darkmode__0\"><span>在C# 中读取游戏基址（例如，在内存中定位一个游戏进程的特定内存地址），通常涉及到使用Windows API来获取游戏进程的内存信息。这可以通过<code class=\"js_darkmode__1\"><span>System.Diagnostics</span></code><span class=\"js_darkmode__2\"><span>命名空间中的<code class=\"js_darkmode__3\"><span>Process</span></code><span class=\"js_darkmode__4\"><span>类和一些P/Invoke（平台调用）技术来实现。以下是一些步骤和示例代码，可以帮助你实现这一功能。</span></span></span></span></span></span>\n<h3 class=\"js_darkmode__5\"><span>步骤 1:打开VS开发工具，新建一个WPF项目，再新建一个Windows窗体。</span></h3>\n<span class=\"js_darkmode__6\"><span class=\"js_darkmode__6\"><span class=\"js_darkmode__7\">步骤 2:在Windows窗体中添加文本框和按钮控件，代码如下：</span></span></span>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">&lt;StackPanel VerticalAlignment=\"Center\"&gt;\n    &lt;StackPanel Orientation=\"Horizontal\" HorizontalAlignment=\"Center\"&gt;\n        &lt;TextBlock Text=\"进程名称：\" VerticalAlignment=\"Center\"/&gt;\n        &lt;TextBox x:Name=\"txtName\" Width=\"190\" Height=\"24\" Text=\"TheIsleServer-Win64-Shipping\"\n                 VerticalContentAlignment=\"Center\"/&gt;\n    &lt;/StackPanel&gt;\n    &lt;Button Content=\"OK\" Width=\"80\" Height=\"26\" Margin=\"0,10\" Click=\"Button_Click\"/&gt;\n    &lt;TextBox IsReadOnly=\"True\" x:Name=\"txtShow\" Height=\"44\" HorizontalAlignment=\"Center\"\n             VerticalContentAlignment=\"Center\" Width=\"210\"/&gt;\n&lt;/StackPanel&gt;\n</pre>\n</div>\n<p>　　其中【TheIsleServer-Win64-Shipping】是恐龙岛游戏服务端进程名称，其它游戏填写对应游戏名称。</p>\n<p><span class=\"js_darkmode__12\">步骤 3:<span><span class=\"js_darkmode__14\">前端界面做好之后，接下来需要添加后台逻辑代码，实现读取游戏基址的功能。</span></span></span></p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:csharp;gutter:true;\">if (GameHelper.GetPidByProcessName(txtName.Text) == 0)\n{\n    MessageBox.Show(\"未找到游戏进程！\");\n    return;\n}\nProcess gameProcess = Process.GetProcessesByName(txtName.Text)[0];\nstring baseAdr = gameProcess.Modules[0].EntryPointAddress.ToString();\nstring baseAdr1 = gameProcess.Modules[0].BaseAddress.ToString();\ntxtShow.Text = \"EntryPointAddress：\" + baseAdr + \"\\nBaseAddress：\" + baseAdr1;\n</pre>\n</div>\n<p>　　<span><span class=\"js_darkmode__17\">代码写完了，运行这个程序，点击界面中的OK按钮，就可以获取恐龙岛游戏的基址。【EntryPointAddress】是程序入口点基址，<span class=\"js_darkmode__18\"><span class=\"js_darkmode__19\"><span><span class=\"js_darkmode__20\">【BaseAddress】就是<span class=\"js_darkmode__21\"><span class=\"js_darkmode__22\"><span><span class=\"js_darkmode__23\">恐龙岛游戏基址。基址<span class=\"js_darkmode__24\"><span>指的是游戏模块（如.exe或.dll文件）在内存中的起始地址，这个地址在游戏每次启动时可能变化，但相对于模块本身是稳定的。</span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p><img alt=\"640\" class=\"lazyload\" /></p>\n<p><span class=\"js_darkmode__25\">读取到游戏<span class=\"js_darkmode__26\"><span>基址<span class=\"js_darkmode__27\"><span>‌后，再<span class=\"js_darkmode__28\"><span>加上偏移地址，就可以获取到比如血量、时间的具体数值，<span class=\"js_darkmode__29\"><span>偏移地址可以是多级的。通过基址+偏移的方式，就可以制作长白天、吃肉回血等功能的<span class=\"js_darkmode__30\"><span class=\"js_darkmode__31\"><span class=\"js_darkmode__32\"><span>服务器插件。</span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<h3><span>为什么需要基址加偏移？</span></h3>\n<p><span class=\"js_darkmode__39\"><span>游戏数据的内存地址在每次重启后可能改变（由于内存随机化机制），直接使用绝对地址会失效。而基址加偏移的方式利用了数据在内存中的相对位置关系，只要基址和偏移不变，就能稳定读取数据。‌</span></span></p>\n</div>\n<div id=\"MySignature\">\n    <p>本文来自博客园，作者：<a href=\"https://www.cnblogs.com/TheIsle/\" target=\"_blank\">逗号TheIsle</a>，转载请注明原文链接：<a href=\"https://www.cnblogs.com/TheIsle/p/19468703\" target=\"_blank\">https://www.cnblogs.com/TheIsle/p/19468703</a></p>\n</div>\n<div class=\"clear\"></div>\n\n\t<div class=\"postDesc\">posted on \n<span id=\"post-date\">2026-01-11 18:04</span>&nbsp;\n<a href=\"https://www.cnblogs.com/TheIsle\">逗号TheIsle</a>&nbsp;\n阅读(<span id=\"post_view_count\">58</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "当我不想再为「小决定」消耗注意力时，我做了一个很小的工具",
      "link": "https://www.cnblogs.com/BigFunny/p/19468424",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/BigFunny/p/19468424\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 15:33\">\n    <span>当我不想再为「小决定」消耗注意力时，我做了一个很小的工具</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p><strong>写这篇文章的起因，其实很简单。</strong></p>\n<p>有一天我发现，自己一天中被打断最多的，并不是复杂的问题，而是一些本来不值得认真思考的小决定：</p>\n<ul>\n<li>先做哪个任务？</li>\n<li>午饭吃什么？</li>\n<li>几个方案里随便选一个，从哪开始？</li>\n</ul>\n<p>这些选择单独看都很轻，但它们反复出现，会不断打断注意力。<br />\n真正消耗人的，往往不是大问题，而是这些「无关紧要但又不得不想一下」的时刻。</p>\n<hr />\n<h2 id=\"我一开始试图用各种工具解决这个问题\">我一开始试图用各种工具解决这个问题</h2>\n<p>但后来发现一个很讽刺的情况：</p>\n<blockquote>\n<p>为了不纠结，我反而花了更多时间在工具本身上。</p>\n</blockquote>\n<p>要登录、要配置、要理解功能，甚至还会被推荐更多「你可能需要的东西」。</p>\n<p>工具并没有让我更快开始行动，反而成了新的注意力黑洞。</p>\n<p>那时候我意识到，我真正想要的并不是一个「更聪明」的系统，而是一个 <strong>更安静的工具</strong>。</p>\n<hr />\n<h2 id=\"一个几乎感觉不到存在的小工具\">一个几乎「感觉不到存在」的小工具</h2>\n<p>WheelPage 就是在这种背景下出现的。</p>\n<p>它不是一个效率系统，也不试图帮你「做出更正确的决定」。<br />\n它的目标只有一个：</p>\n<blockquote>\n<p>在你不想再纠结的时候，给你一个快速、中立、无需解释的结果。</p>\n</blockquote>\n<p>最早做的是一个决策转盘。</p>\n<p>你把选项写进去，转一下，就有结果。</p>\n<p>后来我发现，很多人并不是特别在意转盘转到了哪一项，<br />\n而是在结果出现的那一刻，终于可以继续往下走了。</p>\n<p>这个转盘后来被用在很多意料之外的场景里：</p>\n<ul>\n<li>团队里打破僵局</li>\n<li>课堂互动</li>\n<li>创作卡住时，随机给自己一个起点</li>\n<li>甚至只是帮自己「先开始做点什么」</li>\n</ul>\n<p>如果你想看这个最基础的功能，它在这里：<br />\n👉 <a href=\"https://wheelpage.com/zh/\" rel=\"noopener nofollow\" target=\"_blank\">转盘</a></p>\n<hr />\n<h2 id=\"为什么后来又加了一个-抛硬币\">为什么后来又加了一个 抛硬币</h2>\n<p>在使用过程中，有个反馈反复出现：</p>\n<blockquote>\n<p>「其实我很多时候只是在两件事之间犹豫。」</p>\n</blockquote>\n<p>于是我加了一个更极简的工具：<strong>抛硬币</strong>。</p>\n<ul>\n<li>没有选项配置</li>\n<li>没有说明文字</li>\n<li>打开就能用</li>\n</ul>\n<p>你只需抛一下，正或反。</p>\n<p>有趣的是，很多用户后来跟我说，他们并不是完全把决定交给硬币，<br />\n而是在硬币落下的一瞬间，突然意识到自己心里更偏向哪一边。</p>\n<p>这种体验本身就很微妙。</p>\n<p>如果你也好奇，可以直接试一下这个页面：<br />\n👉 <a href=\"https://wheelpage.com/zh/coin-flip/\" rel=\"noopener nofollow\" target=\"_blank\">抛硬币</a></p>\n<hr />\n<h2 id=\"设计过程中我刻意做的几件反直觉的事\">设计过程中，我刻意做的几件「反直觉」的事</h2>\n<p>在开发 WheelPage 的过程中，我有意避免做三件事情：</p>\n<h3 id=\"1-不做账户系统\">1. 不做账户系统</h3>\n<p>因为「只是用一下」，不应该成为注册的理由。</p>\n<h3 id=\"2-不堆功能\">2. 不堆功能</h3>\n<p>每加一个功能，我都会问自己：</p>\n<blockquote>\n<p>这是在帮用户，<br />\n还是只是让我觉得「这个产品更像个产品了」？</p>\n</blockquote>\n<h3 id=\"3-不制造存在感\">3. 不制造存在感</h3>\n<p>用完就走，是我希望它呈现出来的状态。</p>\n<p>我更愿意把 WheelPage 看成一个「工具抽屉里的小东西」，<br />\n而不是一个需要你持续关注的产品。</p>\n<hr />\n<h2 id=\"写在最后\">写在最后</h2>\n<p>WheelPage 并不是为了解决人生重大决策而存在的。</p>\n<p>它存在的意义，恰恰是为了让你不用在小事上消耗太多注意力。</p>\n<p>有时候，一个外部的、随机的、没有情绪的结果，<br />\n反而能帮你更快行动。</p>\n<p>甚至在某些时刻，它还能让你看清自己真正想要的是什么。</p>\n<p>如果你有类似的困扰，也许这个小工具能帮你节省一点注意力。</p>\n<p>它就在那儿，<br />\n不吵你，也不催你。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-11 15:33</span>&nbsp;\n<a href=\"https://www.cnblogs.com/BigFunny\">OnlyFunny</a>&nbsp;\n阅读(<span id=\"post_view_count\">455</span>)&nbsp;\n评论(<span id=\"post_comment_count\">3</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "听说C++好像偷偷去练什么\"绝世武功\"去了",
      "link": "https://www.cnblogs.com/lixingqiu/p/19468363",
      "published": "",
      "description": "<h1 class=\"postTitle\"><a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/lixingqiu/p/19468363\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 15:07\">\n    <span>听说C++好像偷偷去练什么\"绝世武功\"去了</span>\n    \n\n</a>\n</h1>\n\t<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p><img alt=\"monkey\" class=\"lazyload\" height=\"417\" width=\"417\" /></p>\n<p>&nbsp;</p>\n<p>嘿，大家伙儿以前是不是都听那个老传说了？学C++就像走蜀道，难于上青天；学Python呢，那叫一个顺滑，像吃果冻一样。于是乎，多少怀揣梦想的少男少女，还没看到C++的窗口弹出来，就被那一堆指针、内存管理劝退了，转头抱了Python的大腿。</p>\n<p>但是！时代变了啊朋友们！自从有了这个“C++精灵库”，C++也学会了卖萌了。以前搞个图形界面，又是创建画笔又是设置坐标系，那代码写得比经书还长——尤其是还在捣鼓老开发环境的同学们，那种配置环境的酸爽，谁试谁知道。现在呢？你看这段代码，几行指令，屏幕就亮了，简直像开了挂。</p>\n<p>就拿这《孙悟空的72变动画》来说吧，这代码写得那叫一个随心所欲。那个叫<code>monkey</code>的小家伙，真的是一个小精灵。上一秒还是大圣爷，下一秒变成了光头强，转眼又混进了机器猫的队伍，甚至还能变出个中国结来！这哪是写代码啊，这简直是“连连看”现场版！你让阿童木去抓灰太狼，这脑洞，也就C++加上这精灵库能配合得这么溜。</p>\n<p>最绝的是什么？是那个循环。<code>monkey.next_shape().wait(...)</code>。就这么一句，孙悟空就在屏幕上开始了他的川剧变脸大赏。不需要你懂什么复杂的底层渲染，也不需要你为了一个窗口句柄抓耳挠腮。现在的C++，就像这孙悟空一样，学会72变了。它摇身一变，从那个冷冰冰的“硬汉”，变成了一个能陪你玩、能陪你闹、还能随便你捏造型的“小可爱”。</p>\n<p>所以啊，别再死抱着“C++难如天书”的老黄历了。在这个精灵库里，C++把门槛都拆了，铺上了一层红地毯欢迎大家。这《猴王变形记》演的可不是孙悟空，演的是C++的“变形记”——原来，你离大神之间，只差一个会72变的精灵库！</p>\n<p><span style=\"color: rgba(255, 0, 0, 1);\"><strong>说了这么多，正文在此：</strong></span></p>\n<div class=\"cnblogs_code\">\n<pre>#include <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">sprites.h</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">包含C++精灵库 </span>\nSprite monkey;       <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">建立角色叫monkey </span>\n\n<span style=\"color: rgba(0, 0, 255, 1);\">int</span> main(){        <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">主功能块 </span>\n   monkey.bgcolor(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">black</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>).scale(<span style=\"color: rgba(128, 0, 128, 1);\">0.5</span>);  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">背景黑，角色变小一半\n   </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">抬笔，并且移除索引为0的造型（默认小火箭)</span>\n   monkey.penup().removeshape(<span style=\"color: rgba(128, 0, 128, 1);\">0</span>).color(<span style=\"color: rgba(128, 0, 128, 1);\">200</span>).addy(<span style=\"color: rgba(128, 0, 128, 1);\">200</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n   std::</span><span style=\"color: rgba(0, 0, 255, 1);\">string</span> s = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">C++孙悟空的72变小动画</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n   monkey.write(s,</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">center</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>,{<span style=\"color: rgba(128, 0, 0, 1);\">\"\"</span>,<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">48</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>,<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">normal</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>}).wait(<span style=\"color: rgba(128, 0, 128, 1);\">1</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n   monkey.addy(</span>-<span style=\"color: rgba(128, 0, 128, 1);\">200</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n   monkey.addshape(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/阿童木.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>);   <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">添加造型</span>\n   monkey.addshape(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/光头强.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n   monkey.addshape(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/灰太狼.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n   monkey.addshape(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/机器猫.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n   monkey.addshape(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/monkey.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>); <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">孙悟空</span>\n   monkey.addshape(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/外星人.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n   monkey.addshape(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/中国结.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n   monkey.addshape(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/qirocketboy.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>);  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">骑火箭飞向太空的男孩\n   </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">加载背景音乐</span>\n   Mix_Music *music=Mix_LoadMUS(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/monkeybgmusic.wav</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>);  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">不支持中文名</span>\n   Mix_PlayMusic(music,-<span style=\"color: rgba(128, 0, 128, 1);\">1</span>);  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">-1表示无限循环播放，0表示播放1次，1则是2次。\n   </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> g_screen是角色建立后的全局屏幕指针  </span>\n   <span style=\"color: rgba(0, 0, 255, 1);\">while</span>(g_screen-&gt;exitonclick())  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">单击窗口关闭按钮则循环退出</span>\n     monkey.next_shape().wait(random(<span style=\"color: rgba(128, 0, 128, 1);\">0.9</span>,<span style=\"color: rgba(128, 0, 128, 1);\">1.5</span>)); <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">孙悟空切换造型，并且随机等待一定的时间  </span>\n   <span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n}</span></pre>\n</div>\n<p>哎呀，我忘了看官没有学过C++了，不过这里还有视频：</p>\n<p>................................................................................................(此处过去5分钟)</p>\n<p>完了，找了半天没有找到在哪里插入视频，只得委屈看官去抖音号pxcoding找《孙悟空的72变小动画》吧。</p>\n<p>再见了，我的小宝贝们。</p>\n<p>&nbsp;</p>\n</div>\n<div class=\"clear\"></div>\n\n\t<div class=\"postDesc\">posted on \n<span id=\"post-date\">2026-01-11 15:07</span>&nbsp;\n<a href=\"https://www.cnblogs.com/lixingqiu\">李兴球</a>&nbsp;\n阅读(<span id=\"post_view_count\">291</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}