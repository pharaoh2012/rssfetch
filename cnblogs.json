{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "AI → JSON → UI",
      "link": "https://www.cnblogs.com/guangzan/p/19487446",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/guangzan/p/19487446\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 15:23\">\n    <span>AI → JSON → UI</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"背景\">背景</h2>\n<p>过去两年，AI 生成 UI 的实践基本集中在两种路径上。第一种是直接让模型生成 JSX、HTML 或 CSS。这条路线的优势在于自由度极高，模型几乎不受约束，看起来“什么都能写”。但在真实工程环境中，这种方式几乎不可控：输出结构不稳定，无法保证组件边界，难以做权限与审计控制，生成的代码经常无法编译或违背工程约定，更重要的是，它与实际业务中的组件体系和设计系统严重脱节。</p>\n<p>另一条路线是低代码或 schema 驱动 UI，例如基于 JSON Schema 或表单 schema 的方案。这类方案在工程上是可控的，结构稳定、可校验、可复用，但它们本质上是为“人编写配置”设计的，而不是为“模型生成结构”设计的。schema 表达能力有限，扩展成本高，并且与自然语言之间的映射并不自然，Prompt 往往需要大量人工约束。</p>\n<p>Vercel 刚刚开源了 json-render，json-render 的出现，本质上是对这两条路线的重新切分与组合。它并没有试图让 AI 写前端代码，也没有把 AI 限制在传统低代码 schema 中，而是引入了一个中间层：<strong>JSON UI AST</strong>。AI 只能生成这种 AST，而 AST 的能力边界完全由开发者定义。渲染、状态、行为解释全部留在业务侧完成。开发者因此可以安全地让用户通过自然语言生成仪表盘、小部件或数据视图，而不需要把执行权交给模型。</p>\n<p><img alt=\"iShot_2026-01-15_15.25.26\" src=\"https://img2024.cnblogs.com/blog/1501373/202601/1501373-20260115152759556-1171653435.gif\" /></p>\n<h2 id=\"整体架构json-render-是一个-dsl-解释系统\">整体架构：json-render 是一个 DSL 解释系统</h2>\n<p>从架构视角看，json-render 并不是一个 UI 框架，而是一个 DSL 执行系统。系统由三层构成：最底层是 Catalog，用来声明“系统允许 AI 使用哪些 UI 能力”；中间层是 JSON UI Tree，这是 AI 的唯一输出形式；最上层是 Renderer，由业务侧实现，用于解释 JSON 并渲染真实 UI。</p>\n<p>它们之间的关系可以用下面这张结构图来理解：</p>\n<pre><code>┌────────────┐\n│   Prompt   │\n└─────┬──────┘\n      │\n      ▼\n┌──────────────────┐\n│  LLM / AI Model  │\n└─────┬────────────┘\n      │  JSON UI AST（受 Catalog 严格约束）\n      ▼\n┌──────────────────┐\n│     Catalog      │  ← 能力白名单 / Schema / Grammar\n└─────┬────────────┘\n      │ 校验 + 解析\n      ▼\n┌──────────────────┐\n│    Renderer      │  ← React / Vue / Native\n└─────┬────────────┘\n      │\n      ▼\n┌──────────────────┐\n│   Real UI View   │\n└──────────────────┘\n</code></pre>\n<p>在这个模型中，AI 只参与“结构生成”，不参与“执行”。这也是 json-render 在工程上成立的根本原因。</p>\n<h2 id=\"从-catalog-到-ui\">从 Catalog 到 UI</h2>\n<h3 id=\"1-catalog系统的能力边界定义\">1. Catalog：系统的能力边界定义</h3>\n<p>下面这段代码是整个系统中最重要的入口，它定义了 AI 能使用的全部 UI 能力。</p>\n<pre><code class=\"language-ts\">import { createCatalog } from '@json-render/core'\nimport { z } from 'zod'\n\nexport const catalog = createCatalog({\n  components: {\n    Card: {\n      props: z.object({\n        title: z.string()\n      }),\n      hasChildren: true\n    },\n    Metric: {\n      props: z.object({\n        label: z.string(),\n        valuePath: z.string(),\n        format: z.enum(['number', 'currency', 'percent']).optional()\n      })\n    }\n  },\n  actions: {\n    refresh: {\n      params: z.object({})\n    }\n  }\n})\n</code></pre>\n<p>这里没有任何 UI 代码，只有能力声明。props 使用 Zod 定义，这意味着它不仅是类型提示，还包含运行时校验规则。如果你对 Zod 没有了解，可以看看这篇博文，<a href=\"https://www.cnblogs.com/guangzan/p/19350726\" target=\"_blank\">Zod：TypeScript 类型守卫与数据验证</a>。action 并不是函数实现，而是一个“意图声明”，它只描述“可以发生什么”，不描述“怎么发生”。</p>\n<p>Catalog 在系统中的地位，相当于一门语言的语法定义文件。AI 后续生成的所有 JSON，本质上都必须符合这套 grammar。</p>\n<h3 id=\"2-ai-输出的-json-ui-ast\">2. AI 输出的 JSON UI AST</h3>\n<p>当用户输入类似“生成一个收入仪表盘”的提示时，模型生成的结果不是 JSX，而是下面这样的 JSON：</p>\n<pre><code class=\"language-json\">{\n  \"type\": \"Card\",\n  \"props\": { \"title\": \"Revenue Overview\" },\n  \"children\": [\n    {\n      \"type\": \"Metric\",\n      \"props\": {\n        \"label\": \"Total Revenue\",\n        \"valuePath\": \"/metrics/revenue\",\n        \"format\": \"currency\"\n      }\n    }\n  ]\n}\n</code></pre>\n<p>这个 JSON 有几个非常关键的特征。它不包含任何函数、不包含条件表达式、不包含样式或状态逻辑。它只是结构化地描述“使用哪个组件，用什么参数，组件之间如何嵌套”。所有能力完全来源于 Catalog，因此这个 JSON 是可校验、可存储、可 diff、可审计、可回放的。</p>\n<h3 id=\"3-rendererjson-的解释执行\">3. Renderer：JSON 的解释执行</h3>\n<p>在 React 侧，Renderer 扮演的是解释器的角色。</p>\n<pre><code class=\"language-tsx\">import { Renderer } from '@json-render/react'\nimport { catalog } from './catalog'\n\nfunction App() {\n  return (\n    &lt;Renderer\n      catalog={catalog}\n      components={{\n        Card: ({ title, children }) =&gt; (\n          &lt;div className=\"card\"&gt;\n            &lt;h2&gt;{title}&lt;/h2&gt;\n            {children}\n          &lt;/div&gt;\n        ),\n        Metric: ({ label, value }) =&gt; (\n          &lt;div&gt;\n            {label}: {value}\n          &lt;/div&gt;\n        )\n      }}\n      data={{\n        metrics: { revenue: 120000 }\n      }}\n    /&gt;\n  )\n}\n</code></pre>\n<p>Renderer 并不关心 UI 长什么样，它只做三件事：根据 type 找到对应组件定义，根据 Catalog 校验 props 和 children，根据 valuePath 等规则完成数据注入。</p>\n<h2 id=\"为什么-json-render-是可控的\">为什么 json-render 是“可控的”</h2>\n<p>下面的借助 AI 能力分析基于 <code>vercel-labs/json-render</code> 主仓库。如果你对此不感兴趣，跳过这部分内容。</p>\n<h3 id=\"1-createcatalog能力被冻结的起点\">1. createCatalog：能力被冻结的起点</h3>\n<p>文件路径位于 <code>packages/core/src/create-catalog.ts</code>。这个函数的核心作用不是“注册组件”，而是“冻结能力边界”。</p>\n<p>简化后的核心逻辑可以理解为：</p>\n<pre><code class=\"language-ts\">export function createCatalog(definition) {\n  return {\n    components: definition.components,\n    actions: definition.actions,\n    validateNode(node) {\n      // 校验 type 是否存在\n      // 校验 props 是否符合 Zod schema\n      // 校验 children 是否被允许\n    }\n  }\n}\n</code></pre>\n<p>每一行代码都在服务一个目标：让 Catalog 成为一个不可突破的白名单。Renderer 和 AI 都无法绕过它。这也是为什么 json-render 把 Catalog 放在 core 包中，而不是 React 包中。</p>\n<h3 id=\"2-schema-校验ai-输出必须先编译再执行\">2. Schema 校验：AI 输出必须“先编译再执行”</h3>\n<p>在 JSON Tree 进入 Renderer 之前，系统会逐节点校验。type 是否在 Catalog 中声明，props 是否通过 Zod 校验，children 是否符合 hasChildren 约束，action 是否存在于白名单。这一过程本质上就是一次 AST 校验。</p>\n<p>这意味着 AI 的输出不是“运行时报错”，而是“不通过即拒绝执行”。在 AI UI 系统中，这是一个极其关键但经常被忽视的工程点。</p>\n<h3 id=\"3-renderer真正的解释器模型\">3. Renderer：真正的解释器模型</h3>\n<p>React Renderer 的内部逻辑并不是简单的 switch-case，而是一个递归解释过程。它根据节点的 type 查 Catalog，构造 props，解析 valuePath 注入数据，绑定 action handler，然后递归渲染 children。</p>\n<p>从架构角度看，它更接近一个 JSON AST Interpreter，而不是模板引擎。这也是 json-render 可以跨 React、Vue、Native 复用核心思想的原因。</p>\n<h3 id=\"4-valuepath刻意避免-ai-参与状态逻辑\">4. valuePath：刻意避免 AI 参与状态逻辑</h3>\n<p>valuePath 使用字符串路径描述数据依赖，例如：</p>\n<pre><code class=\"language-json\">\"valuePath\": \"/metrics/revenue\"\n</code></pre>\n<p>这样设计的直接结果是，AI 不需要理解状态结构，也不需要写任何状态逻辑。Renderer 统一负责解析路径、读取数据、触发更新。这在架构上刻意切断了“AI 直接操作状态”的可能性。</p>\n<p>下面是仅包含新增内容的补充章节，重点放在可落到源码层面的机制，避免概念化描述。示例代码与解释均基于 <code>vercel-labs/json-render</code> 当前仓库结构与实现思路。</p>\n<h2 id=\"prompt-与-catalog-的自动对齐\">Prompt 与 Catalog 的自动对齐</h2>\n<p>Prompt 与 Catalog 的自动对齐：不是“调 Prompt”，而是“导出 Grammar”。json-render 中，Prompt 与 Catalog 的对齐并不是通过人肉 Prompt Engineering 完成的，而是通过从 Catalog 派生一份机器可理解的能力描述，并将其注入到模型上下文中。这一点在 <code>packages/core</code> 中的设计非常关键。</p>\n<p>在 core 层，Catalog 本身并不是一个简单的对象，它包含了完整的组件定义、props schema 以及 action 描述。这些信息会被转换为一种“描述性结构”，用于告诉模型当前系统支持的 UI grammar。</p>\n<p>类似这样的逻辑：</p>\n<pre><code class=\"language-ts\">export function catalogToPrompt(catalog) {\n  return `\nYou can generate a JSON UI tree.\nAvailable components:\n${Object.entries(catalog.components).map(([name, def]) =&gt; `\n- ${name}\n  props: ${describeSchema(def.props)}\n  hasChildren: ${def.hasChildren}\n`).join('\\n')}\n\nAvailable actions:\n${Object.keys(catalog.actions).join(', ')}\n\nRules:\n- Output must be valid JSON\n- Only use listed components\n- Follow prop schemas strictly\n`\n}\n</code></pre>\n<p>这里的关键点不在于字符串本身，而在于信息来源完全来自 Catalog。换句话说，Catalog 是 single source of truth，Prompt 只是它的一种序列化视图。当开发者新增或修改组件定义时，Prompt 中允许模型使用的能力会自动发生变化，不存在“代码和 Prompt 不一致”的问题。这也是 json-render 能够避免大量“Prompt 腐化”的根本原因。</p>\n<p>从模型视角看，它面对的不是一段模糊的自然语言说明，而是一套接近 BNF 的 UI grammar 描述。模型生成 JSON UI Tree 的过程，本质上类似于在给定语法约束下生成 AST。这也是为什么 json-render 要使用 Zod 而不是仅靠 TypeScript 类型。Zod schema 可以被同时用于运行时校验和 Prompt 语义描述，形成闭环。</p>\n<h2 id=\"streaming-ui-的实现细节\">Streaming UI 的实现细节</h2>\n<p>流式构建 AST，而不是流式拼字符串。json-render 的 Streaming UI 能力，核心并不在“模型支持流式输出”，而在于 UI 的中间表示是可增量合并的 JSON AST。这一点在 React 包中的实现非常清晰。</p>\n<p>在 <code>packages/react</code> 中，可以看到类似 <code>useUIStream</code> 的 hook，其核心职责是：<br />\n维护一棵当前 UI Tree，并在模型流式输出时不断向这棵树中合并新节点。</p>\n<p>简化后的内部结构大致如下：</p>\n<pre><code class=\"language-ts\">// packages/react/src/use-ui-stream.ts（概念结构）\nexport function useUIStream() {\n  const [tree, setTree] = useState&lt;UITree | null&gt;(null)\n\n  function onChunk(chunk: string) {\n    const partialNode = parseChunkToNode(chunk)\n    if (!partialNode) return\n\n    setTree(prevTree =&gt; {\n      return mergeTree(prevTree, partialNode)\n    })\n  }\n\n  return { tree, onChunk }\n}\n</code></pre>\n<p>这里有两个非常关键但容易被忽略的点。</p>\n<p><code>parseChunkToNode</code> 并不是简单的 <code>JSON.parse</code>。模型在 streaming 模式下输出的通常是不完整 JSON，因此 json-render 采用的是逐段解析、延迟成型的策略。只有当一个节点在结构上是完整且通过 Catalog 校验时，才会被提升为“可合并节点”。<code>mergeTree</code> 是一个纯函数。它不依赖外部状态，只根据已有 UI Tree 和新节点生成下一棵 Tree。这使得每一次更新都是确定性的，也天然适合 React 的状态模型。</p>\n<p>在 Renderer 层，这棵 Tree 会被直接用于递归渲染：</p>\n<pre><code class=\"language-tsx\">function RenderNode({ node }) {\n  const Component = components[node.type]\n\n  const resolvedProps = resolveProps(node.props)\n  const children = node.children?.map(child =&gt;\n    &lt;RenderNode key={child.id} node={child} /&gt;\n  )\n\n  return &lt;Component {...resolvedProps}&gt;{children}&lt;/Component&gt;\n}\n</code></pre>\n<p>由于 Tree 始终是“已校验的合法结构”，Renderer 不需要关心节点是否完整，只需要关心“当前有哪些节点已经存在”。这也是 Streaming UI 能在生成未完成时就安全渲染的根本原因。</p>\n<h2 id=\"streaming-与-catalog-校验如何协同工作\">Streaming 与 Catalog 校验如何协同工作</h2>\n<p>Streaming UI 并不是绕过校验机制的捷径，恰恰相反，它依赖校验机制才能成立。在实际流程中，每一个候选节点在被合并进 UI Tree 之前，都会经过 Catalog 的校验逻辑：</p>\n<pre><code class=\"language-ts\">// packages/core/src/validate-node.ts（概念结构）\nexport function validateNode(node, catalog) {\n  const def = catalog.components[node.type]\n  if (!def) throw new Error('Unknown component')\n\n  def.props.parse(node.props)\n\n  if (node.children &amp;&amp; !def.hasChildren) {\n    throw new Error('Children not allowed')\n  }\n}\n</code></pre>\n<p>Streaming 模式下，这个校验发生得更频繁，但粒度更小。系统宁可“暂时不渲染”，也不会把一个非法节点交给 Renderer。这保证了 UI 在任何时刻都是一个合法子集，而不是半成品垃圾状态。</p>\n<p>Prompt 与 Catalog 的自动对齐，确保模型“不会幻想不存在的能力”；Streaming UI 的 AST 级增量构建，确保 UI“可以在不完整时仍然正确运行”。两者结合，使 json-render 的执行模型更接近编译器与解释器，而不是模板生成器。从工程视角看，这意味着一个重要转变：<strong>UI 生成不再是一次性结果，而是一个可观察、可中断、可回滚的过程。</strong>这也是 json-render 能够真正进入生产系统，而不仅停留在 Demo 层面的根本原因。</p>\n<h2 id=\"json-render-真正解决了什么\">json-render 真正解决了什么</h2>\n<p>json-render 本身并不是一种全新的技术范式。<strong>“用受限结构描述 UI，再由运行时解释执行”这一思想，在前端工程中早已反复出现过。</strong>早期的 JSON Schema Form、react-jsonschema-form、Formily、本质上都是用结构化数据描述界面，再由渲染器生成真实 UI。低代码平台、搭建系统、配置化后台，几乎全部建立在同一逻辑之上。即便在 AI 出现之前，这种模式也已经非常成熟：工程师通过 schema 描述组件、属性和布局，运行时负责校验与渲染，业务侧只操作结构而不直接触碰代码。json-render 并没有发明这种模式，它继承的正是这一整条技术脉络。</p>\n<p>json-render 的不同之处在于，它首次把“模型生成”作为一等公民纳入设计前提。传统 schema UI 假设配置由人编写，因此更强调完整性、可读性和编辑体验；而 json-render 假设结构由模型生成，因此更强调语法边界清晰、失败可恢复、部分结果可执行，以及与 Prompt 的自动对齐能力。从这个角度看，json-render 更像是“为 AI 重新设计的一代 schema UI 执行模型”。它真正解决的问题并不是“怎么用 JSON 渲染 UI”，而是当结构来源变成不可靠的模型时，工程边界应该在哪里。它给出的答案非常明确：AI 只负责生成结构化意图，工程师负责能力定义、执行与渲染，JSON 作为唯一中介和约束层。这使得 AI UI 不再是一次性 Demo，而是可以进入生产系统的工程能力。在当前阶段，这是少数真正站在工程立场思考 AI UI 的方案之一。</p>\n<h2 id=\"参考资料\">参考资料</h2>\n<ul>\n<li><a href=\"https://json-render.dev/\" rel=\"noopener nofollow\" target=\"_blank\">json-render.dev</a></li>\n<li><a href=\"https://github.com/vercel-labs/json-render\" rel=\"noopener nofollow\" target=\"_blank\">github.com/vercel-labs/json-render</a></li>\n<li><a href=\"https://vercel.com/blog\" rel=\"noopener nofollow\" target=\"_blank\">vercel.com/blog</a></li>\n<li><a href=\"https://www.cnblogs.com/guangzan/p/19350726\" target=\"_blank\">Zod：TypeScript 类型守卫与数据验证</a></li>\n</ul>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 15:23</span>&nbsp;\n<a href=\"https://www.cnblogs.com/guangzan\">guangzan</a>&nbsp;\n阅读(<span id=\"post_view_count\">111</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "SeaTunnel(2.3.12)的高级用法（四）：多个source、多个sink",
      "link": "https://www.cnblogs.com/kakarotto-chen/p/19487270",
      "published": "",
      "description": "<h2>\n            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/kakarotto-chen/p/19487270\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 15:08\">\n    <span>SeaTunnel(2.3.12)的高级用法（四）：多个source、多个sink</span>\n    \n\n</a>\n\n        </h2>\n        <div class=\"postbody\">\n            <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"前置知识seatunnel配置中有数据流data-flow转的概念\">前置知识：seatunnel配置中有数据流（Data Flow）转的概念</h2>\n<p>见：<a href=\"https://www.cnblogs.com/kakarotto-chen/p/19487384\" target=\"_blank\">https://www.cnblogs.com/kakarotto-chen/p/19487384</a></p>\n<h2 id=\"demo1两个source汇聚到一个sink\">demo1：两个source汇聚到一个sink</h2>\n<ul>\n<li>\n<p>关键配置：sink的：plugin_input = [\"source_data1\", \"source_data2\"]</p>\n</li>\n<li>\n<p>对应模型</p>\n</li>\n</ul>\n<pre><code>┌──────────┐\n│ Source A │──┐\n└──────────┘  │\n              ├──▶  Sink\n┌──────────┐  │\n│ Source B │──┘\n└──────────┘\n</code></pre>\n<ul>\n<li>执行语句</li>\n</ul>\n<pre><code># ds-st-demo10-2-mysql2pgsql.conf\nsh /data/tools/seatunnel/seatunnel-2.3.12/bin/seatunnel.sh --config /data/tools/seatunnel/myconf/ds-st-demo10-2-mysql2pgsql.conf -i -DJvmOption=\"-Xms2G -Xmx2G\" -m local\n</code></pre>\n<ul>\n<li>建表</li>\n</ul>\n<pre><code>-- ds-st-demo10-2-mysql2pgsql.conf\nCREATE TABLE \"public\".\"t_8_100w_imp_st_ds_demo10\" (\n  id BIGINT PRIMARY KEY,\n  user_name VARCHAR(2000),\n  sex VARCHAR(20),\n  decimal_f NUMERIC(32, 6),\n  phone_number VARCHAR(20),\n  age INT,\n  create_time TIMESTAMP,\n  description TEXT,\n  address VARCHAR(2000) DEFAULT '未知',\n  my_status INT\n);\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"id\" IS '主键';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"user_name\" IS '名字';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"sex\" IS '性别：男；女';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"decimal_f\" IS '大数字';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"phone_number\" IS '电话';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"age\" IS '字符串年龄转数字';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"create_time\" IS '新增时间';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"description\" IS '大文本';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"address\" IS '空地址转默认值：未知';\nCOMMENT ON COLUMN \"public\".\"t_8_100w_imp_st_ds_demo10\".\"my_status\" IS '状态';\n\n</code></pre>\n<ul>\n<li>conf配置</li>\n</ul>\n<pre><code>env {\n  # 任务名字：业务中可以弄表id\n  job.name = \"ds-st-demo10.conf\"\n  # 最大批线程数：并行度（线程数）\n  parallelism = 5\n  # 任务模式：BATCH:批处理模式；STREAMING:流处理模式\n  job.mode = \"BATCH\"\n}\n\nsource {\n  # 第一个数据集\n  jdbc {\n    # 给这个数据集起个名字\n    plugin_output = \"source_data1\"\n  \n    url = \"jdbc:mysql://ip:port/cs1\"\n    driver = \"com.mysql.cj.jdbc.Driver\"\n    user = \"root\"\n    password = \"***\"\n    # sql\n    query = \"select id,name as user_name,sex,decimal_f,phone_number,CAST(age AS SIGNED) as age,create_time,description,address from t_8_100w where id &lt; 10\"\n    \n    # 并行读取配置\n    # 分片的字段：支持：String、Number(int, bigint, decimal, ...)、Date\n    partition_column = \"id\"\n    # 表的分割大小（行数）：每个分片的数据行（默认8096行）。最后分片数=表的总行数 / split.size\n    split.size = 50000\n    # 分片数，匹配并行度parallelism（2.3.12已不推荐配置了，用split.size来代替）\n    # partition_num = 5\n    # 最大批处理数:查询的行提取大小(指定当前任务每次执行时读取数据条数,该值(默认1000)受运行内存影响,若该值较大或单条数据量较大，需适当调整运行内存大小。)\n    fetch_size = 10000\n    \n    # 连接参数\n    # 连接超时时间300ms\n    connection_check_timeout_sec = 300\n    # 其他jdbc的参数\n    properties = {\n      useUnicode = true\n      characterEncoding = \"utf8\"\n      # 时区，不同数据库参数不一样\n      serverTimezone = \"Asia/Shanghai\"\n      # 使用游标提高大结果集性能\n      useCursorFetch = \"true\"\n      # 每次获取行数\n      defaultFetchSize = \"10000\"\n    }\n  }\n  \n  # 第二个数据集\n  jdbc {\n    # 给这个数据集起个名字\n    plugin_output = \"source_data2\"\n  \n    url = \"jdbc:mysql://ip:port/cs1\"\n    driver = \"com.mysql.cj.jdbc.Driver\"\n    user = \"root\"\n    password = \"***\"\n    # \n    query = \"select id,name as user_name,sex,decimal_f,phone_number,CAST(age AS SIGNED) as age,create_time,description,address from t_8_100w where id &gt; 10 and id &lt; 20\"\n    \n    # 并行读取配置\n    # 分片的字段：支持：String、Number(int, bigint, decimal, ...)、Date\n    partition_column = \"id\"\n    # 表的分割大小（行数）：每个分片的数据行（默认8096行）。最后分片数=表的总行数 / split.size\n    split.size = 50000\n    # 分片数，匹配并行度parallelism（2.3.12已不推荐配置了，用split.size来代替）\n    # partition_num = 5\n    # 最大批处理数:查询的行提取大小(指定当前任务每次执行时读取数据条数,该值(默认1000)受运行内存影响,若该值较大或单条数据量较大，需适当调整运行内存大小。)\n    fetch_size = 10000\n    \n    # 连接参数\n    # 连接超时时间300ms\n    connection_check_timeout_sec = 300\n    # 其他jdbc的参数\n    properties = {\n      useUnicode = true\n      characterEncoding = \"utf8\"\n      # 时区，不同数据库参数不一样\n      serverTimezone = \"Asia/Shanghai\"\n      # 使用游标提高大结果集性能\n      useCursorFetch = \"true\"\n      # 每次获取行数\n      defaultFetchSize = \"10000\"\n    }\n  }\n}\n\n# 清洗转换（简单的清洗转换，直接在source的query的sql中处理了就行）\ntransform {\n  # 1. 字段映射：sql中做了，实际生成中不在这里处理。直接在source的query的sql中处理了就行\n  # 还可以用：FieldMapper 插件，来映射字段\n  \n  # 转换age为数字类型（pgsql必须转）\n  \n  # 2. 手机号脱敏：13812341234 -&gt; 138****1234\n  \n  # 3. 年龄转换：字符串转整数（实际生产中，不用转换，也没有内置的转换插件，可以直接保存成功）\n\n  # 4. 性别转换：1-&gt;男，2-&gt;女\n  \n  # 5. 数据过滤：只保留 age &gt; 25 的记录。\n  \n  # 6. 地址默认值：空地址设为'未知'\n}\n\nsink {\n  jdbc {\n    # 接收的最终数据集（汇聚到一个结果中）\n    plugin_input = [\"source_data1\", \"source_data2\"]\n    \n    url = \"jdbc:postgresql://ip:5432/source_db\"\n    driver = \"org.postgresql.Driver\"\n    user = \"postgres\"\n    password = \"123456\"\n    # \n    # query = \"\"\n    \n    # 自动生成sql的配置，和query参数互斥\n    # 生成自动插入sql。如果目标库没有表，也会自动建表\n    generate_sink_sql = true\n    # database必须要，因为generate_sink_sql=true。\n    database = source_db\n    # 自动生成sql时，table必须要。\n    table = \"public.t_8_100w_imp_st_ds_demo10\"\n    # 生成类似：INSERT INTO …… ON CONFLICT (\"主键\") DO UPDATE SET …… 的sql\n    # enable_upsert = true\n    # 判断值唯一的健：此选项用于支持在自动生成 SQL 时进行 insert，delete 和 update 操作。\n    # primary_keys = [\"id\"]\n\n    # 表结构处理策略：表不存在时报错（任务失败），一般用：CREATE_SCHEMA_WHEN_NOT_EXIST（表不存在时创建表；表存在时跳过操作（保留数据））\n    schema_save_mode = \"ERROR_WHEN_SCHEMA_NOT_EXIST\"\n    # 插入数据的处理策略\n    # APPEND_DATA：保留表结构和数据，追加新数据（不删除现有数据）(一般用这个)\n    # DROP_DATA：保留表结构，删除表中所有数据（清空表）——实现清空重灌\n    # CUSTOM_PROCESSING :用户定义处理。需要配合：custom_sql使用\n    data_save_mode = \"DROP_DATA\"\n    # 当 data_save_mode 选择 CUSTOM_PROCESSING 时，您应该填写 CUSTOM_SQL 参数。此参数通常填入可执行的 SQL。SQL 将在同步任务之前执行。\n    #可以实现：同步删除（执行前置update、truncate的sql等）\n    #这个sql未执行，不知道为啥。\n    #这个sql已经执行。原因：因为generate_sink_sql=true的原因。才会执行custom_sql。（只有自动生成sql的时候，这个才会执行）\n    custom_sql = \"\"\"update \"source_db\".\"public\".\"t_8_100w_imp_st_ds_demo10\" set \"my_status\" = 23\"\"\"\n    \n    # 批量写入条数\n    batch_size = 10000\n    # 批次提交间隔\n    batch_interval_ms = 500\n    # 重试次数\n    max_retries = 3\n    \n    # 连接参数\n    # 连接超时时间300ms\n    connection_check_timeout_sec = 300\n    # 其他jdbc的参数\n    properties = {\n      # PostgreSQL专用参数\n      # PostgreSQL的批量优化（注意大小写）\n      reWriteBatchedInserts = \"true\"  \n      # 如果需要时区设置\n      options = \"-c timezone=Asia/Shanghai\"\n    }\n  }\n}\n</code></pre>\n<ul>\n<li>结果(汇聚了19条数据)</li>\n</ul>\n<pre><code>2026-01-15 14:28:15,952 INFO  [s.c.s.s.c.ClientExecuteCommand] [main] - \n***********************************************\n           Job Statistic Information\n***********************************************\nStart Time                : 2026-01-15 14:28:11\nEnd Time                  : 2026-01-15 14:28:15\nTotal Time(s)             :                   4\nTotal Read Count          :                  19\nTotal Write Count         :                  19\nTotal Failed Count        :                   0\n***********************************************\n</code></pre>\n<h2 id=\"demo2一个source分发到两个sink\">demo2：一个source分发到两个sink</h2>\n<p>……………………未完待续</p>\n\n</div>\n<div class=\"clear\"></div>\n\n        </div>\n        <p class=\"postfoot\">\n            posted on \n<span id=\"post-date\">2026-01-15 15:08</span>&nbsp;\n<a href=\"https://www.cnblogs.com/kakarotto-chen\">C_C_菜园</a>&nbsp;\n阅读(<span id=\"post_view_count\">20</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n        </p>"
    },
    {
      "title": "如何通过 C# 将 PPT 文档转换为 PDF 格式",
      "link": "https://www.cnblogs.com/jazz-z/p/19486170",
      "published": "",
      "description": "<div class=\"postcontent\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在日常开发和办公场景中，将 PowerPoint（PPT/PPTX） 转换为 PDF 格式是高频需求。PDF 格式具有跨平台兼容性强、格式固定不易篡改、便于分发归档等优势。本文将介绍如何使用一款 .NET PowerPoint 组件通过 C# 实现 PPT 转 PDF，并提供完整代码示例。</p>\n<h2 id=\"1-安装-net-库\">1. 安装 .NET 库</h2>\n<p>Spire.Presentation 是一款专门用于处理 PowerPoint 文档的 .NET 组件，无需依赖 Microsoft Office 或 PowerPoint 客户端即可完成 PPT 文档的读取、编辑和格式转换。推荐通过 NuGet 包管理器安装，步骤如下：</p>\n<ol>\n<li>打开 Visual Studio，创建任意 C# 项目（如Console App）；</li>\n<li>右键项目→“管理NuGet程序包”；</li>\n<li>搜索“Spire.Presentation”，选择对应版本安装；</li>\n<li>也可通过NuGet命令行安装：</li>\n</ol>\n<pre><code class=\"language-bash\">Install-Package Spire.Presentation\n</code></pre>\n<h2 id=\"2-基础示例单个-powerpoint-文件转-pdf\">2. 基础示例：单个 PowerPoint 文件转 PDF</h2>\n<p>这是最常用的场景，支持 PPT/PPTX 格式输入，直接通过 <code>SaveToFile</code> 方法输出为 PDF 文件：</p>\n<pre><code class=\"language-csharp\">using System;\nusing Spire.Presentation;\n\nnamespace PptToPdfDemo\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            try\n            {\n                // 1. 定义文件路径\n                string pptFilePath = @\"D:\\Demo\\source.pptx\"; // 输入PPT路径\n                string pdfFilePath = @\"D:\\Demo\\output.pdf\";   // 输出PDF路径\n\n                // 2. 加载PPT文档\n                Presentation presentation = new Presentation();\n                presentation.LoadFromFile(pptFilePath);\n\n                // 3. 转换为PDF并保存\n                // 可选参数：PDF导出选项（如压缩、权限等），此处使用默认配置\n                presentation.SaveToFile(pdfFilePath, FileFormat.PDF);\n\n                // 4. 释放资源（关键，避免内存泄漏）\n                presentation.Dispose();\n\n                Console.WriteLine(\"PPT转PDF成功！\");\n            }\n            catch (Exception ex)\n            {\n                // 异常处理：捕获文件不存在、格式不支持、权限不足等问题\n                Console.WriteLine($\"转换失败：{ex.Message}\");\n            }\n        }\n    }\n}\n</code></pre>\n<h3 id=\"3-批量转换转换多个-powerpoint-文件为-pdf\">3. 批量转换：转换多个 PowerPoint 文件为 PDF</h3>\n<p>通过遍历文件夹实现批量转换，适合处理大量 PPT 文件：</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.IO;\nusing Spire.Presentation;\n\nnamespace BatchPptToPdf\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            // 源PPT文件夹路径\n            string pptFolderPath = @\"D:\\Demo\\PptFiles\";\n            // 输出PDF文件夹路径\n            string pdfFolderPath = @\"D:\\Demo\\PdfFiles\";\n\n            // 确保输出文件夹存在\n            if (!Directory.Exists(pdfFolderPath))\n            {\n                Directory.CreateDirectory(pdfFolderPath);\n            }\n\n            // 遍历文件夹中的PPT/PPTX文件\n            string[] pptFiles = Directory.GetFiles(pptFolderPath, \"*\", SearchOption.TopDirectoryOnly)\n                .Where(file =&gt; file.EndsWith(\".ppt\", StringComparison.OrdinalIgnoreCase) \n                             || file.EndsWith(\".pptx\", StringComparison.OrdinalIgnoreCase))\n                .ToArray();\n\n            foreach (string pptFile in pptFiles)\n            {\n                try\n                {\n                    // 获取文件名（不含扩展名），用于生成PDF文件名\n                    string fileName = Path.GetFileNameWithoutExtension(pptFile);\n                    string pdfFile = Path.Combine(pdfFolderPath, $\"{fileName}.pdf\");\n\n                    // 加载并转换\n                    using (Presentation presentation = new Presentation())\n                    {\n                        presentation.LoadFromFile(pptFile);\n                        presentation.SaveToFile(pdfFile, FileFormat.PDF);\n                    }\n\n                    Console.WriteLine($\"已转换：{pptFile} → {pdfFile}\");\n                }\n                catch (Exception ex)\n                {\n                    Console.WriteLine($\"转换失败 {pptFile}：{ex.Message}\");\n                }\n            }\n\n            Console.WriteLine(\"批量转换完成！\");\n        }\n    }\n}\n</code></pre>\n<h3 id=\"4-进阶示例将-powerpoint-转换为加密的-pdf\">4. 进阶示例：将 PowerPoint 转换为加密的 PDF</h3>\n<p>还可以在转换时直接加密保护 PDF 文件，并为 PDF 设置权限：</p>\n<pre><code class=\"language-csharp\">using Spire.Presentation;\nusing Spire.Presentation.External.Pdf;\n\nnamespace ConvertToEncryptedPdf\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            // 定义明确的文件路径（建议替换为你的实际路径）\n            string inputPptPath = @\"C:\\Users\\Administrator\\Desktop\\Input.pptx\";\n            string outputPdfPath = @\"C:\\Users\\Administrator\\Desktop\\ToEncryptedPdf.pdf\";\n            \n            // 使用using语句自动释放Presentation资源（优于手动Dispose）\n            try\n            {\n                using (Presentation presentation = new Presentation())\n                {\n                    // 加载PPT文件\n                    presentation.LoadFromFile(inputPptPath);\n\n                    // 获取PDF保存选项\n                    SaveToPdfOption option = presentation.SaveToPdfOption;\n                    \n                    // 设置PDF密码和权限\n                    // 参数说明：\n                    // 1. 用户密码（打开PDF需要输入的密码）：abc-123\n                    // 2. 所有者密码（用于修改PDF权限的密码）：owner-456（可自定义）\n                    // 3. PDF权限：允许打印 + 允许填写表单\n                    // 4. 加密级别：默认128位（高安全性）\n                  option.PdfSecurity.Encrypt(\"abc-123\", \"owner-456\", \n                        PdfPermissionsFlags.Print | PdfPermissionsFlags.FillFields, \n                        PdfEncryptionKeySize.Key128Bit);\n\n                    // 保存为加密PDF\n                    presentation.SaveToFile(outputPdfPath, FileFormat.PDF, pdfOptions);\n\n                    Console.WriteLine(\"PPT已成功转换为加密PDF！\");\n                    Console.WriteLine($\"输出路径：{outputPdfPath}\");\n                }\n            }\n            catch (Exception ex)\n            {\n                // 捕获所有可能的异常并提示\n                Console.WriteLine($\"转换失败：{ex.Message}\");\n            }\n\n            // 暂停控制台，便于查看结果\n            Console.ReadLine();\n        }\n    }\n}\n</code></pre>\n<h2 id=\"5-关键注意事项\">5. 关键注意事项</h2>\n<ol>\n<li>\n<p><strong>格式兼容性</strong>：</p>\n<ul>\n<li>支持输入格式：PPT、PPTX、PPS、PPSX 等；</li>\n<li>复杂PPT元素（如3D图表、自定义动画、嵌入式视频）转换后可能丢失或显示异常（。</li>\n</ul>\n</li>\n<li>\n<p><strong>资源释放</strong>：</p>\n<ul>\n<li>必须通过 <code>Dispose()</code> 方法释放 <code>Presentation</code> 对象，或使用 <code>using</code> 语句（推荐），否则易导致内存泄漏，尤其批量转换时需注意。</li>\n</ul>\n</li>\n<li>\n<p><strong>权限问题</strong>：</p>\n<ul>\n<li>确保程序对输入/输出路径有读写权限，否则会抛出 <code>UnauthorizedAccessException</code>。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"6-替代方案参考\">6. 替代方案参考</h2>\n<ol>\n<li><strong>LibreOffice SDK</strong>：免费开源，需部署 LibreOffice 服务，API 较复杂；</li>\n<li><strong>OpenXML SDK + iTextSharp</strong>：仅支持 PPTX（OpenXML 格式），需自行处理布局转换，开发成本高；</li>\n<li><strong>GroupDocs.Conversion</strong>：有免费额度，云原生支持，但依赖网络。</li>\n</ol>\n<hr />\n<p>本文提供了可靠的 C# PowerPoint 转 PDF 解决方案，特别适合在服务器环境或无需安装 Microsoft Office 的场景中使用。其优点包括部署简单、API 设计清晰、支持多种输出选项等。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"itemdesc\">\n                发表于 \n<span id=\"post-date\">2026-01-15 10:37</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jazz-z\">LAYONTHEGROUND</a>&nbsp;\n阅读(<span id=\"post_view_count\">64</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n            </div>"
    },
    {
      "title": "不服跑个分？.NET 10 大整数计算对阵 Java，结果令人意外",
      "link": "https://www.cnblogs.com/sdcb/p/19484525/20261113-big-integer-dotnet-10-vs-java",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/sdcb/p/19484525/20261113-big-integer-dotnet-10-vs-java\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 08:50\">\n    <span>不服跑个分？.NET 10 大整数计算对阵 Java，结果令人意外</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"引言从经验值到无限大\">引言：从“经验值”到无限大</h2>\n<p>我对数值计算的执念，来自初中时代烟雾缭绕的网吧。那时玩《伝奇》，最让我着迷的不是打怪爆装备，而是角色面板里那条长长的<strong>经验值</strong>。看着数字不断跳动、累积，最终“叮”一声升级，那种简单的数值驱动整个世界运转的感觉，实在太奇妙了。</p>\n<p>当时自学编程，从 G-BASIC 里只有 16 位的 <code>INTEGER</code>，到第一次发现 QBASIC <code>LONG</code> 能存下“20亿”时的兴奋，再到如今成为一名 .NET 程序员，手握理论上“无限大”的 <code>System.Numerics.BigInteger</code>。我对大数的迷恋从未改变，只是疑惑随之而来：</p>\n<blockquote>\n<p><strong>作为 .NET 开发者，我手里的 <code>BigInteger</code> 到底够不够快？特别是和隔壁 Java 的 <code>BigInteger</code> 相比，究竟谁更胜一筹？</strong></p>\n</blockquote>\n<p>尤其是涉及高精度计算、密码学等关键领域时，这不仅是好奇，更是关乎性能的严肃拷问。今天，我就通过一次详尽的对比测试（含 .NET 10、Java 21以及Java 8），来为大家揭晓答案。结果可能出乎意料，请务必看到最后，文末还有一个高性能的“彩蛋”。</p>\n<hr />\n<h2 id=\"实验目标与范围\">实验目标与范围</h2>\n<p>我们对比这三套实现：</p>\n<ul>\n<li><strong>.NET</strong>：<code>System.Numerics.BigInteger</code>（基于 .NET 10）</li>\n<li><strong>Java</strong>：<code>java.math.BigInteger</code>\n<ul>\n<li><strong>Java 21（最新LTS）</strong>：代表未来趋势</li>\n<li><strong>Java 8（国内存量最大）</strong>：代表庞大的现状</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"测试操作3类覆盖常见大数热点\">测试操作（3类，覆盖常见大数热点）</h3>\n<ol>\n<li><strong>ADD_MOD</strong>：<code>(a + b) mod m</code>（加法 + 取模）</li>\n<li><strong>MUL_MOD</strong>：<code>(a * b) mod m</code>（乘法 + 取模）</li>\n<li><strong>MODPOW</strong>：<code>a^e mod m</code>（模幂，密码学等场景的性能热点）</li>\n</ol>\n<h3 id=\"位宽3档\">位宽（3档）</h3>\n<ul>\n<li>256 / 1024 / 4096 bits</li>\n</ul>\n<h3 id=\"计时口径尽量抑制噪声\">计时口径（尽量抑制噪声）</h3>\n<ul>\n<li><strong>热身</strong>：预热 5 次，让 JIT（即时编译器）充分发挥。</li>\n<li><strong>测量</strong>：正式跑 11 次，取中位数，避免单次抖动干扰。</li>\n<li><strong>指标</strong>：<code>nsPerOp</code>（每次操作耗时多少纳秒），这个值<strong>越小越好</strong>。</li>\n<li><strong>防作弊</strong>：用 <code>XOR</code> 聚合每次计算结果，防止聪明的编译器把整个循环优化掉。</li>\n</ul>\n<hr />\n<h2 id=\"测试环境\">测试环境</h2>\n<blockquote>\n<p>说明：以下是在同一 Linux 容器内完成。容器有资源限制，因此“看到的 CPU 核数/内存”与宿主机不完全一致。</p>\n</blockquote>\n<ul>\n<li><strong>OS</strong>：Ubuntu 24.04.3 LTS</li>\n<li><strong>CPU（宿主机型号）</strong>：AMD EPYC 7763 64-Core Processor<br />\n<strong>容器可用核心数</strong>：2 核（受容器限制）</li>\n<li><strong>内存</strong>：2GB（容器限制）</li>\n<li><strong>.NET</strong>\n<ul>\n<li>SDK：10.0.101</li>\n<li>Runtime：10.0.1</li>\n<li>环境变量：<code>DOTNET_GCServer=1</code></li>\n</ul>\n</li>\n<li><strong>Java</strong>\n<ul>\n<li>Java 21：OpenJDK 21.0.9（LTS）</li>\n<li>Java 8：Temurin(OpenJDK) 1.8.0_472-b08（广泛使用的 8u 系列）</li>\n<li>JVM 参数（两版本一致）：\n<ul>\n<li><code>-Xms1g -Xmx1g</code>（把 Java 堆固定为 1GB，避免堆动态扩张干扰）</li>\n<li><code>-XX:+UseG1GC</code></li>\n<li><code>-XX:+AlwaysPreTouch</code>（尽量减少运行中页分配扰动）</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>一致性约束</strong>\n<ul>\n<li>Java 8/Java 21 使用<strong>同一份源码</strong>（仅使用 Java 8 语法），并且 <strong>用 Java 8 的 <code>javac</code> 编译</strong>（classfile=52.0）后分别在 Java 8 / Java 21 上运行，从而尽量把差异归因到运行时/JIT/库实现，而不是编译器生成差异。</li>\n</ul>\n</li>\n</ul>\n<hr />\n<h2 id=\"赛前插曲net-biginteger-真的不公平吗\">赛前插曲：.NET BigInteger 真的“不公平”吗？</h2>\n<p>有人可能想问：</p>\n<blockquote>\n<p>“.NET BigInteger 每次运算都会创建新的对象（不可变），不公平。”</p>\n</blockquote>\n<p>这句话在“大数场景”里基本成立，但<strong>Java BigInteger 同样是不可变类型</strong>：<code>add/multiply/mod/xor/modPow</code> 都会返回新值，业务代码层面你并没有公开的 in-place API 可以复用内部缓冲。</p>\n<p>因此，在本文的“主对比”（.NET vs Java）里，双方都在不可变范式下运行，这点是公平的。</p>\n<hr />\n<h2 id=\"实验一net-biginteger-vs-java-21-biginteger\">实验一：.NET BigInteger vs Java 21 BigInteger</h2>\n<h3 id=\"31-完整源代码\">3.1 完整源代码</h3>\n<h4 id=\"java-源码bigintbenchjava\">Java 源码：<code>BigIntBench.java</code></h4>\n<blockquote>\n<p>兼容 Java 8 语法；同一份代码在 Java 21 下运行。</p>\n</blockquote>\n<pre><code class=\"language-java\">import java.math.BigInteger;\nimport java.util.*;\n\npublic class BigIntBench {\n    // SplitMix64 RNG (deterministic, fast)\n    static final class SplitMix64 {\n        private long x;\n        SplitMix64(long seed) { this.x = seed; }\n        long nextLong() {\n            long z = (x += 0x9E3779B97F4A7C15L);\n            z = (z ^ (z &gt;&gt;&gt; 30)) * 0xBF58476D1CE4E5B9L;\n            z = (z ^ (z &gt;&gt;&gt; 27)) * 0x94D049BB133111EBL;\n            return z ^ (z &gt;&gt;&gt; 31);\n        }\n        void nextBytes(byte[] dst) {\n            int i = 0;\n            while (i &lt; dst.length) {\n                long v = nextLong();\n                for (int k = 0; k &lt; 8 &amp;&amp; i &lt; dst.length; k++) {\n                    dst[i++] = (byte)(v &gt;&gt;&gt; (56 - 8*k)); // big-endian stream\n                }\n            }\n        }\n    }\n\n    static BigInteger[] genBigInts(int bitSize, int count, long seed) {\n        SplitMix64 rng = new SplitMix64(seed);\n        int byteLen = (bitSize + 7) / 8;\n        BigInteger[] arr = new BigInteger[count];\n        byte[] buf = new byte[byteLen];\n        int topBit = (bitSize - 1) % 8;\n        int keepBits = topBit + 1;\n        int firstMask = (keepBits == 8) ? 0xFF : ((1 &lt;&lt; keepBits) - 1);\n        byte topMask = (byte)(1 &lt;&lt; topBit);\n        for (int i = 0; i &lt; count; i++) {\n            rng.nextBytes(buf);\n            // Ensure exact bit length:\n            // - mask away unused top bits when bitSize is not byte-aligned\n            // - set the top bit so the number has the requested bit length\n            buf[0] &amp;= (byte)firstMask;\n            buf[0] |= topMask;\n            arr[i] = new BigInteger(1, buf);\n        }\n        return arr;\n    }\n\n    static BigInteger genModulus(int bitSize, long seed) {\n        SplitMix64 rng = new SplitMix64(seed);\n        int byteLen = (bitSize + 7) / 8;\n        byte[] buf = new byte[byteLen];\n        rng.nextBytes(buf);\n        int topBit = (bitSize - 1) % 8;\n        int keepBits = topBit + 1;\n        int firstMask = (keepBits == 8) ? 0xFF : ((1 &lt;&lt; keepBits) - 1);\n        buf[0] &amp;= (byte)firstMask;\n        buf[0] |= (byte)(1 &lt;&lt; topBit);\n        buf[buf.length - 1] |= 1; // odd\n        return new BigInteger(1, buf);\n    }\n\n    enum Op { ADD_MOD, MUL_MOD, MODPOW }\n\n    static final class Result {\n        final String lang;\n        final int bits;\n        final Op op;\n        final long ops;\n        final double nsPerOp;\n        final long checksum;\n        Result(String lang, int bits, Op op, long ops, double nsPerOp, long checksum) {\n            this.lang = lang; this.bits = bits; this.op = op; this.ops = ops; this.nsPerOp = nsPerOp; this.checksum = checksum;\n        }\n        String toJson() {\n            return String.format(Locale.ROOT,\n                    \"{\\\"lang\\\":\\\"%s\\\",\\\"bits\\\":%d,\\\"op\\\":\\\"%s\\\",\\\"ops\\\":%d,\\\"nsPerOp\\\":%.3f,\\\"checksum\\\":%d}\",\n                    lang, bits, op.name(), ops, nsPerOp, checksum);\n        }\n    }\n\n    static long runOnce(Op op, BigInteger[] a, BigInteger[] b, BigInteger[] e, BigInteger mod, int outer) {\n        BigInteger acc = BigInteger.ZERO;\n        int n = a.length;\n        switch (op) {\n            case ADD_MOD:\n                for (int o = 0; o &lt; outer; o++) {\n                    for (int i = 0; i &lt; n; i++) {\n                        BigInteger r = a[i].add(b[i]).mod(mod);\n                        acc = acc.xor(r);\n                    }\n                }\n                break;\n\n            case MUL_MOD:\n                for (int o = 0; o &lt; outer; o++) {\n                    for (int i = 0; i &lt; n; i++) {\n                        BigInteger r = a[i].multiply(b[i]).mod(mod);\n                        acc = acc.xor(r);\n                    }\n                }\n                break;\n\n            case MODPOW:\n                // here we use a.length as n; e can be same length\n                for (int o = 0; o &lt; outer; o++) {\n                    for (int i = 0; i &lt; n; i++) {\n                        BigInteger r = a[i].modPow(e[i], mod);\n                        acc = acc.xor(r);\n                    }\n                }\n                break;\n\n            default:\n                throw new IllegalArgumentException(\"Unknown op: \" + op);\n        }\n        return acc.longValue();\n    }\n\n    static Result bench(String lang, int bits, Op op, BigInteger[] a, BigInteger[] b, BigInteger[] e, BigInteger mod, long targetOps, int warmups, int measures) {\n        int n = a.length;\n        int outer = (int)Math.max(1, targetOps / n);\n        long actualOps = (long)n * outer;\n\n        // warmup\n        long ck = 0;\n        for (int i = 0; i &lt; warmups; i++) {\n            ck ^= runOnce(op, a, b, e, mod, outer);\n        }\n\n        long[] times = new long[measures];\n        for (int i = 0; i &lt; measures; i++) {\n            long t0 = System.nanoTime();\n            long c = runOnce(op, a, b, e, mod, outer);\n            long t1 = System.nanoTime();\n            ck ^= c;\n            times[i] = (t1 - t0);\n        }\n        Arrays.sort(times);\n        long median = times[times.length / 2];\n        double nsPerOp = (double)median / (double)actualOps;\n        return new Result(lang, bits, op, actualOps, nsPerOp, ck);\n    }\n\n    static void printHuman(List&lt;Result&gt; results) {\n        System.out.println(\"Java BigInteger benchmark\");\n        System.out.println(\"java.version=\" + System.getProperty(\"java.version\"));\n        System.out.println(\"java.vm.name=\" + System.getProperty(\"java.vm.name\"));\n        System.out.println();\n        System.out.printf(Locale.ROOT, \"%-6s %-9s %-12s %-12s\\n\", \"Bits\", \"Op\", \"ns/op(med)\", \"ops/run\");\n        for (Result r : results) {\n            System.out.printf(Locale.ROOT, \"%-6d %-9s %-12.3f %-12d\\n\", r.bits, r.op.name(), r.nsPerOp, r.ops);\n        }\n        System.out.println();\n        System.out.println(\"checksum=\" + results.stream().mapToLong(x -&gt; x.checksum).reduce(0L, (x,y)-&gt;x^y));\n    }\n\n    public static void main(String[] args) {\n        boolean json = false;\n        for (String a : args) if (a.equals(\"--json\")) json = true;\n\n        int warmups = 5;\n        int measures = 11;\n\n        int[] bitSizes = new int[]{256, 1024, 4096};\n        List&lt;Result&gt; results = new ArrayList&lt;&gt;();\n\n        for (int bits : bitSizes) {\n            BigInteger mod = genModulus(bits, 0xA1B2C3D4E5F60708L ^ bits);\n\n            // add/mul datasets\n            int nAddMul = 1024;\n            BigInteger[] a = genBigInts(bits, nAddMul, 0x1111222233334444L ^ bits);\n            BigInteger[] b = genBigInts(bits, nAddMul, 0x9999AAAABBBBCCCCL ^ bits);\n\n            // modpow datasets (smaller)\n            int nPow = 256;\n            BigInteger[] ap = genBigInts(bits, nPow, 0x13579BDF2468ACE0L ^ bits);\n            BigInteger[] ep = genBigInts(Math.min(bits, 512), nPow, 0x0FEDCBA987654321L ^ bits);\n\n            long addOps;\n            long mulOps;\n            long powOps;\n            if (bits == 256) {\n                addOps = 2_000_000L;\n                mulOps = 500_000L;\n                powOps = 8_000L;\n            } else if (bits == 1024) {\n                addOps = 1_000_000L;\n                mulOps = 120_000L;\n                powOps = 1_500L;\n            } else {\n                addOps = 200_000L;\n                mulOps = 20_000L;\n                powOps = 250L;\n            }\n\n            results.add(bench(\"java\", bits, Op.ADD_MOD, a, b, null, mod, addOps, warmups, measures));\n            results.add(bench(\"java\", bits, Op.MUL_MOD, a, b, null, mod, mulOps, warmups, measures));\n            results.add(bench(\"java\", bits, Op.MODPOW, ap, null, ep, mod, powOps, warmups, measures));\n        }\n\n        if (json) {\n            for (Result r : results) System.out.println(r.toJson());\n        } else {\n            printHuman(results);\n        }\n    }\n}\n</code></pre>\n<h4 id=\"net-源码programcsbiginteger-部分\">.NET 源码：<code>Program.cs</code>（BigInteger 部分）</h4>\n<blockquote>\n<p>这是“主对比”用的 .NET 基准程序。</p>\n</blockquote>\n<pre><code class=\"language-csharp\">using System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Globalization;\nusing System.Linq;\nusing System.Numerics;\n\nsealed class SplitMix64\n{\n    private ulong _x;\n    public SplitMix64(ulong seed) =&gt; _x = seed;\n\n    public ulong NextUInt64()\n    {\n        ulong z = (_x += 0x9E3779B97F4A7C15UL);\n        z = (z ^ (z &gt;&gt; 30)) * 0xBF58476D1CE4E5B9UL;\n        z = (z ^ (z &gt;&gt; 27)) * 0x94D049BB133111EBUL;\n        return z ^ (z &gt;&gt; 31);\n    }\n\n    public void NextBytes(byte[] dst)\n    {\n        int i = 0;\n        while (i &lt; dst.Length)\n        {\n            ulong v = NextUInt64();\n            for (int k = 0; k &lt; 8 &amp;&amp; i &lt; dst.Length; k++)\n            {\n                dst[i++] = (byte)(v &gt;&gt; (56 - 8 * k)); // big-endian stream\n            }\n        }\n    }\n}\n\nenum Op { ADD_MOD, MUL_MOD, MODPOW }\n\nrecord Result(string Lang, int Bits, Op Op, long Ops, double NsPerOp, long Checksum)\n{\n    public string ToJson() =&gt; string.Create(CultureInfo.InvariantCulture,\n        $\"{{\\\"lang\\\":\\\"{Lang}\\\",\\\"bits\\\":{Bits},\\\"op\\\":\\\"{Op}\\\",\\\"ops\\\":{Ops},\\\"nsPerOp\\\":{NsPerOp:F3},\\\"checksum\\\":{Checksum}}}\");\n}\n\nstatic class BigIntBench\n{\n    static BigInteger[] GenBigInts(int bitSize, int count, ulong seed)\n    {\n        var rng = new SplitMix64(seed);\n        int byteLen = (bitSize + 7) / 8;\n        var arr = new BigInteger[count];\n        var buf = new byte[byteLen];\n        int topBit = (bitSize - 1) % 8;\n        byte topMask = (byte)(1 &lt;&lt; topBit);\n\n        for (int i = 0; i &lt; count; i++)\n        {\n            rng.NextBytes(buf);\n            buf[0] |= topMask;\n            // unsigned + big-endian prevents negative\n            arr[i] = new BigInteger(buf, isUnsigned: true, isBigEndian: true);\n        }\n        return arr;\n    }\n\n    static BigInteger GenModulus(int bitSize, ulong seed)\n    {\n        var rng = new SplitMix64(seed);\n        int byteLen = (bitSize + 7) / 8;\n        var buf = new byte[byteLen];\n        rng.NextBytes(buf);\n        int topBit = (bitSize - 1) % 8;\n        buf[0] |= (byte)(1 &lt;&lt; topBit);\n        buf[^1] |= 1; // odd\n        return new BigInteger(buf, isUnsigned: true, isBigEndian: true);\n    }\n\n    static long RunOnce(Op op, BigInteger[] a, BigInteger[] b, BigInteger[] e, BigInteger mod, int outer)\n    {\n        BigInteger acc = BigInteger.Zero;\n        int n = a.Length;\n\n        switch (op)\n        {\n            case Op.ADD_MOD:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                    {\n                        var r = (a[i] + b[i]) % mod;\n                        acc ^= r;\n                    }\n                break;\n\n            case Op.MUL_MOD:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                    {\n                        var r = (a[i] * b[i]) % mod;\n                        acc ^= r;\n                    }\n                break;\n\n            case Op.MODPOW:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                    {\n                        var r = BigInteger.ModPow(a[i], e[i], mod);\n                        acc ^= r;\n                    }\n                break;\n        }\n\n        return (long)(acc &amp; long.MaxValue); // stable checksum\n    }\n\n    static Result Bench(string lang, int bits, Op op, BigInteger[] a, BigInteger[] b, BigInteger[] e, BigInteger mod, long targetOps, int warmups, int measures)\n    {\n        int n = a.Length;\n        int outer = (int)Math.Max(1, targetOps / n);\n        long actualOps = (long)n * outer;\n\n        long ck = 0;\n        for (int i = 0; i &lt; warmups; i++)\n            ck ^= RunOnce(op, a, b, e, mod, outer);\n\n        long[] timesNs = new long[measures];\n        for (int i = 0; i &lt; measures; i++)\n        {\n            var sw = Stopwatch.StartNew();\n            long c = RunOnce(op, a, b, e, mod, outer);\n            sw.Stop();\n            ck ^= c;\n            // Stopwatch ticks to ns\n            timesNs[i] = (long)(sw.ElapsedTicks * (1_000_000_000.0 / Stopwatch.Frequency));\n        }\n\n        Array.Sort(timesNs);\n        long median = timesNs[timesNs.Length / 2];\n        double nsPerOp = (double)median / actualOps;\n        return new Result(lang, bits, op, actualOps, nsPerOp, ck);\n    }\n\n    static void PrintHuman(List&lt;Result&gt; results)\n    {\n        Console.WriteLine(\"C# BigInteger benchmark\");\n        Console.WriteLine($\"dotnet.version={Environment.Version}\");\n        Console.WriteLine($\"os={System.Runtime.InteropServices.RuntimeInformation.OSDescription}\");\n        Console.WriteLine($\"arch={System.Runtime.InteropServices.RuntimeInformation.OSArchitecture}\");\n        Console.WriteLine();\n        Console.WriteLine($\"{ \"Bits\",-6} {\"Op\",-9} {\"ns/op(med)\",-12} {\"ops/run\",-12}\");\n        foreach (var r in results)\n            Console.WriteLine(string.Create(CultureInfo.InvariantCulture, $\"{r.Bits,-6} {r.Op,-9} {r.NsPerOp,-12:F3} {r.Ops,-12}\"));\n        Console.WriteLine();\n        long checksum = 0;\n        foreach (var r in results) checksum ^= r.Checksum;\n        Console.WriteLine($\"checksum={checksum}\");\n    }\n\n    public static int Main(string[] args)\n    {\n        bool json = args.Any(a =&gt; a == \"--json\");\n\n        int warmups = 5;\n        int measures = 11;\n        int[] bitSizes = [256, 1024, 4096];\n\n        var results = new List&lt;Result&gt;();\n\n        foreach (int bits in bitSizes)\n        {\n            var mod = GenModulus(bits, 0xA1B2C3D4E5F60708UL ^ (uint)bits);\n\n            int nAddMul = 1024;\n            var a = GenBigInts(bits, nAddMul, 0x1111222233334444UL ^ (uint)bits);\n            var b = GenBigInts(bits, nAddMul, 0x9999AAAABBBBCCCCUL ^ (uint)bits);\n\n            int nPow = 256;\n            var ap = GenBigInts(bits, nPow, 0x13579BDF2468ACE0UL ^ (uint)bits);\n            var ep = GenBigInts(Math.Min(bits, 512), nPow, 0x0FEDCBA987654321UL ^ (uint)bits);\n\n            long addOps = bits switch { 256 =&gt; 2_000_000L, 1024 =&gt; 1_000_000L, _ =&gt; 200_000L };\n            long mulOps = bits switch { 256 =&gt; 500_000L, 1024 =&gt; 120_000L, _ =&gt; 20_000L };\n            long powOps = bits switch { 256 =&gt; 8_000L, 1024 =&gt; 1_500L, _ =&gt; 250L };\n\n            results.Add(Bench(\"csharp\", bits, Op.ADD_MOD, a, b, null!, mod, addOps, warmups, measures));\n            results.Add(Bench(\"csharp\", bits, Op.MUL_MOD, a, b, null!, mod, mulOps, warmups, measures));\n            results.Add(Bench(\"csharp\", bits, Op.MODPOW, ap, null!, ep, mod, powOps, warmups, measures));\n        }\n\n        if (json)\n        {\n            foreach (var r in results) Console.WriteLine(r.ToJson());\n        }\n        else\n        {\n            PrintHuman(results);\n        }\n\n        return 0;\n    }\n}\n</code></pre>\n<h3 id=\"32-运行方式可复现命令\">3.2 运行方式（可复现命令）</h3>\n<pre><code class=\"language-bash\"># Java 21\ncd /app/bigintbench/java\njavac BigIntBench.java\njava -Xms1g -Xmx1g -XX:+UseG1GC -XX:+AlwaysPreTouch BigIntBench --json\n\n# .NET 10\ncd /app/bigintbench/csharp\ndotnet build -c Release\nDOTNET_GCServer=1 dotnet run -c Release -- --json\n</code></pre>\n<h3 id=\"33-实验一原始输出jsonl\">3.3 实验一原始输出（JSONL）</h3>\n<h4 id=\"java-21results_java21jsonl\">Java 21：<code>results_java21.jsonl</code></h4>\n<pre><code class=\"language-jsonl\">{\"lang\":\"java\",\"bits\":256,\"op\":\"ADD_MOD\",\"ops\":1999872,\"nsPerOp\":139.589,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":256,\"op\":\"MUL_MOD\",\"ops\":499712,\"nsPerOp\":387.031,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":256,\"op\":\"MODPOW\",\"ops\":7936,\"nsPerOp\":17764.884,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":1024,\"op\":\"ADD_MOD\",\"ops\":999424,\"nsPerOp\":284.672,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":1024,\"op\":\"MUL_MOD\",\"ops\":119808,\"nsPerOp\":3094.540,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":1024,\"op\":\"MODPOW\",\"ops\":1280,\"nsPerOp\":264577.852,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":4096,\"op\":\"ADD_MOD\",\"ops\":199680,\"nsPerOp\":900.735,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":4096,\"op\":\"MUL_MOD\",\"ops\":19456,\"nsPerOp\":32062.554,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":4096,\"op\":\"MODPOW\",\"ops\":256,\"nsPerOp\":3422756.113,\"checksum\":0}\n</code></pre>\n<h4 id=\"net-10results_csharpjsonl\">.NET 10：<code>results_csharp.jsonl</code></h4>\n<pre><code class=\"language-jsonl\">{\"lang\":\"csharp\",\"bits\":256,\"op\":\"ADD_MOD\",\"ops\":1999872,\"nsPerOp\":146.261,\"checksum\":0}\n{\"lang\":\"csharp\",\"bits\":256,\"op\":\"MUL_MOD\",\"ops\":499712,\"nsPerOp\":560.246,\"checksum\":0}\n{\"lang\":\"csharp\",\"bits\":256,\"op\":\"MODPOW\",\"ops\":7936,\"nsPerOp\":169713.608,\"checksum\":0}\n{\"lang\":\"csharp\",\"bits\":1024,\"op\":\"ADD_MOD\",\"ops\":999424,\"nsPerOp\":297.335,\"checksum\":0}\n{\"lang\":\"csharp\",\"bits\":1024,\"op\":\"MUL_MOD\",\"ops\":119808,\"nsPerOp\":4792.760,\"checksum\":0}\n{\"lang\":\"csharp\",\"bits\":1024,\"op\":\"MODPOW\",\"ops\":1280,\"nsPerOp\":1938407.720,\"checksum\":0}\n{\"lang\":\"csharp\",\"bits\":4096,\"op\":\"ADD_MOD\",\"ops\":199680,\"nsPerOp\":1280.760,\"checksum\":0}\n{\"lang\":\"csharp\",\"bits\":4096,\"op\":\"MUL_MOD\",\"ops\":19456,\"nsPerOp\":36894.568,\"checksum\":0}\n{\"lang\":\"csharp\",\"bits\":4096,\"op\":\"MODPOW\",\"ops\":256,\"nsPerOp\":20617970.004,\"checksum\":0}\n</code></pre>\n<hr />\n<h2 id=\"实验二加入-java-8-看现状主流处于什么位置\">实验二：加入 Java 8 ——看“现状主流”处于什么位置</h2>\n<p>这一组的关键点是：<strong>同一份 Java 源码用 Java 8 编译</strong>，分别在 Java 8 与 Java 21 上运行，尽量避免“编译器产物差异”。</p>\n<h3 id=\"41-运行方式\">4.1 运行方式</h3>\n<pre><code class=\"language-bash\"># 编译（Java 8）\n/path/to/jdk8/bin/javac BigIntBench.java\n\n# 运行（Java 8）\n/path/to/jdk8/bin/java -Xms1g -Xmx1g -XX:+UseG1GC -XX:+AlwaysPreTouch BigIntBench --json\n\n# 运行（Java 21）\njava -Xms1g -Xmx1g -XX:+UseG1GC -XX:+AlwaysPreTouch BigIntBench --json\n</code></pre>\n<h3 id=\"42-实验二原始输出jsonl\">4.2 实验二原始输出（JSONL）</h3>\n<h4 id=\"java-8results_java8jsonl\">Java 8：<code>results_java8.jsonl</code></h4>\n<pre><code class=\"language-jsonl\">{\"lang\":\"java\",\"bits\":256,\"op\":\"ADD_MOD\",\"ops\":1999872,\"nsPerOp\":207.335,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":256,\"op\":\"MUL_MOD\",\"ops\":499712,\"nsPerOp\":468.248,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":256,\"op\":\"MODPOW\",\"ops\":7936,\"nsPerOp\":17697.113,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":1024,\"op\":\"ADD_MOD\",\"ops\":999424,\"nsPerOp\":390.906,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":1024,\"op\":\"MUL_MOD\",\"ops\":119808,\"nsPerOp\":3089.474,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":1024,\"op\":\"MODPOW\",\"ops\":1280,\"nsPerOp\":277395.652,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":4096,\"op\":\"ADD_MOD\",\"ops\":199680,\"nsPerOp\":990.708,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":4096,\"op\":\"MUL_MOD\",\"ops\":19456,\"nsPerOp\":30692.214,\"checksum\":0}\n{\"lang\":\"java\",\"bits\":4096,\"op\":\"MODPOW\",\"ops\":256,\"nsPerOp\":3468269.539,\"checksum\":0}\n</code></pre>\n<hr />\n<h2 id=\"可视化与总结\">可视化与总结</h2>\n<p>从一个 .NET 程序员的视角来看，这次的测试结果可以说既在情理之中，又有些出乎意料。</p>\n<ol>\n<li><strong>ADD_MOD (加法+取模)</strong>: 在这个项目上，.NET 和 Java 21 几乎打了个平手，差距微乎其微。可以说，在基础的加法运算上，.NET 表现得相当不错。<br />\n<img alt=\"bar_csharp_baseline_add_mod\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260114225530983-1114904260.png\" /></li>\n<li><strong>MUL_MOD (乘法+取模)</strong>: 从这里开始，差距出现了。.NET 明显慢于 Java，性能鸿沟开始变得“肉眼可见”。<br />\n<img alt=\"bar_csharp_baseline_mul_mod\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260114225554833-1748112213.png\" /></li>\n<li><strong>MODPOW (模幂)</strong>: 这是差距最大的地方。.NET 在这项测试中被 Java 21 拉开了 <strong>6到9倍</strong> 的差距。对于从事密码学或需要大量大数运算的开发者来说，这是一个非常刺眼的信号。<br />\n<img alt=\"bar_csharp_baseline_modpow\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260114225547260-463566788.png\" /></li>\n<li><strong>Java 8 vs Java 21</strong>: 毫无疑问，Java 21 在绝大多数情况下都比老迈的 Java 8 要快。不过有趣的是，在 <code>MUL_MOD</code> 的 1024 和 4096 位测试中，Java 8 居然出现了“反超”的现象。这可能是由于 JIT 策略、算法选择的阈值差异，或是单纯的测量误差。虽然这不影响“Java 21更快”的总体结论，但也提醒我们性能测试的复杂性。</li>\n</ol>\n<p>总而言之，这次对决让我们清楚地看到，在复杂的大数运算上，.NET 的 <code>BigInteger</code> 确实还有很长的路要走。</p>\n<hr />\n<h2 id=\"one-more-thing当外援登场\">One More Thing：当“外援”登场</h2>\n<p>在寻找 .NET 大数性能优化方案的过程中，我们自然能想到了业界标杆 GMP ——它是 GNU Multi-Precision Arithmetic Library，很多数学软件/密码学实现都会用它做高性能大整数运算。</p>\n<p>我碰巧也为它做了一个 .NET 封装：<a href=\"https://github.com/sdcb/Sdcb.Arithmetic\" rel=\"noopener nofollow\" target=\"_blank\">Sdcb.Arithmetic</a>。</p>\n<p>但必须提前声明：<strong>让 GMP 作为“外援”加入这场对比，是“不公平”的</strong>。原因很简单：</p>\n<ul>\n<li><strong>语言优势</strong>：GMP 是原生 C/汇编，而 .NET 和 Java 是在虚拟机上运行的托管语言。</li>\n<li><strong>内存策略</strong>：GMP 鼓励使用 <strong>in-place API</strong>，可以直接在原地修改数值，大大减少了内存分配和 GC 压力。而 .NET 和 Java 的 <code>BigInteger</code> 则是不可变对象。</li>\n</ul>\n<p>所以，这部分的结果更像是一个“彩蛋”，展示的是：<strong>如果你愿意引入原生依赖，并改变编码风格，.NET 的大数性能可以达到怎样的高度。</strong></p>\n<h3 id=\"71-客串实验完整源代码net-biginteger--gmpinteger-同场\">7.1 客串实验完整源代码（.NET BigInteger + GmpInteger 同场）</h3>\n<blockquote>\n<p>下面代码会同时输出两套结果：<code>csharp_bigint</code> 与 <code>csharp_gmp_inplace</code>（仍是 JSONL）。</p>\n</blockquote>\n<p><strong><code>Program.cs</code>（客串版，完整）</strong></p>\n<pre><code class=\"language-csharp\">using System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Globalization;\nusing System.Linq;\nusing System.Numerics;\nusing Sdcb.Arithmetic.Gmp;\n\nsealed class SplitMix64\n{\n    private ulong _x;\n    public SplitMix64(ulong seed) =&gt; _x = seed;\n\n    public ulong NextUInt64()\n    {\n        ulong z = (_x += 0x9E3779B97F4A7C15UL);\n        z = (z ^ (z &gt;&gt; 30)) * 0xBF58476D1CE4E5B9UL;\n        z = (z ^ (z &gt;&gt; 27)) * 0x94D049BB133111EBUL;\n        return z ^ (z &gt;&gt; 31);\n    }\n\n    public void NextBytes(byte[] dst)\n    {\n        int i = 0;\n        while (i &lt; dst.Length)\n        {\n            ulong v = NextUInt64();\n            for (int k = 0; k &lt; 8 &amp;&amp; i &lt; dst.Length; k++)\n                dst[i++] = (byte)(v &gt;&gt; (56 - 8 * k));\n        }\n    }\n}\n\nenum Op { ADD_MOD, MUL_MOD, MODPOW }\n\nrecord Result(string Impl, int Bits, Op Op, long Ops, double NsPerOp, long Checksum)\n{\n    public string ToJson() =&gt; string.Create(CultureInfo.InvariantCulture,\n        $\"{{\\\"lang\\\":\\\"{Impl}\\\",\\\"bits\\\":{Bits},\\\"op\\\":\\\"{Op}\\\",\\\"ops\\\":{Ops},\\\"nsPerOp\\\":{NsPerOp:F3},\\\"checksum\\\":{Checksum}}}\");\n}\n\nstatic class BenchUtil\n{\n    public static void MaskToBitSize(byte[] buf, int bitSize)\n    {\n        int topBit = (bitSize - 1) % 8;\n        int keepBits = topBit + 1;\n        int firstMask = keepBits == 8 ? 0xFF : ((1 &lt;&lt; keepBits) - 1);\n        buf[0] &amp;= (byte)firstMask;\n        buf[0] |= (byte)(1 &lt;&lt; topBit);\n    }\n\n    public static string ToHex(byte[] bytes) =&gt; Convert.ToHexString(bytes);\n}\n\nstatic class BigIntegerBench\n{\n    public static BigInteger[] Gen(int bitSize, int count, ulong seed)\n    {\n        var rng = new SplitMix64(seed);\n        int byteLen = (bitSize + 7) / 8;\n        var arr = new BigInteger[count];\n        var buf = new byte[byteLen];\n\n        for (int i = 0; i &lt; count; i++)\n        {\n            rng.NextBytes(buf);\n            BenchUtil.MaskToBitSize(buf, bitSize);\n            arr[i] = new BigInteger(buf, isUnsigned: true, isBigEndian: true);\n        }\n        return arr;\n    }\n\n    public static BigInteger GenModulus(int bitSize, ulong seed)\n    {\n        var rng = new SplitMix64(seed);\n        int byteLen = (bitSize + 7) / 8;\n        var buf = new byte[byteLen];\n        rng.NextBytes(buf);\n        BenchUtil.MaskToBitSize(buf, bitSize);\n        buf[^1] |= 1;\n        return new BigInteger(buf, isUnsigned: true, isBigEndian: true);\n    }\n\n    public static long RunOnce(Op op, BigInteger[] a, BigInteger[] b, BigInteger[] e, BigInteger mod, int outer)\n    {\n        BigInteger acc = BigInteger.Zero;\n        int n = a.Length;\n\n        switch (op)\n        {\n            case Op.ADD_MOD:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                        acc ^= (a[i] + b[i]) % mod;\n                break;\n\n            case Op.MUL_MOD:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                        acc ^= (a[i] * b[i]) % mod;\n                break;\n\n            case Op.MODPOW:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                        acc ^= BigInteger.ModPow(a[i], e[i], mod);\n                break;\n        }\n\n        return (long)(acc &amp; long.MaxValue);\n    }\n}\n\nstatic class GmpIntegerBench\n{\n    public static GmpInteger[] Gen(int bitSize, int count, ulong seed)\n    {\n        var rng = new SplitMix64(seed);\n        int byteLen = (bitSize + 7) / 8;\n        var arr = new GmpInteger[count];\n        var buf = new byte[byteLen];\n\n        for (int i = 0; i &lt; count; i++)\n        {\n            rng.NextBytes(buf);\n            BenchUtil.MaskToBitSize(buf, bitSize);\n            arr[i] = GmpInteger.Parse(BenchUtil.ToHex(buf), 16);\n        }\n        return arr;\n    }\n\n    public static GmpInteger GenModulus(int bitSize, ulong seed)\n    {\n        var rng = new SplitMix64(seed);\n        int byteLen = (bitSize + 7) / 8;\n        var buf = new byte[byteLen];\n        rng.NextBytes(buf);\n        BenchUtil.MaskToBitSize(buf, bitSize);\n        buf[^1] |= 1;\n        return GmpInteger.Parse(BenchUtil.ToHex(buf), 16);\n    }\n\n    public static long RunOnce(Op op, GmpInteger[] a, GmpInteger[] b, GmpInteger[] e, GmpInteger mod, int outer)\n    {\n        using var acc = GmpInteger.From(0);\n        using var tmp = GmpInteger.From(0);\n\n        int n = a.Length;\n        switch (op)\n        {\n            case Op.ADD_MOD:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                    {\n                        GmpInteger.AddInplace(tmp, a[i], b[i]);\n                        GmpInteger.ModInplace(tmp, tmp, mod);\n                        GmpInteger.BitwiseXorInplace(acc, acc, tmp);\n                    }\n                break;\n\n            case Op.MUL_MOD:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                    {\n                        GmpInteger.MultiplyInplace(tmp, a[i], b[i]);\n                        GmpInteger.ModInplace(tmp, tmp, mod);\n                        GmpInteger.BitwiseXorInplace(acc, acc, tmp);\n                    }\n                break;\n\n            case Op.MODPOW:\n                for (int o = 0; o &lt; outer; o++)\n                    for (int i = 0; i &lt; n; i++)\n                    {\n                        GmpInteger.PowerModInplace(tmp, a[i], e[i], mod);\n                        GmpInteger.BitwiseXorInplace(acc, acc, tmp);\n                    }\n                break;\n        }\n\n        return acc.GetHashCode();\n    }\n\n    public static void DisposeAll(GmpInteger[] xs)\n    {\n        foreach (var x in xs) x.Dispose();\n    }\n}\n\nstatic class Runner\n{\n    static Result BenchBigInteger(int bits, Op op, BigInteger[] a, BigInteger[] b, BigInteger[] e, BigInteger mod, long targetOps, int warmups, int measures)\n    {\n        int n = a.Length;\n        int outer = (int)Math.Max(1, targetOps / n);\n        long actualOps = (long)n * outer;\n\n        long ck = 0;\n        for (int i = 0; i &lt; warmups; i++) ck ^= BigIntegerBench.RunOnce(op, a, b, e, mod, outer);\n\n        long[] timesNs = new long[measures];\n        for (int i = 0; i &lt; measures; i++)\n        {\n            var sw = Stopwatch.StartNew();\n            long c = BigIntegerBench.RunOnce(op, a, b, e, mod, outer);\n            sw.Stop();\n            ck ^= c;\n            timesNs[i] = (long)(sw.ElapsedTicks * (1_000_000_000.0 / Stopwatch.Frequency));\n        }\n        Array.Sort(timesNs);\n        long median = timesNs[timesNs.Length / 2];\n        return new Result(\"csharp_bigint\", bits, op, actualOps, (double)median / actualOps, ck);\n    }\n\n    static Result BenchGmpInteger(int bits, Op op, GmpInteger[] a, GmpInteger[] b, GmpInteger[] e, GmpInteger mod, long targetOps, int warmups, int measures)\n    {\n        int n = a.Length;\n        int outer = (int)Math.Max(1, targetOps / n);\n        long actualOps = (long)n * outer;\n\n        long ck = 0;\n        for (int i = 0; i &lt; warmups; i++) ck ^= GmpIntegerBench.RunOnce(op, a, b, e, mod, outer);\n\n        long[] timesNs = new long[measures];\n        for (int i = 0; i &lt; measures; i++)\n        {\n            var sw = Stopwatch.StartNew();\n            long c = GmpIntegerBench.RunOnce(op, a, b, e, mod, outer);\n            sw.Stop();\n            ck ^= c;\n            timesNs[i] = (long)(sw.ElapsedTicks * (1_000_000_000.0 / Stopwatch.Frequency));\n        }\n        Array.Sort(timesNs);\n        long median = timesNs[timesNs.Length / 2];\n        return new Result(\"csharp_gmp_inplace\", bits, op, actualOps, (double)median / actualOps, ck);\n    }\n\n    public static int Run(string[] args)\n    {\n        bool json = args.Any(a =&gt; a == \"--json\");\n\n        int warmups = 5;\n        int measures = 11;\n        int[] bitSizes = [256, 1024, 4096];\n\n        var results = new List&lt;Result&gt;();\n\n        foreach (int bits in bitSizes)\n        {\n            long addOps = bits switch { 256 =&gt; 2_000_000L, 1024 =&gt; 1_000_000L, _ =&gt; 200_000L };\n            long mulOps = bits switch { 256 =&gt; 500_000L, 1024 =&gt; 120_000L, _ =&gt; 20_000L };\n            long powOps = bits switch { 256 =&gt; 8_000L, 1024 =&gt; 1_500L, _ =&gt; 250L };\n\n            // BigInteger data\n            var modB = BigIntegerBench.GenModulus(bits, 0xA1B2C3D4E5F60708UL ^ (uint)bits);\n            var aB = BigIntegerBench.Gen(bits, 1024, 0x1111222233334444UL ^ (uint)bits);\n            var bB = BigIntegerBench.Gen(bits, 1024, 0x9999AAAABBBBCCCCUL ^ (uint)bits);\n            var apB = BigIntegerBench.Gen(bits, 256, 0x13579BDF2468ACE0UL ^ (uint)bits);\n            var epB = BigIntegerBench.Gen(Math.Min(bits, 512), 256, 0x0FEDCBA987654321UL ^ (uint)bits);\n\n            // GmpInteger data (same seeds/bit sizes)\n            using var modG = GmpIntegerBench.GenModulus(bits, 0xA1B2C3D4E5F60708UL ^ (uint)bits);\n            var aG = GmpIntegerBench.Gen(bits, 1024, 0x1111222233334444UL ^ (uint)bits);\n            var bG = GmpIntegerBench.Gen(bits, 1024, 0x9999AAAABBBBCCCCUL ^ (uint)bits);\n            var apG = GmpIntegerBench.Gen(bits, 256, 0x13579BDF2468ACE0UL ^ (uint)bits);\n            var epG = GmpIntegerBench.Gen(Math.Min(bits, 512), 256, 0x0FEDCBA987654321UL ^ (uint)bits);\n\n            try\n            {\n                results.Add(BenchBigInteger(bits, Op.ADD_MOD, aB, bB, null!, modB, addOps, warmups, measures));\n                results.Add(BenchBigInteger(bits, Op.MUL_MOD, aB, bB, null!, modB, mulOps, warmups, measures));\n                results.Add(BenchBigInteger(bits, Op.MODPOW, apB, null!, epB, modB, powOps, warmups, measures));\n\n                results.Add(BenchGmpInteger(bits, Op.ADD_MOD, aG, bG, null!, modG, addOps, warmups, measures));\n                results.Add(BenchGmpInteger(bits, Op.MUL_MOD, aG, bG, null!, modG, mulOps, warmups, measures));\n                results.Add(BenchGmpInteger(bits, Op.MODPOW, apG, null!, epG, modG, powOps, warmups, measures));\n            }\n            finally\n            {\n                GmpIntegerBench.DisposeAll(aG);\n                GmpIntegerBench.DisposeAll(bG);\n                GmpIntegerBench.DisposeAll(apG);\n                GmpIntegerBench.DisposeAll(epG);\n            }\n        }\n\n        if (json)\n        {\n            foreach (var r in results) Console.WriteLine(r.ToJson());\n        }\n\n        return 0;\n    }\n}\n\npublic static class Program\n{\n    public static int Main(string[] args) =&gt; Runner.Run(args);\n}\n</code></pre>\n<p><strong><code>gmpbench.csproj</code>（客串版项目文件）</strong></p>\n<pre><code class=\"language-xml\">&lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;\n  &lt;PropertyGroup&gt;\n    &lt;OutputType&gt;Exe&lt;/OutputType&gt;\n    &lt;TargetFramework&gt;net10.0&lt;/TargetFramework&gt;\n    &lt;Nullable&gt;enable&lt;/Nullable&gt;\n    &lt;ImplicitUsings&gt;enable&lt;/ImplicitUsings&gt;\n  &lt;/PropertyGroup&gt;\n\n  &lt;ItemGroup&gt;\n    &lt;PackageReference Include=\"Sdcb.Arithmetic.Gmp\" Version=\"*\" /&gt;\n    &lt;PackageReference Include=\"Sdcb.Arithmetic.Gmp.runtime.linux-x64\" Version=\"*\" /&gt;\n  &lt;/ItemGroup&gt;\n&lt;/Project&gt;\n</code></pre>\n<h3 id=\"72-客串实验原始输出jsonl\">7.2 客串实验原始输出（JSONL）</h3>\n<pre><code class=\"language-jsonl\">{\"lang\":\"csharp_bigint\",\"bits\":256,\"op\":\"ADD_MOD\",\"ops\":1999872,\"nsPerOp\":146.261,\"checksum\":0}\n{\"lang\":\"csharp_bigint\",\"bits\":256,\"op\":\"MUL_MOD\",\"ops\":499712,\"nsPerOp\":560.246,\"checksum\":0}\n{\"lang\":\"csharp_bigint\",\"bits\":256,\"op\":\"MODPOW\",\"ops\":7936,\"nsPerOp\":169713.608,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":256,\"op\":\"ADD_MOD\",\"ops\":1999872,\"nsPerOp\":76.644,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":256,\"op\":\"MUL_MOD\",\"ops\":499712,\"nsPerOp\":114.690,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":256,\"op\":\"MODPOW\",\"ops\":7936,\"nsPerOp\":13931.914,\"checksum\":0}\n{\"lang\":\"csharp_bigint\",\"bits\":1024,\"op\":\"ADD_MOD\",\"ops\":999424,\"nsPerOp\":297.335,\"checksum\":0}\n{\"lang\":\"csharp_bigint\",\"bits\":1024,\"op\":\"MUL_MOD\",\"ops\":119808,\"nsPerOp\":4792.760,\"checksum\":0}\n{\"lang\":\"csharp_bigint\",\"bits\":1024,\"op\":\"MODPOW\",\"ops\":1280,\"nsPerOp\":1938407.720,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":1024,\"op\":\"ADD_MOD\",\"ops\":999424,\"nsPerOp\":97.260,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":1024,\"op\":\"MUL_MOD\",\"ops\":119808,\"nsPerOp\":562.235,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":1024,\"op\":\"MODPOW\",\"ops\":1280,\"nsPerOp\":218715.147,\"checksum\":0}\n{\"lang\":\"csharp_bigint\",\"bits\":4096,\"op\":\"ADD_MOD\",\"ops\":199680,\"nsPerOp\":1280.760,\"checksum\":0}\n{\"lang\":\"csharp_bigint\",\"bits\":4096,\"op\":\"MUL_MOD\",\"ops\":19456,\"nsPerOp\":36894.568,\"checksum\":0}\n{\"lang\":\"csharp_bigint\",\"bits\":4096,\"op\":\"MODPOW\",\"ops\":256,\"nsPerOp\":20617970.004,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":4096,\"op\":\"ADD_MOD\",\"ops\":199680,\"nsPerOp\":179.720,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":4096,\"op\":\"MUL_MOD\",\"ops\":19456,\"nsPerOp\":5431.441,\"checksum\":0}\n{\"lang\":\"csharp_gmp_inplace\",\"bits\":4096,\"op\":\"MODPOW\",\"ops\":256,\"nsPerOp\":2662198.492,\"checksum\":0}\n</code></pre>\n<h3 id=\"73-客串可视化以gmp为基准\">7.3 客串可视化（以GMP为基准）</h3>\n<p><img alt=\"bar_gmp_baseline_add_mod\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260114225617406-1423853952.png\" /><br />\n<img alt=\"bar_gmp_baseline_mul_mod\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260114225622152-863625745.png\" /><br />\n<img alt=\"bar_gmp_baseline_modpow\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260114225625327-2119127781.png\" /></p>\n<hr />\n<h2 id=\"总结与展望\">总结与展望</h2>\n<p>从这次“硬碰硬”的对决中，我们可以清晰地看到：在基础加法上，.NET <code>BigInteger</code> 与 Java 不分伯仲；但在乘法，尤其是<strong>模幂运算</strong>（对密码学等场景极其重要）上，.NET 目前确实存在明显的短板，大幅落后于 Java。</p>\n<p>承认不足是改进的开始。对于绝大多数业务场景，内置的 <code>BigInteger</code> 依然够用且方便。但如果你的应用处于性能敏感区（如加密算法、科学计算），那么也许是时候考虑一些“重武器”了。</p>\n<p>这也正是我开发并维护 <strong><a href=\"https://github.com/sdcb/Sdcb.Arithmetic\" rel=\"noopener nofollow\" target=\"_blank\">Sdcb.Arithmetic</a></strong> 的初衷。它通过封装 GMP 等高性能原生库，为 .NET 带来了<strong>原地修改（in-place）</strong>以及高达数倍的性能提升（如文中实验所示）。如果你对性能有极致追求，或者想看看 .NET 在大数计算上的极限，欢迎去 GitHub 点个 Star ⭐，试一试这个库。</p>\n<p>感谢阅读！如果你觉得这两个语言的对比分析有意思，或者对 .NET 高性能编程感兴趣，欢迎在评论区留言交流，也欢迎加入我的 <strong>.NET骚操作 QQ群：495782587</strong>，我们一起探索更多技术硬核玩法。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 08:50</span>&nbsp;\n<a href=\"https://www.cnblogs.com/sdcb\">.NET骚操作</a>&nbsp;\n阅读(<span id=\"post_view_count\">992</span>)&nbsp;\n评论(<span id=\"post_comment_count\">11</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Flink源码阅读：JobManager的HA机制",
      "link": "https://www.cnblogs.com/Jackeyzhe/p/19484622",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Jackeyzhe/p/19484622\" id=\"cb_post_title_url\" title=\"发布于 2026-01-14 23:45\">\n    <span>Flink源码阅读：JobManager的HA机制</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"Flink源码阅读：JobManager的HA机制\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/1828322/202601/1828322-20260114234320282-1778052120.png\" />\n        JobManager 在 Flink 集群中发挥着重要的作用，包括任务调度和资源管理等工作。如果 JobManager 宕机，那么整个集群的任务都将失败。为了解决 JobManager 的单点问题，Flink 也设计了 HA 机制来保障整个集群的稳定性。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>JobManager 在 Flink 集群中发挥着重要的作用，包括任务调度和资源管理等工作。如果 JobManager 宕机，那么整个集群的任务都将失败。为了解决 JobManager 的单点问题，Flink 也设计了 HA 机制来保障整个集群的稳定性。</p>\n<h3 id=\"基本概念\">基本概念</h3>\n<p>在 JobManager 启动时，调用 <code>HighAvailabilityServicesUtils.createHighAvailabilityServices</code> 来创建 HA 服务，HA 依赖的服务都被封装在 HighAvailabilityServices 中。当前 Flink 内部支持两种高可用模式，分别是 ZooKeeper 和 KUBERNETES。</p>\n<pre><code class=\"language-java\">case ZOOKEEPER:\n    return createZooKeeperHaServices(configuration, executor, fatalErrorHandler);\ncase KUBERNETES:\n    return createCustomHAServices(\n            \"org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory\",\n            configuration,\n            executor);\n</code></pre>\n<p>HighAvailabilityServices 中提供的关键组件包括：</p>\n<ul>\n<li>\n<p>LeaderRetrievalService：服务发现，用于获取当前 leader 的地址。目前用到服务发现的组件有 ResourceManager、Dispatcher、JobManager、ClusterRestEndpoint。</p>\n</li>\n<li>\n<p>LeaderElection：选举服务，从多个候选者中选出一个作为 leader。用到选举服务的同样是 ResourceManager、Dispatcher、JobManager、ClusterRestEndpoint 这四个。</p>\n</li>\n<li>\n<p>CheckpointRecoveryFactory：Checkpoint 恢复组件的工厂类，提供了创建 CompletedCheckpointStore 和 CheckpointIDCounter 的方法。CompletedCheckpointStore 是用于存储已完成的 checkpoint 的元信息，CheckpointIDCounter 是用于生成 checkpoint ID。</p>\n</li>\n<li>\n<p>ExecutionPlanStore：用于存储执行计划。</p>\n</li>\n<li>\n<p>JobResultStore：用于存储作业结果，这里有两种状态，一种是 dirty，表示作业没有被完全清理，另一种是 clean，表示作业清理工作已经执行完成了。</p>\n</li>\n<li>\n<p>BlobStore：存储作业运行期间的一些二进制文件。</p>\n</li>\n</ul>\n<h4 id=\"选举服务\">选举服务</h4>\n<p>Flink 的选举是依靠 LeaderElection 和 LeaderContender 配合完成的。LeaderElection 是 LeaderElectionService 的代理接口，提供了注册候选者、确认 leader 和 判断候选者是否是 leader 三个接口。LeaderContender 则是用来表示候选者对象。当一个 LeaderContender 当选 leader 后，LeaderElectionService 会为其生成一个 leaderSessionId，LeaderContender 会调用 confirmLeadershipAsync 发布自己的地址。选举服务的具体实现在 LeaderElectionDriver 接口中。</p>\n<h4 id=\"服务发现\">服务发现</h4>\n<p>服务发现的作用是获取各组件的 leader 地址。服务发现依赖 LeaderRetrievalService 和 LeaderRetrievalListener。LeaderRetrievalService 可以启动一个监听，当有新的 leader 当选时，会调用 LeaderRetrievalListener 的 notifyLeaderAddress 方法。</p>\n<h4 id=\"信息保存\">信息保存</h4>\n<p>当 leader 发生切换时，新的 leader 需要获取到旧 leader 存储的信息，这就需要旧 leader 把这些信息存在一个公共的存储上。它可以是 ZooKeeper 或 Kubernetes 的存储，也可以是分布式文件系统的存储。</p>\n<h3 id=\"基于-zookeeper-的-ha\">基于 ZooKeeper 的 HA</h3>\n<h4 id=\"选举服务-1\">选举服务</h4>\n<p>前面我们提到了选举服务主要依赖 LeaderElection 和 LeaderContender 配合完成。我们就以 JobManager 为例，看一下机遇 ZooKeeper 的选举流程的具体实现。</p>\n<p><img alt=\"ZKLeaderElection\" class=\"lazyload\" /></p>\n<p>图中 JobMasterServiceLeadershipRunner 是 LeaderContender 的实现类。在启动服务时，会向 LeaderElection 注册自己的信息，实际执行者是 DefaultLeaderElectionService。它先创建了 LeaderElectionDriver，然后将 LeaderContender 保存在 leaderContenderRegistry 中。选举的核心逻辑封装在 LeaderElectionDriver 中。</p>\n<p>在创建 LeaderElectionDriver 时，会创建 LeaderLatch 对象和 TreeCache 对象， LeaderLatch 封装了与 ZooKeeper 关联的回调，会接收一个 LeaderElectionDriver 作为监听。TreeCache 主要用于监听 ZooKeeper 中 leader 节点的变更。</p>\n<pre><code class=\"language-java\">public ZooKeeperLeaderElectionDriver(\n        CuratorFramework curatorFramework, LeaderElectionDriver.Listener leaderElectionListener)\n        throws Exception {\n    ...\n    this.leaderLatch = new LeaderLatch(curatorFramework, ZooKeeperUtils.getLeaderLatchPath());\n    this.treeCache =\n            ZooKeeperUtils.createTreeCache(\n                    curatorFramework,\n                    \"/\",\n                    new ZooKeeperLeaderElectionDriver.ConnectionInfoNodeSelector());\n\n    treeCache\n            .getListenable()\n            .addListener(\n                    (client, event) -&gt; {\n                        switch (event.getType()) {\n                            case NODE_ADDED:\n                            case NODE_UPDATED:\n                                Preconditions.checkNotNull(\n                                        event.getData(),\n                                        \"The ZooKeeper event data must not be null.\");\n                                handleChangedLeaderInformation(event.getData());\n                                break;\n                            case NODE_REMOVED:\n                                Preconditions.checkNotNull(\n                                        event.getData(),\n                                        \"The ZooKeeper event data must not be null.\");\n                                handleRemovedLeaderInformation(event.getData().getPath());\n                                break;\n                        }\n                    });\n\n    leaderLatch.addListener(this);\n    ...\n    leaderLatch.start();\n    treeCache.start();\n}\n</code></pre>\n<p>我们进入到 LeaderLatch 的 start 方法。它的内部是在 ZooKeeper 上创建 latch-xxx 节点。xxx 是当前 LeaderLatch 的 ID，它由 ZooKeeper 生成，ID 最小的当选 Leader。</p>\n<pre><code class=\"language-java\">private void checkLeadership(List&lt;String&gt; children) throws Exception {\n    if (this.debugCheckLeaderShipLatch != null) {\n        this.debugCheckLeaderShipLatch.await();\n    }\n\n    String localOurPath = (String)this.ourPath.get();\n    List&lt;String&gt; sortedChildren = LockInternals.getSortedChildren(\"latch-\", sorter, children);\n    int ourIndex = localOurPath != null ? sortedChildren.indexOf(ZKPaths.getNodeFromPath(localOurPath)) : -1;\n    this.log.debug(\"checkLeadership with id: {}, ourPath: {}, children: {}\", new Object[]{this.id, localOurPath, sortedChildren});\n    if (ourIndex &lt; 0) {\n        this.log.error(\"Can't find our node. Resetting. Index: \" + ourIndex);\n        this.reset();\n    } else if (ourIndex == 0) {\n        this.lastPathIsLeader.set(localOurPath);\n        this.setLeadership(true);\n    } else {\n        this.setLeadership(false);\n        String watchPath = (String)sortedChildren.get(ourIndex - 1);\n        Watcher watcher = new Watcher() {\n            public void process(WatchedEvent event) {\n                if (LeaderLatch.this.state.get() == LeaderLatch.State.STARTED &amp;&amp; event.getType() == EventType.NodeDeleted) {\n                    try {\n                        LeaderLatch.this.getChildren();\n                    } catch (Exception ex) {\n                        ThreadUtils.checkInterrupted(ex);\n                        LeaderLatch.this.log.error(\"An error occurred checking the leadership.\", ex);\n                    }\n                }\n\n            }\n        };\n        BackgroundCallback callback = new BackgroundCallback() {\n            public void processResult(CuratorFramework client, CuratorEvent event) throws Exception {\n                if (event.getResultCode() == Code.NONODE.intValue()) {\n                    LeaderLatch.this.getChildren();\n                }\n\n            }\n        };\n        ((ErrorListenerPathable)((BackgroundPathable)this.client.getData().usingWatcher(watcher)).inBackground(callback)).forPath(ZKPaths.makePath(this.latchPath, watchPath));\n    }\n\n}\n</code></pre>\n<p>当选 Leader 后，会回调 LeaderElectionDriver 的 isLeader 方法，如果未当选，则继续监听 latch 节点的变更。isLeader 会继续回调 LeaderElection 的 onGrantLeadership 方法，接着调用 LeaderContender 的 grantLeadership。这时会启动 JobMaster 服务，然后调用 LeaderElection 的 confirmLeadershipAsync 来确认当选成功。确认的过程是由 LeaderElectionDriver 来执行的。主要作用是把当前 leader 的信息写回到 ZooKeeper 的 connection_info 节点。</p>\n<pre><code class=\"language-java\">public void publishLeaderInformation(String componentId, LeaderInformation leaderInformation) {\n    Preconditions.checkState(running.get());\n\n    if (!leaderLatch.hasLeadership()) {\n        return;\n    }\n\n    final String connectionInformationPath =\n            ZooKeeperUtils.generateConnectionInformationPath(componentId);\n\n    LOG.debug(\n            \"Write leader information {} for component '{}' to {}.\",\n            leaderInformation,\n            componentId,\n            ZooKeeperUtils.generateZookeeperPath(\n                    curatorFramework.getNamespace(), connectionInformationPath));\n\n    try {\n        ZooKeeperUtils.writeLeaderInformationToZooKeeper(\n                leaderInformation,\n                curatorFramework,\n                leaderLatch::hasLeadership,\n                connectionInformationPath);\n    } catch (Exception e) {\n        leaderElectionListener.onError(e);\n    }\n}\n</code></pre>\n<h4 id=\"服务发现-1\">服务发现</h4>\n<p>梳理完选举服务的源码后，我们再来看一下服务发现的过程。我们以 TaskManager 获取 JobManager 的 leader 为例。</p>\n<p><img alt=\"ZKLeaderRetrieval\" class=\"lazyload\" /></p>\n<p>当我们往 TaskManager 添加任务时，会调用 JobLeaderService 的 addJob 方法。这里会先获取 LeaderRetrieval，然后调用 start 方法注册 LeaderRetrievalListener 监听，并创建 LeaderRetrievalDriver。在 LeaderRetrievalDriver 中主要是向 ZooKeeper 注册 connection_info 节点的变更。</p>\n<p>如果发生变更，ZooKeeper 会回调 <code>LeaderRetrievalDriver.retrieveLeaderInformationFromZooKeeper</code> 方法。我们从 ZooKeeper 获取到 leader 的地址和 sessionId 后，就回调 <code>LeaderRetrievalService.notifyLeaderAddress</code> 方法。最终调用到 JobLeaderService 的 notifyLeaderAddress 方法，这个方法中就是断开与旧 leader 的连接，增加与新 leader 的连接。</p>\n<h4 id=\"信息保存-1\">信息保存</h4>\n<p>最后我们再来看信息保存相关的源码。在 JobManager 完成一次 Checkpoint 时，会执行 <code>CheckpointCoordinator.completePendingCheckpoint</code> 方法，跟随调用链路可以找到 <code>ZooKeeperStateHandleStore.addAndLock</code> 方法，这里会把状态写入到文件系统中，然后把文件路径保存在 ZooKeeper 中。</p>\n<pre><code class=\"language-java\">public RetrievableStateHandle&lt;T&gt; addAndLock(String pathInZooKeeper, T state)\n        throws PossibleInconsistentStateException, Exception {\n    checkNotNull(pathInZooKeeper, \"Path in ZooKeeper\");\n    checkNotNull(state, \"State\");\n    final String path = normalizePath(pathInZooKeeper);\n    final Optional&lt;Stat&gt; maybeStat = getStat(path);\n\n    if (maybeStat.isPresent()) {\n        if (isNotMarkedForDeletion(maybeStat.get())) {\n            throw new AlreadyExistException(\n                    String.format(\"ZooKeeper node %s already exists.\", path));\n        }\n\n        Preconditions.checkState(\n                releaseAndTryRemove(path),\n                \"The state is marked for deletion and, therefore, should be deletable.\");\n    }\n\n    final RetrievableStateHandle&lt;T&gt; storeHandle = storage.store(state);\n    final byte[] serializedStoreHandle = serializeOrDiscard(storeHandle);\n    try {\n        writeStoreHandleTransactionally(path, serializedStoreHandle);\n        return storeHandle;\n    } catch (KeeperException.NodeExistsException e) {\n        // Transactions are not idempotent in the curator version we're currently using, so it\n        // is actually possible that we've re-tried a transaction that has already succeeded.\n        // We've ensured that the node hasn't been present prior executing the transaction, so\n        // we can assume that this is a result of the retry mechanism.\n        return storeHandle;\n    } catch (Exception e) {\n        if (indicatesPossiblyInconsistentState(e)) {\n            throw new PossibleInconsistentStateException(e);\n        }\n        // In case of any other failure, discard the state and rethrow the exception.\n        storeHandle.discardState();\n        throw e;\n    }\n}\n</code></pre>\n<p>至此，基于 ZooKeeper 的 HA 逻辑我们就梳理完了。从 1.12 版本开始，Flink 还支持了 Kubernetes 高可用，下面我们再来一下它是如何实现的。</p>\n<h3 id=\"基于-kubernetes-的-ha\">基于 Kubernetes 的 HA</h3>\n<h4 id=\"选举服务-2\">选举服务</h4>\n<p>通过前面的学习，我们已经了解到，选举的主要逻辑是在 LeaderElectionDriver 中，因此，我们直接来看 KubernetesLeaderElectionDriver 的逻辑即可。创建 KubernetesLeaderElectionDriver 时，创建并启动了 KubernetesLeaderElector。这个类似于 ZooKeeper 逻辑中 LeaderLatch，会跟 Kubernetes 底层的选举逻辑交互，同时注册监听。</p>\n<pre><code class=\"language-java\">public KubernetesLeaderElector(\n        NamespacedKubernetesClient kubernetesClient,\n        KubernetesLeaderElectionConfiguration leaderConfig,\n        LeaderCallbackHandler leaderCallbackHandler,\n        ExecutorService executorService) {\n    this.kubernetesClient = kubernetesClient;\n    this.leaderElectionConfig =\n            new LeaderElectionConfigBuilder()\n                    .withName(leaderConfig.getConfigMapName())\n                    .withLeaseDuration(leaderConfig.getLeaseDuration())\n                    .withLock(\n                            new ConfigMapLock(\n                                    new ObjectMetaBuilder()\n                                            .withNamespace(kubernetesClient.getNamespace())\n                                            .withName(leaderConfig.getConfigMapName())\n                                            // Labels will be used to clean up the ha related\n                                            // ConfigMaps.\n                                            .withLabels(\n                                                    KubernetesUtils.getConfigMapLabels(\n                                                            leaderConfig.getClusterId()))\n                                            .build(),\n                                    leaderConfig.getLockIdentity()))\n                    .withRenewDeadline(leaderConfig.getRenewDeadline())\n                    .withRetryPeriod(leaderConfig.getRetryPeriod())\n                    .withReleaseOnCancel(true)\n                    .withLeaderCallbacks(\n                            new LeaderCallbacks(\n                                    leaderCallbackHandler::isLeader,\n                                    leaderCallbackHandler::notLeader,\n                                    newLeader -&gt;\n                                            LOG.info(\n                                                    \"New leader elected {} for {}.\",\n                                                    newLeader,\n                                                    leaderConfig.getConfigMapName())))\n                    .build();\n    this.executorService = executorService;\n\n    LOG.info(\n            \"Create KubernetesLeaderElector on lock {}.\",\n            leaderElectionConfig.getLock().describe());\n}\n</code></pre>\n<p>选举成功后，会回调 <code>LeaderElectionListener.onGrantLeadership</code> 方法。后续的调用链路还是会调用到 <code>KubernetesLeaderElectionDriver.publishLeaderInformation</code> 方法。这个方法是把 leader 信息写到 Kubernetes 的 configMap 中。</p>\n<pre><code class=\"language-java\">public void publishLeaderInformation(String componentId, LeaderInformation leaderInformation) {\n    Preconditions.checkState(running.get());\n\n    try {\n        kubeClient\n                .checkAndUpdateConfigMap(\n                        configMapName,\n                        updateConfigMapWithLeaderInformation(componentId, leaderInformation))\n                .get();\n    } catch (InterruptedException | ExecutionException e) {\n        leaderElectionListener.onError(e);\n    }\n\n    LOG.debug(\n            \"Successfully wrote leader information {} for leader {} into the config map {}.\",\n            leaderInformation,\n            componentId,\n            configMapName);\n}\n</code></pre>\n<h4 id=\"服务发现-2\">服务发现</h4>\n<p>服务发现的逻辑在 KubernetesLeaderRetrievalDriver 类中，在创建时，会将内部类 ConfigMapCallbackHandlerImpl 注册为监听回调类。</p>\n<p>当 configMap 有新增或变更后，会回调 <code>LeaderRetrievalService.notifyLeaderAddress</code> 方法。</p>\n<pre><code class=\"language-java\">private class ConfigMapCallbackHandlerImpl\n        implements FlinkKubeClient.WatchCallbackHandler&lt;KubernetesConfigMap&gt; {\n\n    @Override\n    public void onAdded(List&lt;KubernetesConfigMap&gt; configMaps) {\n        // The ConfigMap is created by KubernetesLeaderElectionDriver with\n        // empty data. We don't really need to process anything unless the retriever was started\n        // after the leader election has already succeeded.\n        final KubernetesConfigMap configMap = getOnlyConfigMap(configMaps, configMapName);\n        final LeaderInformation leaderInformation = leaderInformationExtractor.apply(configMap);\n        if (!leaderInformation.isEmpty()) {\n            leaderRetrievalEventHandler.notifyLeaderAddress(leaderInformation);\n        }\n    }\n\n    @Override\n    public void onModified(List&lt;KubernetesConfigMap&gt; configMaps) {\n        final KubernetesConfigMap configMap = getOnlyConfigMap(configMaps, configMapName);\n        leaderRetrievalEventHandler.notifyLeaderAddress(\n                leaderInformationExtractor.apply(configMap));\n    }\n    ...\n}\n</code></pre>\n<h4 id=\"信息保存-2\">信息保存</h4>\n<p>信息保存的逻辑和 ZooKeeper 也非常类似。即先把 state 保存在文件系统，然后把存储路径写到 Kubernetes 写到 configMap 中。具体可以看 <code>KubernetesStateHandleStore.addAndLock</code> 方法。</p>\n<h3 id=\"总结\">总结</h3>\n<p>本文我们一起梳理了 Flink 中 JobManager 的 HA 机制相关源码。目前 Flink 支持 ZooKeeper 和 Kubernetes 两种实现。在梳理过程中，我们以 JobManager 为例，其他几个用到高可用的服务的选举逻辑也是一样的。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-14 23:45</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Jackeyzhe\">Jackeyzhe</a>&nbsp;\n阅读(<span id=\"post_view_count\">15</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "小白学AI开发01：创建第一个示例Agent",
      "link": "https://www.cnblogs.com/jyzhao/p/19484508/xiao-bai-xueai-kai-fa01-chuang-jian-di-yi-ge-shi-l",
      "published": "",
      "description": "<a name=\"top\"></a>\n    <h2><a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jyzhao/p/19484508/xiao-bai-xueai-kai-fa01-chuang-jian-di-yi-ge-shi-l\" id=\"cb_post_title_url\" title=\"发布于 2026-01-14 23:43\">\n    <span>小白学AI开发01：创建第一个示例Agent</span>\n    \n\n</a>\n</h2>\n    <small>\n<span id=\"post-date\">2026-01-14 23:43</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jyzhao\">AlfredZhao</a>&nbsp;\n阅读(<span id=\"post_view_count\">20</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</small>\n    <div class=\"entry\">\n        <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在 LangChain 开发中，我们的核心工作是写 Prompt（提示词）。</p>\n<p>除此之外，还需要了解下python基础，简单了解即可，不需要系统学习，这就是AI时代对技术爱好者的红利。</p>\n<h2 id=\"01--构建独立的python环境\">01 | 构建独立的python环境</h2>\n<p>使用miniconda构建独立的python环境。</p>\n<p>官方网站：<a href=\"https://www.anaconda.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://www.anaconda.com/</a></p>\n<p>选择适合你的，我这里直接选择图形化安装的Miniconda：</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/635610/202601/635610-20260114224646258-840865368.jpg\" /></p>\n<p>下载后安装：</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/635610/202601/635610-20260114224646142-163182582.jpg\" /></p>\n<p>关闭并重新打开终端。此时你会看到命令行前面多了个 (base)，这说明 Conda 已经接管了环境。</p>\n<p>然后创建 Python 3.12 专属学习环境：</p>\n<pre><code class=\"language-BASH\"># 创建环境并指定 Python 版本\nconda create -n lc python=3.12 -y\n\n# 激活环境\nconda activate lc\n</code></pre>\n<p>Tips：如果创建环境，遇到让同意服务/条款之类，直接按提示命令执行同意即可。</p>\n<p>后面测试不同功能的python程序肯定免不了引用各种类型的包，我们统一放到 requirements.txt 中，后续新环境也可以直接一键安装需要引入的相关包。</p>\n<pre><code class=\"language-BASH\">pip install -r requirements.txt\n</code></pre>\n<p>定期维护requirements.txt文件：</p>\n<pre><code class=\"language-BASH\">langchain\nlangchain-openai\npython-dotenv\n</code></pre>\n<h2 id=\"02--创建第一个示例agent\">02 | 创建第一个示例Agent</h2>\n<p>这里仿照LangChain官方文档，构建一个简单的Agent。</p>\n<ul>\n<li><a href=\"https://docs.langchain.com/oss/python/langchain/quickstart\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.langchain.com/oss/python/langchain/quickstart</a></li>\n</ul>\n<p>因为笔者暂时没有Claude模型可用，所以替换为国内的模型，在我们学习测试阶段，基本是没啥影响的。</p>\n<p>vi basic.py</p>\n<pre><code class=\"language-python\">import os\nfrom dotenv import load_dotenv\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import tool\n\n# 加载 .env 文件中的 SILICONFLOW_API_KEY\nload_dotenv()\n\n# 1. 定义工具：必须要写清晰的 Docstring（函数注释）\n@tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"获取指定城市的实时天气情况。\"\"\"\n    return f\"{city} 的天气总是阴雨连连！\"\n\n# 2. 初始化 SiliconFlow 模型\nllm = ChatOpenAI(\n    model=\"deepseek-ai/DeepSeek-V3\", \n    api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n    base_url=\"https://api.siliconflow.cn/v1\",\n    temperature=0.1\n)\n\n# 3. 创建 Agent\nagent = create_agent(\n    model=llm,\n    tools=[get_weather],\n    system_prompt=\"你是一个乐于助人的天气助手。\"\n)\n\n# 4. 运行\nif __name__ == \"__main__\":\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"北京天气怎么样？\"}]}\n    )\n    # 打印最后一条消息（AI 的回答）\n    print(response[\"messages\"][-1].content)\n</code></pre>\n<h2 id=\"03--增强这个示例agent\">03 | 增强这个示例Agent</h2>\n<p>汉化并稍微改造下官方的示例，使其适用于我们的测试环境：</p>\n<p>vi enhance.py</p>\n<pre><code class=\"language-PYTHON\">import os\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.tools import tool, ToolRuntime\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain.agents.structured_output import ToolStrategy\n\nload_dotenv()\n\n# 1. 定义系统提示词（要求 AI 做个“谐音梗”专家）\nSYSTEM_PROMPT = \"\"\"你是一位专业的资深天气预报员，说话极其幽默，特别喜欢讲“谐音梗”。\n\n你有权使用以下两个工具：\n- get_weather_for_location: 获取特定城市的天气。\n- get_user_location: 获取当前用户的所在地。\n\n如果用户询问天气，请务必确认地点。如果用户说的是“我这儿”或“我这里”，请调用 get_user_location 获取他们的位置。\"\"\"\n\n# 2. 定义上下文结构（存储用户信息）\n@dataclass\nclass Context:\n    \"\"\"自定义运行时上下文。\"\"\"\n    user_id: str\n\n# 3. 定义工具\n@tool\ndef get_weather_for_location(city: str) -&gt; str:\n    \"\"\"获取指定城市的天气情况。\"\"\"\n    return f\"{city} 的天气总是阳光灿烂！\"\n\n@tool\ndef get_user_location(runtime: ToolRuntime[Context]) -&gt; str:\n    \"\"\"根据用户 ID 获取其地理位置。\"\"\"\n    user_id = runtime.context.user_id\n    # 模拟数据库：用户 1 在北京，其他在珠海\n    return \"北京\" if user_id == \"1\" else \"珠海\"\n\n# 4. 配置你的模型 (SiliconFlow)\nmodel = ChatOpenAI(\n    model=\"deepseek-ai/DeepSeek-V3\",\n    api_key=os.getenv(\"SILICONFLOW_API_KEY\"),\n    base_url=\"https://api.siliconflow.cn/v1\",\n    temperature=0.7  # 稍微提高随机性，让谐音梗更有创意\n)\n\n# 5. 定义响应格式（汉化字段描述，让 AI 明白填什么）\n@dataclass\nclass ResponseFormat:\n    \"\"\"Agent 的结构化回复格式。\"\"\"\n    # 幽默的谐音梗回复\n    punny_response: str\n    # 任何关于天气的有趣信息（如果有的话）\n    weather_conditions: str | None = None\n\n# 6. 设置记忆存储\ncheckpointer = InMemorySaver()\n\n# 7. 创建 Agent\nagent = create_agent(\n    model=model,\n    system_prompt=SYSTEM_PROMPT,\n    tools=[get_user_location, get_weather_for_location],\n    context_schema=Context,\n    response_format=ToolStrategy(ResponseFormat),\n    checkpointer=checkpointer\n)\n\n# --- 运行测试 ---\n\n# 设置会话 ID\nconfig = {\"configurable\": {\"thread_id\": \"test_user_001\"}}\n\n# 第一次提问\nprint(\"--- 第一次对话 ---\")\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"外面天气怎么样？\"}]},\n    config=config,\n    context=Context(user_id=\"1\")\n)\n\n# 访问结构化结果\nres = response['structured_response']\nprint(f\"AI: {res.punny_response}\")\nprint(f\"详情: {res.weather_conditions}\")\n\nprint(\"\\n--- 第二次对话（测试记忆） ---\")\n# 第二次提问\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"谢谢你！我在哪里？\"}]},\n    config=config,\n    context=Context(user_id=\"1\")\n)\n\nres = response['structured_response']\nprint(f\"AI: {res.punny_response}\")\n</code></pre>\n<p>这段示例代码的核心逻辑是：</p>\n<p><strong>第一次对话：</strong></p>\n<p>用户问：“外面天气怎么样？”（没说地点）。<br />\nAI 思考：系统提示词告诉我，如果地点不明，先去查用户在哪。<br />\n动作：AI 调用 get_user_location -&gt; 得到“北京” -&gt; 调用 get_weather_for_location -&gt; 得到天气。<br />\n回复：根据 ResponseFormat 吐出一个谐音梗。</p>\n<p><strong>第二次对话：</strong></p>\n<p>用户问：“谢谢你！我在哪里？”<br />\nAI 思考：由于有 thread_id 和 checkpointer，AI 记得上一次查询的结果。<br />\n动作：它不需要再次查工具，直接从记忆里提取信息并回复。</p>\n<p>然后手工将代码中的 user_id 改为2，再次运行，就会根据工具 <code>get_user_location</code> 中的示例逻辑，得到地点为”珠海“，两次执行的效果参考如下：</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/635610/202601/635610-20260114224646132-69223909.jpg\" /></p>\n<p>该段示例代码虽然简单，但其实已经能展现出一个企业级 AI 智能体（Agent）的缩影。它涵盖了现代 AI 开发中最核心的三个支柱：工具调用（Tool Use）、上下文记忆（Memory）、以及结构化输出（Structured Output）。</p>\n\n</div>\n<div id=\"MySignature\">\n    <a href=\"https://www.cnblogs.com/jyzhao/\" target=\"_blank\">AlfredZhao</a>©版权所有「从Oracle起航，领略精彩的IT技术。」<br />\n转载请注明原文链接：<a href=\"https://www.cnblogs.com/jyzhao/p/19484508/xiao-bai-xueai-kai-fa01-chuang-jian-di-yi-ge-shi-l\" target=\"_blank\">https://www.cnblogs.com/jyzhao/p/19484508/xiao-bai-xueai-kai-fa01-chuang-jian-di-yi-ge-shi-l</a>\n<hr />\n<div style=\"text-align: center; margin-top: 30px;\">\n  <p style=\"font-size: 14px; color: #555;\">\n    👋 感谢阅读，欢迎关注我的公众号 <b>「赵靖宇」</b>\n  </p>\n  <img src=\"https://images.cnblogs.com/cnblogs_com/jyzhao/824234/o_250208075013_qrcode-zjy.jpg\" style=\"border: 1px solid #ddd; border-radius: 8px;\" width=\"160\" />\n</div>\n</div>\n<div class=\"clear\"></div>\n\n        <div class=\"clear\"></div>\n        \n</div>\n    <ul class=\"postmetadata\">\n        \n    </ul>"
    },
    {
      "title": "进阶技巧：在Dash应用中直接使用原生React组件",
      "link": "https://www.cnblogs.com/feffery/p/19486989",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/feffery/p/19486989\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 13:56\">\n    <span>进阶技巧：在Dash应用中直接使用原生React组件</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<center style=\"font-size: 18px; font-weight: bold; padding-top: 40px;\">更多Dash应用开发干货知识、案例，欢迎关注“玩转Dash”微信公众号👇</center>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/1344061/202507/1344061-20250703190053776-1837084116.png\" /></p>\n<h1 id=\"1-简介\">1 简介</h1>\n<p>大家好我是费老师。作为一个<code>Python</code>框架，我们日常在使用<code>Dash</code>构建各种应用的过程中，主流常见的功能可以利用<code>Dash</code>生态中丰富的<em>组件库</em>、<em>工具库</em>等资源，通过编写<code>Python</code>代码实现<em>全栈应用开发</em>，也可以额外配合<code>Dash</code>中的<em>浏览器端回调</em>，在<code>Dash</code>中很方便的调用<code>Echarts</code>等<em>原生JavaScript</em>库实现各种<em>特殊功能拓展</em>。</p>\n<p>除了这些常用形式以外，我们还可以基于今天文章中要给大家介绍的<code>Dash</code>插件<code>dash-vite-plugin</code>，实现将非原生<code>JavaScript</code>库譬如<code>React</code>等传统前端框架相关的功能逻辑，轻松整合到我们的<code>Dash</code>应用中，进一步提升<code>Dash</code>应用功能开发的上限🚀~</p>\n<center><img src=\"https://img2024.cnblogs.com/blog/1344061/202601/1344061-20260115134532456-1814849130.jpg\" /></center>\n<h1 id=\"2-dash-vite-plugin插件的使用\">2 dash-vite-plugin插件的使用</h1>\n<p><code>dash-vite-plugin</code>项目地址，欢迎⭐支持我们：</p>\n<ul>\n<li>Github仓库：<a href=\"https://github.com/HogaStack/dash-vite-plugin\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/HogaStack/dash-vite-plugin</a></li>\n<li>Gitee镜像同步仓库：<a href=\"https://gitee.com/insistence2022/dash-vite-plugin\" rel=\"noopener nofollow\" target=\"_blank\">https://gitee.com/insistence2022/dash-vite-plugin</a></li>\n</ul>\n<p>作为<code>Dash</code>应用的插件，我们可以直接通过<code>pip</code>对<code>dash-vite-plugin</code>进行安装：</p>\n<pre><code class=\"language-bash\">pip install dash-vite-plugin -U\n</code></pre>\n<p>完成安装后，我们直接来看几个实际应用案例，它们对应的完整源码你可以在👆上面提到的项目仓库中的<code>examples</code>目录下找到：</p>\n<h2 id=\"示例应用1shinytext特效\">示例应用1：ShinyText特效</h2>\n<p>这个例子基于非常流行的<code>React</code>动画效果库<code>react-bits</code>中的<code>ShinyText</code>组件（ <a href=\"https://reactbits.dev/text-animations/shiny-text\" rel=\"noopener nofollow\" target=\"_blank\">https://reactbits.dev/text-animations/shiny-text</a> ）。</p>\n<p>对应<code>dash-vite-plugin</code>源码仓库中的<code>examples/react-bits-shiny-text-demo</code>项目，<code>python app.py</code>启动后，初始执行需要稍等一会，等待终端提示相关构建完成后，访问本机<code>http://127.0.0.1:8050</code>就可以看到如下效果，完美还原了<code>react-bits</code>中的文字扫光特效组件功能：</p>\n<center><img src=\"https://img2024.cnblogs.com/blog/1344061/202601/1344061-20260115135151996-1596881536.gif\" /></center>\n<h2 id=\"示例应用2lightning特效\">示例应用2：Lightning特效</h2>\n<p>这个例子基于<code>react-bits</code>中的<code>Lightning</code>组件（ <a href=\"https://reactbits.dev/backgrounds/lightning\" rel=\"noopener nofollow\" target=\"_blank\">https://reactbits.dev/backgrounds/lightning</a> ）。</p>\n<p>对应<code>dash-vite-plugin</code>源码仓库中的<code>examples/react-bits-lightning-demo</code>项目，完美还原了<code>react-bits</code>中的雷电动态背景组件效果：</p>\n<center><img src=\"https://img2024.cnblogs.com/blog/1344061/202601/1344061-20260115135155979-1022491345.gif\" /></center>\n<h2 id=\"示例应用3gridscan特效\">示例应用3：GridScan特效</h2>\n<p>这个例子基于<code>react-bits</code>中的<code>GridScan</code>组件（ <a href=\"https://reactbits.dev/backgrounds/grid-scan\" rel=\"noopener nofollow\" target=\"_blank\">https://reactbits.dev/backgrounds/grid-scan</a> ）。</p>\n<p>对应<code>dash-vite-plugin</code>源码仓库中的<code>examples/react-bits-grid-scan-demo</code>项目，完美还原了<code>react-bits</code>中颇具赛博朋克风格的网格扫光背景组件效果：</p>\n<center><img src=\"https://img2024.cnblogs.com/blog/1344061/202601/1344061-20260115135159305-223594499.gif\" /></center>\n<hr />\n<p>有关<code>dash-vite-plugin</code>的使用说明详见项目仓库文档：</p>\n<ul>\n<li>Github仓库：<a href=\"https://github.com/HogaStack/dash-vite-plugin\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/HogaStack/dash-vite-plugin</a></li>\n<li>Gitee镜像同步仓库：<a href=\"https://gitee.com/insistence2022/dash-vite-plugin\" rel=\"noopener nofollow\" target=\"_blank\">https://gitee.com/insistence2022/dash-vite-plugin</a></li>\n</ul>\n<p>更多<code>Dash</code>应用开发可用插件列表详见：</p>\n<ul>\n<li><a href=\"https://github.com/HogaStack/awesome-dash-hooks\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/HogaStack/awesome-dash-hooks</a></li>\n</ul>\n<hr />\n<p>更多有关<code>Dash</code>应用开发的干货内容，欢迎持续关注我们❤️</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 13:56</span>&nbsp;\n<a href=\"https://www.cnblogs.com/feffery\">费弗里</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "吴恩达深度学习课程五：自然语言处理  第一周：循环神经网络  课后习题与代码实践",
      "link": "https://www.cnblogs.com/Goblinscholar/p/19484728",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Goblinscholar/p/19484728\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 13:03\">\n    <span>吴恩达深度学习课程五：自然语言处理  第一周：循环神经网络  课后习题与代码实践</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>此分类用于记录吴恩达深度学习课程的学习笔记。<br />\n课程相关信息链接如下：</p>\n<ol>\n<li>原课程视频链接：<a href=\"https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&amp;from_spmid=playlist.playlist-detail.0.0&amp;is_story_h5=false&amp;mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&amp;plat_id=116&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&amp;share_source=COPY&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1713085655&amp;unique_k=DfBgvFW&amp;up_id=8654113&amp;vd_source=e035e9878d32f414b4354b839a4c31a4\" rel=\"noopener nofollow\" target=\"_blank\">[双语字幕]吴恩达深度学习deeplearning.ai</a></li>\n<li>github课程资料，含课件与笔记:<a href=\"https://github.com/robbertliu/deeplearning.ai-andrewNG\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达深度学习教学资料</a></li>\n<li>课程配套练习（中英）与答案：<a href=\"https://blog.csdn.net/u013733326/article/details/79827273\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达深度学习课后习题与答案</a></li>\n</ol>\n<p>本篇为第五课第一周的课后习题和代码实践部分。</p>\n<hr />\n<h1 id=\"1理论习题\">1.理论习题</h1>\n<p><a href=\"https://blog.csdn.net/u013733326/article/details/80855595\" rel=\"noopener nofollow\" target=\"_blank\">【中英】【吴恩达课后测验】Course 5 - 序列模型 - 第一周测验</a><br />\n本周的理论习题中有一道较为巧妙，我们来看看：</p>\n<blockquote>\n<p>这里有一些GRU的更新方程：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260115002949730-620648117.png\" /><br />\n爱丽丝建议通过移除&nbsp;<span class=\"math inline\">\\(Γ_u\\)</span> 来简化GRU，即设置 <span class=\"math inline\">\\(Γ_u＝1\\)</span>。贝蒂提出通过移除 <span class=\"math inline\">\\(Γ_r\\)</span> 来简化GRU，即设置 <span class=\"math inline\">\\(Γ_r＝1\\)</span>。哪种模型更容易<strong>在梯度不消失问题的情况下训练</strong>，即使在很长的输入序列上也可以进行训练？<br />\n<strong>答案：&nbsp;贝蒂的模型（即移除<span class=\"math inline\">\\(Γ_r\\)</span>），因为对于一个时间步而言，如果<span class=\"math inline\">\\(Γ_u≈0\\)</span>，梯度可以通过时间步反向传播而不会衰减。</strong></p>\n</blockquote>\n<p>我们先回顾一下<a href=\"https://www.cnblogs.com/Goblinscholar/p/19463351\" target=\"_blank\">GRU</a>中两道门的语义：</p>\n<ol>\n<li><strong>重置门</strong>：控制在计算当前候选隐藏状态时，上一时刻隐藏状态是否被抑制。</li>\n<li><strong>更新门</strong>：控制当前隐藏状态中，上一时刻隐藏状态与当前候选隐藏状态的融合比例。</li>\n</ol>\n<p>现在再来看一下题中的两种简化逻辑：</p>\n<ol>\n<li><strong>移除 <span class=\"math inline\">\\(Γ_u\\)</span></strong>：令更新门恒为 1，此时当前隐藏状态完全由候选隐藏状态决定。</li>\n<li><strong>移除 <span class=\"math inline\">\\(Γ_r\\)</span></strong>：令重置门恒为 1，使候选隐藏状态在计算时不再抑制上一时刻的隐藏状态。</li>\n</ol>\n<p>看到这里，我们可能看不出什么问题，因为只看语义，二者都是一种简化，无法直观看出对训练效果影响的程度。<br />\n这道题的关键在于题目中的：<strong>更容易在梯度不消失问题的情况下训练</strong>。<br />\n也就是说，我们要看的应该是<strong>两种简化对梯度传递的影响</strong>。<br />\n这就要回到公式了,我们来看看两种简化后的传播公式：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260115002949669-1104514762.png\" /><br />\n这里就可以很明显的发现：<strong>如果删去更新门，就是删去了时间步间的梯度直连通道</strong>。而删去重置门则不会造成这一点。<br />\n显然，删除更新门会让梯度在时间步间的传播路径更长，梯度只能通过非线性变换反向传播，更容易衰减，自然就更容易造成梯度消失，由此得到答案。</p>\n<h1 id=\"2代码实践\">2.代码实践</h1>\n<p><a href=\"https://blog.csdn.net/u013733326/article/details/80890454\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达课程5 RNN搭建与应用</a><br />\n依旧还是先摆上这位博主的链接，这篇博客非常详细地展示了手动搭建 RNN、LSTM 等本周理论内容的过程。<br />\n我们依旧还是用成熟框架来演示本周所了解的内容，<strong>任务类型为命名实体识别</strong>，涉及模型列举如下：</p>\n<ol>\n<li><strong>单层单向 RNN</strong></li>\n<li><strong>单层双向 RNN</strong></li>\n<li><strong>单层双向 GRU</strong></li>\n<li><strong>单层双向 LSTM</strong></li>\n<li><strong>多层双向 LSTM</strong></li>\n</ol>\n<h2 id=\"21-数据集conll-2003\">2.1 数据集：CoNLL-2003</h2>\n<p>在本节的代码实践中，我们选用 <strong>CoNLL-2003 命名实体识别数据集</strong> 作为数据集。<br />\n它是序列标注任务中最经典、也最常用的数据集之一，是一个<strong>词级别的序列标注数据集</strong>。</p>\n<p>需要补充的是，关于命名实体识别任务，我们在之前的<a href=\"https://www.cnblogs.com/Goblinscholar/p/19449622\" target=\"_blank\">理论部分</a>里仅使用 0/1 来进行人名的识别，但实际上的命名实体识别显然内容更丰富，并且对于各类实体的标签标注，也有一套主流的成熟标注体系，被称为 <strong>BIO 标注体系</strong>。<br />\n并不复杂，BIO 的三个核心符号含义如下：</p>\n<ul>\n<li><strong>B-类型</strong>（Begin）：表示这个词是一个实体的<strong>开头</strong>（第一个词）。</li>\n<li><strong>I-类型</strong>（Inside）：表示这个词是实体的<strong>内部</strong>（非第一个词）。</li>\n<li><strong>O</strong>（Outside）：表示这个词<strong>不属于</strong>任何实体。</li>\n</ul>\n<p><strong>其中，B、I 后会接任务定义的实体命名。</strong><br />\n来看一个实例：<strong>河北 2026 年 1 月 14 日 天气 很好</strong></p>\n<table>\n<thead>\n<tr>\n<th>词</th>\n<th>标签</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>河北</td>\n<td>B-LOC</td>\n<td>地名的开头</td>\n</tr>\n<tr>\n<td>2026</td>\n<td>B-TIME</td>\n<td>时间表达的开头</td>\n</tr>\n<tr>\n<td>年</td>\n<td>I-TIME</td>\n<td>时间的内部</td>\n</tr>\n<tr>\n<td>1</td>\n<td>I-TIME</td>\n<td>时间的内部</td>\n</tr>\n<tr>\n<td>月</td>\n<td>I-TIME</td>\n<td>时间的内部</td>\n</tr>\n<tr>\n<td>14</td>\n<td>I-TIME</td>\n<td>时间的内部</td>\n</tr>\n<tr>\n<td>日</td>\n<td>I-TIME</td>\n<td>时间的内部</td>\n</tr>\n<tr>\n<td>天气</td>\n<td>O</td>\n<td>非实体</td>\n</tr>\n<tr>\n<td>很</td>\n<td>O</td>\n<td>非实体</td>\n</tr>\n<tr>\n<td>好</td>\n<td>O</td>\n<td>非实体</td>\n</tr>\n</tbody>\n</table>\n<p>CoNLL-2003 同样采用 <strong>BIO 标注体系</strong>，并定义了四类命名实体如下：</p>\n<table>\n<thead>\n<tr>\n<th>实体类型</th>\n<th>含义</th>\n<th>BIO 标签示例</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>PER</td>\n<td>人名（Person）</td>\n<td><code>B-PER</code>, <code>I-PER</code></td>\n<td>表示人物姓名及其组成部分</td>\n</tr>\n<tr>\n<td>ORG</td>\n<td>组织机构（Organization）</td>\n<td><code>B-ORG</code>, <code>I-ORG</code></td>\n<td>公司、政府机构、学校等</td>\n</tr>\n<tr>\n<td>LOC</td>\n<td>地名（Location）</td>\n<td><code>B-LOC</code>, <code>I-LOC</code></td>\n<td>国家、城市、地区等地理名称</td>\n</tr>\n<tr>\n<td>MISC</td>\n<td>其他专有名词（Miscellaneous）</td>\n<td><code>B-MISC</code>, <code>I-MISC</code></td>\n<td>其他不属于上述类别的专名</td>\n</tr>\n<tr>\n<td>O</td>\n<td>非实体</td>\n<td><code>O</code></td>\n<td>不属于任何命名实体的普通词</td>\n</tr>\n</tbody>\n</table>\n<p>了解了 CoNLL-2003 这类数据集的序列标注逻辑后，回到数据本身：<br />\nCoNLL-2003 数据集已经被官方划分为训练集、验证集和测试集三部分，其中：</p>\n<ul>\n<li><strong>训练集</strong>：约 <strong>14,000</strong> 句。</li>\n<li><strong>验证集</strong>：约 <strong>3,000</strong> 句。</li>\n<li><strong>测试集</strong>：约 <strong>3,500</strong> 句。</li>\n</ul>\n<p>总计包含 <strong>20,000+</strong> 条标注句子，词级样本数量在 <strong>20 万级别</strong>。</p>\n<p>CoNLL-2003 的下载接口同样在被封装在 python 的许多第三方库中，其中一个选择是 PyTorch 官方维护的一个 NLP 工具库：<code>torchtext</code> ：</p>\n<pre><code class=\"language-python\">pip install torchtext\n</code></pre>\n<p>安装后，便可以这样导入CoNLL-2003 的下载接口：</p>\n<pre><code class=\"language-python\">from torchtext.datasets import CoNLL2003\n</code></pre>\n<p><strong>但是，由于<code>torchtext</code>并非<code>torch</code> 的子模块，需要单独安装，且高度依赖和<code>torch</code>间的版本匹配，因此在实际安装中可能出现很多兼容性问题。</strong><br />\n因此，我们选择另一个兼容性更好且实际上在这类使用上也更流行的库：HuggingFace 的 <code>datasets</code>:</p>\n<pre><code class=\"language-python\">pip install datasets\n</code></pre>\n<p>安装后，便可以这样导入：</p>\n<pre><code class=\"language-python\">from datasets import load_dataset\n</code></pre>\n<p>这样，我们就完成了数据集本身的准备，打印几条示例数据如下：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260115002949955-1373934732.png\" /></p>\n<h2 id=\"22-数据预处理\">2.2 数据预处理</h2>\n<p>完成数据集准备后，自然下一步就要开始让数据输入模型，但序列数据不同于我们之前的图像，统一预处理后就可以输入网络，在 RNN 中，我们需要在数据预处理中花更多功夫，具体如下：</p>\n<h4 id=\"1导入相关库\">（1）导入相关库</h4>\n<p>开局当然还是先导库，在 NLP 相关任务中，这些相关库都很常见，尤其是python内置的<code>collections</code>库，我们常用它来<strong>统计语料库，构建词典</strong>。</p>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence  # 用于对不同长度的序列进行填充，使得批次张量对齐。\nfrom datasets import load_dataset # 导入 CoNLL-2003 数据集。\nfrom collections import Counter # 用于统计词频和标签频率，便于构建词表和标签表。\nfrom torch.utils.data import DataLoader # 标配，提供批次化处理和打乱数据集功能。\n</code></pre>\n<h4 id=\"2加载数据集\">（2）加载数据集</h4>\n<p>这步很简单，就不多解释了：</p>\n<pre><code class=\"language-python\"># 使用 HuggingFace datasets 加载数据\ndataset = load_dataset(\"conll2003\")\n# 分别获取训练集、验证集、测试集\ntrain_data = dataset['train']\nvalid_data = dataset['validation']\ntest_data = dataset['test']\n</code></pre>\n<h4 id=\"3构建词表和标签表\">（3）构建词表和标签表</h4>\n<p>就像我们在<a href=\"https://www.cnblogs.com/Goblinscholar/p/19437798\" target=\"_blank\">理论部分</a>介绍的，在做序列任务中，我们需要把文本和标签都转成索引形式，即计算机能理解的词典。<br />\n这里我们先用 <code>Counter</code> 统计词频和标签频率，然后构建词表和标签表。<br />\n在这里需要补充的内容是<strong>填充符<code>&lt;PAD&gt;</code></strong>,因为模型需要固定维度的输入，而序列往往长度不同，因此我们需要<strong>对不同长度的序列进行填充</strong>来适配模型输入。<br />\n举个简单的例子：</p>\n<table>\n<thead>\n<tr>\n<th>填充前</th>\n<th>填充后</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Hello my firend</td>\n<td>Hello my firend</td>\n</tr>\n<tr>\n<td>Hi</td>\n<td>Hi <code>&lt;PAD&gt;``&lt;PAD&gt;</code></td>\n</tr>\n</tbody>\n</table>\n<p>同样，当输入序列中出现<code>&lt;PAD&gt;</code>时，其对应的标签也是<code>&lt;PAD&gt;</code>。<br />\n在代码中，我们一般设置<strong>输入</strong>序列中的<code>&lt;PAD&gt;</code>的 ID 为 0，但在<strong>标签</strong>序列中则不同，我们一般设定为一个特殊值，比如下面的 -100，<strong>这个特殊值会被特殊记录，在计算损失时会忽略该位置的损失计算。</strong><br />\n因为它本身的作用就是填充序列长度，模型不需要在这里浪费计算性能，也无法从这里学习信息。<br />\n编码如下：</p>\n<pre><code class=\"language-python\"># 统计词频和标签频率\n# Counter()：用来统计元素出现次数的字典\nword_counter = Counter() \ntag_counter = Counter()\n\nfor item in train_data:\n    words = item['tokens']      # 单词列表\n    tags = item['ner_tags']     # 标签索引\n    tag_counter.update(tags)    # 统计标签\n    word_counter.update(words)  # 统计词频\n\n# 构建词表\nword_vocab = {w: i + 2 for i, (w, _) in enumerate(word_counter.most_common())}\n# word_counter.most_common() ：返回按词频从高到低排列的 (单词, 出现次数) 列表。\n# i + 2 我们给单词分配整数 ID，从 2 开始。\n# {w: i + 2 for i, (w, _) in ...} ：通过字典推导式，把每个单词 w 映射为对应的整数 ID。\n# 最终 word_vocab = {'I':2, 'love':3, 'Python':4, ...}\nword_vocab[\"&lt;PAD&gt;\"] = 0   # 填充符\nword_vocab[\"&lt;UNK&gt;\"] = 1   # 未知词\n\n# 构建标签表\ntag_vocab = {t: t for t in tag_counter.keys()}  # 标签索引直接映射自己\npad_tag_id = -100        # 定义 PAD 标签索引，在后续损失函数参数中使用。\nnum_classes = len(tag_vocab) # 预测类别，定义输出层\nvocab_size = len(word_vocab)  # 用于独热编码\n</code></pre>\n<h4 id=\"4定义编码函数\">（4）定义编码函数</h4>\n<p>定义好好词表和标签表本身后，我们还需要定义第一个编码函数，它的作用就是<strong>把文本信息对照词典转换为计算机能理解的编码。</strong> 我们下一步就会通过这个函数来处理数据集。<br />\n同时，这一步还像是之前卷积网络中的 transform 方法，把在输入模型前，把数据转换成 PyTorch 要求的Tensor 张量。<br />\n不过和图像不同的是，这里处理的是文本序列。而 transform 针对的是图像数据，且处理逻辑也不同，因此并不能在这里直接使用。</p>\n<pre><code class=\"language-python\"># 将单条样本的单词和标签编码为 Tensor\ndef encode(item):\n    words = item['tokens']\n    tags = item['ner_tags'] \n    x = torch.tensor([word_vocab.get(w, 1) for w in words], dtype=torch.long) \n    # 这段逻辑是，对于每个词：如果在词表里找到就返回其索引，如果没找到，就返回 1，即 &lt;UNK&gt; 的索引\n    y = torch.tensor(tags, dtype=torch.long)  \n    # 标签索引\n    return x, y\n</code></pre>\n<h4 id=\"4数据填充和构建迭代器\">（4）数据填充和构建迭代器</h4>\n<p>到这一步，我们已经拥有了词典和相应的编码函数，自然，我们可以使用它们<strong>对数据进行预处理</strong>了，这一步的工作分为三部分：</p>\n<ol>\n<li><strong>定义填充函数。</strong></li>\n<li><strong>使用编码函数对划分好的数据进行编码。</strong></li>\n<li><strong>使用编码好的数据与填充函数，完成最终的批次迭代器定义。</strong></li>\n</ol>\n<pre><code class=\"language-python\"># 自定义填充函数\ndef collate_fn(batch): # 参数为一批次数据 \n    xs, ys = zip(*batch) # 把（样本，标签）拆成（样本，样本）和（标签，标签）\n    xs_pad = pad_sequence(xs, batch_first=True, padding_value=word_vocab[\"&lt;PAD&gt;\"])     # pad_sequence 方法会自动找到最长序列并补充填充同一批次中其他序列到该长度。\n    # 不同批次的填充长度可以不同。\n    ys_pad = pad_sequence(ys, batch_first=True, padding_value=pad_tag_id)  \n    return xs_pad.to(device), ys_pad.to(device)  \n    \n# 对所有数据进行编码\ntrain_dataset = [encode(item) for item in train_data]  \nvalid_dataset = [encode(item) for item in valid_data]  \ntest_dataset  = [encode(item) for item in test_data] \n \n# 使用编码好的数据划分批次，进行填充 \ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)  \nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)  \ntest_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n</code></pre>\n<p>至此，我们才完成了在数据集在输入模型前的预处理工作，下一步，自然就是要定义模型了。</p>\n<h2 id=\"23-定义-rnn-模型\">2.3 定义 RNN 模型</h2>\n<p>同样，对于 RNN ，PyTorch 也有封装好的方法，我们定义要用的 RNN 模型如下：</p>\n<pre><code class=\"language-python\">class RNNTagger(nn.Module):  \n    def __init__(self, vocab_size, hidden_dim, num_classes,  \n                 rnn_type='RNN', bidirectional=False, num_layers=1):\n        # 使用多个参数，方便我们调用不同模型,默认为单层单向 RNN\n        super().__init__()  \n        self.vocab_size = vocab_size  # 词表\n        self.bidirectional = bidirectional  # 是否双向\n        self.rnn_type = rnn_type.upper()  # 使用的循环单元\n         \n        input_size = vocab_size  # 使用独热编码，独热编码输入维度 = 词表大小  \n        # 使用普通 RNN\n        if self.rnn_type == 'RNN':  \n            self.rnn = nn.RNN(input_size, hidden_dim, batch_first=True,  \n                              bidirectional=bidirectional, num_layers=num_layers)\n        # 使用 LSTM  \n        elif self.rnn_type == 'LSTM':  \n            self.rnn = nn.LSTM(input_size, hidden_dim, batch_first=True,  \n                               bidirectional=bidirectional, num_layers=num_layers)         # 使用 GRU\n        elif self.rnn_type == 'GRU':  \n            self.rnn = nn.GRU(input_size, hidden_dim, batch_first=True,  \n                              bidirectional=bidirectional, num_layers=num_layers)  \n        else:  \n            raise ValueError(\"rnn_type must be 'RNN','LSTM','GRU'\")  \n        # 输出层\n        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)  \n    def forward(self, x):  \n\t# x: [batch, seq_len], 转独热  \n\t    x_onehot = torch.nn.functional.one_hot(x, num_classes=self.vocab_size).float()  \n\t    out, _ = self.rnn(x_onehot)  #在这里就封装了按时间步传播的逻辑\n\t    out = self.fc(out)  \n\t    return out\n</code></pre>\n<p>在这里，你会发现，RNN、LSTM、GRU 都被封装成了单独的模型，我们只需要更改其参数，即可修改其深度和是否双向等。<br />\n最后，就是定义相应训练和验证逻辑了。</p>\n<h2 id=\"24-训练与验证\">2.4 训练与验证</h2>\n<p>我们这次把整个训练与验证逻辑也定义为函数，方便之后使用不同的模型调用：</p>\n<pre><code class=\"language-python\">def train_validate(model, train_loader, valid_loader, epochs=5, lr=0.001):\n    model.to(device)\n     # 在损失函数种使用ignore_index参数来忽略对&lt;PAD&gt;的损失计算\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_tag_id)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    train_loss_history = []\n    train_acc_history  = []\n    val_acc_history    = []\n    val_f1_history     = []\n\n    epoch_times = []\n    total_start_time = time.time()\n\n    for epoch in range(epochs):\n        epoch_start_time = time.time()\n\n        # ========= 训练 =========\n        model.train()\n        total_loss = 0\n        total_correct = 0\n        total_tokens = 0\n\n        for x_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(x_batch)\n\n            outputs_reshape = outputs.view(-1, num_classes)\n            y_batch_reshape = y_batch.view(-1)\n\n            loss = criterion(outputs_reshape, y_batch_reshape)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            preds = outputs.argmax(dim=-1)\n            for i in range(y_batch.size(0)):\n                mask = y_batch[i] != pad_tag_id\n                total_correct += (preds[i][mask] == y_batch[i][mask]).sum().item()\n                total_tokens += mask.sum().item()\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_acc = total_correct / total_tokens\n\n        train_loss_history.append(avg_train_loss)\n        train_acc_history.append(train_acc)\n\n        # ========= 验证 =========\n        model.eval()\n        val_total_correct = 0\n        val_total_tokens = 0\n\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for x_val, y_val in valid_loader:\n                outputs = model(x_val)\n                preds = outputs.argmax(dim=-1)\n\n                for i in range(y_val.size(0)):\n                    mask = y_val[i] != pad_tag_id\n\n                    val_total_correct += (preds[i][mask] == y_val[i][mask]).sum().item()\n                    val_total_tokens += mask.sum().item()\n\n                    all_preds.extend(preds[i][mask].cpu().tolist())\n                    all_labels.extend(y_val[i][mask].cpu().tolist())\n\n        val_acc = val_total_correct / val_total_tokens\n\n        # ⭐ token-level F1（macro）\n        val_f1 = f1_score(all_labels, all_preds, average='macro')\n\n        val_acc_history.append(val_acc)\n        val_f1_history.append(val_f1)\n\n        epoch_time = time.time() - epoch_start_time\n        epoch_times.append(epoch_time)\n\n        print(\n            f\"轮次 {epoch+1}: \"\n            f\"训练损失={avg_train_loss:.4f}, \"\n            f\"训练准确率={train_acc:.4f}, \"\n            f\"验证准确率={val_acc:.4f}, \"\n            f\"验证F1={val_f1:.4f}, \"\n            f\"本轮用时={epoch_time:.2f} 秒\"\n        )\n\n    total_time = time.time() - total_start_time\n    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n\n    print(\"\\n======== 训练时间统计 ========\")\n    print(f\"总训练时间：{total_time:.2f} 秒\")\n    print(f\"平均每轮用时：{avg_epoch_time:.2f} 秒\")\n\n    history = {\n        'train_loss': train_loss_history,\n        'train_acc': train_acc_history,\n        'val_acc': val_acc_history,\n        'val_f1': val_f1_history\n    }\n    return model, history\n\n</code></pre>\n<p>自此，我们终于完成了主要的代码工作，下面就来运行看看吧。</p>\n<h2 id=\"25-运行结果\">2.5 运行结果</h2>\n<h4 id=\"1-单层单向-rnn\">(1) 单层单向 RNN</h4>\n<p>首先，先试试最基础的单层单向 RNN,我们在主函数里这样调用它：</p>\n<pre><code class=\"language-python\">if __name__ == '__main__':  \n    # 定义参数  \n    cfg = {'rnn_type':'RNN','bidirectional':False,'num_layers':1}\n    # 传入模型   \n    model = RNNTagger(vocab_size=len(word_vocab),   \n                      hidden_dim=128,   \n                      num_classes=num_classes,  \n                      rnn_type=cfg['rnn_type'],   \n                      bidirectional=cfg['bidirectional'],  \n                      num_layers=cfg['num_layers']) \n    # 进行训练\n    model, history = train_validate(model, train_loader, valid_loader, epochs=5)      # 可视化\n    plot_training_curves(history) \n</code></pre>\n<p>需要强调的是：<strong>RNN 通常不需要设置过多的训练轮次</strong>，这是因为其训练采用时间反向传播，梯度需要沿时间维度逐步传递，时间步一长就容易出现梯度消失或爆炸的问题，使得后期训练收益迅速降低。<br />\n实践中，RNN 往往在前几轮就学到主要的序列模式，继续增加 epoch 不仅提升有限，还可能导致验证性能波动甚至过拟合，因此相比其他模型，RNN 的训练轮次通常设置得相对较少。</p>\n<p>现在来看看运行结果：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260115002949968-1870151032.png\" /><br />\n这里有一个问题，你会发现：<strong>F1 分数明显要比准确率低一截。</strong></p>\n<p>原因在于，<strong>序列标注任务中的类别分布通常是极度不均衡的</strong>。以命名实体识别为例，大部分词实际上都属于非实体类别（如 <code>O</code>），模型即使只是“保守地”把绝大多数位置预测为 <code>O</code>，也能获得相当可观的准确率。这种情况下，<strong>准确率更多反映的是模型对“多数类”的拟合能力，而非对实体边界和类别的真实识别能力</strong>。<br />\n相比之下，<strong>F1 分数同时考虑了精确率和召回率</strong>，只有当模型既能正确识别实体、又不过度漏掉实体时，F1 才会提高。因此，在命名实体识别这类关注“少数但关键结构”的任务中，F1 能更真实地刻画模型的实际效果。<br />\n也正因如此，<strong>在序列标注任务中，准确率往往具有较强的迷惑性，而 F1 才是更具判别力、也更被广泛采用的核心评价指标</strong>。<br />\n我们继续，看看其他模型的表现。</p>\n<h4 id=\"2单层双向-rnn\">（2）单层双向 RNN</h4>\n<p>第二个实验对象，单层双向 RNN，我们只需要更改一个参数：</p>\n<pre><code class=\"language-python\">if __name__ == '__main__':  \n    # 定义参数: 'bidirectional':True\n    cfg = {'rnn_type':'RNN','bidirectional':True,'num_layers':1}\n    # 传入模型   \n    model = RNNTagger(vocab_size=len(word_vocab),   \n                      hidden_dim=128,   \n                      num_classes=num_classes,  \n                      rnn_type=cfg['rnn_type'],   \n                      bidirectional=cfg['bidirectional'],  \n                      num_layers=cfg['num_layers']) \n    # 进行训练\n    model, history = train_validate(model, train_loader, valid_loader, epochs=5)      # 可视化\n    plot_training_curves(history) \n</code></pre>\n<p>来看结果：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260115002950498-1918940564.png\" /><br />\n很明显，验证集上的 F1 分数有所提升从之前的 79% 左右到了 83% 左右，但随着计算量的增加，训练用时也相应的增加了约 20%。</p>\n<p>我们继续。</p>\n<h4 id=\"3单层双向-gru\">（3）单层双向 GRU</h4>\n<p>第三个实验对象，单层双向 GRU，同样修改参数如下：</p>\n<pre><code class=\"language-python\">cfg = {'rnn_type': 'GRU', 'bidirectional': True, 'num_layers': 1}  \n</code></pre>\n<p>来看结果：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260115003322668-1088493097.png\" /><br />\n观察结果，你会发现，GRU 再次实现了 F1 分数的提升，但同时，更加复杂的循环单元也让其运行时间大幅提升。</p>\n<h4 id=\"4单层双向-lstm\">（4）单层双向 LSTM</h4>\n<p>继续，来看看LSTM 的表现：</p>\n<pre><code class=\"language-python\">cfg = {'rnn_type': 'LSTM', 'bidirectional': True, 'num_layers': 1}  \n</code></pre>\n<p>结果如下：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260115002950457-1403414203.png\" /></p>\n<p>结果显示：<strong>在当前设置下， LSTM 并不如 GRU</strong>。<br />\n其实原因也比较直观，LSTM 并不是一定比 GRU 好，它只是“上限更高、成本也更高”，同时也<strong>更难训练</strong>，我们继续下一步来看看：</p>\n<h4 id=\"5多层双向-lstm\">（5）多层双向 LSTM</h4>\n<p>最后一次，我们选择增加 LSTM 的深度，同时增加轮次为10。</p>\n<pre><code class=\"language-python\">cfg = {'rnn_type': 'LSTM', 'bidirectional': True, 'num_layers': 3}  \n</code></pre>\n<p>结果如下：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260115002950526-729492931.png\" /><br />\n结果好像也并没有像我们所想一样有所提升，这也确实说明了，<strong>更复杂的模型，需要更多的训练技巧。</strong></p>\n<p>我们的演示部分就到这里。<br />\n你会发现，单层单向 RNN 训练快、准确率高但 F1 偏低，说明对少数类识别有限。<br />\n单层双向 RNN 利用前后文信息提升了 F1，但训练时间也有所提升。<br />\n单层双向 GRU 在保持训练稳定的同时进一步提升了 F1，性价比最好。<br />\n单层双向 LSTM 参数多、训练难度高，在当前设置下不如 GRU，多层双向 LSTM 即便增加轮次和深度，F1 仍未显著提升，说明复杂模型需要更精细的训练策略。<br />\n<strong>总体来看，对于中等规模的序列标注任务，浅层 GRU 是最稳健的选择。</strong></p>\n<p>当然，这只是针对我们现在所介绍的技术而言，下一周的内容关于词嵌入，是相比独热编码而言，更加合适的文本编码技术，到时我们再来看看各个模型的性能如何。</p>\n<h1 id=\"3-附录\">3. 附录</h1>\n<h2 id=\"31-rnn-模型训练代码-pytorch版\">3.1 RNN 模型训练代码-PyTorch版</h2>\n<pre><code class=\"language-python\">import time  \nimport torch  \nimport torch.nn as nn  \nfrom torch.nn.utils.rnn import pad_sequence  \nfrom datasets import load_dataset  \nfrom collections import Counter  \nfrom torch.utils.data import DataLoader  \nfrom sklearn.metrics import f1_score  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n  \ndataset = load_dataset(\"conll2003\")  \ntrain_data = dataset['train']  \nvalid_data = dataset['validation']  \ntest_data  = dataset['test']  \n  \nword_counter = Counter()  \ntag_counter = Counter()  \n  \nfor item in train_data:  \n    word_counter.update(item['tokens'])  \n    tag_counter.update(item['ner_tags'])  \n  \n# 词表\nword_vocab = {w:i+2 for i,(w,_) in enumerate(word_counter.most_common())}  \nword_vocab[\"&lt;PAD&gt;\"] = 0  \nword_vocab[\"&lt;UNK&gt;\"] = 1  \n  \n# 标签表  \ntag_vocab = {t:t for t in tag_counter.keys()}  \npad_tag_id = -100  \nnum_classes = len(tag_vocab)  \nvocab_size = len(word_vocab)  # 用于独热编码  \n  \n \n# 编码函数  \ndef encode(item):  \n    x = torch.tensor([word_vocab.get(w,1) for w in item['tokens']], dtype=torch.long)  \n    y = torch.tensor(item['ner_tags'], dtype=torch.long)  \n    return x, y  \n  \n\n# DataLoader + 填充函数  \ndef collate_fn(batch):  \n    xs, ys = zip(*batch)  \n    xs_pad = pad_sequence(xs, batch_first=True, padding_value=word_vocab[\"&lt;PAD&gt;\"])  \n    ys_pad = pad_sequence(ys, batch_first=True, padding_value=pad_tag_id)  \n    return xs_pad.to(device), ys_pad.to(device)  \n  \ntrain_dataset = [encode(item) for item in train_data]  \nvalid_dataset = [encode(item) for item in valid_data]  \ntest_dataset  = [encode(item) for item in test_data]  \n  \ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)  \nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)  \ntest_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)  \n  \n\n# RNN 模型  \nclass RNNTagger(nn.Module):  \n    def __init__(self, vocab_size, hidden_dim, num_classes,  \n                 rnn_type='RNN', bidirectional=False, num_layers=1):  \n        super().__init__()  \n        self.vocab_size = vocab_size  \n        self.bidirectional = bidirectional  \n        self.rnn_type = rnn_type.upper()  \n  \n        input_size = vocab_size  # 独热编码输入维度 = 词表大小  \n  \n        if self.rnn_type == 'RNN':  \n            self.rnn = nn.RNN(input_size, hidden_dim, batch_first=True,  \n                              bidirectional=bidirectional, num_layers=num_layers)  \n        elif self.rnn_type == 'LSTM':  \n            self.rnn = nn.LSTM(input_size, hidden_dim, batch_first=True,  \n                               bidirectional=bidirectional, num_layers=num_layers)  \n        elif self.rnn_type == 'GRU':  \n            self.rnn = nn.GRU(input_size, hidden_dim, batch_first=True,  \n                              bidirectional=bidirectional, num_layers=num_layers)  \n        else:  \n            raise ValueError(\"rnn_type must be 'RNN','LSTM','GRU'\")  \n  \n        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes)  \n  \n    def forward(self, x):  \n        # x: [batch, seq_len], 转独热  \n        x_onehot = torch.nn.functional.one_hot(x, num_classes=self.vocab_size).float()  \n        out, _ = self.rnn(x_onehot) # 过RNN  \n        out = self.fc(out) #过输出层  \n        return out  \n  \n  \nimport matplotlib.pyplot as plt  \nfrom matplotlib import rcParams  \n  \n \nrcParams['font.sans-serif'] = ['SimHei']    \nrcParams['axes.unicode_minus'] = False    \n  \n\n# 训练 + 验证函数\ndef train_validate(model, train_loader, valid_loader, epochs=5, lr=0.001):  \n    model.to(device)  \n    criterion = nn.CrossEntropyLoss(ignore_index=pad_tag_id)  \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  \n  \n    train_loss_history = []  \n    train_acc_history  = []  \n    val_acc_history    = []  \n    val_f1_history     = []  \n  \n    epoch_times = []  \n    total_start_time = time.time()  \n  \n    for epoch in range(epochs):  \n        epoch_start_time = time.time()  \n  \n        # ========= 训练 =========        \n        model.train()  \n        total_loss = 0  \n        total_correct = 0  \n        total_tokens = 0  \n  \n        for x_batch, y_batch in train_loader:  \n            optimizer.zero_grad()  \n            outputs = model(x_batch)  \n  \n            outputs_reshape = outputs.view(-1, num_classes)  \n            y_batch_reshape = y_batch.view(-1)  \n  \n            loss = criterion(outputs_reshape, y_batch_reshape)  \n            loss.backward()  \n            optimizer.step()  \n            total_loss += loss.item()  \n  \n            preds = outputs.argmax(dim=-1)  \n            for i in range(y_batch.size(0)):  \n                mask = y_batch[i] != pad_tag_id  \n                total_correct += (preds[i][mask] == y_batch[i][mask]).sum().item()  \n                total_tokens += mask.sum().item()  \n  \n        avg_train_loss = total_loss / len(train_loader)  \n        train_acc = total_correct / total_tokens  \n  \n        train_loss_history.append(avg_train_loss)  \n        train_acc_history.append(train_acc)  \n  \n        # ========= 验证 =========        \n        model.eval()  \n        val_total_correct = 0  \n        val_total_tokens = 0  \n  \n        all_preds = []  \n        all_labels = []  \n  \n        with torch.no_grad():  \n            for x_val, y_val in valid_loader:  \n                outputs = model(x_val)  \n                preds = outputs.argmax(dim=-1)  \n  \n                for i in range(y_val.size(0)):  \n                    mask = y_val[i] != pad_tag_id  \n  \n                    val_total_correct += (preds[i][mask] == y_val[i][mask]).sum().item()  \n                    val_total_tokens += mask.sum().item()  \n  \n                    all_preds.extend(preds[i][mask].cpu().tolist())  \n                    all_labels.extend(y_val[i][mask].cpu().tolist())  \n  \n        val_acc = val_total_correct / val_total_tokens  \n  \n      \n        val_f1 = f1_score(all_labels, all_preds, average='macro')  \n  \n        val_acc_history.append(val_acc)  \n        val_f1_history.append(val_f1)  \n  \n        epoch_time = time.time() - epoch_start_time  \n        epoch_times.append(epoch_time)  \n  \n        print(  \n            f\"轮次 {epoch+1}: \"  \n            f\"训练损失={avg_train_loss:.4f}, \"  \n            f\"训练准确率={train_acc:.4f}, \"  \n            f\"验证准确率={val_acc:.4f}, \"  \n            f\"验证F1={val_f1:.4f}, \"  \n            f\"本轮用时={epoch_time:.2f} 秒\"  \n        )  \n  \n    total_time = time.time() - total_start_time  \n    avg_epoch_time = sum(epoch_times) / len(epoch_times)  \n  \n    print(\"\\n======== 训练时间统计 ========\")  \n    print(f\"总训练时间：{total_time:.2f} 秒\")  \n    print(f\"平均每轮用时：{avg_epoch_time:.2f} 秒\")  \n  \n    history = {  \n        'train_loss': train_loss_history,  \n        'train_acc': train_acc_history,  \n        'val_acc': val_acc_history,  \n        'val_f1': val_f1_history  \n    }  \n    return model, history  \n  \n  \n  \n\ndef plot_training_curves(history):  \n    epochs = range(1, len(history['train_loss']) + 1)  \n    fig, ax1 = plt.subplots(figsize=(10, 5))    \n    ax1.plot(epochs, history['train_loss'], 'b-o', label='训练损失')  \n    ax1.set_xlabel(\"轮次\", fontsize=12)  \n    ax1.set_ylabel(\"训练损失\", color='b', fontsize=12)  \n    ax1.tick_params(axis='y', labelcolor='b')  \n    ax1.grid(True)  \n    ax2 = ax1.twinx()  \n    ax2.plot(epochs, history['train_acc'], 'g-o', label='训练准确率')  \n    ax2.plot(epochs, history['val_acc'],   'r-o', label='验证准确率')  \n    ax2.plot(epochs, history['val_f1'],    'm-o', label='验证 F1')  \n    ax2.set_ylabel(\"指标值\", fontsize=12)  \n    ax2.tick_params(axis='y')   \n    lines1, labels1 = ax1.get_legend_handles_labels()  \n    lines2, labels2 = ax2.get_legend_handles_labels()  \n    ax2.legend(lines1 + lines2, labels1 + labels2, loc='lower right')  \n    plt.title(\"训练损失、准确率与 F1 曲线对比\", fontsize=14)  \n    plt.tight_layout()  \n    plt.show()  \n  \nif __name__ == '__main__':  \n    # cfg = {'rnn_type': 'RNN', 'bidirectional': False, 'num_layers': 1}  \n    # cfg = {'rnn_type':'RNN','bidirectional':True,'num_layers':1}    # cfg = {'rnn_type': 'GRU', 'bidirectional': True, 'num_layers': 1}    # cfg = {'rnn_type': 'LSTM', 'bidirectional': True, 'num_layers': 1}    cfg = {'rnn_type': 'LSTM', 'bidirectional': True, 'num_layers': 3}  \n    model = RNNTagger(vocab_size=len(word_vocab),  \n                      hidden_dim=128,  \n                      num_classes=num_classes,  \n                      rnn_type=cfg['rnn_type'],  \n                      bidirectional=cfg['bidirectional'],  \n                      num_layers=cfg['num_layers'])  \n    model, history = train_validate(model, train_loader, valid_loader, epochs=10)  \n    plot_training_curves(history)\n</code></pre>\n<h2 id=\"32-rnn-模型训练代码-tf版\">3.2 RNN 模型训练代码-TF版</h2>\n<pre><code class=\"language-python\">import time  \nimport numpy as np  \nimport tensorflow as tf  \nfrom tensorflow.keras.layers import Layer, SimpleRNN, GRU, LSTM, Dense, Bidirectional, Input  \nfrom tensorflow.keras.models import Model  \nfrom tensorflow.keras.optimizers import Adam  \nfrom datasets import load_dataset  \nfrom collections import Counter  \nfrom sklearn.metrics import f1_score  \nimport matplotlib.pyplot as plt  \nfrom matplotlib import rcParams  \n  \n  \nrcParams['font.sans-serif'] = ['SimHei']  \nrcParams['axes.unicode_minus'] = False  \n  \ndataset = load_dataset(\"conll2003\")  \ntrain_data = dataset['train']  \nvalid_data = dataset['validation']  \ntest_data  = dataset['test']  \n  \nword_counter = Counter()  \ntag_counter = Counter()  \n  \nfor item in train_data:  \n    word_counter.update(item['tokens'])  \n    tag_counter.update(item['ner_tags'])  \n  \n# 词表  \nword_vocab = {w:i+2 for i,(w,_) in enumerate(word_counter.most_common())}  \nword_vocab[\"&lt;PAD&gt;\"] = 0  \nword_vocab[\"&lt;UNK&gt;\"] = 1  \nvocab_size = len(word_vocab)  \n  \n# 标签表  \ntag_vocab = {t:t for t in tag_counter.keys()}  \nnum_classes = len(tag_vocab)  \npad_tag_id = -100  # 训练时忽略  \n  \n  \n# 编码函数  \ndef encode(item):  \n    x = [word_vocab.get(w, 1) for w in item['tokens']]  \n    y = item['ner_tags']  \n    return x, y  \n  \ntrain_dataset = [encode(item) for item in train_data]  \nvalid_dataset = [encode(item) for item in valid_data]  \ntest_dataset  = [encode(item) for item in test_data]  \n  \n  \n# 填充函数  \ndef pad_sequences(seqs, pad_value=0):  \n    max_len = max(len(s) for s in seqs)  \n    padded = [s + [pad_value]*(max_len - len(s)) for s in seqs]  \n    return np.array(padded)  \n  \ndef prepare_batch(dataset):  \n    X = pad_sequences([x for x, y in dataset], pad_value=0)  \n    y = pad_sequences([y for x, y in dataset], pad_value=pad_tag_id)  \n    return X, y  \n  \nX_train, y_train = prepare_batch(train_dataset)  \nX_valid, y_valid = prepare_batch(valid_dataset)  \nX_test,  y_test  = prepare_batch(test_dataset)  \n  \n  \nclass OneHotLayer(Layer):  \n    def __init__(self, depth, **kwargs):  \n        super().__init__(**kwargs)  \n        self.depth = depth  \n  \n    def call(self, inputs):  \n        return tf.one_hot(inputs, depth=self.depth, dtype=tf.float32)  \n  \n  \ndef build_rnn_model(vocab_size, num_classes, hidden_dim=128, rnn_type='RNN', bidirectional=False, num_layers=1):  \n    inputs = Input(shape=(None,), dtype=tf.int32)  \n    x = OneHotLayer(depth=vocab_size)(inputs)  \n  \n    for i in range(num_layers):  \n        return_seq = True  \n        rnn_layer = None  \n        if rnn_type.upper() == 'RNN':  \n            rnn_layer = SimpleRNN(hidden_dim, return_sequences=return_seq)  \n        elif rnn_type.upper() == 'GRU':  \n            rnn_layer = GRU(hidden_dim, return_sequences=return_seq)  \n        elif rnn_type.upper() == 'LSTM':  \n            rnn_layer = LSTM(hidden_dim, return_sequences=return_seq)  \n        else:  \n            raise ValueError(\"rnn_type must be 'RNN','LSTM','GRU'\")  \n        if bidirectional:  \n            rnn_layer = Bidirectional(rnn_layer)  \n        x = rnn_layer(x)  \n  \n    outputs = Dense(num_classes, activation='softmax')(x)  \n    model = Model(inputs, outputs)  \n    return model  \n  \ndef train_validate(model, X_train, y_train, X_valid, y_valid, batch_size=32, epochs=5, lr=0.001):  \n    optimizer = Adam(learning_rate=lr)  \n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=pad_tag_id)  \n  \n    train_loss_history = []  \n    train_acc_history  = []  \n    val_acc_history    = []  \n    val_f1_history     = []  \n  \n    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(batch_size)  \n    valid_data = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(batch_size)  \n  \n    for epoch in range(epochs):  \n        start_time = time.time()  \n        total_loss = 0  \n        total_correct = 0  \n        total_tokens = 0  \n  \n        # ========= 训练 =========        \n        for step, (x_batch, y_batch) in enumerate(train_data):  \n            with tf.GradientTape() as tape:  \n                logits = model(x_batch, training=True)  \n                loss = loss_fn(y_batch, logits)  \n            grads = tape.gradient(loss, model.trainable_variables)  \n            optimizer.apply_gradients(zip(grads, model.trainable_variables))  \n            total_loss += loss.numpy()  \n  \n            preds = tf.argmax(logits, axis=-1,output_type=tf.int32)  \n            mask = y_batch != pad_tag_id  \n            total_correct += tf.reduce_sum(tf.cast(preds[mask] == y_batch[mask], tf.float32)).numpy()  \n            total_tokens += tf.reduce_sum(tf.cast(mask, tf.float32)).numpy()  \n  \n        train_loss_history.append(total_loss / len(train_data))  \n        train_acc_history.append(total_correct / total_tokens)  \n  \n        # ========= 验证 =========        \n        all_preds = []  \n        all_labels = []  \n        total_correct_val = 0  \n        total_tokens_val = 0  \n  \n        for x_batch, y_batch in valid_data:  \n            logits = model(x_batch, training=False)  \n            preds = tf.argmax(logits, axis=-1,output_type=tf.int32)  \n            mask = y_batch != pad_tag_id  \n  \n            total_correct_val += tf.reduce_sum(tf.cast(preds[mask] == y_batch[mask], tf.float32)).numpy()  \n            total_tokens_val += tf.reduce_sum(tf.cast(mask, tf.float32)).numpy()  \n  \n            all_preds.extend(preds[mask].numpy())  \n            all_labels.extend(y_batch[mask].numpy())  \n  \n        val_acc = total_correct_val / total_tokens_val  \n        val_f1  = f1_score(all_labels, all_preds, average='macro')  \n  \n        val_acc_history.append(val_acc)  \n        val_f1_history.append(val_f1)  \n  \n        epoch_time = time.time() - start_time  \n        print(f\"轮次 {epoch+1}: 训练损失={train_loss_history[-1]:.4f}, \"  \n              f\"训练准确率={train_acc_history[-1]:.4f}, \"  \n              f\"验证准确率={val_acc:.4f}, 验证 F1={val_f1:.4f}, \"  \n              f\"本轮用时={epoch_time:.2f} 秒\")  \n  \n    history = {  \n        'train_loss': train_loss_history,  \n        'train_acc': train_acc_history,  \n        'val_acc': val_acc_history,  \n        'val_f1': val_f1_history  \n    }  \n    return history  \n  \ndef plot_training_curves(history):  \n    epochs = range(1, len(history['train_loss']) + 1)  \n    fig, ax1 = plt.subplots(figsize=(10, 5))  \n    ax1.plot(epochs, history['train_loss'], 'b-o', label='训练损失')  \n    ax1.set_xlabel(\"轮次\", fontsize=12)  \n    ax1.set_ylabel(\"训练损失\", color='b', fontsize=12)  \n    ax1.tick_params(axis='y', labelcolor='b')  \n    ax1.grid(True)  \n  \n    ax2 = ax1.twinx()  \n    ax2.plot(epochs, history['train_acc'], 'g-o', label='训练准确率')  \n    ax2.plot(epochs, history['val_acc'], 'r-o', label='验证准确率')  \n    ax2.plot(epochs, history['val_f1'], 'm-o', label='验证 F1')  \n    ax2.set_ylabel(\"指标值\", fontsize=12)  \n    ax2.tick_params(axis='y')  \n  \n    lines1, labels1 = ax1.get_legend_handles_labels()  \n    lines2, labels2 = ax2.get_legend_handles_labels()  \n    ax2.legend(lines1 + lines2, labels1 + labels2, loc='lower right')  \n  \n    plt.title(\"训练损失、准确率与 F1 曲线对比\", fontsize=14)  \n    plt.tight_layout()  \n    plt.show()  \n  \nif __name__ == '__main__':  \n    cfg = {'rnn_type':'RNN','bidirectional':True,'num_layers':1}  \n    model = build_rnn_model(vocab_size=vocab_size,  \n                            num_classes=num_classes,  \n                            hidden_dim=128,  \n                            rnn_type=cfg['rnn_type'],  \n                            bidirectional=cfg['bidirectional'],  \n                            num_layers=cfg['num_layers'])  \n    history = train_validate(model, X_train, y_train, X_valid, y_valid, batch_size=32, epochs=10, lr=0.001)  \n    plot_training_curves(history)\n</code></pre>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 13:03</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Goblinscholar\">哥布林学者</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "FastAPI缓存提速实战：手把手教你用Redis为接口注入“记忆”",
      "link": "https://www.cnblogs.com/ymtianyu/p/19485724",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ymtianyu/p/19485724\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 11:14\">\n    <span>FastAPI缓存提速实战：手把手教你用Redis为接口注入“记忆”</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        本文将详细介绍如何在FastAPI应用中集成Redis实现高效的数据缓存。内容涵盖Redis的核心作用与优势、在主要操作系统上的安装与配置步骤、必须掌握的常用命令，并通过一个完整的实战示例，展示如何编写一个可复用的缓存装饰器来显著提升接口性能，最后分享缓存策略的注意事项与进阶思考。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>你的FastAPI接口是不是在高并发下越来越慢，数据库频频告警？</p>\n<p>一个案例，一个核心查询接口，在日活仅5万时，平均响应时间就飙升到了1.2秒。排查后发现，超过80%的请求都在重复查询数据库里那几条几乎不变的热点数据。在引入Redis缓存后，这个接口的<strong style=\"color: rgba(186, 55, 42, 1);\">平均响应时间直接降到了0.2秒以内</strong>，数据库负载下降了70%。这，就是缓存的魔力。</p>\n<p>今天，我们就来聊聊如何为你FastAPI项目装上Redis这个“高速缓存”，让它拥有“记忆”，不再每次都傻傻地重复劳动。</p>\n<h2>📖 本文你将学到：</h2>\n<div>\n<p>🎯 1. Redis是什么？为什么它是缓存的首选？</p>\n<p>🎯 2. 如何快速安装、配置Redis。</p>\n<p>🎯 3. 必须掌握的Redis核心命令。</p>\n<p>🎯 4. 编写一个通用的FastAPI缓存装饰器，一劳永逸。</p>\n</div>\n<hr />\n<h2>🔧 第一部分：问题与背景 - 为什么需要缓存？</h2>\n<p>想象一下，你是餐厅（你的Web服务）的服务员（API接口）。每次客人（客户端）点一份“今日特色菜”（热门数据），你都非得跑回后厨（数据库）问厨师一遍，尽管这道菜一天都不会变。结果就是，你累趴了，后厨也被你问烦了，客人还嫌上菜慢。</p>\n<p>缓存，就像是你在前厅放了个小本子（Redis）。第一次有客人点“今日特色菜”，你去后厨问了，然后<strong style=\"color: rgba(186, 55, 42, 1);\">把菜名和价格记在本子上</strong>。接下来再有客人点，你直接看一眼本子就告诉他，速度快了十倍。只有当特色菜更换了（数据变更），你才需要去更新小本子。</p>\n<p>在技术层面，缓存主要解决两个问题：<strong style=\"color: rgba(186, 55, 42, 1);\">1. 提升数据读取速度</strong>（内存远快于磁盘/网络）；<strong style=\"color: rgba(186, 55, 42, 1);\">2. 减轻后端数据库压力</strong>。</p>\n<h2>⚙️ 第二部分：Redis核心与安装配置</h2>\n<h3>🎯 Redis是什么？</h3>\n<p>Redis是一个开源、基于内存、可持久化的<code style=\"color: rgba(186, 55, 42, 1);\">键值对(Key-Value)</code>存储系统。它支持多种数据结构（字符串、哈希、列表、集合等），性能极高，常被用作数据库、缓存和消息中间件。对于缓存场景，我们主要看中它：<strong style=\"color: rgba(186, 55, 42, 1);\">内存存储速度极快</strong>、<strong style=\"color: rgba(186, 55, 42, 1);\">数据结构丰富</strong>、<strong style=\"color: rgba(186, 55, 42, 1);\">支持设置过期时间</strong>。</p>\n<h3>🎯 安装Redis（各平台通用步骤）</h3>\n<div>\n<p><strong>1. macOS (使用Homebrew):</strong></p>\n<div>\n<p>- 打开终端，执行：<code style=\"color: rgba(186, 55, 42, 1);\">brew install redis</code></p>\n<p>- 启动服务：<code style=\"color: rgba(186, 55, 42, 1);\">brew services start redis</code></p>\n</div>\n<p><strong>2. Linux (以Ubuntu为例):</strong></p>\n<div>\n<p>- 更新包管理器：<code style=\"color: rgba(186, 55, 42, 1);\">sudo apt update</code></p>\n<p>- 安装Redis：<code style=\"color: rgba(186, 55, 42, 1);\">sudo apt install redis-server</code></p>\n<p>- 启动服务：<code style=\"color: rgba(186, 55, 42, 1);\">sudo systemctl start redis</code></p>\n</div>\n<p><strong>3. Windows:</strong></p>\n<div>\n<p>官方不支持Windows原生安装，但可以通过：</p>\n<p>- 使用WSL2（推荐，在WSL的Ubuntu中按Linux方法安装）。</p>\n<p>- 或下载微软维护的旧版本Windows移植版（不推荐用于生产）。</p>\n</div>\n</div>\n<p>安装完成后，在终端输入 <code style=\"color: rgba(186, 55, 42, 1);\">redis-cli ping</code>，如果返回 <code style=\"color: rgba(186, 55, 42, 1);\">PONG</code>，恭喜你，Redis服务已成功运行！</p>\n<h3>🎯 你必须掌握的5个Redis缓存核心命令</h3>\n<div>\n<p><strong>1. SET： 设置键值对</strong></p>\n<pre class=\"language-dockerfile highlighter-hljs\"><code>SET user:1001 '{\"name\": \"Alice\", \"age\": 30}' EX 60\n# 键：user:1001， 值：JSON字符串， EX 60表示60秒后过期</code></pre>\n<p><strong>2. GET： 获取键对应的值</strong></p>\n<pre class=\"language-sql highlighter-hljs\"><code>GET user:1001</code></pre>\n<p><strong>3. EXISTS： 检查键是否存在</strong></p>\n<pre class=\"language-dockerfile highlighter-hljs\"><code>EXISTS user:1001 # 返回1(存在)或0(不存在)</code></pre>\n<p><strong>4. DEL： 删除一个或多个键</strong></p>\n<pre class=\"language-sql highlighter-hljs\"><code>DEL user:1001 user:1002</code></pre>\n<p><strong>5. TTL： 查看键的剩余生存时间(秒)</strong></p>\n<pre class=\"language-dockerfile highlighter-hljs\"><code>TTL user:1001 # 返回剩余秒数，-1表示永不过期，-2表示键不存在</code></pre>\n</div>\n<h2>🚀 第三部分：FastAPI整合Redis实战演示</h2>\n<p>理论说再多，不如一行代码。让我们开始实战，构建一个带缓存的FastAPI应用。</p>\n<h3>🎯 第一步：安装依赖</h3>\n<pre class=\"highlighter-hljs\" style=\"background-color: rgba(246, 248, 250, 1); padding: 15px; border-radius: 5px;\"><code>pip install fastapi uvicorn redis python-dotenv</code></pre>\n<h3>🎯 第二步：项目结构与配置</h3>\n<pre class=\"highlighter-hljs\" style=\"background-color: rgba(246, 248, 250, 1); padding: 15px; border-radius: 5px;\"><code>your_project/\n├── main.py\n├── cache.py\n├── .env\n└── requirements.txt</code></pre>\n<p>在<code style=\"color: rgba(186, 55, 42, 1);\">.env</code>文件中配置Redis连接：</p>\n<pre class=\"highlighter-hljs\" style=\"background-color: rgba(246, 248, 250, 1); padding: 15px; border-radius: 5px;\"><code>REDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_DB=0\nREDIS_PASSWORD=  # 默认无密码，生产环境一定要设！\nCACHE_DEFAULT_TTL=3600  # 默认缓存过期时间1小时</code></pre>\n<h3>🎯 第三步：创建Redis连接与缓存工具类 (cache.py)</h3>\n<pre class=\"language-python highlighter-hljs\"><code>import redis.asyncio as redis  # 使用异步客户端\nimport json\nfrom functools import wraps\nfrom typing import Any, Optional\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass RedisCache:\n    def __init__(self):\n        self.redis_client: Optional[redis.Redis] = None\n        self.default_ttl = int(os.getenv(“CACHE_DEFAULT_TTL”, 3600))\n\n    async def connect(self):\n        \"\"\"连接Redis\"\"\"\n        if not self.redis_client:\n            self.redis_client = redis.Redis(\n                host=os.getenv(“REDIS_HOST”, “localhost”),\n                port=int(os.getenv(“REDIS_PORT”, 6379)),\n                db=int(os.getenv(“REDIS_DB”, 0)),\n                password=os.getenv(“REDIS_PASSWORD”),\n                decode_responses=True  # 自动解码返回字符串\n            )\n        return self.redis_client\n\n    async def disconnect(self):\n        \"\"\"关闭连接\"\"\"\n        if self.redis_client:\n            await self.redis_client.close()\n\n    def cache_key(self, func_name: str, *args, **kwargs) -&gt; str:\n        \"\"\"生成唯一的缓存键\"\"\"\n        # 简单示例：将函数名和参数序列化后拼接\n        arg_str = “_”.join([str(arg) for arg in args])\n        kwarg_str = “_”.join([f“{k}_{v}” for k, v in sorted(kwargs.items())])\n        return f“fastapi_cache:{func_name}:{arg_str}:{kwarg_str}”.strip(“:”)\n\n    async def get(self, key: str) -&gt; Any:\n        \"\"\"从缓存获取数据\"\"\"\n        if not self.redis_client:\n            await self.connect()\n        data = await self.redis_client.get(key)\n        if data:\n            try:\n                return json.loads(data)  # 反序列化JSON\n            except json.JSONDecodeError:\n                return data  # 如果不是JSON，返回原始字符串\n        return None\n\n    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -&gt; bool:\n        \"\"\"设置缓存\"\"\"\n        if not self.redis_client:\n            await self.connect()\n        if isinstance(value, (dict, list)):\n            value = json.dumps(value)  # 序列化复杂对象\n        expire_time = ttl if ttl is not None else self.default_ttl\n        return await self.redis_client.setex(key, expire_time, value)\n\n    async def delete(self, key: str) -&gt; int:\n        \"\"\"删除缓存\"\"\"\n        if not self.redis_client:\n            await self.connect()\n        return await self.redis_client.delete(key)\n\n# 全局缓存实例\ncache = RedisCache()\n\ndef cached(ttl: Optional[int] = None):\n    \"\"\"缓存装饰器：可复用于任何异步函数\"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # 生成缓存键\n            key = cache.cache_key(func.__name__, *args, **kwargs)\n            # 尝试从缓存获取\n            cached_result = await cache.get(key)\n            if cached_result is not None:\n                print(f“Cache HIT for key: {key}”)\n                return cached_result\n            # 缓存未命中，执行原函数\n            print(f“Cache MISS for key: {key}”)\n            result = await func(*args, **kwargs)\n            # 将结果存入缓存\n            await cache.set(key, result, ttl)\n            return result\n        return wrapper\n    return decorator</code></pre>\n<h3>🎯 第四步：在FastAPI应用中使用 (main.py)</h3>\n<pre class=\"highlighter-hljs\" style=\"background-color: rgba(246, 248, 250, 1); padding: 15px; border-radius: 5px;\"><code>from fastapi import FastAPI, Depends, HTTPException\nfrom cache import cache, cached\nimport asyncio\n\napp = FastAPI(title=“FastAPI Redis缓存演示”)\n\n# 应用启动和关闭事件\n@app.on_event(“startup”)\nasync def startup_event():\n    await cache.connect()\n    print(“✅ Redis connected”)\n\n@app.on_event(“shutdown”)\nasync def shutdown_event():\n    await cache.disconnect()\n    print(“👋 Redis disconnected”)\n\n# --- 模拟一个耗时的数据查询函数 ---\nasync def fetch_user_data_from_db(user_id: int):\n    \"\"\"模拟从数据库查询用户数据（耗时操作）\"\"\"\n    await asyncio.sleep(2)  # 模拟2秒的IO延迟\n    return {“id”: user_id, “name”: f“用户_{user_id}”, “score”: user_id * 10}\n\n# --- 应用缓存的接口 ---\n@app.get(“/user/{user_id}”)\n@cached(ttl=30)  # 为此接口单独设置30秒缓存\nasync def get_user(user_id: int):\n    \"\"\"获取用户信息（带缓存）\"\"\"\n    data = await fetch_user_data_from_db(user_id)\n    return {“source”: “database (cached later)”, “data”: data}\n\n@app.get(“/user/{user_id}/fresh”)\nasync def get_user_fresh(user_id: int):\n    \"\"\"获取用户信息（强制查数据库，不缓存）\"\"\"\n    data = await fetch_user_data_from_db(user_id)\n    return {“source”: “database (fresh)”, “data”: data}\n\n@app.delete(“/cache/user/{user_id}”)\nasync def delete_user_cache(user_id: int):\n    \"\"\"手动删除某个用户的缓存\"\"\"\n    # 注意：这里需要模拟生成和接口一致的缓存键，实战中可能需要更复杂的键管理\n    key_pattern = f“fastapi_cache:get_user:{user_id}”\n    deleted = await cache.delete(key_pattern)\n    if deleted:\n        return {“message”: f”Cache for user {user_id} deleted.”}\n    raise HTTPException(status_code=404, detail=“Cache key not found”)\n\nif __name__ == “__main__”:\n    import uvicorn\n    uvicorn.run(“main:app”, host=“0.0.0.0”, port=8000, reload=True)</code></pre>\n<p>现在，运行<code style=\"color: rgba(186, 55, 42, 1);\">python main.py</code>并访问 <code style=\"color: rgba(186, 55, 42, 1);\">http://localhost:8000/docs</code> 查看自动生成的API文档。</p>\n<p><strong style=\"color: rgba(186, 55, 42, 1);\">测试效果：</strong></p>\n<div>\n<p>- 首次访问 <code style=\"color: rgba(186, 55, 42, 1);\">/user/1</code>，会等待约2秒，返回来源为<span style=\"color: rgba(186, 55, 42, 1);\"><code>database</code></span>。</p>\n<p>- <strong>30秒内</strong>再次访问 <code style=\"color: rgba(186, 55, 42, 1);\">/user/1</code>，瞬间返回，来源数据来自缓存，控制台会打印<span style=\"color: rgba(186, 55, 42, 1);\"><code>Cache HIT</code></span>。</p>\n<p>- 访问 <code style=\"color: rgba(186, 55, 42, 1);\">/user/1/fresh</code> 则总是访问“数据库”。</p>\n<p>- 调用 <code style=\"color: rgba(186, 55, 42, 1);\">DELETE /cache/user/1</code> 可以手动清除缓存。</p>\n</div>\n<h2>💡 第四部分：注意事项与进阶思考</h2>\n<div>\n<p><strong>1. 缓存穿透：</strong> 查询一个不存在的数据（如<span style=\"color: rgba(186, 55, 42, 1);\"><code>user_id=-1</code></span>），缓存永远不会命中，请求每次都打到数据库。</p>\n<div>\n<p>- <strong>解决方案：</strong> 即使没查到数据，也缓存一个空值或特殊标记（如<span style=\"color: rgba(186, 55, 42, 1);\"><code>NULL</code></span>），并设置一个较短的过期时间。</p>\n</div>\n<p><strong>2. 缓存雪崩：</strong> 大量缓存键在同一时刻过期，导致所有请求瞬间涌向数据库。</p>\n<div>\n<p>- <strong>解决方案：</strong> 为缓存过期时间添加一个随机值（如<span style=\"color: rgba(186, 55, 42, 1);\"><code>基础TTL + random.randint(0, 300)</code></span>），避免集体失效。</p>\n</div>\n<p><strong>3. 缓存更新策略：</strong> 数据变更时，如何同步更新缓存？常用“写时删除”（Cache-Aside）。</p>\n<div>\n<p>- 更新数据库后，<strong style=\"color: rgba(186, 55, 42, 1);\">立即删除</strong>对应的缓存键。下次读取时自然回源并重新缓存。</p>\n</div>\n<p><strong>4. 序列化：</strong> 缓存复杂对象（如Pydantic模型）时，要确保它们能被JSON序列化。可以使用<span style=\"color: rgba(186, 55, 42, 1);\"><code>.dict()</code></span>方法将其转为字典。</p>\n<p><strong>5. 键的设计：</strong> 清晰的键命名空间（如<span style=\"color: rgba(186, 55, 42, 1);\"><code>app:entity:id</code></span>）便于管理和批量操作（使用<span style=\"color: rgba(186, 55, 42, 1);\"><code>KEYS</code></span>或<span style=\"color: rgba(186, 55, 42, 1);\"><code>SCAN</code></span>命令，生产环境慎用<span style=\"color: rgba(186, 55, 42, 1);\"><code>KEYS</code></span>）。</p>\n<p><strong>6. 最重要的一点：</strong> <strong style=\"color: rgba(186, 55, 42, 1);\">缓存不是万能的，它是一种用空间换时间的权衡。</strong> 不要缓存频繁变化的数据、极小结果集或已经很快的查询。始终监控缓存命中率，它是衡量缓存效益的关键指标。</p>\n</div>\n<hr style=\"border: none; height: 1px; margin: 40px 0;\" />\n<p style=\"text-align: center;\">---<strong>写在最后</strong>---<br />希望这份总结能帮你避开一些坑。如果觉得有用，不妨点个 赞👍 或 收藏⭐ 标记一下，方便随时回顾。也欢迎关注我，后续为你带来更多类似的实战解析。有任何疑问或想法，我们评论区见，一起交流开发中的各种心得与问题。</p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 11:14</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ymtianyu\">一名程序媛呀</a>&nbsp;\n阅读(<span id=\"post_view_count\">60</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "如何一步步将 ASP.NET MVC 升级为.NET",
      "link": "https://www.cnblogs.com/powertoolsteam/p/19486260",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/powertoolsteam/p/19486260\" id=\"cb_post_title_url\" title=\"发布于 2026-01-15 10:55\">\n    <span>如何一步步将 ASP.NET MVC 升级为.NET</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"引言\">引言</h2>\n<p>将 ASP.NET MVC 应用从.NET Framework 升级到现代.NET 并不是简单的版本提升。此次迁移代表了运行时、托管模型、配置系统、依赖注入和 HTTP 流水线架构的转变。许多团队低估了这一点，把它当作标准的框架升级，结果在流程后期才发现他们应用中的核心假设已经不再成立。</p>\n<p>好消息是，Microsoft 提供了明确的指导和模式，使得正确操作时迁移过程可预测。本文介绍了一个实用的逐步策略，如何将基于.NET Framework 构建的 ASP.NET MVC 5 应用迁移到运行在现代.NET 上的 ASP.NET Core，同时最大限度地减少风险和停机时间。</p>\n<p>本指南是为已经了解 ASP.NET MVC 并希望有一条切实可行、准备好生产的开发者写的。</p>\n<h3 id=\"步骤1根据应用大小选择迁移路径\">步骤1：根据应用大小选择迁移路径</h3>\n<p>选择这两条路中的一条：</p>\n<ol>\n<li>\n<p><strong>大多数真实应用的增量迁移</strong><br />\n你会在旧应用前面搭建一个新的 ASP.NET Core 应用，逐步迁移路由和功能，迁移时保持旧应用的存在。Microsoft 推荐这种方法用于大规模迁移。</p>\n</li>\n<li>\n<p><strong>小型应用的 Big Bang 迁移</strong><br />\n你创建一个新的 ASP.NET Core MVC 应用，移植控制器、视图和服务，直到一切运行完毕，然后切换。</p>\n</li>\n</ol>\n<p>如果不确定，默认采用增量迁移，因为这样可以降低切换风险。</p>\n<h3 id=\"步骤2清点什么会阻挡你\">步骤2：清点什么会阻挡你</h3>\n<p>列出迁移过程中常需要改进的依赖和应用功能：</p>\n<ul>\n<li>\n<p><strong>System.Web 的使用</strong><br />\n像 HttpContext.Current、HttpModules、HttpHandlers、Global.asax 以及经典流水线功能这些功能不会直接迁移。</p>\n</li>\n<li>\n<p><strong>认证与授权</strong><br />\nASP.NET Core 采用了不同的模式。</p>\n</li>\n<li>\n<p><strong>会话、缓存、配置、日志</strong><br />\nAPI 和模式各不相同，需要有意识的迁移规划。</p>\n</li>\n</ul>\n<p>快速对你的 NuGet 包做个兼容性检查。对于每个软件包，检查它是否支持 netstandard2.0 或现代网络版本，或者是否有 ASP.NET Core 替代品。标记所有与 System.Web 相关的内容。</p>\n<h3 id=\"步骤3在接触代码前建立安全基线\">步骤3：在接触代码前建立安全基线</h3>\n<p>冻结当前状态：</p>\n<ol>\n<li>给仓库标记</li>\n<li>确保你能构建解决方案并运行测试套件</li>\n<li>如果覆盖率较少，可以增加或加强测试<br />\n重点关注关键路由、授权流程和最重要的业务操作。</li>\n</ol>\n<p>如果无法快速添加测试，至少要记录烟雾测试脚本和预期输出。</p>\n<h3 id=\"步骤4先升级库再考虑网页应用\">步骤4：先升级库，再考虑网页应用</h3>\n<p>这是杠杆最高的操作。如果还没拆分，就把解决方案分成几层：网页项目、应用服务、域名、数据访问、共享工具。</p>\n<p>先移植类库。如果可能，把库转换成目标 NetStandard2.0 作为桥接，或者直接面向现代 .NET。修复编译错误并替换不支持的 API。</p>\n<p>Microsoft 的通用移植指导是评估项目，并以降低初始升级后复杂性的方式推进各个部分。</p>\n<h3 id=\"步骤5决定是否使用迁移工具\">步骤5：决定是否使用迁移工具</h3>\n<p>你可以使用两个 Microsoft 支持的工具指令来加快部分工作速度：</p>\n<ol>\n<li>\n<p><strong>.NET 升级助手</strong><br />\n它帮助升级项目并分析不兼容问题，包括 .NET Framework 项目。在有帮助的地方使用它，但不要指望它能完全把 ASP.NET MVC 5 应用自动转化为 ASP.NET Core MVC。</p>\n</li>\n<li>\n<p><strong>ASP.NET Framework 应用的迁移工具</strong><br />\nMicrosoft 指出，将 ASP.NET 框架 MVC、Web API 和 Web Forms 项目升级到 ASP.NET Core，提供专用的现代化工具。</p>\n</li>\n</ol>\n<p>如果你愿意，你仍然可以手动完成所有操作，但工具可以减少枯燥的机械编辑。</p>\n<h3 id=\"步骤6创建新的-aspnet-core-mvc-主机应用\">步骤6：创建新的 ASP.NET Core MVC 主机应用</h3>\n<p>即使是增量迁移，这一步也必须，因为你需要一个 ASP.NET Core 应用才能在现代.NET 上运行。</p>\n<p>创建一个新的 ASP.NET 核心 MVC 项目，使用标准的 ASP.NET 核心 MVC 模板。添加你的核心包、日志、配置和 DI 结构。</p>\n<p>在 ASP.NET Core 中，Program.cs 配置主机和中间件流水线。没有 Global.asax。添加一些你肯定需要的基础中间件：路由、静态文件、认证和授权（如适用）、MVC 端点。</p>\n<p>最小示例形状：</p>\n<pre><code class=\"language-csharp\">var builder = WebApplication.CreateBuilder(args);\n\nbuilder.Services.AddControllersWithViews();\n\nvar app = builder.Build();\n\napp.UseStaticFiles();\napp.UseRouting();\n\napp.MapControllerRoute(\n    name: \"default\",\n    pattern: \"{controller=Home}/{action=Index}/{id?}\");\n\napp.Run();\n</code></pre>\n<h3 id=\"步骤7先移动路由然后是控制器最后是视图\">步骤7：先移动路由，然后是控制器，最后是视图</h3>\n<p>Microsoft 的 MVC 到核心示例攻略从设置开始，然后是控制器和视图，最后是静态内容和客户端依赖。遵循这个顺序，因为这样可以让迁移在每一步都可测试。</p>\n<p><strong>迁移路由</strong><br />\n在 MVC 5 中，你很可能用过 RouteConfig，可能还用了属性路由。在 ASP.NET Core 中，你配置中间件流水线和端点映射中的路由。</p>\n<p><strong>迁移控制器</strong><br />\n一次迁移一个控制器。复制一个控制器类。通过更换命名空间和替换依赖 System.Web 的代码来修复编译错误。用控制器 HttpContext 访问替代 HttpContext.Current 模式。</p>\n<p><strong>迁移视图</strong><br />\nRazor 语法类似，但辅助器和一些功能有所不同。需要的地方转换 HTML 辅助工具，必要时使用标签辅助工具。</p>\n<p>你很可能会引入 ViewImports.cshtml 和 ViewStart.cshtml 的模式。</p>\n<h3 id=\"步骤8移动静态文件和客户端依赖\">步骤8：移动静态文件和客户端依赖</h3>\n<p><strong>静态文件</strong><br />\n在 MVC 5 中，你可能用过内容和脚本的约定。在 ASP.NET Core 中，默认是 wwwroot 加 UseStaticFiles 中间件。</p>\n<p><strong>捆绑与精简</strong><br />\n如果你依赖 System.Web.Optimization，可以用现代前端构建流水线或其他支持的方法替代它。</p>\n<p>Microsoft 的示例迁移流程明确指出静态内容和客户端依赖是早期迁移步骤。</p>\n<h3 id=\"步骤9将配置从-webconfig-迁移到-appsettings\">步骤9：将配置从 Web.config 迁移到 appsettings</h3>\n<p>确定你在 Web.config 中存储了什么：连接串、应用设置、认证设置、自定义配置部分。</p>\n<p>将设置迁移到 appsettings.json 和环境特定的 appsettings 文件中。通过构建器读取 ASP.NET 核心配置系统中的设置。</p>\n<p>Microsoft 有专门的 MVC 迁移示例中的迁移配置指南。</p>\n<h3 id=\"步骤10迁移认证和授权\">步骤10：迁移认证和授权</h3>\n<p>确定你当前的认证模型：表单认证、OWIN cookies、ASP.NET MVC 5 中的身份、Windows 认证。</p>\n<p>在 ASP.NET Core 中实现类似的：</p>\n<ol>\n<li>在 DI 中配置认证服务</li>\n<li>在流程中加入中间件</li>\n<li>端口授权属性和策略</li>\n</ol>\n<p>Microsoft 的 MVC 迁移示例指向了针对认证和身份迁移的专门指导，因为这通常是较为繁重的部分之一。</p>\n<h3 id=\"步骤11迁移数据访问\">步骤11：迁移数据访问</h3>\n<p><strong>如果你使用实体框架6</strong><br />\n你通常可以保留 EF6 一段时间，即使是在现代.NET 上，这取决于你的配置。从长远来看，许多团队转向 EF Core 以获取现代化的模式和功能。</p>\n<p><strong>如果你用 ADO.NET 或 Dapper</strong><br />\n这些通常能干净地移植，但你会更新配置和依赖注入模式。</p>\n<h3 id=\"步骤12运行应用修复运行时问题然后进行优化\">步骤12：运行应用，修复运行时问题，然后进行优化</h3>\n<p>按这个典型顺序修复运行时问题：</p>\n<ol>\n<li>配置与环境差异</li>\n<li>中间件排序问题</li>\n<li>身份验证与 Cookie</li>\n<li>会话与缓存行为</li>\n</ol>\n<p>运行自动化测试和烟雾测试。在适当情况下添加结构化日志和健康检查。</p>\n<h3 id=\"步骤13如果你选择了增量迁移设置前门core-应用\">步骤13：如果你选择了增量迁移，设置\"前门\"Core 应用</h3>\n<p>对于增量迁移，关键举措是将 ASP.NET 核心应用置于现有的.NET Framework 应用之前，并逐路由迁移。Microsoft 关于增量迁移的\"入门指南\"推荐采用代理前线方法进行大规模迁移。</p>\n<p>你的流程变成：</p>\n<ol>\n<li>路由 A 由 ASP.NET 核心处理，其他所有路由代理到 MVC 5</li>\n<li>一次移动一个垂直切片</li>\n<li>当所有路线都被迁移后，关闭旧的 MVC 5 应用</li>\n</ol>\n<h3 id=\"步骤14生产切换清单\">步骤14：生产切换清单</h3>\n<p><strong>主持</strong><br />\n决定 IIS 在流程中、IIS 在流程外，还是容器。</p>\n<p><strong>可观测性</strong><br />\n在增量迁移过程中，两个应用的日志、指标和追踪都对齐了。</p>\n<p><strong>安全</strong><br />\nCookie 设置、数据保护密钥、TLS、头部。</p>\n<p><strong>性能</strong><br />\n迁移后确认缓存和会话行为。</p>\n<h2 id=\"结论\">结论</h2>\n<p>将 ASP.NET MVC 应用从.NET Framework 迁移到现代.NET 是一项战略投资，而非机械升级。正确执行时，它能带来更清晰的架构、性能提升、云准备度提升以及长期平台支持。</p>\n<p>通过将迁移拆分为有计划、可测试的步骤，并优先关注架构而非语法，团队可以自信地现代化，同时不扰乱生产系统。</p>\n<p>这种方法在现实企业迁移中已被反复证明有效，并且符合 Microsoft 推荐的 ASP.NET 应用现代化战略。</p>\n\n</div>\n<div id=\"MySignature\">\n    <hr />\n<br />\n<p>本文是由葡萄城技术开发团队发布，转载请注明出处：<a href=\"https://www.grapecity.com.cn/\" target=\"_blank\">葡萄城官网</a></p>\n<!--p style=\"font-size: 16px; font-family: 微软雅黑, 黑体, Arial; color: #000\">了解企业级低代码开发平台，请前往<a href=\"https://www.grapecity.com.cn/solutions/huozige\" target=\"_blank\">活字格</a>\n</p><p style=\"font-size: 16px; font-family: 微软雅黑, 黑体, Arial; color: #000\">了解可嵌入您系统的在线 Excel，请前往<a href=\"https://www.grapecity.com.cn/developer/spreadjs\" target=\"_blank\">SpreadJS纯前端表格控件</a></p>\n<p style=\"font-size: 16px; font-family: 微软雅黑, 黑体, Arial; color: #000\">了解嵌入式的商业智能和报表软件，请前往<a href=\"https://www.grapecity.com.cn/solutions/wyn\" target=\"_blank\">Wyn Enterprise\n</a></p-->\n\n<br />\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-15 10:55</span>&nbsp;\n<a href=\"https://www.cnblogs.com/powertoolsteam\">葡萄城技术团队</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}