{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "AI开发-python-langchain框架（1-6“示例学习”—— 少样本提示（Few-Shot Prompting））",
      "link": "https://www.cnblogs.com/yclh/p/19532721",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/yclh/p/19532721\" id=\"cb_post_title_url\" title=\"发布于 2026-01-26 12:44\">\n    <span>AI开发-python-langchain框架（1-6“示例学习”—— 少样本提示（Few-Shot Prompting））</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p><span class=\"qwen-markdown-text\">今天我们一起来看一段非常实用的 <strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">LangChain 代码</span></strong><span class=\"qwen-markdown-text\">，它展示了如何用 <strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">少样本提示（Few-Shot Prompting）</span></strong><span class=\"qwen-markdown-text\"> 的方式，让大模型更聪明、更准确地回答数学问题。我会把核心知识点拆解清楚，帮助大家理解每一步的作用和背后的原理。</span></span></span></p>\n<h3 class=\"qwen-markdown-heading\"><span class=\"qwen-markdown-text\">什么是“少样本提示”（Few-Shot Prompting）？</span></h3>\n<ul class=\"qwen-markdown-list\" dir=\"auto\">\n<li><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">定义</span></strong><span class=\"qwen-markdown-text\">：在给大模型提问前，先提供几个 <strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">输入-输出的示例</span></strong><span class=\"qwen-markdown-text\">，让模型“模仿”这些例子来回答新问题。</span></span></li>\n<li><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">作用</span></strong><span class=\"qwen-markdown-text\">：提升模型在特定任务上的表现，尤其适合结构化、规则明确的任务（比如数学计算、格式转换等）。</span></li>\n<li><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">类比</span></strong><span class=\"qwen-markdown-text\">：就像老师先讲两道例题，再让学生做第三道——学生更容易掌握规律。</span></li>\n</ul>\n<table class=\"qwen-markdown-table\">\n<thead class=\"qwen-markdown-table-thead\">\n<tr class=\"qwen-markdown-table-thead-tr\"><th class=\"qwen-markdown-table-thead-tr-th\" scope=\"col\">\n<div class=\"qwen-markdown-table-thead-tr-th-col\"><span class=\"qwen-markdown-text\">知识点</span></div>\n</th><th class=\"qwen-markdown-table-thead-tr-th\" scope=\"col\">\n<div class=\"qwen-markdown-table-thead-tr-th-col\"><span class=\"qwen-markdown-text\">说明</span></div>\n</th></tr>\n</thead>\n<tbody class=\"qwen-markdown-table-tbody\">\n<tr class=\"qwen-markdown-table-tbody-tr\">\n<td class=\"qwen-markdown-table-tbody-tr-td\">\n<div class=\"qwen-markdown-table-tbody-tr-td-col\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">Few-Shot Prompting</span></strong></div>\n</td>\n<td class=\"qwen-markdown-table-tbody-tr-td\">\n<div class=\"qwen-markdown-table-tbody-tr-td-col\"><span class=\"qwen-markdown-text\">通过少量示例引导模型行为，提升任务准确性</span></div>\n</td>\n</tr>\n<tr class=\"qwen-markdown-table-tbody-tr\">\n<td class=\"qwen-markdown-table-tbody-tr-td\">\n<div class=\"qwen-markdown-table-tbody-tr-td-col\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">ChatPromptTemplate</span></strong></div>\n</td>\n<td class=\"qwen-markdown-table-tbody-tr-td\">\n<div class=\"qwen-markdown-table-tbody-tr-td-col\"><span class=\"qwen-markdown-text\">构建多轮对话式提示的标准工具</span></div>\n</td>\n</tr>\n<tr class=\"qwen-markdown-table-tbody-tr\">\n<td class=\"qwen-markdown-table-tbody-tr-td\">\n<div class=\"qwen-markdown-table-tbody-tr-td-col\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">FewShotChatMessagePromptTemplate</span></strong></div>\n</td>\n<td class=\"qwen-markdown-table-tbody-tr-td\">\n<div class=\"qwen-markdown-table-tbody-tr-td-col\"><span class=\"qwen-markdown-text\">自动将示例列表转为对话历史</span></div>\n</td>\n</tr>\n<tr class=\"qwen-markdown-table-tbody-tr\">\n<td class=\"qwen-markdown-table-tbody-tr-td\">\n<div class=\"qwen-markdown-table-tbody-tr-td-col\"><strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">MessagesPlaceholder</span></strong><span class=\"qwen-markdown-text\">（本例未用但重要）</span></div>\n</td>\n<td class=\"qwen-markdown-table-tbody-tr-td\">\n<div class=\"qwen-markdown-table-tbody-tr-td-col\"><span class=\"qwen-markdown-text\">用于动态插入中间消息（如聊天历史）</span></div>\n</td>\n</tr>\n</tbody>\n</table>\n<p>看代码：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:python;gutter:true;\">from langchain_core.prompts import (\n    ChatPromptTemplate,\n    FewShotChatMessagePromptTemplate,\n    MessagesPlaceholder,\n)\n\n# 定义示例\nexamples = [\n    {\"input\": \"2+2\", \"output\": \"4\"},\n    {\"input\": \"2+3\", \"output\": \"5\"},\n]\n\n# 定义示例提示模板\nexample_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{input}\"),\n        (\"ai\", \"{output}\"),\n    ]\n)\n\n# 创建少样本提示模板\nfew_shot_prompt = FewShotChatMessagePromptTemplate(\n    example_prompt=example_prompt,\n    examples=examples,\n)\n\n# 组装最终提示模板\nfinal_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"你是一位非常厉害的数学天才。\"),\n        few_shot_prompt,\n        (\"human\", \"{input}\"),\n    ]\n)\n\n# 测试提示模板\ntest_input = \"3+3\"\nformatted_prompt = final_prompt.format_messages(input=test_input)\nprint(\"格式化后的提示：\")\nfor msg in formatted_prompt:\n    print(f\"{msg.type}: {msg.content}\")\n\n\n#调用大模型\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nimport os\n\n#配置 Deepseek 密钥和模型参数\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n    base_url=os.getenv(\"BASE_URL\"),  # Deepseek 的 API 基础地址\n    model=\"deepseek-v3:671b\",  # Deepseek 对话模型（可选：deepseek-chat-pro 等高级模型）\n    temperature=0.7,  # 温度参数（0-1，越低越稳定）\n    max_tokens=1024  # 最大生成 tokens\n)\n\noutput_parser = StrOutputParser()\n\nchain = final_prompt | llm | output_parser\n\nresponse = chain.invoke({\"input\":\"3的平方是多少？\"})\nprint(response)\n</pre>\n</div>\n<p>&nbsp;运行结果：</p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:python;gutter:true;\">格式化后的提示：\nsystem: 你是一位非常厉害的数学天才。\nhuman: 2+2\nai: 4\nhuman: 2+3\nai: 5\nhuman: 3+3\n3的平方是 **9**。  \n\n计算过程：  \n3² = 3 × 3 = **9**\n</pre>\n</div>\n<p>&nbsp;</p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-26 12:44</span>&nbsp;\n<a href=\"https://www.cnblogs.com/yclh\">万笑佛</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "如何用 Python 将 Markdown 转换为 Word 文档",
      "link": "https://www.cnblogs.com/jazz-z/p/19532573",
      "published": "",
      "description": "<div class=\"postcontent\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p class=\"p\">在当今的技术文档工作流中，Markdown 因其简洁的语法和版本控制友好的特性，已成为开发者和技术写作者的首选格式。然而，在企业环境中，Word 文档仍然是正式报告、客户交付物和标准化文档的主流格式。</p>\n<p class=\"p\">本文将分享如何使用 Free Spire.Doc for Python—一款免费的 Python 文档处理库，快速实现 Markdown 到 Word 的转换，涵盖基础转换、批量处理等实用场景，新手也能轻松上手。</p>\n<hr />\n<p class=\"p\">&nbsp;</p>\n<h2 class=\"h4\">一、环境准备</h2>\n<p class=\"p\">Free Spire.Doc for Python 是免费 Python 文档处理库，无需依赖 Microsoft Word，支持 Word 文档的创建、编辑、转换等操作，其中内置的 Markdown 解析能力，能高效实现 Markdown 到 Doc/Docx 格式的转换，且兼容常见的 Markdown 语法（标题、列表、图片、链接等）。</p>\n<p class=\"listitem\"><strong class=\"strong\">安装</strong>：<br />打开终端/命令提示符，执行以下pip安装命令：</p>\n<div class=\"cnblogs_code\">\n<pre>pip <span style=\"color: rgba(0, 0, 255, 1);\">install</span> Spire.Doc.Free</pre>\n</div>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2 class=\"h4\" id=\"3\">二、基础实现：单篇 Markdown 转 Word</h2>\n<h3 class=\"h5\" id=\"4\">场景1：将 Markdown 文本直接转换为 Word</h3>\n<p class=\"p\">适用于 Markdown 内容较短、无需读取文件的场景，核心代码如下：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">from</span> spire.doc <span style=\"color: rgba(0, 0, 255, 1);\">import</span> *\n<span style=\"color: rgba(0, 0, 255, 1);\">from</span> spire.doc.common <span style=\"color: rgba(0, 0, 255, 1);\">import</span> *\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 定义要转换的Markdown文本（涵盖常见语法）</span>\nmarkdown_text = <span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\n# 一级标题：Markdown转Word测试\n## 二级标题：功能演示\n### 三级标题：基础语法支持\n\n#### 1. 段落与强调\n这是一段普通段落，支持**粗体**、*斜体*、`行内代码`，以及[超链接](https://www.google.com/)。\n\n#### 2. 列表\n- 无序列表项1\n- 无序列表项2\n  - 子列表项\n\n1. 有序列表项1\n2. 有序列表项2\n\n#### 3. 代码块\n```python\nprint(\"Hello, Markdown to Word!\")\na = 1 + 2\n```\n</span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span>\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 将markdown文本写入md文档</span>\nmarkdown_path = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">input.md</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\nwith open(markdown_path, </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">w</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span>, encoding=<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">utf-8</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">) as f:\n    f.write(markdown_text)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 创建Document对象</span>\ndoc =<span style=\"color: rgba(0, 0, 0, 1);\"> Document()\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 加载md文档</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">doc.LoadFromFile(markdown_path, FileFormat.Markdown)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 保存为Word文档（支持.doc和.docx格式）</span>\noutput_path = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Markdown转Word.docx</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\ndoc.SaveToFile(output_path, FileFormat.Docx)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 5. 释放资源</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">doc.Close()\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">转换完成！Word文档已保存至：{output_path}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<p>&nbsp;</p>\n<h3 class=\"h5\" id=\"5\">场景2：读取 Markdown 文件转换为 Word</h3>\n<p class=\"p\">适用于已有.md文件的场景（如<code class=\"codespan\">test.md</code>），代码更简洁：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">from</span> spire.doc <span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> Document\n</span><span style=\"color: rgba(0, 0, 255, 1);\">from</span> spire.doc <span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> FileFormat\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 创建Document对象</span>\ndoc =<span style=\"color: rgba(0, 0, 0, 1);\"> Document()\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 直接加载Markdown文件（指定文件路径）</span>\nmarkdown_file_path = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">test.md</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\ndoc.LoadFromFile(markdown_file_path, FileFormat.Markdown)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 保存为Word文档</span>\noutput_path = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Markdown转Word.docx</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\ndoc.SaveToFile(output_path, FileFormat.Docx)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 释放资源</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">doc.Close()\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">文件转换完成！路径：{output_path}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<p>&nbsp;</p>\n<h5 class=\"h5\" id=\"6\">代码关键说明：</h5>\n<ul class=\"ul\">\n<li class=\"listitem\"><code class=\"codespan\">Document()</code>：创建一个空的 Word 文档对象，是所有操作的核心载体；</li>\n<li class=\"listitem\"><code class=\"codespan\">LoadFromFile()</code>：加载 Markdown 文件，第二个参数&nbsp;<code class=\"codespan\">FileFormat.Markdown</code>&nbsp;指定解析格式；</li>\n<li class=\"listitem\"><code class=\"codespan\">SaveToFile()</code>：接收输出路径和文件格式（<code class=\"codespan\">FileFormat.Docx</code>/<code class=\"codespan\">FileFormat.Doc</code>），完成保存；</li>\n<li class=\"listitem\"><code class=\"codespan\">Close()</code>：释放文档资源，避免内存占用。</li>\n</ul>\n<hr />\n<p>&nbsp;</p>\n<h2 class=\"h4\" id=\"7\">三、批量转换多个 Markdown 文件</h2>\n<p class=\"p\">Free Spire.Doc for Python 支持批量转换一个文件夹中的多个 Markdown 文档。</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> os\n</span><span style=\"color: rgba(0, 0, 255, 1);\">from</span> spire.doc <span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> Document\n</span><span style=\"color: rgba(0, 0, 255, 1);\">from</span> spire.doc <span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> FileFormat\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 定义Markdown文件所在文件夹和输出文件夹</span>\nmd_folder = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">./markdown_files</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\noutput_folder </span>= <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">./word_files</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 创建输出文件夹（若不存在）</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(0, 0, 255, 1);\">not</span><span style=\"color: rgba(0, 0, 0, 1);\"> os.path.exists(output_folder):\n    os.makedirs(output_folder)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 遍历文件夹中的所有.md文件</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">for</span> filename <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> os.listdir(md_folder):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> filename.endswith(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">.md</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 拼接文件路径</span>\n        md_path =<span style=\"color: rgba(0, 0, 0, 1);\"> os.path.join(md_folder, filename)\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 生成输出Word文件名（替换后缀为.docx）</span>\n        output_filename = os.path.splitext(filename)[0] + <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">.docx</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\n        output_path </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> os.path.join(output_folder, output_filename)\n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 转换逻辑</span>\n        doc =<span style=\"color: rgba(0, 0, 0, 1);\"> Document()\n        doc.LoadFromFile(md_path, FileFormat.Markdown)\n        doc.SaveToFile(output_path, FileFormat.Docx)\n        doc.Close()\n\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">已转换：{filename} -&gt; {output_filename}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">所有Markdown文件批量转换完成！</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2 class=\"h4\" id=\"8\">常见问题与注意事项</h2>\n<ol class=\"ol\">\n<li class=\"listitem\"><strong class=\"strong\">格式兼容问题</strong>：部分小众 Markdown 语法（如 Mermaid 流程图、LaTeX 公式）暂不支持，转换后可能显示异常，建议提前简化这类内容；</li>\n<li class=\"listitem\"><strong class=\"strong\">编码问题</strong>：若 Markdown 文件含中文，建议保存为 UTF-8 编码，避免转换后出现乱码；</li>\n<li class=\"listitem\"><strong class=\"strong\">免费版限制</strong>：Free Spire.Doc for Python 免费版对文档页数有限制，满足日常轻量使用。</li>\n</ol><hr />\n<h4 class=\"h4\" id=\"9\">&nbsp;</h4>\n<p class=\"p\">通过本文介绍的方法，我们可以通过几行 Python 代码实现 Markdown 转 Word 文档，同时支持批量处理等扩展功能，完美适配日常办公、文档交付等场景。相比其他转换工具，Free Spire.Doc 无需依赖第三方服务，本地运行更安全，且 Python 接口友好，新手易上手。</p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"itemdesc\">\n                发表于 \n<span id=\"post-date\">2026-01-26 12:00</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jazz-z\">LAYONTHEGROUND</a>&nbsp;\n阅读(<span id=\"post_view_count\">5</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n            </div>"
    },
    {
      "title": "如何正确的 DDD",
      "link": "https://www.cnblogs.com/xiaozhuang/p/19532510",
      "published": "",
      "description": "<h1 class=\"postTitle\"><a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xiaozhuang/p/19532510\" id=\"cb_post_title_url\" title=\"发布于 2026-01-26 11:51\">\n    <span>如何正确的 DDD</span>\n    \n\n</a>\n</h1>\n\t<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>在架构设计领域，DDD（领域驱动设计）被讨论得最多，也被误解得最深。很多公司所谓的“官方指导文件”其实是在南辕北辙。如果不纠正这些根源上的错误，所谓的架构优化只能是空中楼阁。</p>\n<p>以下是对当前行业内、甚至是某些大厂指导文件中典型错误做法的深度批判。</p>\n<hr />\n<h2>一、 批判：指导文件建议“不使用领域服务”</h2>\n<p><strong>【现状批判】</strong>：很多公司迷信“绝对充血模型”，要求逻辑必须全写在领域实体中。然而，现实中大量业务涉及<strong>同领域内多个实体的对比与关联校验</strong>（并非跨领域协作）。因为逻辑在单个实体里塞不进去，而指导文件又禁用了领域服务，程序员被迫将核心业务规则“偷渡”到<strong>应用层</strong>。</p>\n<p><strong>【正确方式】</strong>：<strong>领域服务是逻辑的指挥官。</strong> 凡是涉及本领域内多实体协作、业务主键查重、规则验证，必须由领域服务承载。它是领域层对外的唯一逻辑入口，守住领域核心逻辑。</p>\n<hr />\n<h2>二、 批判：在应用层加入“公共方法目录”</h2>\n<p><strong>【现状批判】</strong>：由于领域逻辑被驱逐到了应用层，为了复用，开发者只能在应用层搞一个 <code>Common</code> 或 <code>Util</code>。这本质上是由于领域逻辑无法归位而导致的“架构流产”。</p>\n<p><strong>【正确方式】</strong>：<strong>业务逻辑必须回归领域。</strong> 任何涉及规则的复用，必须沉淀在各自的领域服务中。应用层只负责调度流程，绝不负责存储业务规则。</p>\n<hr />\n<h2>三、 批判：指导文件明确“不使用聚合”</h2>\n<p><strong>【现状批判】</strong>：有些指导文档主张平铺实体。没有聚合，实体就像散沙，应用层可以随意绕过业务规则修改实体状态，导致一致性防线全面崩溃。</p>\n<p><strong>【正确方式】</strong>：<strong>每一个领域必须是一个聚合。</strong> 聚合根是唯一的入口。只有通过聚合根，才能保证业务规则（不变量）在任何时候都是有效的。聚合不是负担，而是保护业务逻辑的盔甲。</p>\n<hr />\n<h2>四、 批判：在应用层定义外部调用防腐接口</h2>\n<p><strong>【现状批判】</strong>：文档建议在应用层定义防腐接口，这会导致<strong>基础服务反向依赖应用层</strong>，造成严重的依赖环和逻辑污染。</p>\n<p><strong>【正确方式】</strong>：<strong>新建领域对象，在领域层定义接口。</strong> 应该在领域层创建接口契约，由基础架构层（Infrastructure）去实现细节。最后，将该能力封装成<strong>领域服务</strong>供应用层调用。确保领域层自持，基础架构层只负责“插件式”实现。</p>\n<hr />\n<h2>五、 批判：领域服务调用其它领域的仓储</h2>\n<p><strong>【现状批判】</strong>：在订单领域直接注入用户领域的 Repository。这种“跨界伸手段”让领域边界瞬间消失，两个领域在物理存储层面死死捆绑，未来根本无法拆分。</p>\n<p><strong>【正确方式】</strong>：<strong>领域间绝对零感知。</strong> 领域服务只能调用本领域的仓储。跨领域的交互必须上浮到应用层，由应用层担任“导演”进行编排。</p>\n<hr />\n<h2>六、 批判：把领域对象用来接收领域事件</h2>\n<p><strong>【现状批判】</strong>：让实体或领域对象去监听 MQ 消息。这不仅引入了技术噪音，最致命的是<strong>让领域之间形成了依赖</strong>。一旦领域 A 的对象去监听领域 B 的事件，领域 A 就不再孤立，它被迫感知了外部世界的变化，破坏了领域自治。</p>\n<p><strong>【正确方式】</strong>：<strong>应用层监听，领域服务处理。</strong> 应用层负责接收外部事件并将其“翻译”为本领域能理解的普通请求。领域对象应保持清静，它不该知道“事件”是从哪来的，更不该知道外部领域的存在。</p>\n<hr />\n<h2>领域对象设计原则：全业务封装与绝对隔离</h2>\n<ol start=\"1\">\n<li>\n<p><strong>领域必须封装本领域的所有业务</strong>：通过<strong>领域服务</strong>实现业务逻辑的完全闭环。领域外层（应用层）不需要知道领域内部是如何判决的。</p>\n</li>\n<li>\n<p><strong>领域之间必须绝对隔离</strong>：领域之间要达到“物理级”的互不感知。它们不知道彼此的存在，更不能直接通信。</p>\n</li>\n<li>\n<p><strong>应用层是唯一的导演</strong>：只有应用层才有资格编排各个领域。应用层指挥 A 领域判决、B 领域执行，而领域本身只专注于自己。</p>\n</li>\n</ol><hr />\n<h2>总结：正确 DDD 的标准化原则</h2>\n<ul>\n<li>\n<p><strong>1-1-N 模组结构</strong>：<strong>1 个仓储接口、1 个领域服务、N 个实体（构成 1 个聚合）</strong>。</p>\n</li>\n<li>\n<p><strong>契约自持</strong>：接口定义在领域层，拒绝反向依赖。</p>\n</li>\n<li>\n<p><strong>物理级隔离</strong>：领域间零感知，逻辑全封装，由应用层统一编排。</p>\n</li>\n<li>\n<p><strong>首错即断（Fail-fast）</strong>：领域内遇错直接抛出业务异常。</p>\n</li>\n</ul>\n<p><strong>只有正确的 DDD，才能正确的“微”服务。</strong></p>\n<p>微服务不是拆出来的，而是领域边界清晰后自然“长”出来的。</p>\n<hr />\n<p><strong>博主结语</strong>： 架构的优雅来自于对领域边界的极端克制。那些要求你“禁用领域服务”、“平铺实体”的指导文件，其实是在毁掉你的架构。拒绝虚伪的 DDD，守住领域层的尊严。</p>\n</div>\n<div class=\"clear\"></div>\n\n\t<div class=\"postDesc\">posted on \n<span id=\"post-date\">2026-01-26 11:51</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xiaozhuang\">小庄</a>&nbsp;\n阅读(<span id=\"post_view_count\">3</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "开源一个自己的作品浏览器插件ChaTab，一键提交Prompt到多个AI应用",
      "link": "https://www.cnblogs.com/jqbird/p/19531428/chatab",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jqbird/p/19531428/chatab\" id=\"cb_post_title_url\" title=\"发布于 2026-01-26 09:06\">\n    <span>开源一个自己的作品浏览器插件ChaTab，一键提交Prompt到多个AI应用</span>\n    \n\n</a>\n\n\t\t</h1>\n\t\t<div class=\"clear\"></div>\n\t\t<div class=\"postBody\">\n\t\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"chatab-简介\">ChaTab 简介</h1>\n<p>好看也好用的chrome浏览器首页插件，已经在谷歌浏览器商店上架4个月了，代码调整到基本稳定状态了，所以打算把它开源出来。<br />\n每天在用各种AI工具，不同平台切换，非常烦人，所以就自己做了一个这样的工具给自己用，挺好用，所以分享给有类似痛点的用户。</p>\n<h1 id=\"开发工具\">开发工具</h1>\n<p>cursor + claude code</p>\n<p>现在软件开发的成本极大降低，再也不是手工业时代的一针一线的编码了，只要基础架构设计好，AI基本可以完成 90%的设计和编码工作，</p>\n<h1 id=\"插件功能说明\">插件功能说明</h1>\n<ol>\n<li>批量Prompt提交，一键快速批量发送提示词至 ChatGPT、DeepSeek 等多个 AI 应用，提升效率。</li>\n</ol>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<ol start=\"2\">\n<li>智能背景切换 ， 随机更换 Chrome 启动页背景，让每次开启浏览器都有新鲜感。</li>\n</ol>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<p>3.应用导航，下拉的时候点击icon快速链接到应用地址。</p>\n<p><img alt=\"2\" class=\"lazyload\" /></p>\n<ol start=\"4\">\n<li>左侧历史记录，点击可以快速填充历史记录到tab里的输入框中。</li>\n</ol>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<h1 id=\"使用说明\">使用说明</h1>\n<p><img alt=\"image\" class=\"lazyload\" /></p>\n<p>1、谷歌浏览器商店搜索 \"chatab\"，或者复制链接地址安装：</p>\n<p><a href=\"https://chromewebstore.google.com/detail/chatab/gcbcmnekbambjebgfjnbgopmgcgcealn?hl=zh-CN&amp;utm_source=ext_sidebar\" rel=\"noopener nofollow\" target=\"_blank\">https://chromewebstore.google.com/detail/chatab/gcbcmnekbambjebgfjnbgopmgcgcealn?hl=zh-CN&amp;utm_source=ext_sidebar</a></p>\n<p>2、使用前请先在浏览器中登录 ChatGPT、DeepSeek 等相关平台。插件将为您提供一键提交、多平台切换的便捷体验。</p>\n<p>注意：这个插件会修改浏览器首页。</p>\n<h1 id=\"开源地址\">开源地址</h1>\n<ul>\n<li><a href=\"https://github.com/robotbird/chatab\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/robotbird/chatab</a></li>\n</ul>\n<h1 id=\"关于作者\">关于作者</h1>\n<p>产品经理，热爱哲学，个人博客 <a href=\"http://www.jqpress.com\" rel=\"noopener nofollow\" target=\"_blank\">jqpress.com</a>，微信公众号：产品经理随想曲</p>\n<p><img alt=\"产品经理随想曲\" class=\"lazyload\" /></p>\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-26 09:06</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jqbird\">叶鹏</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "HagiCode 实践：如何利用 GitHub Actions 实现 Docusaurus 自动部署",
      "link": "https://www.cnblogs.com/newbe36524/p/19531383",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/newbe36524/p/19531383\" id=\"cb_post_title_url\" title=\"发布于 2026-01-26 08:56\">\n    <span>HagiCode 实践：如何利用 GitHub Actions 实现 Docusaurus 自动部署</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"为-hagicode-添加-github-pages-自动部署支持\">为 HagiCode 添加 GitHub Pages 自动部署支持</h1>\n<blockquote>\n<p>本项目早期代号为 PCode，现已正式更名为 HagiCode。本文记录了如何为项目引入自动化静态站点部署能力，让内容发布像喝水一样简单。</p>\n</blockquote>\n\n<h2 id=\"背景引言\">背景/引言</h2>\n<p>在 HagiCode 的开发过程中，我们遇到了一个很现实的问题：随着文档和提案越来越多，如何高效地管理和展示这些内容成了当务之急。我们决定引入 GitHub Pages 来托管我们的静态站点，但是手动构建和部署实在是太麻烦了——每次改动都要本地构建、打包，然后手动推送到 <code>gh-pages</code> 分支。这不仅效率低下，还容易出错。</p>\n<p>为了解决这个问题（主要是为了偷懒），我们需要一套自动化的部署流程。本文将详细记录如何为 HagiCode 项目添加 GitHub Actions 自动部署支持，让我们只需专注于内容创作，剩下的交给自动化流程。</p>\n<h2 id=\"关于-hagicode\">关于 HagiCode</h2>\n<blockquote>\n<p>嘿，介绍一下我们正在做的东西</p>\n</blockquote>\n<p>我们正在开发 <strong>HagiCode</strong>——一款 AI 驱动的代码智能助手，让开发体验变得更智能、更便捷、更有趣。</p>\n<p><strong>智能</strong>——AI 全程辅助，从想法到代码，让编码效率提升数倍。<strong>便捷</strong>——多线程并发操作，充分利用资源，开发流程顺畅无阻。<strong>有趣</strong>——游戏化机制和成就系统，让编码不再枯燥，充满成就感。</p>\n<p>项目正在快速迭代中，如果你对技术写作、知识管理或者 AI 辅助开发感兴趣，欢迎来 <a href=\"https://github.com/HagiCode-org/site\" rel=\"noopener nofollow\" target=\"_blank\">GitHub</a> 看看～</p>\n<h2 id=\"目标分析\">目标分析</h2>\n<p>在动手之前，我们得先明确这次任务到底要干啥。毕竟磨刀不误砍柴工嘛。</p>\n<h3 id=\"核心需求\">核心需求</h3>\n<ol>\n<li><strong>自动化构建</strong>：当代码推送到 <code>main</code> 分支时，自动触发构建流程。</li>\n<li><strong>自动部署</strong>：构建成功后，自动将生成的静态文件部署到 GitHub Pages。</li>\n<li><strong>环境一致性</strong>：确保 CI 环境和本地构建环境一致，避免\"本地能跑，线上报错\"的尴尬。</li>\n</ol>\n<h3 id=\"技术选型\">技术选型</h3>\n<p>考虑到 HagiCode 是基于 Docusaurus 构建的（一种非常流行的 React 静态站点生成器），我们可以利用 GitHub Actions 来实现这一目标。</p>\n<h2 id=\"配置-github-actions-工作流\">配置 GitHub Actions 工作流</h2>\n<p>GitHub Actions 是 GitHub 提供的 CI/CD 服务。通过在代码仓库中定义 YAML 格式的工作流文件，我们可以定制各种自动化任务。</p>\n<h3 id=\"创建工作流文件\">创建工作流文件</h3>\n<p>我们需要在项目根目录下的 <code>.github/workflows</code> 文件夹中创建一个新的配置文件，比如叫 <code>deploy.yml</code>。如果文件夹不存在，记得先手动创建一下。</p>\n<p>这个配置文件的核心逻辑如下：</p>\n<ol>\n<li><strong>触发条件</strong>：监听 <code>main</code> 分支的 <code>push</code> 事件。</li>\n<li><strong>运行环境</strong>：最新版的 Ubuntu。</li>\n<li><strong>构建步骤</strong>：\n<ul>\n<li>检出代码</li>\n<li>安装 Node.js</li>\n<li>安装依赖 (<code>npm install</code>)</li>\n<li>构建静态文件 (<code>npm run build</code>)</li>\n</ul>\n</li>\n<li><strong>部署步骤</strong>：使用官方提供的 <code>action-gh-pages</code> 将构建产物推送到 <code>gh-pages</code> 分支。</li>\n</ol>\n<h3 id=\"关键配置代码\">关键配置代码</h3>\n<p>以下是我们最终采用的配置模板：</p>\n<pre><code class=\"language-yaml\">name: Deploy to GitHub Pages\n\n# 触发条件：当推送到 main 分支时\non:\n  push:\n    branches:\n      - main\n    # 可以根据需要添加路径过滤，比如只有文档变动才构建\n    # paths:\n    #   - 'docs/**'\n    #   - 'package.json'\n\n# 设置权限，这对于部署到 GitHub Pages 很重要\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\n# 并发控制：取消同一分支的旧构建\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: false\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        # 注意：必须设置 fetch-depth: 0，否则可能导致构建版本号不准确\n        with:\n          fetch-depth: 0\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 20 # 建议与本地开发环境保持一致\n          cache: 'npm'     # 启用缓存可以加速构建过程\n\n      - name: Install dependencies\n        run: npm ci\n        # 使用 npm ci 而不是 npm install，因为它更快、更严格，适合 CI 环境\n\n      - name: Build website\n        run: npm run build\n        env:\n          # 如果你的站点构建需要环境变量，在这里配置\n          # NODE_ENV: production\n          # PUBLIC_URL: /your-repo-name\n          \n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: ./build # Docusaurus 默认输出目录\n\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: ubuntu-latest\n    needs: build\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\n</code></pre>\n<h2 id=\"实施过程中的坑点\">实施过程中的坑点</h2>\n<p>在实际操作中，我们遇到了一些问题，这里分享出来希望大家能避开（或者提前准备好解决方案）。</p>\n<h3 id=\"1-github-token-权限问题\">1. GitHub Token 权限问题</h3>\n<p>最开始配置的时候，部署总是报错 403 (Forbidden)。查了好久才发现，是因为 GitHub 默认的 <code>GITHUB_TOKEN</code> 并没有写入 Pages 的权限。</p>\n<p><strong>解决方案</strong>：在仓库的 <code>Settings</code> -&gt; <code>Actions</code> -&gt; <code>General</code> -&gt; <code>Workflow permissions</code> 中，务必选择 <strong>\"Read and write permissions\"</strong>。</p>\n<h3 id=\"2-构建目录路径错误\">2. 构建目录路径错误</h3>\n<p>Docusaurus 默认把构建好的静态文件放在 <code>build</code> 目录。但是有些项目（比如 Create React App 默认是 <code>build</code>，Vite 默认是 <code>dist</code>）可能配置不一样。如果在 Actions 中报错找不到文件，记得去 <code>docusaurus.config.js</code> 里检查一下输出路径配置。</p>\n<h3 id=\"3-子路径问题\">3. 子路径问题</h3>\n<p>如果你的仓库不是用户主页（即不是 <code>username.github.io</code>），而是项目主页（比如 <code>username.github.io/project-name</code>），你需要配置 <code>baseUrl</code>。</p>\n<p>在 <code>docusaurus.config.js</code> 中：</p>\n<pre><code class=\"language-javascript\">module.exports = {\n  // ...\n  url: 'https://HagiCode-org.github.io', // 你的 GitHub URL\n  baseUrl: '/site/', // 如果你的仓库叫 site，这里就填 /site/\n  // ...\n};\n</code></pre>\n<p>这一点很容易被忽略，配置不对会导致页面打开全是白屏，因为资源路径加载不到。</p>\n<h2 id=\"验证成果\">验证成果</h2>\n<p>配置完所有东西并推送代码后，我们就可以去 GitHub 仓库的 <strong>Actions</strong> 标签页看戏了。</p>\n<p>你会看到黄色的圆圈（工作流正在运行），变绿就代表成功啦！如果变红了，点击进去查看日志，通常都能排查出问题（大部分时候是拼写错误或者路径配置不对）。</p>\n<p>构建成功后，访问 <code>https://&lt;你的用户名&gt;.github.io/&lt;仓库名&gt;/</code> 就能看到崭新的站点了。</p>\n<h2 id=\"总结\">总结</h2>\n<p>通过引入 GitHub Actions，我们成功实现了 HagiCode 文档站的自动化部署。这不仅节省了手动操作的时间，更重要的是保证了发布流程的标准化。现在不管是哪位小伙伴更新了文档，只要合并到 <code>main</code> 分支，几分钟后就能在线上看到最新的内容。</p>\n<p><strong>核心收益</strong>：</p>\n<ul>\n<li><strong>效率提升</strong>：从\"手动打包、手动上传\"变成\"代码即发布\"。</li>\n<li><strong>降低错误</strong>：消除了人为操作失误的可能性。</li>\n<li><strong>体验优化</strong>：让开发者更专注于内容质量，而不是被繁琐的部署流程困扰。</li>\n</ul>\n<p>虽然配置 CI/CD 刚开始有点麻烦（尤其是各种权限和路径问题），但这是一次性投入，长期回报巨大的工作。强烈建议所有静态站点项目都接入类似的自动化流程。</p>\n<h2 id=\"参考资料\">参考资料</h2>\n<ul>\n<li><a href=\"https://docs.github.com/en/actions\" rel=\"noopener nofollow\" target=\"_blank\">GitHub Actions 官方文档</a></li>\n<li><a href=\"https://docusaurus.io/docs/deployment\" rel=\"noopener nofollow\" target=\"_blank\">Docusaurus 部署指南</a></li>\n<li>[actions-gh-pages Action 使用说明](<a href=\"https://github.com\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com</a> peaceiris/actions-gh-pages)</li>\n</ul>\n<hr />\n<p>感谢您的阅读,如果您觉得本文有用,快点击下方点赞按钮👍,让更多的人看到本文。</p>\n<p>本内容采用人工智能辅助协作,经本人审核,符合本人观点与立场。</p>\n<h2 id=\"元信息\">元信息</h2>\n<ul>\n<li><strong>本文作者:</strong> <a href=\"https://www.newbe.pro\" rel=\"noopener nofollow\" target=\"_blank\">newbe36524</a></li>\n<li><strong>本文链接:</strong> <a href=\"https://hagicode-org.github.io/site/blog/2026/01/25/docusaurus-auto-deployment-with-github-actions\" rel=\"noopener nofollow\" target=\"_blank\">https://hagicode-org.github.io/site/blog/2026/01/25/docusaurus-auto-deployment-with-github-actions</a></li>\n<li><strong>版权声明:</strong> 本博客所有文章除特别声明外,均采用 BY-NC-SA 许可协议。转载请注明出处!</li>\n</ul>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-26 08:56</span>&nbsp;\n<a href=\"https://www.cnblogs.com/newbe36524\">Newbe36524</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "复刻 ChatGPT 高级数据分析！Sdcb Chats 1.10 重磅发布：能分析Excel、做PPT",
      "link": "https://www.cnblogs.com/sdcb/p/19528764/chats-1-10",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/sdcb/p/19528764/chats-1-10\" id=\"cb_post_title_url\" title=\"发布于 2026-01-26 08:45\">\n    <span>复刻 ChatGPT 高级数据分析！Sdcb Chats 1.10 重磅发布：能分析Excel、做PPT</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>Chats 1.10.0 发布了！这是一个我个人非常喜欢，也期待已久的版本。</p>\n<p>距离 1.9.0 发布（2025 年 11 月 27 日）已经过去了近两个月。这期间，我并不是在摸鱼，而是在憋一个“大招”——<strong>内置代码执行器（Code Interpreter）</strong>。</p>\n<blockquote>\n<p>如果你还不了解 <strong>Sdcb Chats</strong>：简单说，这是一个支持多家主流模型服务商的 AI 网关。它不只能让你在一个统一界面里聚合管理所有模型，同时也兼容标准 API 协议，支持 Docker 一键部署与多数据库支持。</p>\n</blockquote>\n<h2 id=\"为什么是代码执行器\">为什么是代码执行器？</h2>\n<p>Sdcb Chats 1.9发布之后，很多人好奇我下一步准备怎么走，我当时想的两个方向：</p>\n<p><strong>方向一：Dify 模式（Dify化）</strong><br />\n一种是支持发布成 App 的功能，比如通过一定的系统提示词、工具集选择、模型参数设置（如温度等），可以将这样的东西打包发布成一个像 App 一样的网页，或者是一个 js 入口。用户可以通过这个网页直接使用 Chats 的预定功能和 AI 大模型聊天、完成指定任务。打包成的 js 甚至可以嵌入用户（通常是企业用户）的网页中，这样一来用户就可以直接在自己的网站上使用定制化的 AI 助手了。这个方向很有商业潜力，很多客户都在问。</p>\n<p><strong>方向二：Code Interpreter 模式（ADA）</strong><br />\n另外一个方向就是实现内置的基于 Docker 的沙箱功能。因为我经常看到 ChatGPT 网站（或者像 Manus 一样）中可以执行一段 Python 脚本——或者经过一系列多步骤的过程，然后生成一张图片或者一个图表、Excel、PPT。这个功能之前 ChatGPT 叫作 Code Interpreter（代码执行器），后来为了显得更专业，改名叫“高级数据分析”（Advanced Data Analysis，ADA）。</p>\n<p>这个功能的核心在于：<strong>它可以让 AI 模型直接操作文件、生成各种格式的输出，而不需要用户手动去处理数据和文件。</strong></p>\n<p>我发现，目前市面上除了 OpenAI 提供了完整体验外，像 Gemini、Grok、Qwen（基于OpenWebUI）等都没有提供类似的功能支持。</p>\n<p><strong>我的选择</strong><br />\n从 Chats 的商业化角度来说，Dify 方向肯定更有“钱”景。但我个人对 ADA 方向更感兴趣一些，因为我觉得这个功能更“硬核”，也更能体现 AI 模型的能力和潜力。</p>\n<p>所以我最终选择了 ADA 方向作为 1.10 版本发布的核心功能。经过几周的努力，1.10 版本终于完成了内置 Docker Code Interpreter 的功能！现在用户可以在 Chats 中直接请 AI 创建一个 Docker 会话，上传文件，让 AI 模型执行代码、分析数据、生成 PPT 等，非常方便实用。</p>\n<h2 id=\"强大的-docker-沙箱不只是-python\">强大的 Docker 沙箱：不只是 Python</h2>\n<p>为了实现这个功能，我不仅仅是加了几个 API 那么简单。</p>\n<p>这个功能的 PoC 其实早在去年 11 月我就完成了，之后我陆续打磨，直到 2026 年元旦的时候我和 AI 做了多轮的设计，最终定稿并创建了 7 个内置工具：</p>\n<ul>\n<li><code>create_docker_session</code>：创建环境</li>\n<li><code>run_command</code>：执行命令</li>\n<li><code>read_file</code> / <code>write_file</code> / <code>patch_file</code>：文件操作</li>\n<li><code>download_chat_files</code>：文件流转</li>\n<li><code>destroy_session</code>：资源回收</li>\n</ul>\n<p>我还内置了一套 <strong>Artifact（工件）文件夹跟踪机制</strong>。简单来说，如果大模型操作命令行工具或者其它什么脚本将文件放到了指定的 artifact 文件夹，Chats 系统就会自动帮用户保存下来，用户在聊天界面可以直接点击下载。</p>\n<h3 id=\"专属镜像sdcbcode-interpreter\">专属镜像：sdcb/code-interpreter</h3>\n<p>为了让体验达到极致，我不想让大家每次都去拉取巨大的通用镜像，也不想让大家费劲去配置环境。因此，我专门构建了一个 docke 镜像：<code>sdcb/code-interpreter</code>。</p>\n<p>和 OpenAI 的 ADA 功能类似，这个镜像里预装了常用的工具链和依赖库：</p>\n<ul>\n<li><strong>语言环境</strong>：Python, .NET (Dotnet), Node.js, GCC</li>\n<li><strong>数据处理</strong>：SQLite3, Pandas, Numpy</li>\n<li><strong>文档办公</strong>：LibreOffice, Pandoc</li>\n<li><strong>多媒体</strong>：FFmpeg, ImageMagick</li>\n<li><strong>Web自动化</strong>：Playwright Chromium</li>\n</ul>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260125102326917-629370352.png\" /></p>\n<p>大模型在使用这个镜像时，会自动加载 <code>/app/skills.md</code> 这个文件，里面列出了镜像中预装的工具和库：</p>\n<pre><code class=\"language-md\">This environment is pre-configured with the following tools and libraries:\n* utilities: git, LibreOffice, Pandoc, Poppler (`pdftotext`, `pdfinfo`), sqlite3, file, FFmpeg {ffmpegVersion}, git, imagemagick, playwright[chromium]\n* dotnet {dotnetVersion}, commands: `dotnet build`, `dotnet run single-file.cs`, `dotnet add package`, pre-cached NuGet packages in `/opt/nuget-local`\n* python {pythonVersion}, commands: `python3 - &lt;&lt;'PY' ...`, `pip install --break-system-packages package-name`, IMPORTANT: Many packages are pre-installed, Always check with `pip list` BEFORE installing to avoid unnecessary waits.\n* C/C++ Toolchain (gcc {gccVersion}), tools: `gcc`, `g++`, `make`, `cmake`\n* node.js {nodeVersion}, commands: `node`, `npm`, IMPORTANT: Many packages are pre-installed globally. Always check with `npm -g ls` BEFORE installing.\n</code></pre>\n<p>大模型可以直接调用这些工具，非常方便。</p>\n<p>值得一提的是，作为一名 .NET 爱好者，这个镜像我是基于 .NET 做的（Base Image），因此碰巧你像我一样使用 .NET/C# 来验证一些东西的话，让大模型直接使用这个镜像就行了！</p>\n<h2 id=\"场景演示它能做什么\">场景演示：它能做什么？</h2>\n<p>光说不练假把式，我们来看看它在实际场景中的威力。</p>\n<h3 id=\"场景一分析-excel-并生成图表\">场景一：分析 Excel 并生成图表</h3>\n<p>以前我们让 AI 分析 Excel，通常是将数据格式发给AI，然后让它帮忙写代码，用户再自己运行代码生成图表。现在可以直接让 AI 在沙箱中操作 Excel 文件，生成图表。</p>\n<p>比如你有一个这样的excel：<a href=\"https://cv-public.sdcb.pub/2026/changsha_weather_2025.xlsx\" rel=\"noopener nofollow\" target=\"_blank\">https://cv-public.sdcb.pub/2026/changsha_weather_2025.xlsx</a><br />\n里面的数据是长沙 2025 年的每日天气记录：<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260125102327825-611206051.png\" /></p>\n<blockquote>\n<p><strong>用户</strong>：请帮我分析这个 Excel 文件，生成一个包含每月平均气温和降水量的报告，并附上图表。</p>\n</blockquote>\n<p>AI 会直接在 Docker 沙箱中运行 Python 代码，使用 Pandas 和 Matplotlib 生成图表，然后把结果打包成报告发给你（视频有加速）。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260125102328487-114968639.avif\" /></p>\n<h3 id=\"场景二一句话生成-ppt\">场景二：一句话生成 PPT</h3>\n<p>这是我最爱的功能之一。</p>\n<blockquote>\n<p><strong>用户</strong>：请根据上面的天气分析，帮我生成一份以“2025年长沙天气分析报告”为主题的PPT，包含封面、目录、数据分析、图表展示和结论五个部分。</p>\n</blockquote>\n<p>AI 会调用 <code>python-pptx</code> 库，在沙箱中生成 <code>.pptx</code> 文件，然后给你一个下载链接。你下载下来直接就可以去汇报了（当然最好还是微调一下）。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260125102329000-1999082053.png\" /></p>\n<p>这是原始 PPT 的下载链接，有兴趣的可以看看感受一下效果：<a href=\"https://cv-public.sdcb.pub/2026/changsha_weather_report_2025.pptx\" rel=\"noopener nofollow\" target=\"_blank\">https://cv-public.sdcb.pub/2026/changsha_weather_report_2025.pptx</a></p>\n<h3 id=\"场景三做实验\">场景三：做实验</h3>\n<p>这其实也是我最喜欢的一个场景。比如我想测试一个新的算法，或者验证一个代码片段，我可以直接让 AI 在沙箱中帮我跑代码。</p>\n<p>比如之前我写过两篇博客：</p>\n<ul>\n<li>《不服跑个分？.NET 10 大整数计算对阵 Java，结果令人意外》（博客园链接：<a href=\"https://www.cnblogs.com/sdcb/p/19484525/20261113-big-integer-dotnet-10-vs-java%EF%BC%89\" target=\"_blank\">https://www.cnblogs.com/sdcb/p/19484525/20261113-big-integer-dotnet-10-vs-java）</a></li>\n<li>《.NET 10了，HttpClient还是不能用using吗？我做了一个实验》（博客园链接：<a href=\"https://www.cnblogs.com/sdcb/p/19500792/20260119-using-httpclient%EF%BC%89\" target=\"_blank\">https://www.cnblogs.com/sdcb/p/19500792/20260119-using-httpclient）</a></li>\n</ul>\n<p>这里面有大量的代码、实验数据和图表，我可以直接让 AI 在沙箱中帮我跑这些代码，生成数据，然后帮我分析结果、生成图表，极大地提高了我的写作效率：</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260125102329414-26338747.png\" /></p>\n<h2 id=\"其它重磅更新\">其它重磅更新</h2>\n<p>除了代码执行器，1.10 还有很多硬核更新：</p>\n<h3 id=\"1-交错思考改进\">1. 交错思考改进</h3>\n<p>在 1.9 版本中，我引入了交错思考（Interleaved Thinking）的概念，允许大模型在回答问题时分多步思考和行动。但只支持 Minimax-M2、Anthropic Claude等模型支持交错思考。</p>\n<p>在 1.10 版本中，我扩展了对更多模型的支持，包括 OpenAI Responses API、DeepSeek V3.2模型。</p>\n<p>比如你看下面的使用量统计（来自 Chats 的截图）：</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260125102329774-510845930.png\" /></p>\n<p>我当时使用的模型是 OpenAI 的 gpt-5.2，它通过 Response API 的交错思考将思考信息回传，在总结 201K Tokens 的对话中，有 189K 触发了请求缓存，节省了大量的计算资源和费用，还提升了模型能力，这都依赖于交错思考的功能。</p>\n<p>到此，Sdcb Chats中已经有这些模型提供商确认支持完整的交错思考功能：</p>\n<table>\n<thead>\n<tr>\n<th>Id</th>\n<th>Name</th>\n<th>加入时间</th>\n<th>交错思考</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>Azure AI Foundry</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td>✅</td>\n</tr>\n<tr>\n<td>2</td>\n<td>腾讯混元</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td>❓</td>\n</tr>\n<tr>\n<td>3</td>\n<td>零一万物</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td></td>\n</tr>\n<tr>\n<td>4</td>\n<td>月之暗面</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td>✅</td>\n</tr>\n<tr>\n<td>5</td>\n<td>OpenAI</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td>✅</td>\n</tr>\n<tr>\n<td>6</td>\n<td>百度千帆</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td></td>\n</tr>\n<tr>\n<td>7</td>\n<td>阿里百炼</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td>❓</td>\n</tr>\n<tr>\n<td>8</td>\n<td>讯飞星火</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td></td>\n</tr>\n<tr>\n<td>9</td>\n<td>智谱AI</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/3b3918af\" rel=\"noopener nofollow\" target=\"_blank\">2024-09-05</a></td>\n<td>❓</td>\n</tr>\n<tr>\n<td>10</td>\n<td>DeepSeek</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/30db0079\" rel=\"noopener nofollow\" target=\"_blank\">2024-12-06</a></td>\n<td>✅</td>\n</tr>\n<tr>\n<td>11</td>\n<td>x.ai</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/0d1cab20\" rel=\"noopener nofollow\" target=\"_blank\">2024-12-11</a></td>\n<td></td>\n</tr>\n<tr>\n<td>12</td>\n<td>Github Models</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/0d1cab20\" rel=\"noopener nofollow\" target=\"_blank\">2024-12-11</a></td>\n<td></td>\n</tr>\n<tr>\n<td>13</td>\n<td>谷歌AI</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/a4effc1b\" rel=\"noopener nofollow\" target=\"_blank\">2025-01-10</a></td>\n<td>✅</td>\n</tr>\n<tr>\n<td>14</td>\n<td>Ollama</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/6a5288e7\" rel=\"noopener nofollow\" target=\"_blank\">2025-01-20</a></td>\n<td></td>\n</tr>\n<tr>\n<td>15</td>\n<td>MiniMax</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/6a5288e7\" rel=\"noopener nofollow\" target=\"_blank\">2025-01-20</a></td>\n<td>✅</td>\n</tr>\n<tr>\n<td>16</td>\n<td>火山方舟</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/843510ff\" rel=\"noopener nofollow\" target=\"_blank\">2025-01-24</a></td>\n<td>❓</td>\n</tr>\n<tr>\n<td>17</td>\n<td>硅基流动</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/889144cf\" rel=\"noopener nofollow\" target=\"_blank\">2025-02-08</a></td>\n<td>❓</td>\n</tr>\n<tr>\n<td>18</td>\n<td>OpenRouter</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/15adedfe\" rel=\"noopener nofollow\" target=\"_blank\">2025-03-05</a></td>\n<td>❓</td>\n</tr>\n<tr>\n<td>19</td>\n<td>小马算力</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/32e4a0d5\" rel=\"noopener nofollow\" target=\"_blank\">2025-11-07</a></td>\n<td>❓</td>\n</tr>\n<tr>\n<td>20</td>\n<td>Anthropic</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/22ebef98\" rel=\"noopener nofollow\" target=\"_blank\">2025-11-24</a></td>\n<td>✅</td>\n</tr>\n<tr>\n<td>21</td>\n<td>小米Mimo</td>\n<td><a href=\"https://github.com/sdcb/chats/commit/026f1a4e\" rel=\"noopener nofollow\" target=\"_blank\">2025-12-17</a></td>\n<td>✅</td>\n</tr>\n</tbody>\n</table>\n<p>注：</p>\n<ul>\n<li>❓代表模型提供商使用了基于 Anthropic Messages API 的实现，按协议推断应该支持，但由于未做过端到端测试，因此不确定是否能实现完整的交错思考能力——我相当肯定不能简单地说“支持”，因为有些模型在实现上可能会有差异化。</li>\n<li>还有一些模型提供商基于 OpenAI Responses API 实现的，比如文心千帆，理论上只要支持 Responses API 也能实现交错思考，但我还没有做过测试，因此暂时不列入表格。</li>\n</ul>\n<h3 id=\"2-用户体验大升级\">2. 用户体验大升级</h3>\n<ul>\n<li>\n<p><strong>工具调用展示</strong>：为了配合 Code Interpreter，我重写了工具调用的 UI（<code>ToolCallBlock</code>）。现在可以看到实时的控制台输出流（Log Stream），体验就像自己在看终端一样爽。<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260125102330184-33469646.avif\" /></p>\n</li>\n<li>\n<p><strong>拖拽上传</strong>：支持直接 Ctrl+V 粘贴文件，或者拖拽文件到输入框。<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260125102330573-196654545.png\" /></p>\n</li>\n<li>\n<p><strong>生成信息统计</strong>：现在气泡上会显示 Step 数量、平均耗时等极客信息（如上已经有截图示例）。</p>\n</li>\n</ul>\n<h2 id=\"幕后花絮\">幕后花絮</h2>\n<h3 id=\"那个让人头秃的-ef-core-bug\">那个让人头秃的 EF Core Bug</h3>\n<p>开发后期有一个很烦人的 EF Core 问题，表现是会话追踪的数据有时候会莫名其妙地状态不对。当时我把日志丢给 AI 大模型，让它帮我分析了好久都没解决，给出的建议都在“隔靴搔痒”。</p>\n<p>后来没办法，我只能拿出传统的“调试器大法”，一步一步地断点调试，才发现是 EF Core 在处理并发更新时的状态跟踪（Change Tracking）导致了数据污染。果然，关键时刻还是得靠人类工程师的直觉和经验啊！😅</p>\n<h3 id=\"为什么迟到了\">为什么迟到了？</h3>\n<p>1.10 其实早在 <strong>2026-01-14</strong> 就发布了，为什么今天是 26 号我才发文章宣传？</p>\n<p>一方面是我自己在进行高强度的“狗粮测试”（Dogfooding），确保在这个 Docker 沙箱里跑 rm -rf / 不会把我的宿主机炸了（开玩笑的，有安全限制）。</p>\n<p>另一方面，我补了大量的文档。新功能的配置参数比较多，为了让大家能顺利部署，我花了几天时间重新梳理了 <a href=\"https://github.com/sdcb/chats/blob/main/README.md\" rel=\"noopener nofollow\" target=\"_blank\">README</a> 和 <a href=\"https://github.com/sdcb/chats/blob/main/doc/zh-CN/release-notes/1.10.0.md\" rel=\"noopener nofollow\" target=\"_blank\">Release Note</a>，大家部署前一定要去看看。</p>\n<hr />\n<h2 id=\"结语\">结语</h2>\n<p>Chats 1.10 是一个里程碑，它让 Chats 从一个单纯的“聊天窗口”进化成了一个“生产力工场”。我非常期待看到大家用这个功能玩出什么花样！</p>\n<p>喜欢的朋友请给我的 GitHub 项目一个 star：<br />\n🌟 <strong><a href=\"https://github.com/sdcb/chats\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/sdcb/chats</a></strong></p>\n<p>这是完整的更新日志：<br />\n<a href=\"https://github.com/sdcb/chats/blob/main/doc/zh-CN/release-notes/README.md\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/sdcb/chats/blob/main/doc/zh-CN/release-notes/README.md</a></p>\n<p>有什么想法也欢迎在评论区留言交流，也欢迎加入我的新创建的微信群：</p>\n<p><img alt=\"微信群\" src=\"https://io.starworks.cc:88/cv-public/2026/chats-wxg-qr.png\" /></p>\n<p>如果你更习惯用 QQ 群或者上面的微信群链接失效的话，也可以加入 Chats QQ 群：<strong>498452653</strong>，我们一起探索更多 AI 技术硬核玩法。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-26 08:45</span>&nbsp;\n<a href=\"https://www.cnblogs.com/sdcb\">.NET骚操作</a>&nbsp;\n阅读(<span id=\"post_view_count\">258</span>)&nbsp;\n评论(<span id=\"post_comment_count\">5</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "上周热点回顾（1.19-1.25）",
      "link": "https://www.cnblogs.com/cmt/p/19531319",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/cmt/p/19531319\" id=\"cb_post_title_url\" title=\"发布于 2026-01-26 08:34\">\n    <span>上周热点回顾（1.19-1.25）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>热点随笔：</p>\n<p>· <a href=\"https://www.cnblogs.com/HaiJun-Aion/archive/2026/01/23/19521040.html\" rel=\"noopener\" target=\"_blank\">32岁程序员猝死背后，我的一些真实感受</a> (<a href=\"https://www.cnblogs.com/HaiJun-Aion/\" rel=\"noopener\" target=\"_blank\">程序员海军</a>) <br />· <a href=\"https://www.cnblogs.com/sdcb/archive/2026/01/23/20260119-using-httpclient.html\" rel=\"noopener\" target=\"_blank\">.NET 10了，HttpClient还是不能用using吗？我做了一个实验</a> (<a href=\"https://www.cnblogs.com/sdcb/\" rel=\"noopener\" target=\"_blank\">.NET骚操作</a>) <br />· <a href=\"https://www.cnblogs.com/sdcb/archive/2026/01/21/20260120-chats-190.html\" rel=\"noopener\" target=\"_blank\">两天烧掉200美元！我AI大模型网关终于支持了Claude模型</a> (<a href=\"https://www.cnblogs.com/sdcb/\" rel=\"noopener\" target=\"_blank\">.NET骚操作</a>) <br />· <a href=\"https://www.cnblogs.com/yupi/archive/2026/01/20/19505500.html\" rel=\"noopener\" target=\"_blank\">20 个神级 AI 编程扩展，爽爆了！</a> (<a href=\"https://www.cnblogs.com/yupi/\" rel=\"noopener\" target=\"_blank\">程序员鱼皮</a>) <br />· <a href=\"https://www.cnblogs.com/xdesigner/archive/2026/01/19/19501590.html\" rel=\"noopener\" target=\"_blank\">MWGA - 为了复活1000亿行C#代码</a> (<a href=\"https://www.cnblogs.com/xdesigner/\" rel=\"noopener\" target=\"_blank\">袁永福 电子病历，医疗信息化</a>) <br />· <a href=\"https://www.cnblogs.com/haibindev/archive/2026/01/19/19503791.html\" rel=\"noopener\" target=\"_blank\">国内四大AI编程IDE对比（一）：直观印象与模型能力</a> (<a href=\"https://www.cnblogs.com/haibindev/\" rel=\"noopener\" target=\"_blank\">haibindev</a>) <br />· <a href=\"https://www.cnblogs.com/xzqcsj/archive/2026/01/22/19510445.html\" rel=\"noopener\" target=\"_blank\">一个月搞定100+表迁移：我的“偷师”Navicat实战复盘</a> (<a href=\"https://www.cnblogs.com/xzqcsj/\" rel=\"noopener\" target=\"_blank\">一旅人</a>) <br />· <a href=\"https://www.cnblogs.com/MeteorSeed/archive/2026/01/22/19505675.html\" rel=\"noopener\" target=\"_blank\">【译】Visual Studio 2026 来了：更快、更智能，深受老用户的喜爱</a> (<a href=\"https://www.cnblogs.com/MeteorSeed/\" rel=\"noopener\" target=\"_blank\">MeteorSeed</a>) <br />· <a href=\"https://www.cnblogs.com/sheng-jie/archive/2026/01/20/19508743.html\" rel=\"noopener\" target=\"_blank\">.NET+AI | Workflow | 核心概念速通（1）</a> (<a href=\"https://www.cnblogs.com/sheng-jie/\" rel=\"noopener\" target=\"_blank\">「圣杰」</a>) <br />· <a href=\"https://www.cnblogs.com/diamondhusky/archive/2026/01/20/19508626.html\" rel=\"noopener\" target=\"_blank\">告别 throw exception！为什么 Result&lt;T&gt; 才是业务逻辑的正确选择</a> (<a href=\"https://www.cnblogs.com/diamondhusky/\" rel=\"noopener\" target=\"_blank\">呆萌哈士奇</a>) <br />· <a href=\"https://www.cnblogs.com/powertoolsteam/archive/2026/01/21/19511415.html\" rel=\"noopener\" target=\"_blank\">如何提升 C# 应用中的性能</a> (<a href=\"https://www.cnblogs.com/powertoolsteam/\" rel=\"noopener\" target=\"_blank\">葡萄城技术团队</a>) <br />· <a href=\"https://www.cnblogs.com/cmt/archive/2026/01/19/19499970.html\" rel=\"noopener\" target=\"_blank\">上周热点回顾（1.12-1.18）</a> (<a href=\"https://www.cnblogs.com/cmt/\" rel=\"noopener\" target=\"_blank\">博客园团队</a>) </p>\n<p>热点新闻：</p>\n<p>· <a href=\"https://news.cnblogs.com/n/813132/\" rel=\"noopener\" target=\"_blank\">睡眠对寿命的影响仅次于吸烟！建议每晚至少睡7小时</a><br />· <a href=\"https://news.cnblogs.com/n/813264/\" rel=\"noopener\" target=\"_blank\">马斯克兑现承诺，开源X推荐算法！100% AI驱动，0人工规则</a><br />· <a href=\"https://news.cnblogs.com/n/813133/\" rel=\"noopener\" target=\"_blank\">每天多动5分钟、少坐30分钟，显著降低死亡风险</a><br />· <a href=\"https://news.cnblogs.com/n/813178/\" rel=\"noopener\" target=\"_blank\">国产第一、全球第四，巨亏400亿</a><br />· <a href=\"https://news.cnblogs.com/n/813193/\" rel=\"noopener\" target=\"_blank\">设计师开窍了！比亚迪海洋网「8 系双旗舰」亮相，纯电续航或超 1000 km</a><br />· <a href=\"https://news.cnblogs.com/n/813255/\" rel=\"noopener\" target=\"_blank\">Gemini准确率从21%飙到97%！谷歌只用了这一招：复制粘贴</a><br />· <a href=\"https://news.cnblogs.com/n/813323/\" rel=\"noopener\" target=\"_blank\">DeepSeek R1发布一年了，不卷功能、不融资、不着急，凭什么「硬控」硅谷</a><br />· <a href=\"https://news.cnblogs.com/n/813184/\" rel=\"noopener\" target=\"_blank\">罗永浩被禁言后现身B站颁奖典礼 获年度新人奖 并上台发表感言</a><br />· <a href=\"https://news.cnblogs.com/n/813260/\" rel=\"noopener\" target=\"_blank\">Cursor一夜翻车，AI 300万代码写浏览器被打假！全网群嘲「AI泔水」</a><br />· <a href=\"https://news.cnblogs.com/n/813326/\" rel=\"noopener\" target=\"_blank\">IPv4 和 IPv6 地址现状</a><br />· <a href=\"https://news.cnblogs.com/n/813348/\" rel=\"noopener\" target=\"_blank\">看了300期神秘园，我能挑战「鳌太线」了吗</a><br />· <a href=\"https://news.cnblogs.com/n/813242/\" rel=\"noopener\" target=\"_blank\">小米汽车回应两起火事件！</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-26 08:34</span>&nbsp;\n<a href=\"https://www.cnblogs.com/cmt\">博客园团队</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "收藏！LLM开发全链路：5大步骤+15大框架，从数据治理到RLHF一文通关",
      "link": "https://www.cnblogs.com/aifrontiers/p/19529366",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/aifrontiers/p/19529366\" id=\"cb_post_title_url\" title=\"发布于 2026-01-26 08:12\">\n    <span>收藏！LLM开发全链路：5大步骤+15大框架，从数据治理到RLHF一文通关</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>原文：<a href=\"https://mp.weixin.qq.com/s/oRUjkoUcHOrMtHfVHkr5Cw\" rel=\"noopener nofollow\" target=\"_blank\">https://mp.weixin.qq.com/s/oRUjkoUcHOrMtHfVHkr5Cw</a></p>\n<p><strong>LLM往期文章推荐</strong></p>\n<p><a href=\"https://mp.weixin.qq.com/s/cx3qY42Lp0L3RaSOgsH77A\" rel=\"noopener nofollow\" target=\"_blank\">小白也能看懂的RL-PPO</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/nfN0dWT3ZfDuW7ZGfaG6dA\" rel=\"noopener nofollow\" target=\"_blank\">收藏！强化学习从入门到封神：5 本经典教材 + 8 大实战项目 + 7个免费视频，一站式搞定</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/4_6CBXMJhqmiYKSzsAXncg\" rel=\"noopener nofollow\" target=\"_blank\">小白也能看懂的RLHF：基础篇</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/8O7W8--x14-b1d3M9IS_3w\" rel=\"noopener nofollow\" target=\"_blank\">小白也能看懂的RLHF-PPO：原理篇</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/9KT9LrMTXDGHSvGFrQhRkg\" rel=\"noopener nofollow\" target=\"_blank\">小白也能看懂的LLM-RL算法：PPO/DPO/GRPO/GSPO</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/9f4mqYVGKNS-LhmHLl6CXw\" rel=\"noopener nofollow\" target=\"_blank\">收藏！LLM-RL训练框架：3大流派+6大框架，一文搞定</a></p>\n<p>在上一篇<a href=\"https://mp.weixin.qq.com/s/9f4mqYVGKNS-LhmHLl6CXw\" rel=\"noopener nofollow\" target=\"_blank\">收藏！LLM-RL训练框架：3大流派+6大框架，一文搞定</a>中，我们重点讨论了LLM训练技术的开源框架，并未涉及LLM训练的其他环节。在人工智能领域从模型中心化向数据中心化范式转移的背景下，LLM的成功不仅依赖于模型参数规模的爆炸式增长，更取决于全链路工程化的精细程度。</p>\n<p>这一链路涵盖了从海量异构数据的精炼、超大规模分布式环境下的模型训练、特定任务驱动的指令微调，到最终模型输出与人类价值观对齐的RLHF阶段。如近的开源生态系统已涌现出一批高性能、模块化且落地性强的代码框架，这些工具极大地降低了开发者训练、微调和部署私有化大模型的门槛。本篇将对这一全链路中的核心开源框架进行深度的技术解构，分析其底层机制、性能指标及行业应用场景。</p>\n<h1 id=\"1-分布式数据清洗与编排引擎\">1 分布式数据清洗与编排引擎</h1>\n<p>数据质量是LLM性能的生命线。当前工业界的共识是，高质量的合成数据和经过严苛清洗的NLP语料对提升模型逻辑推理能力至关重要。当数据规模达到PB级时，单机处理变得不可行。异构脏数据的处理流程需要复杂的任务编排和大规模分布式计算的支持。</p>\n<h2 id=\"11-data-juicer\">1.1 <strong>Data-Juicer</strong></h2>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<ul>\n<li>\n<p>gitHub：<a href=\"https://github.com/datajuicer/data-juicer\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/datajuicer/data-juicer</a> 5.8k⭐</p>\n</li>\n<li>\n<p>说明文档：<a href=\"https://datajuicer.github.io/data-juicer/zh_CN/main/docs_index_ZH.html\" rel=\"noopener nofollow\" target=\"_blank\">https://datajuicer.github.io/data-juicer/zh_CN/main/docs_index_ZH.html</a></p>\n</li>\n<li>\n<p>论文：<a href=\"https://arxiv.org/pdf/2501.14755v2\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2501.14755v2</a></p>\n</li>\n</ul>\n<h3 id=\"核心特点\"><strong>核心特点</strong></h3>\n<ul>\n<li>\n<p><strong>一站式与系统化</strong>：涵盖了数据分析、清洗、过滤、转换、去重及合成的完整链路。它不仅是一个工具包，更是一个完整的系统，提供了100多个核心算子。</p>\n</li>\n<li>\n<p><strong>多模态支持</strong>：除了基础的文本数据，Data-Juicer 2.0及后续版本深度支持图像、视频、音频等多种模态，能够处理复杂的交织多模态数据。</p>\n</li>\n<li>\n<p><strong>高效扩展</strong>：基于Ray和CUDA优化，支持单机到数千核集群的弹性扩展，性能经工业级验证。</p>\n</li>\n<li>\n<p><strong>数据-模型共开发（Sandbox）</strong>：提供沙盒机制，允许开发者在小规模数据上快速迭代实验，通过反馈循环和可视化工具快速验证数据改进对模型效果的影响。</p>\n</li>\n</ul>\n<h3 id=\"适用场景\">适用场景</h3>\n<ul>\n<li>\n<p><strong>预训练/微调加速</strong>：对海量网页数据去噪，或筛选高质量、高多样性的指令微调数据。</p>\n</li>\n<li>\n<p><strong>多模态生成训练</strong>：为类似Sora的视频生成或多模态大模型准备精细化标注与清洗后的语料。</p>\n</li>\n<li>\n<p><strong>自动化数据工程</strong>：利用AI算子自动生成、重写数据，或探索最优数据混合比例。</p>\n</li>\n</ul>\n<h3 id=\"优缺点\">优缺点</h3>\n<p><strong>优点</strong>：① 工业级成熟度：源自阿里巴巴通义实验室，经过大规模生产环境验证，算子丰富且性能优异；② 生态集成度高：与ModelScope（魔搭社区）、LLaMA-Factory、Ray等主流大模型生态深度打通，方便开发者集成到现有流水线；③ 灵活易用：对于新手，可以直接使用官方提供的最佳实践配置；对于高级用户，可以通过 Python 灵活自定义算子。</p>\n<p><strong>缺点</strong>：① 学习成本：算子库庞大，需一定时间摸索最佳参数组合；② 资源需求：部分高级算子（如模型打分）依赖计算资源，处理海量数据时成本较高。</p>\n<h2 id=\"12-datatrove\">1.2 Datatrove</h2>\n<ul>\n<li>github: <a href=\"https://github.com/huggingface/datatrove\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/huggingface/datatrove</a> 2.8k⭐</li>\n</ul>\n<h3 id=\"关键特性\">关键特<strong>性</strong></h3>\n<ul>\n<li>\n<p><strong>平台无关的流水线</strong>：代码在本地机器、Slurm集群或 Ray集群上运行时几乎不需要改动。它通过执行器机制抽象了底层算力。</p>\n</li>\n<li>\n<p><strong>低<strong><strong>内存</strong></strong>占用与流式处理</strong>：采用生成器模式，数据以流的形式通过处理模块，即便处理数百TB的数据，内存消耗也能控制在较低水平。</p>\n</li>\n<li>\n<p><strong>强大的去重功能</strong>：内置了工业级的去重算法，包括MinHash（模糊去重）和Exact Substring（精确子串去重），这是处理网页抓取数据的关键。</p>\n</li>\n<li>\n<p><strong>容错与断点续传</strong>：能够自动跟踪已完成的任务，如果作业在集群中崩溃，重启后会自动跳过已处理的部分。</p>\n</li>\n</ul>\n<h3 id=\"适用场景-1\">适用场景</h3>\n<ul>\n<li>\n<p><strong>LLM</strong> <strong>预训练清洗</strong>：处理Common Crawl等原始网页快照，提取纯净文本并剔除低质量内容。。</p>\n</li>\n<li>\n<p><strong>超大规模去重</strong>：在海量数据中精准剔除重复或高度相似的文档。</p>\n</li>\n<li>\n<p><strong>分布式数据工程</strong>：利用Slurm或Ray等集群环境快速处理万亿规模的数据集。</p>\n</li>\n</ul>\n<h3 id=\"优缺点-1\">优缺点</h3>\n<p><strong>优点</strong>：① 极致的扩展性：不是为了处理小样本设计的，而是为了处理万亿级Token设计的，在Slurm或Ray分布式环境下表现极佳；②简洁的API：Pythonic风格，模块化程度高，易于自定义扩展；③ 与生态深度集成：与Hugging Face Hub和fsspec深度整合，支持直接读写S3、Hugging Face数据仓库。</p>\n<p><strong>缺点</strong>：①主要侧重文本：理论上虽然可以处理其他数据，但目前其生态和预置算子主要集中在文本领域。在多模态（图像、视频）算子的丰富度上，目前弱于Data-Juicer；② 文档相对精简：相比一些商业化或历史悠久的框架，其详细文档和教程仍在完善中，更多依赖示例代码（Examples）。</p>\n<h1 id=\"2-分布式预训练与模型训练底层架构\">2 分布式预训练与模型训练底层架构</h1>\n<p>当数据准备就绪后，如何将其高效地输入到分布式计算集群中进行训练成为核心挑战。因单个GPU的显存（如H100的80GB）远不足以容纳100+B参数的模型、及其优化器状态和梯度，分布式并行策略成为了现代训练框架的基石。</p>\n<h2 id=\"21-megatron-lm\">2.1 Megatron-LM</h2>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<ul>\n<li>\n<p>Github : <a href=\"https://github.com/NVIDIA/Megatron-LM\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/NVIDIA/Megatron-LM</a> 15k⭐</p>\n</li>\n<li>\n<p>官方文档：<a href=\"https://docs.nvidia.com/megatron-core/index.html\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.nvidia.com/megatron-core/index.html</a></p>\n</li>\n<li>\n<p>论文1：<a href=\"https://arxiv.org/pdf/1909.08053\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/1909.08053</a></p>\n</li>\n<li>\n<p>论文2：<a href=\"https://arxiv.org/pdf/2104.04473\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2104.04473</a></p>\n</li>\n<li>\n<p>论文3：<a href=\"https://arxiv.org/pdf/2205.05198\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2205.05198</a></p>\n</li>\n</ul>\n<p>Megatron-LM作为由NVIDIA深度开发的分布式训练框架，核心贡献在于提出并完善了多维并行体系，特别是针对 Transformer 结构的深度优化，在大模型预训练领域占据着举足轻重的地位。其设计哲学始终围绕着如何榨干NVIDIA GPU的每一分性能，特别是利用高性能NVLink互联和CUDA内核融合技术。</p>\n<h3 id=\"关键特性与底层架构分析\">关键特性与底层架构分析</h3>\n<p>核心贡献在于提出并完善了多维并行体系，特别是针对 Transformer结构的深度优化。</p>\n<ul>\n<li>\n<p><strong>多维<strong><strong>并行计算</strong></strong>架构</strong>：将计算任务在三个维度进行解耦：层内计算（张量并行）、层间计算（流水线并行）以及批次数据（数据并行）。张量并行（Tensor Parallelism）是一种层内并行技术，将Transformer层的矩阵乘法操作沿列或行进行拆分。例如，在注意力机制的QKV投影层，通过将输出维度（列）切分到不同GPU，每个进程仅需存储和计算部分参数，最后通过All-Reduce操作聚合梯度。这种精细化的切分使得单卡无法容纳的大型层得以在节点内高效运行。</p>\n</li>\n<li>\n<p><strong>流水线<strong><strong>并行</strong></strong>的1F1B调度</strong>：为了解决跨层并行的负载均衡问题，Megatron-LM引入了1F1B（One-Forward-One-Backward）调度算法。通过将全局批次切分为多个微批次（Micro-batches），1F1B允许不同的流水线阶段在同一时间点并行处理不同的微批次，极大地压缩了流水线气泡（Pipeline Bubble）占用的时间比例，从而提升了集群的整体利用率。</p>\n</li>\n<li>\n<p><strong>序列<strong><strong>并行</strong></strong>与上下文并行</strong>：针对长文本训练需求，实现了序列并行（Sequence Parallelism），它将非张量并行部分（如LayerNorm和Dropout）沿序列维度进一步拆分，有效减少了冗余的显存占用。而在处理超长上下文（如 32K及以上tokens）时，上下文并行（Context Parallelism）则通过跨设备分配序列片段来应对激活值内存激增的挑战。</p>\n</li>\n<li>\n<p><strong>Megatron Core (MCore)</strong>：作为该框架的最新演进版本，MCore采用了模块化、组件化的设计理念。它通过Composable APIs允许用户灵活构建自定义训练流程，并集成了混合专家模型的先进支持，包括针对 DeepSeek-V3等架构的深度优化，支持 DeepEP、HybridEP等高效的Token调度算法，旨在实现异构数据中心规模下的高弹性能。</p>\n</li>\n</ul>\n<h3 id=\"适合场景与性能边界\">适合场景与性能边界</h3>\n<p>Megatron-LM专为拥有高性能计算集群（尤其是具备 NVLink 节点内互联的高端NVIDIA GPU环境）的团队设计。它是训练基础大模型（如Deepseek等级模型）的参考实现，特别是在需要追求极致的TFLOPS吞吐量时，其定制化的CUDA内核融合技术（能减少显存访问开销约 40%）展现出了巨大的技术优势。</p>\n<h3 id=\"优缺点-2\">优缺点</h3>\n<p><strong>优点</strong>：① 性能极致：通过高度优化的算子融合和硬件感知通信，实现业界最高的显存和算力效率 ；② 稳定性强：作为NVIDIA官方维护项目，对Hopper/Blackwell等新架构的支持最为迅速且深度；③ 工业标准：其提出的3D并行方案已成为大规模训练的事实标准。</p>\n<p><strong>缺点</strong>：① 开发难度高：代码侵入性强，对非Transformer架构的适配极其复杂，需要深厚的系统编程功底；② 灵活性受限：由于过度依赖NVLink和专有算子，在非同构网络或显存极度受限的异构环境下表现不如DeepSpeed。</p>\n<h2 id=\"22--deepspeed\">2.2  DeepSpeed</h2>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<ul>\n<li>\n<p>gitHub: <a href=\"https://github.com/deepspeedai/DeepSpeed\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/deepspeedai/DeepSpeed</a> 41.3k⭐</p>\n<ul>\n<li><a href=\"https://github.com/microsoft/DeepSpeedExamples\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/microsoft/DeepSpeedExamples</a> 6.8k⭐</li>\n</ul>\n</li>\n<li>\n<p>官方文档：<a href=\"https://deepspeed.readthedocs.io/en/stable/zero3.html\" rel=\"noopener nofollow\" target=\"_blank\">https://deepspeed.readthedocs.io/en/stable/zero3.html</a></p>\n</li>\n<li>\n<p>官网：<a href=\"https://www.deepspeed.ai/\" rel=\"noopener nofollow\" target=\"_blank\">https://www.deepspeed.ai/</a></p>\n</li>\n<li>\n<p>论文：<a href=\"https://arxiv.org/pdf/2308.01320\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2308.01320</a></p>\n</li>\n</ul>\n<p>DeepSpeed是微软在大模型训练领域推出的另一力作，其设计重心在于解决大模型训练中的显存瓶颈问题。通过零冗余优化器（ZeRO）系列技术，DeepSpeed极大地降低了训练超大规模模型的准入门槛。</p>\n<h3 id=\"关键技术深度解析\">关键技术深度解析</h3>\n<p>DeepSpeed的核心架构由ZeRO优化器及其衍生的异构存储技术构成。</p>\n<ul>\n<li>\n<p><strong>ZeRO<strong><strong>优化器</strong></strong>（ZeRO-1/2/3）</strong>：ZeRO技术的精髓在于消除模型状态的冗余存储。在传统数据并行中，每个GPU都保存完整的模型状态，而ZeRO则将这些状态进行分片（Sharding）。ZeRO-1仅对优化器状态进行分片；ZeRO-2进一步对梯度进行分片；ZeRO-3则实现了对参数、梯度和优化器状态的全量分片存储。这种策略使得显存消耗随设备数的增加而线性下降。</p>\n</li>\n<li>\n<p><strong>ZeRO-Offload与ZeRO-Infinity</strong>：针对显存严重短缺的场景，DeepSpeed提出了异构内存利用方案。ZeRO-Offload利用CPU内存存储并处理优化器状态更新，而ZeRO-Infinity则将这一思路推向极致，通过高效的Infinity Offload引擎同时利用GPU显存、CPU内存和NVMe SSD，支持在单个GPU上训练高达130亿参数的模型，或在集群上支持万亿参数规模。</p>\n</li>\n<li>\n<p><strong>DeepSpeed-Ulysses序列****并行</strong>：相比Megatron的序列并行，Ulysses采用了一种更通用的All-to-All通信机制。它在注意力计算前后重新分配数据，使得每个GPU能够完整地看到全局上下文，但仅负责计算部分注意力头，这在长序列训练中展现出了极佳的带宽效率。</p>\n</li>\n<li>\n<p><strong>混合3D<strong><strong>并行</strong></strong>集成</strong>：DeepSpeed通过与Megatron-LM的深度融合，将ZeRO技术与张量并行、流水线并行相结合，形成了一套完整的万亿级模型训练方案。其提供的TiledLinear模块进一步通过内存平铺技术减少了算子的峰值内存需求。</p>\n</li>\n</ul>\n<h3 id=\"适合场景\">适合场景</h3>\n<p>DeepSpeed是资源异构或受限环境下的不二之选。无论是需要在中小规模集群上微调百亿参数模型，还是在跨节点的非高速互联环境下进行预训练，其强大的显存管理能力都能确保持续运行。此外，由于其与 Hugging Face等开源生态结合紧密，它是研究人员和企业进行快速实验和模型私有化部署的首选框架。</p>\n<h3 id=\"优缺点-3\">优缺点</h3>\n<p><strong>优点</strong>：① 显存效率极高：ZeRO-3与Infinity技术极大地打破了内存墙，支持单机训练超大规模模型；② 易用性与兼容性：通过简单的JSON配置文件即可启用复杂功能，无需大规模重构PyTorch代码；③ 生态开放：支持包括AMD、Intel 在内的多种硬件平台，广泛集成于第三方库。</p>\n<p><strong>缺点</strong>：① 通信开销风险：在开启深度Offload模式下，由于受限于PCIe带宽，计算效率可能大幅下降；② 复杂系统集成：在将ZeRO与3D并行结合使用时，参数配置与负载均衡的调优极其繁琐。</p>\n<h2 id=\"23-colossal-ai\">2.3 Colossal-AI</h2>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/hpcaitech/ColossalAI\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/hpcaitech/ColossalAI</a> 41.3k⭐</p>\n</li>\n<li>\n<p>官网：<a href=\"https://colossalai.org/\" rel=\"noopener nofollow\" target=\"_blank\">https://colossalai.org/</a></p>\n</li>\n<li>\n<p>论文：<a href=\"https://arxiv.org/pdf/2110.14883\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2110.14883</a></p>\n</li>\n</ul>\n<p>Colossal-AI由潞晨科技团队开发，致力于通过统一的并行接口和智能化的内存管理，将复杂的分布式训练大众化。其最大的特色在于对异构内存的精细化控制以及自动并行化搜索技术。</p>\n<h3 id=\"关键技术特性分析\">关键技术特性分析</h3>\n<p>Colossal-AI引入了一系列旨在提升资源利用率和开发效率的底层机制。</p>\n<ul>\n<li>\n<p><strong>Gemini动态异构<strong><strong>内存</strong></strong>管理器</strong>：Gemini是Colossal-AI的核心组件，其设计灵感源于PatrickStar。它采用块状内存管理策略，将参数、梯度及优化器状态组织成连续的内存块。Gemini的先进之处在于其Warmup机制：在训练的第一步，系统会实时监测非模型数据（如激活值）的内存波动，据此动态调整模型数据在GPU与CPU间的存放比例。这种自适应的逐出策略能有效减少内存碎片，并充分利用空闲的CPU显存空间。</p>\n</li>\n<li>\n<p><strong>自动并行化搜索</strong>：区别于其他框架的显著标志。通过ColoTracer静态图分析技术，系统可以将并行策略的制定转化为一个约束优化问题。它不仅能自动搜索张量并行的切分方案，还能结合激活值检查点的插入位置，找到比人类专家手动配置更优的执行计划。这极大降低了对系统架构师的依赖。</p>\n</li>\n<li>\n<p><strong>多维<strong><strong>张量并行</strong></strong>架构</strong>：除了传统的1D张量并行，Colossal-AI还实现了2D、2.5D和3D张量并行。利用复杂的分布式矩阵乘法算法，它能够在更大规模的集群中进一步减少通信量，实现超线性的并行加速比。</p>\n</li>\n<li>\n<p><strong>Booster与Shardformer生态</strong>：Colossal-AI提供了Booster插件系统，支持对开源社区（如Hugging Face, Timm）中的模型进行非侵入式的分布式包装。Shardformer可以在不改变原始模型定义的情况下，通过JIT编译和内核替换，自动注入并行算子、FlashAttention及FusedNorm等性能增强项。</p>\n</li>\n</ul>\n<h3 id=\"适合场景-1\">适合场景</h3>\n<p>Colossal-AI非常适合那些需要训练多样化模型结构、且希望减少手动调优工作量的团队。在视觉Transformer (ViT) 以及对显存利用率要求极高的长序列任务中，其动态管理机制表现卓越。此外，其作为PyTorch Lightning的合作伙伴，也吸引了大量追求开发效率的科研用户。</p>\n<h3 id=\"优缺点-4\">优缺点</h3>\n<p><strong>优点</strong>：① 极佳的显存容忍度：Chunk机制相比原生ZeRO能显著降低内存碎片，提升训练稳定性；② 自动化程度最高：一键式并行策略搜索极大地简化了从单机到集群的迁移过程；③ 多维并行方案丰富：针对不同集群拓扑提供了更灵活的算法选择。</p>\n<p><strong>缺点</strong>：① 稳定性尚需打磨：部分前沿特性（如某些自动并行模式）在复杂异构环境下的健壮性仍有提升空间；② 维护复杂性：底层集成的各种优化插件较多，对初学者的排错能力有一定要求。</p>\n<h2 id=\"24-torchtitan\">2.4 torchtitan</h2>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/pytorch/torchtitan\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/pytorch/torchtitan</a> 5k⭐</p>\n</li>\n<li>\n<p>论文：<a href=\"https://openreview.net/pdf?id=WuQtmIkiUL\" rel=\"noopener nofollow\" target=\"_blank\">https://openreview.net/pdf?id=WuQtmIkiUL</a></p>\n</li>\n</ul>\n<p>torchtitan是PyTorch官方团队近期推出的原生分布式预训练参考框架。它的出现标志着PyTorch正致力于在核心库层面统一各种分布式并行原语，从而提供一个轻量级、模块化且高度可组合的标杆实现。</p>\n<h3 id=\"架构设计哲学与关键原语\">架构设计哲学与关键原语</h3>\n<p>torchtitan的核心优势在于其完全基于PyTorch原生组件，如DTensor、DeviceMesh和FSDP2，而非依赖繁重的外部封装。</p>\n<ul>\n<li>\n<p><strong>DTensor与DeviceMesh 抽象</strong>：这是torchtitan的基石。DeviceMesh将集群中的计算节点抽象为一个多维逻辑网格，管理底层的进程组通信。DTensor（Distributed Tensor）则允许开发者以单设备语义编写代码，通过在DTensor上定义分片、复制或部分聚合等状态，系统会自动推导并执行必要的重分布通信操作。这种设计极大地简化了多维并行的逻辑复杂性。</p>\n</li>\n<li>\n<p><strong>FSDP2 (Fully Sharded Data Parallel 2)</strong>：torchtitan率先集成了FSDP2。相比于第一代FSDP 的FlatParameter设计，FSDP2采用了基于参数维度的DTensor分片，具有更强的组合性，并能与torch.compile实现深度融合。实验数据显示，FSDP2在吞吐量上提升了约1.5%，且内存占用降低了7%。</p>\n</li>\n<li>\n<p><strong>硬件软件协同设计与可组合4D并行</strong>：框架原生支持数据并行、张量并行、流水线并行及上下文并行的叠加（即4D并行）。torchtitan引入了Float8混合精度训练、SymmetricMemory共享内存通信优化以及异步检查点保存。其设计的精妙之处在于模块的解耦，模型定义保持算法中立，而并行策略作为独立的层进行包装。</p>\n</li>\n<li>\n<p><strong>编译器驱动的内核融合</strong>：深度集成torch.compile，支持区域级编译优化。通过将Transformer Block作为一个整体进行编译，框架能自动实现算子融合与显存复用，减少了对昂贵的手写CUDA内核的依赖，同时也提升了跨硬件平台的迁移能力。</p>\n</li>\n</ul>\n<h3 id=\"适合场景-2\">适合场景</h3>\n<p>torchtitan适用于希望紧随PyTorch技术栈、进行最前沿架构实验或追求代码长期可维护性的团队。它是训练Llama 3.1等尖端开源模型的官方推荐参考系统。由于其代码量极简且遵循标准的PyTorch编程模式，它也非常适合用于教学和分布式系统原理的研究。</p>\n<h3 id=\"优缺点-5\">优缺点</h3>\n<p><strong>优点</strong>：① 极致的原生性：与PyTorch核心深度绑定，无第三方框架带来的兼容性包袱；② 模块化与可组合性：4D并行的实现逻辑清晰，便于研究人员自定义并行策略；③ 先进的优化技术：率先集成Float8、torch.compile等官方最新成果，具备极高的扩展潜力。</p>\n<p><strong>缺点</strong>：① 成熟度稍欠：相比Megatron或DeepSpeed，其在万亿参数级的大规模工业化生产环境中，相关辅助工具（如监控、故障恢复）仍处于快速完善阶段；② 功能覆盖面：目前的重心主要集中在基础LLM训练，对MoE、多模态等复杂场景的开箱即用支持尚未达到DeeSpeed的丰富程度。</p>\n<h1 id=\"3-合成数据与问答对生成\">3 合成数据与问答对生成</h1>\n<p>AI领域从判别式模型向生成式大语言模型演进中，高质量指令数据成为决定模型性能、领域专业性与对齐效果的关键。传统监督微调高度依赖人类标注，不仅规模化生产成本高，且在深层逻辑推理、长链数学证明和垂直行业知识相关标注中，易因标注者能力局限导致质量波动。为突破该瓶颈，学界与工业界研发出多种自动化数据合成框架，依托教师模型的生成能力，将少量种子指令或原始文档转化为海量结构化问答对。</p>\n<h2 id=\"31-self-instruct\">3.1 Self-Instruct</h2>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/yizhongw/self-instruct\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/yizhongw/self-instruct</a> 4.6k⭐</p>\n</li>\n<li>\n<p>论文：<a href=\"https://arxiv.org/pdf/2212.10560\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2212.10560</a></p>\n</li>\n</ul>\n<p>Self-Instruct框架是由华盛顿大学等机构在2022年底提出的，其核心贡献在于提出了一种几乎不需要人工干预的指令数据自增长路径。在LLM发展早期，该框架的出现极大地民主化了模型微调的过程，使得研究者能够以极低的成本将预训练模型转化为指令遵循模型。</p>\n<h3 id=\"迭代自引导的生成机制\">迭代自引导的生成机制</h3>\n<p>其底层逻辑是一种迭代的自引导算法。该过程始于一个包含175个由人类编写的种子任务的微型集合。每个任务由一条指令和相应的&lt;输入, 输出&gt;示例组成。框架通过利用LLM的上下文学习能力，从任务池中随机抽取少量示例作为提示词，引导模型生成新的指令、输入以及对应的输出。</p>\n<p>在具体的任务生成阶段，Self-Instruct展现了严密的逻辑分层。为了确保生成的实例具有高质量的格式，框架首先要求模型生成指令，随后对其进行任务类型识别。这种识别机制将任务分为“分类任务”与“非分类任务”两类，并针对性地应用不同的实例生成策略：</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>生成策略</strong></td>\n<td><strong>应用场景</strong></td>\n<td><strong>执行逻辑</strong></td>\n</tr>\n<tr>\n<td><strong>输入优先</strong></td>\n<td>非分类任务</td>\n<td>先引导模型根据指令生成输入背景，再根据指令和输入生成输出回复。</td>\n</tr>\n<tr>\n<td><strong>输出优先</strong></td>\n<td>分类任务</td>\n<td>先让模型列出所有可能的类别标签，再针对每个标签反向生成符合逻辑的输入内容，以避免模型倾向于生成单一标签的偏见。</td>\n</tr>\n</tbody>\n</table>\n<p>这种差异化的生成路径确保了在处理诸如情感分析等简单分类任务时，数据集的分布不会因模型的逻辑惯性而向某一类别倾斜。</p>\n<h3 id=\"启发式过滤与多样性保障\">启发式过滤与多样性保障</h3>\n<p>为了解决模型自生成过程中的<strong>幻觉</strong>与<strong>重复</strong>问题，框架采用两层筛选机制：</p>\n<ul>\n<li>\n<p><strong>相似度去重：</strong> 利用ROUGE-L指标，剔除与现有任务池相似度超过0.7的冗余指令。</p>\n</li>\n<li>\n<p><strong>针对性清洗：</strong></p>\n<ul>\n<li>\n<p><strong>长度过滤：</strong> 确保任务完整且重点突出。</p>\n</li>\n<li>\n<p><strong>依赖排除：</strong> 屏蔽包含图像、文件等模型无法处理的外部关键词。</p>\n</li>\n<li>\n<p><strong>格式规范：</strong> 移除无效标点开头或非英文的生成内容。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"实验成果与性能提升\">实验成果与性能提升</h3>\n<p>Self-Instruct成功从GPT-3中提取了5.2万条指令及8.2万个实例：</p>\n<ul>\n<li>\n<p><strong>性能飞跃：</strong> 在SuperNI基准测试中，GPT-3的指令遵循能力提升了33.1%。</p>\n</li>\n<li>\n<p><strong>对标结果：</strong> 仅依靠合成数据微调，其实际表现已接近InstructGPT-001的水平。</p>\n</li>\n</ul>\n<h2 id=\"32-wizardlm\">3.2 <strong>WizardLM</strong></h2>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/nlpxucan/WizardLM\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/nlpxucan/WizardLM</a> 9.5k⭐</p>\n</li>\n<li>\n<p>论文：<a href=\"https://proceedings.iclr.cc/paper_files/paper/2024/file/82eec786fdfbbfa53450c5feb7d1ac92-Paper-Conference.pdf\" rel=\"noopener nofollow\" target=\"_blank\">https://proceedings.iclr.cc/paper_files/paper/2024/file/82eec786fdfbbfa53450c5feb7d1ac92-Paper-Conference.pdf</a></p>\n</li>\n</ul>\n<p>如果说Self-Instruct解决了数据从无到有的问题，WizardL则专注于数据由简入繁的质变。</p>\n<h3 id=\"evol-instruct的进化逻辑与操作算子\">Evol-Instruct的进化逻辑与操作算子</h3>\n<p>Evol-Instruct是WizardLM的核心算法，它不再是从种子任务中水平扩张，而是通过垂直进化不断改写指令。该机制通过提示词引导LLM执行六种具体的进化操作：</p>\n<ul>\n<li>\n<p><strong>深度进化</strong>：</p>\n<ul>\n<li>\n<p><strong>添加约束</strong>：向原指令引入更多限制性条件。例如将\"写一个故事\"进化为\"写一个关于量子力学的悬疑故事，且不能使用‘科学’这个词\"。</p>\n</li>\n<li>\n<p><strong>深化</strong>：要求模型在更高、更抽象的认知层面上探讨原有的主题。</p>\n</li>\n<li>\n<p><strong>具体化</strong>：将泛化的概念指令转化为细节丰富的特定场景，增加处理实际问题的复杂性。</p>\n</li>\n<li>\n<p><strong>增加推理步骤</strong>：强制要求模型展示多步逻辑推导过程，而非直接给出答案。</p>\n</li>\n<li>\n<p><strong>复杂化输入</strong>：通过引入干扰信息、错误逻辑或极其复杂的上下文背景来测试模型的稳健性。</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>广度进化</strong>：</p>\n<ul>\n<li><strong>变异</strong>：基于原指令生成一条全新的、在主题或技能上更具挑战性的指令，旨在扩展数据集的领域覆盖范围和技能多样性</li>\n</ul>\n</li>\n</ul>\n<p>通过定制Prompt模板实现操作，模型按进化指令生成回复，经多轮迭代将简单种子指令转化为复杂推理任务。</p>\n<h3 id=\"指令消除器严苛的质量守门人\">指令消除器：严苛的质量守门人</h3>\n<p>在进化过程中，模型可能会生成逻辑不通、无法完成或毫无增量的指令。WizardLM引入了\"指令消除器\"作为过滤机制，将以下四类失败任务予以剔除：</p>\n<ul>\n<li>\n<p><strong>无信息增量</strong>: 进化的指令与原指令内容基本一致，没有实质性的复杂度提升，通常利用GPT-4进行自动判定。</p>\n</li>\n<li>\n<p><strong>执行失败</strong>: 模型生成的回复包含\"对不起/sorry\"等道歉词汇，且回复长度少于80字，表明任务已超出模型处理能力。</p>\n</li>\n<li>\n<p><strong>输出退化</strong>: 回复仅包含标点符号、停用词或生成过程中产生的无效字符。</p>\n</li>\n<li>\n<p><strong>模板泄露</strong>: 进化的指令中直接包含了进化 Prompt 中的引导性词汇，如\"#Rewritten Prompt#\"等。</p>\n</li>\n</ul>\n<p>这种基于\"进化-淘汰\"的物竞天择机制，确保了最终产生的数据集（如WizardLM-70k）在质量分布上向高难度区间倾斜。</p>\n<h3 id=\"专用领域的衍生应用\">专用领域的衍生应用</h3>\n<p>Evol-Instruct普适性强，可助力多垂直领域打造顶尖模型：WizardCoder针对编程任务进化，在代码生成领域超越更大参数量模型；WizardMath结合强化学习形成RLEIF机制，通过复杂数学推理链生成与步骤级评分，大幅提升模型逻辑严密性。</p>\n<h2 id=\"33-easy-dataset\">3.3 Easy Dataset</h2>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/ConardLi/easy-dataset\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/ConardLi/easy-dataset</a> 12.9k⭐</p>\n</li>\n<li>\n<p>官网：<a href=\"https://docs.easy-dataset.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.easy-dataset.com/</a></p>\n</li>\n<li>\n<p>论文：<a href=\"https://arxiv.org/pdf/2507.04009v1\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2507.04009v1</a></p>\n</li>\n</ul>\n<p>与探索算法边界的Self-Instruct、WizardLM不同，Easy Dataset框架聚焦垂直领域模型训练的工程痛点。金融、医疗、法律等行业的高质量指令数据多隐藏在各类非结构化文档中，难以高效利用，而这款集成化GUI 数据工程套件，实现了从文档解析到模型评估的全链路闭环。</p>\n<h3 id=\"自适应文档处理与语义切分\">自适应文档处理与语义切分</h3>\n<p>Easy Dataset的首要核心功能是其\"自适应文档处理\"模块。针对传统文本切分工具容易导致语义断裂的问题，该框架集成了多种智能切分算法：</p>\n<ul>\n<li>\n<p><strong>Markdown结构感知切分</strong>：能够识别文档的分级标题结构，确保每一个切分出的文本块在逻辑上是完整的。</p>\n</li>\n<li>\n<p><strong>代码感知切分</strong>：针对技术文档，自动识别代码块并防止其在切分过程中被截断 。</p>\n</li>\n<li>\n<p><strong>混合切分策略</strong>：结合语义相似度与固定长度约束，产生既符合计算效率又具备语义连贯性的文本块。</p>\n</li>\n</ul>\n<h3 id=\"角色驱动的合成\">角色驱动的合成</h3>\n<p>为了提升合成数据的多样性和专业性，Easy Dataset引入了创新的\"角色驱动合成\"机制。系统会根据文档内容自动生成\"流派-受众对\"。例如，当处理一份复杂的法律合规文档时，系统会设定不同的提问与回答角色：</p>\n<ul>\n<li>\n<p><strong>角色A</strong>：法律专家（针对专业条文进行深度解析）。</p>\n</li>\n<li>\n<p><strong>角色B</strong>：初级法务（针对合规操作流程进行提问）。</p>\n</li>\n<li>\n<p><strong>角色C</strong>：企业高管（针对合规风险点进行总结性询问）。</p>\n</li>\n</ul>\n<p>这种角色驱动的Prompting策略，使得从同一段非结构化文本中可以延伸出多条视角迥异、风格多元的QA对，极大地模拟了真实世界中复杂的人机交互场景。</p>\n<h3 id=\"人机协作的视觉化管理\">人机协作的视觉化管理</h3>\n<p>Easy Dataset最大的工程优势在于其直观的图形用户界面，这使得非技术出身的行业专家也能深度参与数据建设过程。</p>\n<ul>\n<li>\n<p><strong>文档预处理</strong>：视觉化调节切分阈值，实时查看切分后的文本块，并支持全局标签树构建。</p>\n</li>\n<li>\n<p><strong>指令生成</strong>：批量构建基于文本块的任务，支持手动修改生成的指令，通过标签树管理知识点分布。</p>\n</li>\n<li>\n<p><strong>回复增强</strong>：利用LLM生成包含思维链的详细回答，并提供AI辅助纠偏和质量分级。</p>\n</li>\n<li>\n<p><strong>数据集导出</strong>：一键导出为Alpaca或ShareGPT格式，并自动生成 LLaMA Factory配置文件。</p>\n</li>\n</ul>\n<p>人机协作模式有效缓解纯合成数据的幻觉问题，用户可在界面一键优化逻辑、事实有误的回答，保障训练集的严谨性。</p>\n<h3 id=\"评估闭环与-arena-系统\">评估闭环与 Arena 系统</h3>\n<p>Easy Dataset v1.7.0进一步强化了其评估能力，引入了模型竞技场（Arena）、自动化评估模块。用户不仅可以合成训练数据，还可以同步合成包含判断、单选、多选、简答、开放式五大题型的测试集。教师模型量化评分并生成评估报告，形成生成 — 训练准备 — 评估反馈闭环，是工业界落地便捷的数据工程套件。</p>\n<h2 id=\"34-技术选型建议\">3.4 技术选型建议</h2>\n<ul>\n<li>\n<p><strong>通用对齐阶段</strong>：预训练基础模型先通过Self-Instruct构建广覆盖指令基座，掌握基础对话范式。</p>\n</li>\n<li>\n<p><strong>专业逻辑突破阶段</strong>：面向代码、数学、复杂咨询的模型，引入WizardLM算法，多轮进化打造人类标注难穷尽的逻辑难点。</p>\n</li>\n<li>\n<p><strong>行业模型落地阶段</strong>：特定行业模型采用Easy Dataset路径，以行业规档为底座，通过角色驱动合成专业贴合业务的问答数据集。</p>\n</li>\n</ul>\n<h1 id=\"4-有监督微调sft与参数高效适配框架\">4 有监督微调（SFT）与参数高效适配框架</h1>\n<p>预训练赋予了模型博学，而有监督微调（SFT）则赋予了模型听从指令的能力。随着模型规模的增长，全参数微调的成本变得难以承受，为了平衡模型性能与资源利用率，有监督微调（SFT）结合参数高效适配（PEFT）技术成为了行业主流方案。</p>\n<h2 id=\"41-llamafactory\">4.1 LlamaFactory</h2>\n<ul>\n<li>\n<p>gitHub: <a href=\"https://github.com/hiyouga/LlamaFactory\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/hiyouga/LlamaFactory</a> 66.1k⭐</p>\n</li>\n<li>\n<p>官方文档: <a href=\"https://docs.llamafactory.com.cn/docs/documents/introduct\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.llamafactory.com.cn/docs/documents/introduct</a></p>\n</li>\n</ul>\n<p>LlamaFactory以微调的民主化为核心理念，是易用、高效、高集成的LLM训练平台。它通过抽象底层逻辑，将复杂的分布式训练、数据格式转化、模型兼容等问题封装于统一接口，其特色的LLaMA Board可视化界面，支持用户无代码完成从数据准备、模型加载到微调启动、损失监控及权重合并的全流程操作。更多内容见<a href=\"https://mp.weixin.qq.com/s/9f4mqYVGKNS-LhmHLl6CXw\" rel=\"noopener nofollow\" target=\"_blank\">收藏！LLM-RL训练框架：3大流派+6大框架，一文搞定</a>。</p>\n<h3 id=\"关键技术特性与算法矩阵\">关键技术特性与算法矩阵</h3>\n<p>LlamaFactory展现了极强的Day 0支持能力，即在 Llama 3/4、Qwen 2.5/3、DeepSeek-R1 等前沿模型发布的第一时间提供适配。在算法层面，该框架集成了目前最广泛的PEFT算法矩阵。</p>\n<ul>\n<li>\n<p><strong>基础微调模式</strong>: 全量参数微调、冻结微调、LoRA、QLoRA (2/3/4/5/6/8-bit)</p>\n</li>\n<li>\n<p><strong>先进适配器算法</strong>: DoRA、LoRA+、rsLoRA、PiSSA、LoftQ、OFT</p>\n</li>\n<li>\n<p><strong>优化器与内存管理</strong>: GaLore、BAdam、Adam-mini、Muon、FlashAttention-2、Liger Kernel</p>\n</li>\n<li>\n<p><strong>对齐与强化学习</strong>: SFT、PPO、DPO、KTO、ORPO、SimPO</p>\n</li>\n</ul>\n<p>LlamaFactory在数据处理上支持主流数据集格式及多轮对话、多模态数据处理，内置Native DDP、DeepSpeed、FSDP等分布式训练策略，可在中大规模GPU集群运行，还深度适配昇腾NPU，覆盖驱动安装、CANN配置及加速算子集成。</p>\n<h3 id=\"适合场景-3\">适合场景</h3>\n<ul>\n<li>\n<p><strong>科研实验与原型验证</strong>：研究人员可以利用WebUI快速测试不同PEFT算法对特定任务的效果，无需在工程细节上浪费时间。</p>\n</li>\n<li>\n<p><strong>快速领域定制</strong>：中小企业或开发团队需要将通用模型转化为特定垂直领域的助手时，LlamaFactory提供了最快的落地路径。</p>\n</li>\n<li>\n<p><strong>国产化替代</strong>：在需要基于昇腾NPU进行自主可控的大模型微调时，LlamaFactory的NPU训练方案提供了详尽的工程参考。</p>\n</li>\n</ul>\n<h3 id=\"优缺点-6\">优缺点</h3>\n<p><strong>优点</strong>：① 交互极致友好：LLaMA Board可视化界面是目前开源社区中成熟度最高的GUI工具，极大降低了非工程背景用户的门槛；② 兼容性极其广泛：支持模型超过100种，涵盖了当前市场上绝大多数主流及小众架构；③ 算法跟进迅速：社区活跃度极高，新算法（如GaLore、DoRA）的集成速度处于行业领先水平。</p>\n<p><strong>缺点</strong>：①单卡极限优化不足：虽然集成了Unsloth的加速算子，但在纯粹的单卡极致效率上，仍略逊于原生Unsloth框架；② 工程复杂度较高：由于功能过于丰富，底层代码库较为庞大，对于希望进行深度二次开发的用户来说，代码的学习曲线可能较陡。</p>\n<h2 id=\"42-unsloth\">4.2 Unsloth</h2>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/unslothai/unsloth\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/unslothai/unsloth</a> 51.1k⭐</p>\n</li>\n<li>\n<p>说明文档：<a href=\"https://unsloth.ai/docs\" rel=\"noopener nofollow\" target=\"_blank\">https://unsloth.ai/docs</a></p>\n</li>\n</ul>\n<h3 id=\"核心机制triton内核与手动反向传播\">核心机制：Triton内核与手动反向传播</h3>\n<p>Unsloth颠覆了开发者对显存占用的认知，由Daniel Han和Michael Han兄弟开发，核心是极致性能优化与深度挖掘硬件潜力。该框架舍弃PyTorch默认算子实现，基于OpenAI的Triton语言手写线性层、RoPE位置嵌入等几乎所有关键计算内核。</p>\n<p>其核心价值在于消除冗余的中间张量存储：标准PyTorch流程中，反向传播需保留大量激活值，是显存消耗的主要来源。Unsloth通过手动编写反向传播逻辑，结合内核融合技术，让单次GPU调用完成多个计算步骤，大幅减少VRAM的读写次数。</p>\n<h3 id=\"性能表现与显存节省数据\">性能表现与显存节省数据</h3>\n<p>根据官方及社区的实测数据，Unsloth在微调任务中相比基准（HF+FA2）的优势。</p>\n<ul>\n<li>\n<p><strong>训练速度</strong>：提速 2x 至 5x，部分场景甚至达到 10x 以上</p>\n</li>\n<li>\n<p><strong>显存占用</strong>：减少70%至80%，极大地缓解了OOM问题</p>\n</li>\n<li>\n<p><strong>精度保持</strong>：0%精度损失，通过数学上的精确等价实现优化而非近似</p>\n</li>\n<li>\n<p><strong>上下文支持</strong>：在80GB显存下支持高达500K的上下文窗口训练</p>\n</li>\n</ul>\n<p>此外，Unsloth的动态4-bit量化技术优化自bitsandbytes，通过动态保护关键权重，以不足10%的显存开销提升4-bit模型微调精度，在MMLU等基准上表现更优。</p>\n<h3 id=\"适合场景-4\">适合场景</h3>\n<ul>\n<li>\n<p><strong>消费级<strong><strong>显卡</strong></strong>训练</strong>：在RTX 3060 (12GB) 或RTX 4090 (24GB) 上微调Llama 7B甚至20B规模的模型，这是传统框架难以完成的任务。</p>\n</li>\n<li>\n<p><strong>长文本任务训练</strong>：对于需要极长上下文（如法律文书、长篇小说分析）的微调，Unsloth的内存管理能力是核心优势。</p>\n</li>\n<li>\n<p><strong>高频迭代任务</strong>：在需要频繁调整参数、快速看到训练结果的场景下，2-5倍的速度提升意味着研发周期的指数级缩短。</p>\n</li>\n</ul>\n<h3 id=\"优缺点-7\">优缺点</h3>\n<p><strong>优点</strong>：① 效率断层领先：在单卡微调领域，Unsloth的速度和显存效率目前处于无敌地位；② 零部署痛苦：提供丰富的Google Colab和Kaggle笔记本模板，且支持一键导出至GGUF、vLLM等多种部署格式；③ RL支持卓越：针对GRPO算法进行了内存优化，使其成为当前复现DeepSeek-R1风格推理模型最经济的工具。</p>\n<p><strong>缺点</strong>：① 多卡支持较弱：长期以来其开源版本主要针对单卡优化，虽然可以通过一些非官方手段接入多卡，但成熟的分布式支持（Pro版）尚未全面开源；② 模型结构敏感：由于算子是手写的，当模型引入全新的算子或结构（如非Transformer架构）时，需要开发者手动更新内核，兼容性扩展相对滞后。</p>\n<h2 id=\"43-ms-swift\">4.3 ms-swift</h2>\n<p><img alt=\"\" class=\"lazyload\" /></p>\n<ul>\n<li>\n<p>Github: <a href=\"https://github.com/modelscope/ms-swift\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/modelscope/ms-swift</a> 12.3k⭐</p>\n</li>\n<li>\n<p>论文：<a href=\"https://arxiv.org/abs/2408.05517\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/abs/2408.05517</a></p>\n</li>\n</ul>\n<p>ms-swift（Scalable lightWeight Infrastructure for Fine-Tuning)阿里是ModelScope社区推出的全能型大模型微调框架，区别于LlamaFactory的易用性和Unsloth的性能追求，其以工业级的深度与广度为设计目标。该框架深度融合ModelScope生态，支持600余种纯文本模型、300余种多模态模型，覆盖图像、视频、音频等全模态输入。</p>\n<p>ms-swift实现了辅助数据标注、微调训练、EvalScope评测、量化及vLLM/SGLang部署的全链路闭环，一站式工具链大幅降低企业生产环境中跨平台迁移的工程损耗。</p>\n<h3 id=\"核心技术megatron-扩展与国产硬件适配\">核心技术：Megatron 扩展与国产硬件适配</h3>\n<p>在处理超大规模模型时，ms-swift引入了Megatron并行技术，支持张量并行（TP）、流水线并行（PP）、上下文并行（CP）等复杂策略。通过Megatron-SWIFT模块，该框架在微调 MoE（专家混合）模型时的加速比可达10倍以上。</p>\n<p>ms-swift对国产化算力的支持最为彻底。它不仅支持 NVIDIA A100/H100，还针对昇腾NPU、寒武纪、寒武纪及国内主流算力平台进行了深度调优。其在昇腾NPU上支持基于CANN的底层加速，并能够配合DeepSpeed实现大规模分布式训练。</p>\n<ul>\n<li>\n<p><strong>模型支持</strong>：支持Qwen-VL, InternVL, GLM-4.5V, Ovis等300+多模态模型</p>\n</li>\n<li>\n<p><strong>多模态优化</strong>：引入多模态打包技术（Packing），训练速度提升100%+</p>\n</li>\n<li>\n<p><strong>强化学习族群</strong>：内置GRPO, DAPO, GSPO, RLOO, Reinforce++等全系列强化学习算法</p>\n</li>\n<li>\n<p><strong>部署后端</strong>: 集成vLLM, SGLang, LMDeploy，提供标准的OpenAI API接口</p>\n</li>\n</ul>\n<h3 id=\"适合场景-5\">适合场景</h3>\n<p>ms-swift是企业级项目及超大规模任务的首选：</p>\n<ul>\n<li>\n<p><strong>多模态模型微调</strong>：在处理包含视觉、语音、视频的跨模态对齐任务时，ms-swift提供了最成熟的模板与算子支持。</p>\n</li>\n<li>\n<p><strong>大规模集群训练</strong>：当微调任务需要跨越数十台甚至上百台服务器，并利用复杂的并行策略时，ms-swift的Megatron架构是核心保障。</p>\n</li>\n<li>\n<p><strong>国产化工业落地</strong>：对于必须部署在国产硬件环境（如金融、政务云）的AI项目，ms-swift提供了最完备的驱动与框架兼容性支持。</p>\n</li>\n</ul>\n<h3 id=\"优缺点-8\">优缺点</h3>\n<p><strong>优点</strong>：① 多模态支持之王：目前开源界对视觉、语音模型微调支持最全面、优化最深入的框架之一; ② 国产算力之友：在昇腾等国产芯片上的表现最为稳定，且有ModelScope社区的强大技术支持; ③并行策略完备：真正支持千亿级参数模型的并行训练与优化，具备极强的可扩展性。</p>\n<p><strong>缺点</strong>：① 学习难度最高：由于集成了Megatron和海量的工业级参数，其配置复杂度高于LlamaFactory，对开发者的工程水平有较高要求; ② 环境依赖较重：在安装分布式及国产硬件支持模块时，对系统库、编译器版本有较严苛的要求，初次部署可能耗时较长。</p>\n<h1 id=\"5-强化学习与人类偏好对齐rlhf\">5 强化学习与人类偏好对齐（RLHF）</h1>\n<p>RLHF是大模型通往智能的最后一步，通过与人类价值观的对齐，模型能够学会拒绝有害指令并在复杂任务中表现得更有逻辑。随着DPO（直接偏好优化）和 GRPO（组相对策略优化）等算法的普及，对齐框架正在向高性能生成和灵活策略切换方向演进。</p>\n<p>因前一篇<a href=\"https://mp.weixin.qq.com/s/9f4mqYVGKNS-LhmHLl6CXw\" rel=\"noopener nofollow\" target=\"_blank\">收藏！LLM-RL训练框架：3大流派+6大框架，一文搞定</a>做过介绍，这里不过多阐述，只给一个汇总表格。</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th></th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>框架名称</td>\n<td>核心技术方案</td>\n<td>适用场景</td>\n</tr>\n<tr>\n<td>OpenRLHF</td>\n<td>Ray + vLLM架构、分布式多模型分发</td>\n<td>大规模集群下的高吞吐 PPO/DPO/GRPO训练</td>\n</tr>\n<tr>\n<td>veRL</td>\n<td>3D-HybridEngine、训练与采样高效重塑</td>\n<td>超大规模MoE模型的高性能强化学习对齐</td>\n</tr>\n<tr>\n<td>TRL</td>\n<td>深度集成PEFT、Hugging Face原生支持</td>\n<td>使用HF生态进行轻量级对齐实验</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"6-选型建议\">6 选型建议</h1>\n<p>大模型全链路已形成高度协同的开源生态，数据制备环节可通过Easy-dataset将企业内部文档直接抽离为问答对，并搭配Data-Juicer完成质量清洗；模型微调环节针对处理好的数据集，可采用Llama-Factory调用生成的配置开展微调，若资源受限还能结合Unsloth实现加速；智能对齐环节则借助OpenRLHF进行强化学习对齐，让模型具备更优的逻辑推理能力与价值观对齐效果，其中逻辑推理可达到O1级思考水平。建议企业开发者优先尝试Easy-dataset与Llama-Factory的组合方案，该方案是目前上手最快、工程化程度最高的数据—微调闭环方案。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-26 08:12</span>&nbsp;\n<a href=\"https://www.cnblogs.com/aifrontiers\">AI-Frontiers</a>&nbsp;\n阅读(<span id=\"post_view_count\">10</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "深入浅出了解生成模型-8：生成加速策略概述",
      "link": "https://www.cnblogs.com/Big-Yellow/p/19530732",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Big-Yellow/p/19530732\" id=\"cb_post_title_url\" title=\"发布于 2026-01-25 23:16\">\n    <span>深入浅出了解生成模型-8：生成加速策略概述</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        扩散模型生成加速策略主要包括加速框架优化、Cache策略及量化技术。加速框架方面，可通过指定attention计算后端（如flash_attn）、torch.compile编译、torch.channels_last优化内存访问，或使用xFormers加速attention计算并降低显存，配合CPU卸载、设备分配等显存优化措施。Cache策略利用扩散过程时间冗余，如DeepCache缓存UNet高层特征、FORA复用DiT的Attn和MLP层特征，FBCache基于First Block L1误差判断是否复用残差，CacheDit结合前n层缓存与阈值判断实现加速。量化技术通过PTQ或QAT降低显存并加速，如Bitsandbytes的即时可逆int4/int8量化、SVDQuant分解权重吸收异常值后量化残差、GGUF格式的紧凑编码与多种PTQ量化级别。测试显示，结合channel优化、flash_attn及cache-dit等策略可有效缩短生图时间。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>原文：<a href=\"https://www.big-yellow-j.top/posts/2025/12/29/SDAcceralate.html\" rel=\"noopener nofollow\" target=\"_blank\">https://www.big-yellow-j.top/posts/2025/12/29/SDAcceralate.html</a></p>\n<h2 id=\"扩散模型生成加速策略\">扩散模型生成加速策略</h2>\n<p>Diffusion推理加速的方案，主要包括Cache、量化、分布式推理、采样器优化和蒸馏等。下面内容主要是去对Cache、计算加速框架以及量化技术进行介绍</p>\n<blockquote>\n<p>SD模型加速方式：<a href=\"https://github.com/xlite-dev/Awesome-DiT-Inference?tab=readme-ov-file#Quantization\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/xlite-dev/Awesome-DiT-Inference?tab=readme-ov-file#Quantization</a></p>\n</blockquote>\n<p>不过值得注意的是对于下面内容，首先介绍加速框架（这部分内容主要是介绍进行加速的一些小trick，主要是直接通过api去加速）、cache以及量化一般就会涉及到一些算法的基本原理。所有的测试代码：<a href=\"https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/DFModelCode/DF_acceralate/df_acceralate.ipynb\" rel=\"noopener nofollow\" target=\"_blank\">df_acceralate.ipynb</a></p>\n<h3 id=\"一般加速框架以及显存优化措施\">一般加速框架以及显存优化措施</h3>\n<p>这部分内容的话比较杂（直接总结<a href=\"https://huggingface.co/docs/diffusers/optimization/fp16#scaled-dot-product-attention\" rel=\"noopener nofollow\" target=\"_blank\">huggingface</a>内容），1、<strong>直接使用attn计算加速后端</strong>，比如说一般就是直接使用比如说<code>flash_attn</code>进行attention计算加速，比如说：</p>\n<pre><code class=\"language-python\">pipeline.transformer.set_attention_backend(\"_flash_3_hub\") # 启用flash attn计算加速\npipeline.transformer.reset_attention_backend()             # 关闭flash attn计算加速\n</code></pre>\n<p>不过值得注意的是<code>_flash_3_hub</code> 只支持非hopper架构，因此可以直接就使用<code>set_attention_backend(\"flash\")</code>。2、<strong>直接使用</strong><code>torch.compile</code>进行加速，不过值得注意的是<strong>在开始使用过程中会比较慢</strong>，因为在执行时，它会将模型编译为优化的内核，所以相对会比较慢，但是如果对编译后模型进行批量测试在时间上就会有所提升比如说在代码<a href=\"https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/DFModelCode/DF_acceralate/df_acceralate.ipynb\" rel=\"noopener nofollow\" target=\"_blank\">df_acceralate.ipynb</a>中测试结果使用compile在z-image上生成5张图片耗时：86.49s（<strong>平均生图时间</strong>4s）不使用compile：29.92（<strong>平均生图时间</strong>5s）；3、使用<code>torch.channels_last</code>去优化数据结构（<a href=\"https://docs.pytorch.org/tutorials/intermediate/memory_format_tutorial.html#performance-gains\" rel=\"noopener nofollow\" target=\"_blank\">torch文档</a>）：最主要的一点是通过channel_last让 GPU 在计算卷积 / attention 时，内存访问更连续，比如说一般数据的输入是NCHW那么在内存访问中格式是：<code>N0C0H0W0, N0C0H0W1, ..., N0C0H1W0, ...</code>这个里面通道C变化最慢，使用channel_list数据格式变为NHWC在内存中访问顺序是：<code>N0H0W0C0, N0H0W0C1, N0H0W0C2, ...</code>值得注意的是两部分数据在shape上是一致的只是strid不一致。使用方式也比较简单：</p>\n<pre><code class=\"language-python\"># 修改模型\nmodel = model.to(memory_format=torch.channels_last)\n# 修改输入\ninput = input.to(memory_format=torch.channels_last)\noutput = model(input)\n...\npipeline.unet.to(memory_format=torch.channels_last)\n</code></pre>\n<h4 id=\"1xformers加速\">1、xFormers加速</h4>\n<blockquote>\n<p>项目地址：<a href=\"https://github.com/facebookresearch/xformers\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/facebookresearch/xformers</a></p>\n</blockquote>\n<p>在SD模型中对于xformers基本使用方式如下所示：</p>\n<pre><code class=\"language-python\">import torch\nfrom diffusers import StableDiffusionXLPipeline\n\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n).to(\"cuda\")\n# 使用xformer加速\npipeline.enable_xformers_memory_efficient_attention()\n# 关闭xformer加速\npipeline.disable_xformers_memory_efficient_attention()\n</code></pre>\n<p>xformers作用在于<strong>加速attention计算并降低显存</strong>，除此之外还提供了多种注意力实现方式，如casual attention等。根据<a href=\"https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.fmha.cutlass.FwOp\" rel=\"noopener nofollow\" target=\"_blank\">官方文档</a>中的描述，对于对于<code>xformers.ops.memory_efficient_attention</code>在使用上参数主要是：1、输入数据也就是QKV的格式上必须满足为：<code>[B, M, H, K]</code>分别表示的是其中B 为batch size, N为序列长度, num_heads为多头注意力头的个数, dim_head则为每个头对应的embeding size；2、attn_bias实际上充当为在使用mask attention时的mask；3、p也就是dropout对应值；4、op为Tuple，用于指定优化self-attention计算所采用的算子。基本使用方式如下：</p>\n<pre><code class=\"language-python\">import xformers.ops as xops\ny = xops.memory_efficient_attention(q, k, v)\ny = xops.memory_efficient_attention(q, k, v, p=0.2) # 使用dropout\ny = xops.memory_efficient_attention(\n    q, k, v,\n    attn_bias=xops.LowerTriangularMask()\n)# 使用casual 注意力\n</code></pre>\n<p>值得着重了解的就是其中<code>attn_bias</code>参数，简单直观的理解：用于控制注意力可见性和结构的统一接口，<strong>既可以表示 mask，也可以表示稀疏/局部/因果等高级注意力模式</strong>，并且以高性能方式融入 attention 内核。比如说：<br />\n1、<code>xops.LowerTriangularMask()</code>：常规的causal注意力也就是下三角mask<br />\n2、<code>xops.LocalAttentionFromBottomRightMask</code>：局部注意力，每个token只能看最近的window_size个token</p>\n<h4 id=\"2显存优化\">2、显存优化</h4>\n<blockquote>\n<p>这部分内容直接总结：<a href=\"https://huggingface.co/docs/diffusers/en/optimization/memory?device-map=pipeline+level#reduce-memory-usage\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/docs/diffusers/en/optimization/memory?device-map=pipeline+level#reduce-memory-usage</a></p>\n</blockquote>\n<p>对于模型的显存过大可以考虑根据自身的设备进行分配，比如说将模型卸载到CPU或者将VAE等放到其它显卡上，在diffusers就提供了这些方法（这块内容直接问AI进行总结）：<br />\n<strong>1、CPU卸载</strong><br />\n它启用了一种极致级别的逐层（leaf-level / sequential）CPU offloading机制，核心思路是：把模型的计算图中<strong>最底层的参数（leaf modules，即最细粒度的子模块、层或权重块）默认放在 CPU 内存里存储</strong>。在前向传播（forward pass）过程中，只在真正需要计算某个具体层的时候，才把那一小块参数临时从 CPU 拷贝（onload）到 GPU。计算完这层之后，立刻把这块参数再 offload 回 CPU，释放 GPU 显存。然后再加载下一层，以此类推，一层一层顺序执行（sequential）。<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602481-1040500181.webp\" /><br />\n<strong>2、设备分配</strong><br />\n这部分主要是将生成模型中不同模型结构如VAE、CLIP去分配到其它显卡上：</p>\n<pre><code class=\"language-python\">import torch\nfrom diffusers import AutoModel, StableDiffusionXLPipeline\npipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    device_map=\"balanced\" # 使用balance就可以实现不同设备分配\n)\nprint(pipeline.hf_device_map)\n{'unet': 1, 'vae': 1, 'safety_checker': 0, 'text_encoder': 0}\n</code></pre>\n<p>亦或者直接自己定分配：</p>\n<pre><code class=\"language-python\">import torch\nfrom diffusers import AutoModel\ndevice_map = {\n    'pos_embed': 0, 'time_text_embed': 0, 'context_embedder': 0, 'x_embedder': 0, 'transformer_blocks': 0, 'single_transformer_blocks.0': 0, 'single_transformer_blocks.1': 0, 'single_transformer_blocks.2': 0, 'single_transformer_blocks.3': 0, 'single_transformer_blocks.4': 0, 'single_transformer_blocks.5': 0, 'single_transformer_blocks.6': 0, 'single_transformer_blocks.7': 0, 'single_transformer_blocks.8': 0, 'single_transformer_blocks.9': 0, 'single_transformer_blocks.10': 1, 'single_transformer_blocks.11': 1, 'single_transformer_blocks.12': 1, 'single_transformer_blocks.13': 1, 'single_transformer_blocks.14': 1, 'single_transformer_blocks.15': 1, 'single_transformer_blocks.16': 1, 'single_transformer_blocks.17': 1, 'single_transformer_blocks.18': 1, 'single_transformer_blocks.19': 1, 'single_transformer_blocks.20': 1, 'single_transformer_blocks.21': 'cpu', 'single_transformer_blocks.22': 'cpu', 'single_transformer_blocks.23': 'cpu', 'single_transformer_blocks.24': 'cpu', 'single_transformer_blocks.25': 'cpu', 'single_transformer_blocks.26': 'cpu', 'single_transformer_blocks.27': 'cpu', 'single_transformer_blocks.28': 'cpu', 'single_transformer_blocks.29': 'cpu', 'single_transformer_blocks.30': 'cpu', 'single_transformer_blocks.31': 'cpu', 'single_transformer_blocks.32': 'cpu', 'single_transformer_blocks.33': 'cpu', 'single_transformer_blocks.34': 'cpu', 'single_transformer_blocks.35': 'cpu', 'single_transformer_blocks.36': 'cpu', 'single_transformer_blocks.37': 'cpu', 'norm_out': 'cpu', 'proj_out': 'cpu'\n}\ntransformer = AutoModel.from_pretrained(\n    \"black-forest-labs/FLUX.1-dev\", \n    subfolder=\"transformer\",\n    device_map=device_map,\n    torch_dtype=torch.bfloat16\n)\n</code></pre>\n<h3 id=\"cache策略概述\">cache策略概述</h3>\n<p>cache指的是：<strong>缓存通过存储和重用不同层（例如注意力层和前馈层）的中间输出来加速推理，而不是在每个推理步骤执行整个计算</strong>。它以更多内存为代价显着提高了生成速度，并且不需要额外的训练。主要详细介绍两种：1、DeepCache；2、FORA。对于更加多的cache策略可以看<a href=\"https://zhuanlan.zhihu.com/p/711223667\" rel=\"noopener nofollow\" target=\"_blank\">知乎</a>，<strong>推荐直接使用</strong><a href=\"#cachedit\" rel=\"noopener nofollow\">CacheDit</a>来进行加速。</p>\n<h4 id=\"deepcache策略\">DeepCache策略</h4>\n<blockquote>\n<p>Paper:<a href=\"https://arxiv.org/pdf/2312.00858\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2312.00858</a><br />\nCode:<a href=\"https://link.zhihu.com/?target=https%3A//github.com/horseee/DeepCache\" rel=\"noopener nofollow\" target=\"_blank\">https://link.zhihu.com/?target=https%3A//github.com/horseee/DeepCache</a></p>\n</blockquote>\n<p><strong>主要针对UNet架构</strong>的Diffusion模型进行推理加速。DeepCache 是一种Training-free的扩散模型加速算法，核心思想是<strong>利用扩散模型序列去噪步骤中固有的时间冗余来减少计算开销</strong>。<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231603400-1714104454.webp\" /><br />\n基于 U-Net 结构特性，发现相邻去噪步骤的高层特征具有显著时间一致性（Adjacent steps in the denoising process exhibit significant temporal similarity in high-level features.），比如说上图中作者在测试上采用block <span class=\"math inline\">\\(U_2\\)</span>的特征和其它所有的采样步之间相似性计算（图b），因此缓存这些高层特征并仅以低成本更新低层特征，从而避免重复计算。具体方法为：<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602316-18977204.webp\" /><br />\n比如说在官方的使用中有参数：<code>helper.set_params(cache_interval=3,cache_branch_id=0,)</code>表示是每3个时间步进行一次完成forward然后刷新cache，而其中参数cache_branch_id值得是一般而言在UNet中会定义<code>branch 0 → early / down blocks</code>等就是选择哪些层的输出。具体过程如下：t=1进行计算缓存，t=2,3都直接使用缓存，t=4完整计算得到缓存。</p>\n<h4 id=\"fora\">FORA</h4>\n<blockquote>\n<p>Paper: <a href=\"https://arxiv.org/pdf/2407.01425\" rel=\"noopener nofollow\" target=\"_blank\">https://arxiv.org/pdf/2407.01425</a><br />\nCode: <a href=\"https://github.com/prathebaselva/FORA\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/prathebaselva/FORA</a></p>\n</blockquote>\n<p><strong>主要是争对Dit架构</strong>的Diffusion模型进行推理加速。利用 Diffusion Transformer 扩散过程的重复特性实现了可用于DiT的Training-free的Cache加速算法。<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602779-569782593.webp\" /><br />\nFORA的核心在于发现Dit在去噪过程中，<strong>相邻时间步的Attn和MLP层特征存在显著重复性</strong>（如上图所示:在layer0、9、18、27这些层以及250步采样中，随后采样步约往后特征之间相似性也就越高。）。通过Caching特征，FORA 将这些重复计算的中间特征保存并在后续时间步直接复用，避免逐步重新计算。<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231603384-1139937541.webp\" /><br />\n具体而言，模型以固定间隔 N 重新计算并缓存特征：当时间步 t 满足 t mod N=0 时，更新所有层的缓存；在后续 N-1 步中，直接检索cached的 Attn 和 MLP 特征，跳过重复计算。这种策略利用了 DiT 架构在邻近时间时间步的特征相似性，在不修改DiT模型结构的前提下实现加速。例如，在 250 步 DDIM 采样中，当 N=3 时，模型仅需在第 3、6、9... 步重新计算特征，其余步骤复用Cache，使计算量减少约 2/3。实验表明，FORA对后期去噪阶段的特征相似性利用更为高效，此时特征变化缓慢，缓存复用的性价比最高。</p>\n<h4 id=\"fbcache\">FBCache</h4>\n<blockquote>\n<p>项目地址：<a href=\"https://github.com/chengzeyi/ParaAttention/blob/main/doc/fastest_flux.md\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/chengzeyi/ParaAttention/blob/main/doc/fastest_flux.md</a></p>\n</blockquote>\n<p>通过缓存变换器模型中变换器块的输出，并在下一步推理中重新使用它们，可以降低计算成本，加快推理速度。然而，很难决定何时重新使用缓存以确保生成图像的质量。最近，TeaCache 提出，可以使用时间步嵌入来近似模型输出之间的差异。AdaCache 也表明，在多个图像和视频 DiT 基线中，<strong>缓存可以在不牺牲生成质量的情况下显著提高推理速度</strong>。不过，TeaCache 仍然有点复杂，因为它需要重新缩放策略来确保缓存的准确性。在 ParaAttention 中，<strong>发现可以直接使用第一个transformer输出的残差来近似模型输出之间的差异。当差值足够小时，我们可以重复使用之前推理步骤的残差</strong>，这意味着我们实际上跳过了去噪步骤。我们的实验证明了这一方法的有效性，我们可以在 FLUX.1-dev 推理上实现高达 1.5 倍的速度，而且质量非常好<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\" rel=\"noopener nofollow\">[1]</a></sup>。<br />\n简单来说就是上面提到的DeepCache/FORA在使用上太粗糙直接通过固定时间步去cache缓存这样忽视输出差异的非均匀性，因此后续的TeaCache发现模型输入与输出的强相关性，通过Timestep Emebdding（输入）来估计输出差异。而后FBCache又做了新的改进：<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602704-899465973.webp\" /><br />\n利用residual cache实现了一个基于First Block L1误差的Cache方案，误差小于指定阈值，就跳过当前步计算，复用residual cache，对当前步的输出进行估计。</p>\n<h4 id=\"cachedit\">CacheDit</h4>\n<p><a href=\"https://github.com/vipshop/cache-dit\" rel=\"noopener nofollow\" target=\"_blank\">cache-dit</a>这个框架主要是适用于Dit结构的扩散模型使用，其具体<a href=\"https://cache-dit.readthedocs.io/en/latest/user_guide/DBCACHE_DESIGN/\" rel=\"noopener nofollow\" target=\"_blank\">模型框架</a>如下：<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231603414-1076713638.webp\" /><br />\n对于上述框架首先了解CacheDit中几个概念：1、<code>Fn</code>：表示需要计算前n层transformer block在时间步t并且详细解释一下CacheDit原理；2、<code>Bn</code>:表示进一步的融合后n层transformer block的信息去强化预测准确性。其中n=1时候就是FBCache。<br />\n因此对于CacheDit具体过程为：<strong>在t-1步时候</strong>，前n块block去计算他们的结果得到输出结果hidden state并且写入缓存中<span class=\"math inline\">\\(C_{t-1}\\)</span>，而后后几层进行完整结算。<strong>在t步时候</strong>，前n块block不完整计算，而是直接复用/近似 t-1 步的缓存<span class=\"math inline\">\\(C_{t-1}\\)</span>得到近似的结果，计算近似结果和缓存结果中差异（L1 范数），如果差异小于阈值直接复用缓存输入到后续的块中计算，反之就重新计算这n块结果。<br />\n其中具体使用如下：<a href=\"https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/DFModelCode/DF_acceralate/df_acceralate.ipynb\" rel=\"noopener nofollow\" target=\"_blank\">df_acceralate.ipynb</a></p>\n<h3 id=\"量化技术概述\">量化技术概述</h3>\n<p><a href=\"https://www.big-yellow-j.top/posts/2025/10/11/Quantized.html\" rel=\"noopener nofollow\" target=\"_blank\">量化技术</a>是一种模型压缩的常见方法，将模型权重从高精度（如FP16或FP32）量化为低比特位（如INT8、INT4）去实现<strong>降低显存+生成加速</strong>。量化过程的基本范式，量化过程：<span class=\"math inline\">\\(Q=\\frac{W}{S}\\)</span>其中 <span class=\"math inline\">\\(S\\)</span>表示scale，反量化过程：<span class=\"math inline\">\\(\\hat{w}=QS\\)</span>，因此对于量化只需要保存：1、量化后权重；2、scale值（不同量化模型计算方式不同）。比如说（对称量化过程）对于：<code>1.21, -1.13, 0.22, 0.83, 2.11, -1.53, 0.79\t-0.54, 0.84</code>其中最大值为2.11那么可以计算出缩放系数为：<span class=\"math inline\">\\(\\frac{2.11}{127}=0.01661417\\)</span>（127代表int8数值范围-127，127）那么可以对数据缩放（量化）得到：<code>72, -69, 13, 49, 127, -93, 47, -33, 50</code>反量化可以得到：<code>1.19622024,....</code>（直接乘scale即可）具体计算数字之间差异，都是存在误差的。<br />\n常见的量化策略可以分为PTQ和QAT两大类。量化感知训练（Quantization-Aware Training）：在<strong>模型训练过程中进行量化</strong>，一般效果会更好一些，但需要额外训练数据和大量计算资源比如说qlora。后量化（PTQ）：在<strong>模型训练完成后，对模型进行量化</strong>，无需重新训练。对于线性量化下，浮点数与定点数之间的转换公式如下：<span class=\"math inline\">\\(Q=\\frac{R}{S}+Z;R=(Q-Z)*S\\)</span>，其中R 表示量化前的浮点数、Q 表示量化后的定点数、S（Scale）表示缩放因子的数值、Z（Zero）表示零点的数值。除此之外还会听到几个概念：<strong>1、对称量化</strong>：对称量化的核心思想是将浮点数量化为整数，且量化后的分布是关于零对称的；<strong>2、非对称量化</strong>：是一种用于将浮点数转换为整数表示的量化方法。与对称量化不同的是，这种方法在数据具有偏移（即非对称分布）时更有效，因为它可以减少量化误差。非对称量化会分别找出浮点数的最小值和最大值，分别量化到目标整数范围的最小值和最大值，充分利用量化后的整数范围。这可以使用一个缩放因子（scale）和偏移量（zero-point）来实现<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\" rel=\"noopener nofollow\">[2]</a></sup>。<br />\n比如说在LLM中常用的两种<strong>后量化技术</strong>：1、<strong>GPTQ量化技术</strong>：通过量化——补偿——量化迭代方法，首先量化<span class=\"math inline\">\\(W_{:,j}\\)</span>，而后去计算误差并且补充到 <span class=\"math inline\">\\(W_{:,j:(i+B)}\\)</span>而后进行迭代实现所有参数的量化；2、<strong>AWQ量化技术</strong>：模型计算过程中只有关键参数起作用因此对于关键参数保持原来的精度(FP16)，对其他权重进行低比特量化，但是这样不同进度参数会导致硬件问题，因此在AWQ中<strong>对所有权重均进行低比特量化，但是，在量化时，对于显著权重乘以较大的scale，相当于降低其量化误差；同时，对于非显著权重，乘以较小的scale，相当于给予更少的关注。</strong></p>\n<blockquote>\n<p>补充一个小知识，一般量化看到比较多就是W4A4这个一般指的就是权重和激活的4bit量化，其中权重一般就是<strong>对应该层的模型权重</strong>，激活就是<strong>对应该层的输入</strong></p>\n</blockquote>\n<h4 id=\"bitsandbytes-量化\">Bitsandbytes 量化</h4>\n<p>通过使用bitsandbytes量化来实现8-bit（int8）或者4-bit（int4、Qlora中一般就会使用）量化，不过区别上面提到的AWQ以及GPTQ量化，bitsandbytes不需要对模型进行训练（AWQ、GPTQ可能需要输入数据然后计算误差进行量化），前者需要通过数据来保证量化精度（量化过程是离线、一次性过程），后者量化过程是即时的可逆的。其技术原理如下：<span class=\"math inline\">\\(w≈s q\\)</span>其中w表示原始的FP16权重，q代表int4/int8权重，s缩放因子，其量化过程为对每一个block权重计算：<span class=\"math inline\">\\(\\max(\\text{abs}(w))\\)</span>而后去计算scale：<span class=\"math inline\">\\(s=\\frac{amx(\\| w\\|)}{2^{b-1}-1}\\)</span>而后代入公式就可以得到量化后权重，推理过程中进行：反量化 + 矩阵乘法融合在一个 CUDA kernel 中完成：<span class=\"math inline\">\\(Y=X(sq)\\)</span>。因此对于其使用也很简单，比如说在代码中：<a href=\"https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/DFModelCode/DF_acceralate/cache_acceralate.py\" rel=\"noopener nofollow\" target=\"_blank\">cache_acceralate.py</a></p>\n<pre><code class=\"language-python\"># 在ZImagePipeline中参数为：\nclass ZImagePipeline(DiffusionPipeline, ZImageLoraLoaderMixin, FromSingleFileMixin):\n    def __init__(,..,vae, text_encoder, tokenizr, transformer):\n        ...\n# 因此可以直接对里面的text_encoder使用量化处理\n\nfrom diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\nquantization_config = DiffusersBitsAndBytesConfig(\n    load_in_4bit=True,# 在模型加载阶段，将权重以 4-bit 量化形式加载\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,# 指定 反量化后参与计算的 dtype\n    bnb_4bit_use_double_quant=True,#启用 Double Quantization（双重量化），也就是对block的scale在进行一次量化\n    llm_int8_skip_modules=[\"transformer_blocks.0.img_mod\"],# 指定 不参与 bitsandbytes 量化的模块\n)\ntransformer = AutoModel.from_pretrained(\n    model_name,\n    cache_dir=cache_dir,\n    subfolder=\"transformer\",\n    quantization_config=quantization_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    mirror='https://hf-mirror.com'\n)\n</code></pre>\n<p>去对你的<code>model_name</code>里面的transformer进行量化处理，除此之外还有使用例子就是进行优化器量化，比如说</p>\n<pre><code class=\"language-python\"># 和使用adamw方式一样，使用qlora使用一般带上这个优化器\nimport bitsandbytes as bnb\noptimizer_class = bnb.optim.AdamW8bit\n</code></pre>\n<h4 id=\"svdquant量化\">SVDQuant量化</h4>\n<blockquote>\n<p><a href=\"https://github.com/nunchaku-ai/nunchaku\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/nunchaku-ai/nunchaku</a></p>\n</blockquote>\n<p>在扩散模型中，权重（Weights）和激活（Activations）往往包含大量异常值（极端大或小的值），这些值在低位量化（如4-bit INT4）时会引起严重误差，导致生成的图像失真或噪声增多。<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231601731-775108244.webp\" /></p>\n<blockquote>\n<p>a：权重和激活值中都存在异常值，b：将激活值的异常值移动到权重中，c：将权重进行分解低秩的<span class=\"math inline\">\\(L_1L_2\\)</span>以及残差</p>\n</blockquote>\n<p>因此对于SVDQuant过程描述如下，对于权重和激活值：<span class=\"math inline\">\\(\\mathbf{X}\\)</span> 以及 <span class=\"math inline\">\\(\\mathbf{X}\\)</span>，在最初这两部分值都是存在大量异常值，因此首先通过平滑操作将激活<span class=\"math inline\">\\(\\mathbf{X}\\)</span>中的异常值迁移到权重 <span class=\"math inline\">\\(\\mathbf{X}\\)</span>中得到更新后权重 <span class=\"math inline\">\\(\\hat{\\mathbf{W}}\\)</span>，这部分数据表述为：<span class=\"math inline\">\\(\\hat{\\mathbf{W}}=\\mathbf{W}\\odot S\\)</span>其中S是平滑因子，用于转移异常（⊙表示逐元素乘法）。这部分操作主要是因为：<strong>将异常值集中到权重侧，因为权重是静态的，更容易后续处理。激活侧的异常值减少后，量化难度降低</strong>。而后，进行SVD分解与低秩吸收，对更新后权重进行奇异值分解：$  \\hat{𝑾} = 𝑼 \\Sigma 𝑽^T  <span class=\"math inline\">\\(, 其中𝑼和𝑽是正交矩阵，\\)</span>\\Sigma<span class=\"math inline\">\\(是奇异值对角矩阵。保留前k个最大奇异值（低秩r，通常r &lt;&lt; min(m,n)，其中m,n是权重矩阵维度），形成低秩近似：\\)</span>  𝑳_1 𝑳_2 = 𝑼[:,:r] \\cdot \\Sigma[:r,:r] \\cdot 𝑽^T[:r,:]  <span class=\"math inline\">\\(。然后，计算残差：\\)</span>  𝑹 = \\hat{𝑾} - 𝑳_1 𝑳_2<span class=\"math inline\">\\(，其中只对残差\\)</span>𝑹<span class=\"math inline\">\\(进行量化（\\)</span>Q(𝑹)=\\text{round}(\\frac{𝑹}{S_𝑹})S_𝑹<span class=\"math inline\">\\(，其中\\)</span>S_𝑹<span class=\"math inline\">\\(为缩放因子）处理。低秩分支\\)</span>  𝑳_1 𝑳_2  $使用高精度（16-bit float）运行，专门“吸收”异常值和主要信息，而残差𝑹中的异常值和幅度显著减少，只需量化到4-bit。量化误差界限分析（从论文中）：量化误差上界可通过F范数和奇异值控制，证明低秩吸收后残差的量化难度降低（误差 ≤ $  \\frac{\\sqrt{\\log(\\text{size}(𝑹)\\pi)}}{q_{\\max}} \\mathbb{E}[|𝑹|_F]  $，其中q_max是量化最大值），<strong>最后的整体近似计算</strong>：</p>\n<p></p><div class=\"math display\">\\[\\hat{\\mathbf{X}}\\hat{\\mathbf{W}}=\\hat{\\mathbf{X}}(R+L_1L_2)≈\\hat{\\mathbf{X}}(L_1L_2)+Q(\\hat{\\mathbf{X}})Q(R)\n\\]</div><p></p><p>第一项是16-bit低秩分解，第二项为4-bit残差分支。整个过程对应：<br />\n<img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602130-1215683201.webp\" /></p>\n<h4 id=\"gguf\">GGUF</h4>\n<blockquote>\n<p>HF文档：<a href=\"https://huggingface.co/docs/hub/en/gguf\" rel=\"noopener nofollow\" target=\"_blank\">https://huggingface.co/docs/hub/en/gguf</a><br />\n<a href=\"https://unsloth.ai/docs/basics/inference-and-deployment/saving-to-gguf\" rel=\"noopener nofollow\" target=\"_blank\">https://unsloth.ai/docs/basics/inference-and-deployment/saving-to-gguf</a></p>\n</blockquote>\n<p>GGUF格式是用于存储大型模型预训练结果的，相较于Hugging Face和torch的bin文件，它采用了紧凑的二进制编码格式、优化的数据结构以及内存映射等技术，提供了更高效的数据存储和访问方式。GGUF 本身支持多种量化级别（Q2_K ~ Q8_0、IQ2 ~ IQ4 等），这些量化方式属于后训练量化（PTQ），和 AWQ、GPTQ、bitsandbytes 4bit 一样，都是在预训练模型上直接执行量化（不需要重新训练）。不过需要区别的是AWQ、GPTQ、SVDQuant都需要一小批数据而bitsandbytes以及GGUF不需要，在GGUF中可以实现量化方式有两类：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\"><strong>传统Q系列</strong>（按照权重逐层量化）</th>\n<th style=\"text-align: center;\"><strong>K-Quant系列</strong>（通过 block-wise + scale 优化）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602722-1391696036.webp\" /></td>\n<td style=\"text-align: center;\"><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602534-100439133.webp\" /></td>\n</tr>\n</tbody>\n</table>\n<p>其中<strong>传统Q系列</strong>主要是一整块权重共享一个 scale（缩放因子），每个权重用低 bit 整数表示，容易受到极端值的影响。 <strong>K-Quant系列</strong>一个 block 内，再分“子块”，每个子块有自己的 scale，其中S代表子块少、scale少；M代表子块多、scale多。</p>\n<h2 id=\"总结\">总结</h2>\n<p>本文主要是介绍一些在SD模型中加快生图的策略，1、直接使用加速框架进行优化，比如说指定attention计算后端方式、通过<code>torch.compile</code>进行编译、使用<code>torch.channels_last</code>去优化内存访问方式等；2、cache策略，发现在生成过程中在某些层/时间布之间图像的特征比较相似，因此就可以考虑将这些计算结果进行缓存在后续n步中直接加载缓存好的特征来实现生成加速，主要介绍框架是<code>cache-dit</code>；3、量化技术概述，<br />\n最后简单对比一下生成加速时间</p>\n<blockquote>\n<p>测试prompt: <code>超写实亚洲中年男性，年龄约45-55岁。面容坚毅、憔悴，带有生活阅历的痕迹（如眼角的细纹）。他穿着质感柔软的深灰色高领毛衣，外搭一件经典的卡其色风衣，站在寒风中周围是高楼大厦</code><br />\n从测试结果上图像的差异还是不大，时间的话从5.97--&gt;5.48（<strong>不一定严谨！</strong>）还是有效的</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th style=\"text-align: center;\">正常生图</th>\n<th style=\"text-align: center;\">+使用channel+ flash_attn</th>\n<th style=\"text-align: center;\">+使用cachedit</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align: center;\"><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602310-1840320611.webp\" /></td>\n<td style=\"text-align: center;\"><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602417-1468973015.webp\" /></td>\n<td style=\"text-align: center;\"><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/3395559/202601/3395559-20260125231602717-1497084798.webp\" /></td>\n</tr>\n<tr>\n<td style=\"text-align: center;\"><code>5.97</code></td>\n<td style=\"text-align: center;\"><code>5.67</code></td>\n<td style=\"text-align: center;\"><code>5.48</code></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"参考\">参考</h2>\n<hr class=\"footnotes-sep\" />\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li class=\"footnote-item\" id=\"fn1\"><p><a href=\"https://github.com/chengzeyi/ParaAttention/blob/main/doc/fastest_flux.md\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/chengzeyi/ParaAttention/blob/main/doc/fastest_flux.md</a> <a class=\"footnote-backref\" href=\"#fnref1\" rel=\"noopener nofollow\">↩︎</a></p>\n</li>\n<li class=\"footnote-item\" id=\"fn2\"><p><a href=\"https://juejin.cn/post/7436976221068148786\" rel=\"noopener nofollow\" target=\"_blank\">https://juejin.cn/post/7436976221068148786</a> <a class=\"footnote-backref\" href=\"#fnref2\" rel=\"noopener nofollow\">↩︎</a></p>\n</li>\n</ol>\n</section>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-25 23:16</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Big-Yellow\">Big-Yellow-J</a>&nbsp;\n阅读(<span id=\"post_view_count\">51</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "MCP 进化：让静态 Tool 进化为具备“上下文感知”的远程 Skills",
      "link": "https://www.cnblogs.com/noear/p/19530698",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/noear/p/19530698\" id=\"cb_post_title_url\" title=\"发布于 2026-01-25 22:47\">\n    <span>MCP 进化：让静态 Tool 进化为具备“上下文感知”的远程 Skills</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        摘要： Solon AI 通过将 Model Context Protocol (MCP) 封装为动态感知的 Skill，解决了传统静态工具交互的三大痛点：上下文噪音、权限真空和行为失控。Skill 通过智能准入（isSupported）、指令注入（getInstruction）和三态路由（getToolsName）实现动态分发，使模型仅感知当前场景所需的工具。实战示例展示了客户端与服务端的协作，支持多租户隔离和权限控制。该方案适用于复杂业务场景，但需注意其非标准化特性及适用边界。最终，Skill 架构提升\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在 AI Agent 的工程实践中，Model Context Protocol (MCP) 已成为连接大模型与外部世界的标准桥梁。然而，随着应用场景从“个人助手”向“企业级复杂业务”迈进，传统的 MCP 交互模式开始显露其 <strong>“静态化”</strong> 的瓶颈。</p>\n<p>Solon AI 支持将 MCP 封装为 Skill，实现了从“冷冰冰的 API 集合”到“具备感知能力的智能技能”的跨越。</p>\n<h3 id=\"一静态-tools-的三大痛点\">一、静态 Tools 的三大痛点</h3>\n<p>传统的 MCP 交互类似于一个“无法关闭的工具箱”，无论场景如何，所有工具一涌而上：</p>\n<ul>\n<li><strong>上下文噪音（Context Noise）：</strong> 即使是一个简单的问候，模型也会被注入成百上千行的工具 Schema 定义，白白浪费 Token，更干扰模型的推理专注度。</li>\n<li><strong>权限真空（Security Risks）：</strong> 模型对工具的可见性是“全量”的。难以根据当前登录用户的角色（如普通用户 vs 管理员）动态隐藏敏感操作（如：删除订单）。</li>\n<li><strong>行为失控（Instruction Gap）：</strong> 工具只提供了“能做什么”，却无法告诉模型“在当前背景下该怎么做”。模型缺乏针对特定业务场景的即时指令约束。</li>\n</ul>\n<h3 id=\"二核心解决方式感知挂载与动态分发\">二、核心解决方式：感知、挂载与动态分发</h3>\n<p>Solon AI 通过引入 Skill（Solon AI Skills） 生命周期 来包裹 MCP 协议，实现以下机制解决上述痛点：</p>\n<h4 id=\"a-智能准入-issupported\">A. 智能准入 (isSupported)：</h4>\n<p>只有当 Prompt 上下文（意图、租户信息、环境变量）满足条件时，技能才会被激活。</p>\n<h4 id=\"b-指令注入-getinstruction\">B. 指令注入 (getInstruction)：</h4>\n<p>在技能挂载时，自动为模型注入针对当前上下文的“行为准则”（System Message）。</p>\n<h4 id=\"c-三态路由-gettoolsname\">C. 三态路由 (getToolsName)：</h4>\n<p>服务端根据 Prompt 属性，动态决定给模型展示哪些工具。支持三种形态的路由方式：</p>\n<ul>\n<li>全量使用：未定义过滤逻辑时，显示所有业务工具。</li>\n<li>精准授权：仅展示当前用户权限范围内的工具。</li>\n<li>完全拒绝：即便技能激活，也可能因安全策略在此时封锁所有工具调用。</li>\n</ul>\n<h3 id=\"三实战示例\">三、实战示例</h3>\n<h4 id=\"1-客户端像本地技能一样调用\">1. 客户端：像本地技能一样调用</h4>\n<p>开发者只需关注业务属性的注入，无需操心工具的过滤逻辑，一切由 MCP Skill 代理与远程服务端约定与协商。</p>\n<pre><code class=\"language-java\">import org.noear.solon.ai.chat.ChatModel;\nimport org.noear.solon.ai.chat.prompt.Prompt;\nimport org.noear.solon.ai.mcp.McpChannel;\nimport org.noear.solon.ai.mcp.client.McpClientProvider;\nimport org.noear.solon.ai.mcp.client.McpSkillClient;\n\n//构建 mcp 客户端\nMcpClientProvider mcpClient = McpClientProvider.builder()\n                .channel(McpChannel.STREAMABLE)\n                .url(\"http://localhost:8081//skill/order\")\n                .build();\n\n// 构建带有业务属性的提示词\nPrompt prompt = Prompt.of(\"帮我取消订单 A001\")\n                .attrPut(\"tenant_id\", \"solon_001\")\n                .attrPut(\"user_role\", \"ADMIN\"); // 模拟管理员身份\n\n// 注入技能，模型将只看到“管理员”权限下的工具\nchatModel.prompt(prompt)\n         .options(o -&gt; o.skillAdd(new McpSkillClient(mcpClient))) //将 mcp 客户端 包装为 Solon AI Skills\n         .call();\n</code></pre>\n<h4 id=\"2-服务端实现具备感知力的技能\">2. 服务端：实现具备“感知力”的技能</h4>\n<p>服务端不再是盲目响应，而是通过解析 Prompt 决定自己的表现。</p>\n<pre><code class=\"language-java\">import org.noear.solon.ai.annotation.ToolMapping;\nimport org.noear.solon.ai.chat.prompt.Prompt;\nimport org.noear.solon.ai.mcp.McpChannel;\nimport org.noear.solon.ai.mcp.server.McpSkillServer;\nimport org.noear.solon.ai.mcp.server.annotation.McpServerEndpoint;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\n@McpServerEndpoint(channel = McpChannel.STREAMABLE_STATELESS, mcpEndpoint = \"/skill/order\")\npublic class OrderSkillServer extends McpSkillServer {\n    \n    @Override\n    public boolean isSupported(Prompt prompt) {\n        // 感知意图：只有涉及“订单”且租户合规时才激活\n        return prompt.getUserContent().contains(\"订单\") \n               &amp;&amp; prompt.attr(\"tenant_id\") != null;\n    }\n\n    @Override\n    public String getInstruction(Prompt prompt) {\n        // 动态指令：注入租户特定的业务规则\n        return \"你现在是租户[\" + prompt.attr(\"tenant_id\") + \"]的订单助手。\";\n    }\n\n    @Override\n    public List&lt;String&gt; getToolsName(Prompt prompt) {\n        // 权限隔离：根据用户角色动态下发工具名\n        List&lt;String&gt; tools = new ArrayList&lt;&gt;();\n        tools.add(\"OrderQuery\"); // 基础权限\n\n        if (\"ADMIN\".equals(prompt.attr(\"user_role\"))) {\n            tools.add(\"OrderCancel\"); // 仅管理员可见\n        }\n        return tools;\n    }\n\n    @ToolMapping(description = \"查询订单\")\n    public String OrderQuery(String id) { ... }\n\n    @ToolMapping(description = \"取消订单\")\n    public String OrderCancel(String id) { ... }\n}\n</code></pre>\n<h3 id=\"四skills-架构反思与局限性补充\">四、Skills 架构反思与局限性补充</h3>\n<p>尽管将 MCP 进化为 Skills 带来了显著的工程优势，但开发者仍需理清其技术边界：</p>\n<ul>\n<li>非标准化的架构增强：</li>\n</ul>\n<p>LLM 的底层标准仅包含 Prompt 和 Tool-Call。Skills 并非模型原生标准，也不属于 MCP 的公共协议规范，而是一种 架构设计模式（模式，是通用的）。它通常由 AI 开发框架（如 Solon AI）在消费侧实现，用于解决复杂业务下的能力调度问题。</p>\n<ul>\n<li>消费侧驱动的定制：</li>\n</ul>\n<p>MCP 向 Skills 的进化本质上是“业务驱动”或“领域驱动”的。在设计远程 MCP Skill 时，必须参考消费侧（即 Agent 执行引擎）的具体规范进行深度定制。</p>\n<ul>\n<li>适用场景的选择：</li>\n</ul>\n<p>Tool：适用于原子化、无状态、全量公开的简单功能插件。</p>\n<p>Skill：适用于需要上下文感知、多租户隔离、动态指令约束的复杂业务逻辑块。</p>\n<h3 id=\"五-好处总结\">五、 好处总结</h3>\n<p>将 MCP 进化为 Skills 之后，您的 AI Agent 架构将获得：</p>\n<ul>\n<li>极致的上下文纯净度：</li>\n</ul>\n<p>模型只看到此时此刻该看的工具（通过 getToolsName 实现按需加载，或权限控制）。</p>\n<ul>\n<li>天然的权限安全：</li>\n</ul>\n<p>通过服务端感知的动态分发，实现真正的跨进程角色权限控制（RBAC for Tools）。</p>\n<ul>\n<li>低耦合的业务演进：</li>\n</ul>\n<p>业务逻辑和规则变更集中在服务端，客户端 “无需” 任何代码改动即可获得最新能力。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-25 22:47</span>&nbsp;\n<a href=\"https://www.cnblogs.com/noear\">带刺的坐椅</a>&nbsp;\n阅读(<span id=\"post_view_count\">27</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}