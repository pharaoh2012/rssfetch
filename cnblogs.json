{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "Julia, 科学计算与高性能编程语言",
      "link": "https://www.cnblogs.com/boyogala/p/19521607",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/boyogala/p/19521607\" id=\"cb_post_title_url\" title=\"发布于 2026-01-23 12:34\">\n    <span>Julia, 科学计算与高性能编程语言</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<section id=\"nice\"><h1><span class=\"prefix\"></span><span class=\"content\">Julia, 科学计算与高性能编程语言</span><span class=\"suffix\"></span></h1>\n<p>Julia（julialang.org）由Stefan Karpinski、Jeff Bezanson等在2009年创建，目标是融合Python的易用性、C的高性能、R的统计能力、Matlab的科学计算生态。</p>\n<p><strong>其核心设计哲学是</strong>：</p>\n<ul>\n<li><section>高性能：编译型语言（JIT），接近C/Fortran性能。</section></li><li><section>多领域统一：一个语言解决科学计算、数据科学、机器学习、可视化等全栈问题。</section></li><li><section>生态活跃：2023年PyPI包下载量超500万次，社区年增长40%+。</section></li></ul>\n<p>✅ 关键优势总结：</p>\n<ul>\n<li><section>速度：数值计算性能≈C/Fortran，远超Python/R（实测：矩阵乘法快20-100倍）。</section></li><li><section>易用性：语法类似Python，但类型系统提供编译优化。</section></li><li><section>生态整合：无需切换语言，一个环境完成从数据到部署的全流程。</section></li></ul>\n<hr />\n<p>作为一门新兴的科学计算语言，Julia正在迅速改变科研和工程领域的计算范式。自2012年由MIT团队推出以来，Julia以其独特的设计哲学——**\"一次编写，高效运行\"**，成功融合了动态语言的易用性与静态语言的高性能，为解决\"两语言问题\"提供了革命性方案。</p>\n<h3><span class=\"prefix\"></span><span class=\"content\">一、Julia语言核心优势</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 高性能计算能力</span><span class=\"suffix\"></span></h4>\n<p>Julia的<strong>JIT编译</strong>机制是其高性能的基础，通过基于LLVM的即时编译器，Julia能够将动态类型代码编译为接近C/Fortran性能的原生机器码。在实际应用中，Julia的性能表现如下：</p>\n<ul>\n<li><section><strong>数值计算</strong>：矩阵乘法比Python快20-100倍</section></li><li><section><strong>循环计算</strong>：100万次循环求和比Python快75倍</section></li><li><section><strong>高精度计算</strong>：BigFloat的乘法操作仅比C的MPFR实现慢5-10%</section></li><li><section><strong>科学计算</strong>：微分方程求解性能与Fortran相当或更优</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. 类型系统与多分派机制</span><span class=\"suffix\"></span></h4>\n<p>Julia的<strong>多分派（Multiple Dispatch）</strong>机制是其最核心的创新，也是性能优化的关键。多分派允许函数根据<strong>所有参数类型</strong>动态选择最优实现，而非仅基于接收者类型，这使得代码既保持了动态类型的灵活性，又获得了接近静态语言的性能。</p>\n<ul>\n<li><section><strong>类型推断</strong>：编译器自动推断类型，减少运行时开销</section></li><li><section><strong>类型稳定性</strong>：通过<code>@code_warntype</code>可视化类型推断过程</section></li><li><section><strong>参数多态</strong>：支持泛型编程，提高代码复用性</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. 统一的全栈生态系统</span><span class=\"suffix\"></span></h4>\n<p>Julia提供了一个<strong>统一的全栈环境</strong>，使开发者能够在一个语言环境中完成从数据处理到模型训练、可视化展示再到部署的完整工作流，无需在Python、R、Matlab和C/Fortran之间切换。</p>\n<ul>\n<li><section><strong>数据科学</strong>：DataFrames.jl、CSV.jl等工具包</section></li><li><section><strong>可视化</strong>：Plots.jl、GLMakie等可视化库</section></li><li><section><strong>机器学习</strong>：Flux.jl、MLJ.jl等深度学习和机器学习框架</section></li><li><section><strong>科学计算</strong>：DifferentialEquations.jl等专业计算包</section></li><li><section><strong>并行计算</strong>：Distributed.jl、CUDA.jl等并行和GPU加速库</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">4. 易用性与开发效率</span><span class=\"suffix\"></span></h4>\n<p>Julia的语法设计借鉴了Python、Matlab和R等语言，提供了<strong>接近Python的易用性和开发效率</strong>，同时保持了科学计算所需的严谨性。</p>\n<ul>\n<li><section><strong>代码简洁性</strong>：与Python相比，相同功能的代码行数减少30-50%</section></li><li><section><strong>交互式开发</strong>：支持Jupyter Notebook、Pluto.jl等交互式环境</section></li><li><section><strong>可读性</strong>：语法直观，接近数学表达，便于科研协作</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">二、数值系统与高性能计算</span><span class=\"suffix\"></span></h3>\n<p>Julia的数值系统是其高性能的基础，专为科学计算和数值分析设计。</p>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 高效数值类型</span><span class=\"suffix\"></span></h4>\n<p>Julia提供了丰富的数值类型，覆盖从8位整数到任意精度浮点数的全谱系：</p>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>类型</th>\n<th>位数</th>\n<th>范围</th>\n<th>特点</th>\n</tr><tr>\n<td>Int8</td>\n<td>8位</td>\n<td>-128至127</td>\n<td>内存占用小，适合分类数据</td>\n</tr>\n<tr>\n<td>Int16</td>\n<td>16位</td>\n<td>-32768至32767</td>\n<td>常规整数计算</td>\n</tr>\n<tr>\n<td>Int32</td>\n<td>32位</td>\n<td>-2^31至2^31-1</td>\n<td>默认整数类型</td>\n</tr>\n<tr>\n<td>Int64</td>\n<td>64位</td>\n<td>-2^63至2^63-1</td>\n<td>大规模整数计算</td>\n</tr>\n<tr>\n<td>Big Int</td>\n<td>任意位</td>\n<td>无限制</td>\n<td>高精度整数运算</td>\n</tr>\n<tr>\n<td>Float16</td>\n<td>16位</td>\n<td>±6.55e±04</td>\n<td>GPU加速友好</td>\n</tr>\n<tr>\n<td>Float32</td>\n<td>32位</td>\n<td>±3.4e±38</td>\n<td>默认浮点类型</td>\n</tr>\n<tr>\n<td>Float64</td>\n<td>64位</td>\n<td>±1.7e±308</td>\n<td>高精度科学计算</td>\n</tr>\n<tr>\n<td>BigFloat</td>\n<td>任意位</td>\n<td>无限制</td>\n<td>基于MPFR/GMP库</td>\n</tr>\n<tr>\n<td>Complex{F}</td>\n<td>128位</td>\n<td>±3.4e±38</td>\n<td>复数计算，如Complex{Float64}</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">2. 高性能计算优化</span><span class=\"suffix\"></span></h4>\n<p>Julia通过多种机制实现数值计算的高性能：</p>\n<ul>\n<li><section><strong>向量化操作</strong>：通过<code>@.</code>语法实现自动向量化</section></li><li><section><strong>SIMD指令</strong>：支持<code>@simd</code>并行指令</section></li><li><section><strong>BLAS调用</strong>：默认使用优化的BLAS库（如OpenBLAS、Intel MKL）</section></li><li><section><strong>高精度计算</strong>：BigFloat基于GMP/MPFR库，性能接近C</section></li></ul>\n<p><strong>性能实测</strong>：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;BenchmarkTools<br />A&nbsp;=&nbsp;rand(<span class=\"hljs-built_in\">Float32</span>,&nbsp;<span class=\"hljs-number\">1000</span>,&nbsp;<span class=\"hljs-number\">1000</span>);&nbsp;B&nbsp;=&nbsp;rand(<span class=\"hljs-built_in\">Float32</span>,&nbsp;<span class=\"hljs-number\">1000</span>,&nbsp;<span class=\"hljs-number\">1000</span>)<br /><span class=\"hljs-meta\">@btime</span>&nbsp;$A&nbsp;*&nbsp;$B&nbsp;&nbsp;<span class=\"hljs-comment\">#&nbsp;Julia:&nbsp;0.8ms&nbsp;(Float32)</span><br /></code></pre>\n<p>相比之下，Python（NumPy）在相同任务上需要约3.2ms，R则需要约12.3ms，Julia的性能优势明显。</p>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. 矩阵运算优化</span><span class=\"suffix\"></span></h4>\n<p>Julia的<code>LinearAlgebra</code>包提供了高度优化的矩阵运算接口：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;LinearAlgebra<br /><span class=\"hljs-comment\">#&nbsp;矩阵乘法</span><br />C&nbsp;=&nbsp;A&nbsp;*&nbsp;B<br /><span class=\"hljs-comment\">#&nbsp;矩阵点乘</span><br />C&nbsp;.+=&nbsp;A&nbsp;.+&nbsp;B<br /><span class=\"hljs-comment\">#&nbsp;矩阵求逆</span><br />inv(A)<br /><span class=\"hljs-comment\">#&nbsp;特征值分解</span><br />eigen(A)<br /></code></pre>\n<p>通过<code>Octavian.jl</code>等优化库，Julia的矩阵乘法性能甚至可以超越OpenBLAS和Intel MKL。</p>\n<h3><span class=\"prefix\"></span><span class=\"content\">三、类型系统与多分派机制</span><span class=\"suffix\"></span></h3>\n<p>Julia的类型系统是其高性能与易用性结合的关键，核心是<strong>多分派机制</strong>。</p>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 多分派原理</span><span class=\"suffix\"></span></h4>\n<p>多分派允许函数根据<strong>所有参数类型</strong>动态选择实现：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;定义两个版本的add函数</span><br /><span class=\"hljs-keyword\">function</span>&nbsp;add(x::<span class=\"hljs-built_in\">Int</span>,&nbsp;y::<span class=\"hljs-built_in\">Int</span>)<br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">return</span>&nbsp;x&nbsp;+&nbsp;y<br /><span class=\"hljs-keyword\">end</span><br /><br /><span class=\"hljs-keyword\">function</span>&nbsp;add(x::<span class=\"hljs-built_in\">Float64</span>,&nbsp;y::<span class=\"hljs-built_in\">Float64</span>)<br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">return</span>&nbsp;x&nbsp;+&nbsp;y<br /><span class=\"hljs-keyword\">end</span><br /><br /><span class=\"hljs-comment\">#&nbsp;调用函数，Julia会根据参数类型自动选择</span><br />add(<span class=\"hljs-number\">1</span>,&nbsp;<span class=\"hljs-number\">2</span>)&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-comment\">#&nbsp;调用Int版本</span><br />add(<span class=\"hljs-number\">1.0</span>,&nbsp;<span class=\"hljs-number\">2.0</span>)&nbsp;<span class=\"hljs-comment\">#&nbsp;调用Float64版本</span><br /></code></pre>\n<p>这种机制使得代码既保持了动态类型的灵活性，又获得了接近静态语言的性能。</p>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. 类型推断与性能优化</span><span class=\"suffix\"></span></h4>\n<p>Julia的编译器能够进行<strong>高效的类型推断</strong>，将动态类型代码编译为高性能机器码：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;显式类型注解</span><br /><span class=\"hljs-keyword\">function</span>&nbsp;sum_loop(n::<span class=\"hljs-built_in\">Int</span>)<br />&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;=&nbsp;<span class=\"hljs-number\">0.0</span><br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">for</span>&nbsp;i&nbsp;<span class=\"hljs-keyword\">in</span>&nbsp;<span class=\"hljs-number\">1</span>:n;&nbsp;s&nbsp;+=&nbsp;i;&nbsp;<span class=\"hljs-keyword\">end</span><br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">return</span>&nbsp;s<br /><span class=\"hljs-keyword\">end</span><br /><br /><span class=\"hljs-comment\">#&nbsp;隐式类型推断</span><br /><span class=\"hljs-keyword\">function</span>&nbsp;sum_loop(n)<br />&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;=&nbsp;<span class=\"hljs-number\">0.0</span><br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">for</span>&nbsp;i&nbsp;<span class=\"hljs-keyword\">in</span>&nbsp;<span class=\"hljs-number\">1</span>:n;&nbsp;s&nbsp;+=&nbsp;i;&nbsp;<span class=\"hljs-keyword\">end</span><br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">return</span>&nbsp;s<br /><span class=\"hljs-keyword\">end</span><br /><br /><span class=\"hljs-comment\">#&nbsp;查看类型推断过程</span><br /><span class=\"hljs-meta\">@code_warntype</span>&nbsp;sum_loop(<span class=\"hljs-number\">1_000_000</span>)<br /></code></pre>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>Julia</strong>：200ns</section></li><li><section><strong>Python</strong>：15μs（慢75倍）</section></li><li><section><strong>R</strong>：约30μs</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. 类型稳定性</span><span class=\"suffix\"></span></h4>\n<p>Julia鼓励开发者编写<strong>类型稳定的代码</strong>，以获得最佳性能：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;类型不稳定代码</span><br /><span class=\"hljs-keyword\">function</span>&nbsp;unstable_sum(v)<br />&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;=&nbsp;<span class=\"hljs-number\">0</span><br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">for</span>&nbsp;x&nbsp;<span class=\"hljs-keyword\">in</span>&nbsp;v;&nbsp;s&nbsp;+=&nbsp;x;&nbsp;<span class=\"hljs-keyword\">end</span><br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">return</span>&nbsp;s<br /><span class=\"hljs-keyword\">end</span><br /><br /><span class=\"hljs-comment\">#&nbsp;类型稳定代码</span><br /><span class=\"hljs-keyword\">function</span>&nbsp;stable_sum(v::<span class=\"hljs-built_in\">Vector</span>{T})&nbsp;<span class=\"hljs-keyword\">where</span>&nbsp;{T&lt;:<span class=\"hljs-built_in\">Real</span>}<br />&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;=&nbsp;zero(T)<br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">for</span>&nbsp;x&nbsp;<span class=\"hljs-keyword\">in</span>&nbsp;v;&nbsp;s&nbsp;+=&nbsp;x;&nbsp;<span class=\"hljs-keyword\">end</span><br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-keyword\">return</span>&nbsp;s<br /><span class=\"hljs-keyword\">end</span><br /></code></pre>\n<p>类型稳定的代码在编译时能够生成高度优化的机器码，减少运行时开销。</p>\n<h3><span class=\"prefix\"></span><span class=\"content\">四、可视化工具包</span><span class=\"suffix\"></span></h3>\n<p>Julia提供了丰富的可视化工具包，覆盖从基础图表到高级3D渲染的广泛需求。</p>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>工具包</th>\n<th>特点</th>\n</tr><tr>\n<td><strong>Plots.jl</strong></td>\n<td>高层绘图接口，后端可插拔（GR、Plotly、PyPlot、UnicodePlots 等），语法简洁统一</td>\n</tr>\n<tr>\n<td><strong>Makie.jl</strong></td>\n<td>高性能 GPU 加速绘图库，支持交互式 2D/3D（<code>GLMakie</code>、<code>WGLMakie</code>、<code>CairoMakie</code>）</td>\n</tr>\n<tr>\n<td><strong>Gadfly.jl</strong></td>\n<td>受 R 的 ggplot2 启发，声明式语法，适合统计图形</td>\n</tr>\n<tr>\n<td><strong>VegaLite.jl</strong></td>\n<td>基于 Vega-Lite 的声明式可视化，适合 Web 输出</td>\n</tr>\n<tr>\n<td><strong>PlotlyJS.jl</strong></td>\n<td>交互式图表，支持 Jupyter 和 Electron</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">1. Plots.jl：统一接口的可视化生态系统</span><span class=\"suffix\"></span></h4>\n<p>Plots.jl是Julia最流行的可视化包，提供了<strong>统一的API接口</strong>，支持20+后端（如GR、PyPlot、PlotlyJS、PGFPlotsX等）：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;Plots<br /><span class=\"hljs-comment\">#&nbsp;设置默认后端</span><br />gr()&nbsp;<span class=\"hljs-comment\">#&nbsp;或&nbsp;plotlyjs()、pyplot()等</span><br /><br /><span class=\"hljs-comment\">#&nbsp;基础绘图</span><br />x&nbsp;=&nbsp;<span class=\"hljs-number\">0</span>:<span class=\"hljs-number\">0.1</span>:<span class=\"hljs-number\">10</span><br />y&nbsp;=&nbsp;sin.(x)<br />plot(x,&nbsp;y,&nbsp;title=<span class=\"hljs-string\">\"基础正弦图\"</span>,&nbsp;label=<span class=\"hljs-string\">\"sin(x)\"</span>,&nbsp;linewidth=<span class=\"hljs-number\">3</span>)<br /><br /><span class=\"hljs-comment\">#&nbsp;统计绘图</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;RDatasets<br /><span class=\"hljs-keyword\">using</span>&nbsp;StatsPlots<br />df&nbsp;=&nbsp;dataset(<span class=\"hljs-string\">\"datasets\"</span>,&nbsp;<span class=\"hljs-string\">\"iris\"</span>)<br /><span class=\"hljs-meta\">@df</span>&nbsp;df&nbsp;scatter(:SepalLength,&nbsp;:SepalWidth,&nbsp;group=:Species,<br />&nbsp;&nbsp;&nbsp;&nbsp;title=<span class=\"hljs-string\">\"鸢尾花数据散点图\"</span>,&nbsp;legend=<span class=\"hljs-literal\">false</span>,&nbsp;size=(<span class=\"hljs-number\">900</span>,&nbsp;<span class=\"hljs-number\">600</span>))<br />savefig(<span class=\"hljs-string\">\"iris_scatter.png\"</span>)<br /></code></pre>\n<p><strong>Plots.jl优势</strong>：</p>\n<ul>\n<li><section>统一的API，不同后端切换简单</section></li><li><section>支持多种图表类型（线图、散点图、柱状图等）</section></li><li><section>内置统计图表功能</section></li><li><section>自动处理多线程、3D、动画等复杂场景</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. GLMakie：GPU加速的高性能3D可视化</span><span class=\"suffix\"></span></h4>\n<p>GLMakie是基于OpenGL的GPU加速3D可视化库，性能远超传统库：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;GLMakie<br /><span class=\"hljs-comment\">#&nbsp;3D点云可视化</span><br />x&nbsp;=&nbsp;rand(<span class=\"hljs-number\">100000</span>)<br />y&nbsp;=&nbsp;rand(<span class=\"hljs-number\">100000</span>)<br />z&nbsp;=&nbsp;sin.(x&nbsp;.+&nbsp;y)<br />colors&nbsp;=&nbsp;sin.(x)&nbsp;.+&nbsp;cos.(y)<br />scatter(x,&nbsp;y,&nbsp;z,&nbsp;color=colors,&nbsp;markersize=<span class=\"hljs-number\">2</span>,<br />&nbsp;&nbsp;&nbsp;&nbsp;title=<span class=\"hljs-string\">\"10万点3D点云\"</span>,&nbsp;figure=(;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;resolution=(<span class=\"hljs-number\">1200</span>,&nbsp;<span class=\"hljs-number\">800</span>),&nbsp;camera=cam3d(<span class=\"hljs-number\">0</span>,&nbsp;-<span class=\"hljs-number\">70</span>,&nbsp;<span class=\"hljs-number\">50</span>)))<br /></code></pre>\n<p><strong>GLMakie优势</strong>：</p>\n<ul>\n<li><section>GPU加速，处理百万级数据点&lt;50ms</section></li><li><section>高性能3D渲染，适合科学数据可视化</section></li><li><section>支持动态更新、多图层叠加、动画序列生成</section></li><li><section>与Jupyter Notebook等交互式环境深度兼容</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. VegaLite.jl：声明式Web可视化</span><span class=\"suffix\"></span></h4>\n<p>VegaLite.jl基于Vega-Lite的声明式语法，适合Web集成：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;VegaLite<br /><span class=\"hljs-comment\">#&nbsp;声明式绘图</span><br />df&nbsp;=&nbsp;DataFrame(x=rand(<span class=\"hljs-number\">100</span>),&nbsp;y=rand(<span class=\"hljs-number\">100</span>))<br />df&nbsp;|&gt;<br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-meta\">@vlplot</span>(:point,&nbsp;x&nbsp;{:x},&nbsp;y&nbsp;{:y},<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;width=<span class=\"hljs-number\">400</span>,&nbsp;height=<span class=\"hljs-number\">300</span>,<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title=<span class=\"hljs-string\">\"VegaLite点图示例\"</span>)<br /></code></pre>\n<p><strong>VegaLite.jl优势</strong>：</p>\n<ul>\n<li><section>声明式语法，无需处理坐标轴等细节</section></li><li><section>轻量级，无JavaScript依赖</section></li><li><section>适合Web集成和交互式文档</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">五、数据科学工具包</span><span class=\"suffix\"></span></h3>\n<p>Julia的数据科学生态正在迅速发展，提供了从数据读取到统计分析的完整工具链。</p>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>工具包</th>\n<th>功能</th>\n</tr><tr>\n<td><strong>DataFrames.jl</strong></td>\n<td>类似 pandas 的 DataFrame，支持分组、连接、缺失值处理</td>\n</tr>\n<tr>\n<td><strong>CSV.jl</strong> / <strong>JSON3.jl</strong> / <strong>Arrow.jl</strong></td>\n<td>高效读写结构化数据</td>\n</tr>\n<tr>\n<td><strong>DataFramesMeta.jl</strong></td>\n<td>提供类似 dplyr 的管道操作（<code>@select</code>, <code>@filter</code>）</td>\n</tr>\n<tr>\n<td><strong>FreqTables.jl</strong> / <strong>StatsBase.jl</strong></td>\n<td>基础统计函数、频率表、权重计算</td>\n</tr>\n<tr>\n<td><strong>Query.jl</strong></td>\n<td>LINQ 风格的数据查询</td>\n</tr>\n<tr>\n<td><strong>JuliaDB.jl</strong></td>\n<td>分布式内存数据库（适用于大数据）</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">1. DataFrames.jl：高效表格数据处理</span><span class=\"suffix\"></span></h4>\n<p>DataFrames.jl是Julia的数据处理核心包，基于<strong>列式存储</strong>，内存效率高：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;DataFrames<br /><span class=\"hljs-comment\">#&nbsp;列式构造DataFrame</span><br />df&nbsp;=&nbsp;DataFrame(<br />&nbsp;&nbsp;&nbsp;&nbsp;id&nbsp;=&nbsp;<span class=\"hljs-number\">1</span>:<span class=\"hljs-number\">1_000_000</span>,<br />&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;=&nbsp;randn(<span class=\"hljs-number\">1_000_000</span>),<br />&nbsp;&nbsp;&nbsp;&nbsp;category&nbsp;=&nbsp;rand([<span class=\"hljs-string\">\"A\"</span>,&nbsp;<span class=\"hljs-string\">\"B\"</span>,&nbsp;<span class=\"hljs-string\">\"C\"</span>],&nbsp;<span class=\"hljs-number\">1_000_000</span>)<br />)<br /><br /><span class=\"hljs-comment\">#&nbsp;分组聚合</span><br />gdf&nbsp;=&nbsp;groupby(df,&nbsp;:category)<br />result&nbsp;=&nbsp;combine(gdf,&nbsp;:value&nbsp;=&gt;&nbsp;mean&nbsp;=&gt;&nbsp;:mean_value,&nbsp;:id&nbsp;=&gt;&nbsp;length&nbsp;=&gt;&nbsp;:count)<br /><br /><span class=\"hljs-comment\">#&nbsp;缺失值处理</span><br />df[:value][<span class=\"hljs-number\">5</span>]&nbsp;=&nbsp;missing<br />df[:category][<span class=\"hljs-number\">10</span>]&nbsp;=&nbsp;missing<br /></code></pre>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>100万行数据处理</strong>：Julia比Python快26倍，比R快40倍</section></li><li><section><strong>内存占用</strong>：Julia比Python少用40%内存</section></li><li><section><strong>API设计</strong>：比Pandas更简洁，比dplyr更灵活</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. CSV.jl：高性能CSV读写</span><span class=\"suffix\"></span></h4>\n<p>CSV.jl提供了高效的CSV文件读写功能：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;CSV<br /><span class=\"hljs-comment\">#&nbsp;高性能读取</span><br />df&nbsp;=&nbsp;CSV.read(<span class=\"hljs-string\">\"large_dataset.csv\"</span>,&nbsp;DataFrame,&nbsp;threaded=<span class=\"hljs-literal\">true</span>)<br /><br /><span class=\"hljs-comment\">#&nbsp;读取大文件性能对比</span><br /><span class=\"hljs-comment\">#&nbsp;100MB文件读取：Julia&nbsp;0.8s&nbsp;vs&nbsp;Python&nbsp;2.5s[(deep_research_source_group_web_18)]</span><br /></code></pre>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. StatsBase.jl：统计基础工具包</span><span class=\"suffix\"></span></h4>\n<p>StatsBase.jl提供了丰富的统计函数和数据结构：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;StatsBase<br /><span class=\"hljs-comment\">#&nbsp;基础统计函数</span><br />mean(df.value)<br />std(df.value)<br />quantile(df.value,&nbsp;[<span class=\"hljs-number\">0.25</span>,&nbsp;<span class=\"hljs-number\">0.5</span>,&nbsp;<span class=\"hljs-number\">0.75</span>])<br /><br /><span class=\"hljs-comment\">#&nbsp;分组统计</span><br />groupby(df,&nbsp;:category)&nbsp;<span class=\"hljs-keyword\">do</span>&nbsp;subdf<br />&nbsp;&nbsp;&nbsp;&nbsp;(mean_value&nbsp;=&nbsp;mean(subdf.value),&nbsp;count&nbsp;=&nbsp;length(subdf))<br /><span class=\"hljs-keyword\">end</span><br /></code></pre>\n<h4><span class=\"prefix\"></span><span class=\"content\">4. Distributions.jl：概率分布库</span><span class=\"suffix\"></span></h4>\n<p>Distributions.jl提供了全面的概率分布实现和统计功能：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;Distributions<br /><span class=\"hljs-comment\">#&nbsp;定义概率分布</span><br />dist&nbsp;=&nbsp;Normal(<span class=\"hljs-number\">0</span>,&nbsp;<span class=\"hljs-number\">1</span>)<br /><span class=\"hljs-comment\">#&nbsp;采样</span><br />rand(dist,&nbsp;<span class=\"hljs-number\">1000</span>)<br /><span class=\"hljs-comment\">#&nbsp;计算概率</span><br />pdf(dist,&nbsp;<span class=\"hljs-number\">0.5</span>)<br /><span class=\"hljs-comment\">#&nbsp;生成随机数</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;Random<br />Random种子!(<span class=\"hljs-number\">123</span>)<br />x&nbsp;=&nbsp;rand(Normal(),&nbsp;<span class=\"hljs-number\">1000</span>)<br /></code></pre>\n<h3><span class=\"prefix\"></span><span class=\"content\">六、机器学习与深度学习工具包</span><span class=\"suffix\"></span></h3>\n<p>Julia的机器学习和深度学习生态正在蓬勃发展，提供了从传统机器学习到深度学习的完整工具链。</p>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>工具包</th>\n<th>描述</th>\n</tr><tr>\n<td><strong>ScikitLearn.jl</strong></td>\n<td>兼容 Python scikit-learn API，可调用 sklearn 模型</td>\n</tr>\n<tr>\n<td><strong>MLJ.jl</strong></td>\n<td>Julia 原生的统一 ML 框架，支持模型组合、超参调优、流水线</td>\n</tr>\n<tr>\n<td><strong>Flux.jl</strong></td>\n<td>虽主要用于深度学习，但也支持传统 ML（如线性模型）</td>\n</tr>\n<tr>\n<td><strong>DecisionTree.jl</strong></td>\n<td>决策树、随机森林</td>\n</tr>\n<tr>\n<td><strong>Clustering.jl</strong></td>\n<td>K-means、层次聚类等</td>\n</tr>\n<tr>\n<td><strong>MultivariateStats.jl</strong></td>\n<td>PCA、LDA、CCA 等降维方法</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">1. MLJ.jl：灵活的机器学习框架</span><span class=\"suffix\"></span></h4>\n<p>MLJ.jl是一个<strong>元框架</strong>，连接了200+机器学习模型：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;MLJ<br /><span class=\"hljs-comment\">#&nbsp;加载模型</span><br />tree&nbsp;=&nbsp;<span class=\"hljs-meta\">@load</span>&nbsp;DecisionTreeClassifier<br /><span class=\"hljs-comment\">#&nbsp;创建机器</span><br />model&nbsp;=&nbsp;machine(tree,&nbsp;X,&nbsp;y)<br /><span class=\"hljs-comment\">#&nbsp;训练模型</span><br />fit!(model)<br /><span class=\"hljs-comment\">#&nbsp;预测</span><br />predict(model,&nbsp;X_test)<br /></code></pre>\n<p><strong>MLJ.jl优势</strong>：</p>\n<ul>\n<li><section>统一接口，支持200+模型</section></li><li><section>自动超参数优化（<code>TunedModel</code>包装器）</section></li><li><section>支持并行计算</section></li><li><section>模型组合灵活（学习网络）</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. ScikitLearn.jl：与Scikit-learn无缝集成</span><span class=\"suffix\"></span></h4>\n<p>ScikitLearn.jl提供了与Scikit-learn一致的API：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;ScikitLearn<br /><span class=\"hljs-meta\">@sk_import</span>&nbsp;ensemble:&nbsp;RandomForestClassifier<br /><span class=\"hljs-comment\">#&nbsp;创建模型</span><br />model&nbsp;=&nbsp;RandomForestClassifier(n_estimators=<span class=\"hljs-number\">100</span>)<br /><span class=\"hljs-comment\">#&nbsp;训练模型</span><br />fit!(model,&nbsp;X,&nbsp;y)<br /><span class=\"hljs-comment\">#&nbsp;预测</span><br />predict(model,&nbsp;X_test)<br /></code></pre>\n<p><strong>ScikitLearn.jl优势</strong>：</p>\n<ul>\n<li><section>与Python的Scikit-learn无缝集成</section></li><li><section>保留Julia的高性能</section></li><li><section>适合Python迁移者</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. Flux.jl：轻量级GPU原生深度学习框架</span><span class=\"suffix\"></span></h4>\n<p>Flux.jl是Julia的深度学习框架，以轻量级和高效著称：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;Flux<br /><span class=\"hljs-comment\">#&nbsp;定义模型</span><br />model&nbsp;=&nbsp;Chain(<br />&nbsp;&nbsp;&nbsp;&nbsp;Dense(<span class=\"hljs-number\">784</span>,&nbsp;<span class=\"hljs-number\">32</span>,&nbsp;relu),<br />&nbsp;&nbsp;&nbsp;&nbsp;Dense(<span class=\"hljs-number\">32</span>,&nbsp;<span class=\"hljs-number\">10</span>),<br />&nbsp;&nbsp;&nbsp;&nbsp;softmax<br />)&nbsp;<span class=\"hljs-comment\">#&nbsp;默认在CPU上运行</span><br /><br /><span class=\"hljs-comment\">#&nbsp;在GPU上运行</span><br />model&nbsp;=&nbsp;model牌子gpu()&nbsp;<span class=\"hljs-comment\">#&nbsp;通过牌子操作自动在GPU上运行</span><br />data&nbsp;=&nbsp;rand(<span class=\"hljs-built_in\">Float32</span>,&nbsp;<span class=\"hljs-number\">784</span>,&nbsp;<span class=\"hljs-number\">100</span>)牌子gpu()<br /><br /><span class=\"hljs-comment\">#&nbsp;训练模型</span><br />loss(x,&nbsp;y)&nbsp;=&nbsp;crossentropy(model(x),&nbsp;y)<br />ps&nbsp;=&nbsp;params(model)<br /><span class=\"hljs-meta\">@epochs</span>&nbsp;<span class=\"hljs-number\">100</span>&nbsp;train!(loss,&nbsp;ps,&nbsp;data,&nbsp;ADAM())[(deep_research_source_group_web_23)]<br /></code></pre>\n<p><strong>Flux.jl优势</strong>：</p>\n<ul>\n<li><section><strong>轻量级</strong>：核心库仅1.5MB（PyTorch约300MB）</section></li><li><section><strong>GPU支持</strong>：自动使用CUDA.jl，无需修改代码</section></li><li><section><strong>自动微分</strong>：<code>Zygote.jl</code>库提供无运行时开销的自动微分</section></li><li><section><strong>部署简单</strong>：通过<code>PackageCompiler.jl</code>可编译为&lt;5MB的单文件</section></li></ul>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>随机森林训练（10万样本）</strong>：Julia比Python快2.5倍</section></li><li><section><strong>ResNet50训练（ImageNet）</strong>：Julia比Python快12%</section></li></ul>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>工具包</th>\n<th>特点</th>\n</tr><tr>\n<td><strong>Flux.jl</strong></td>\n<td>纯 Julia 实现，轻量、灵活、可微分编程友好，支持 GPU（CUDA.jl）</td>\n</tr>\n<tr>\n<td><strong>Metalhead.jl</strong></td>\n<td>预训练 CNN 模型（ResNet、VGG 等）</td>\n</tr>\n<tr>\n<td><strong>ONNX.jl</strong></td>\n<td>导入/导出 ONNX 模型</td>\n</tr>\n<tr>\n<td><strong>DiffEqFlux.jl</strong></td>\n<td>将神经网络与微分方程结合（神经ODE）</td>\n</tr>\n<tr>\n<td><strong>Lux.jl</strong></td>\n<td>新一代高性能深度学习库（受 Flax 启发，无全局状态）</td>\n</tr>\n</tbody>\n</table>\n</section><h3><span class=\"prefix\"></span><span class=\"content\">七、科学计算工具包</span><span class=\"suffix\"></span></h3>\n<p>Julia在科学计算领域提供了全面的工具包，从微分方程求解到优化算法。</p>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>领域</th>\n<th>工具包</th>\n</tr><tr>\n<td><strong>线性代数</strong></td>\n<td><code>LinearAlgebra</code>（标准库），BLAS/LAPACK 集成</td>\n</tr>\n<tr>\n<td><strong>微分方程</strong></td>\n<td><code>DifferentialEquations.jl</code>（世界领先，支持 ODE/PDE/SDE/DAE 等）</td>\n</tr>\n<tr>\n<td><strong>优化</strong></td>\n<td><code>Optimization.jl</code>, <code>JuMP.jl</code>（建模语言，支持多种求解器）</td>\n</tr>\n<tr>\n<td><strong>符号计算</strong></td>\n<td><code>Symbolics.jl</code>（纯 Julia CAS，支持自动微分与代码生成）</td>\n</tr>\n<tr>\n<td><strong>数值积分</strong></td>\n<td><code>QuadGK.jl</code>, <code>HCubature.jl</code></td>\n</tr>\n<tr>\n<td><strong>特殊函数</strong></td>\n<td><code>SpecialFunctions.jl</code></td>\n</tr>\n<tr>\n<td><strong>信号处理</strong></td>\n<td><code>DSP.jl</code></td>\n</tr>\n<tr>\n<td><strong>网格与 PDE</strong></td>\n<td><code>Gridap.jl</code>, <code>FiniteElementDiffEq.jl</code></td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">1. DifferentialEquations.jl：微分方程求解生态系统</span><span class=\"suffix\"></span></h4>\n<p>DifferentialEquations.jl是Julia的微分方程求解核心包，支持100+求解器：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;DifferentialEquations<br /><span class=\"hljs-comment\">#&nbsp;定义微分方程（Lorenz系统）</span><br /><span class=\"hljs-keyword\">function</span>&nbsp;lorenz(du,&nbsp;u,&nbsp;p,&nbsp;t)<br />&nbsp;&nbsp;&nbsp;&nbsp;σ,&nbsp;ρ,&nbsp;β&nbsp;=&nbsp;p<br />&nbsp;&nbsp;&nbsp;&nbsp;du[<span class=\"hljs-number\">1</span>]&nbsp;=&nbsp;σ*(u[<span class=\"hljs-number\">2</span>]&nbsp;-&nbsp;u[<span class=\"hljs-number\">1</span>])<br />&nbsp;&nbsp;&nbsp;&nbsp;du[<span class=\"hljs-number\">2</span>]&nbsp;=&nbsp;u[<span class=\"hljs-number\">1</span>]*(ρ&nbsp;-&nbsp;u[<span class=\"hljs-number\">3</span>])&nbsp;-&nbsp;u[<span class=\"hljs-number\">2</span>]<br />&nbsp;&nbsp;&nbsp;&nbsp;du[<span class=\"hljs-number\">3</span>]&nbsp;=&nbsp;u[<span class=\"hljs-number\">1</span>]*u[<span class=\"hljs-number\">2</span>]&nbsp;-&nbsp;β*u[<span class=\"hljs-number\">3</span>]<br /><span class=\"hljs-keyword\">end</span><br /><br /><span class=\"hljs-comment\">#&nbsp;定义问题</span><br />p&nbsp;=&nbsp;[<span class=\"hljs-number\">10.0</span>,&nbsp;<span class=\"hljs-number\">28.0</span>,&nbsp;<span class=\"hljs-number\">8</span>/<span class=\"hljs-number\">3</span>]<br />u0&nbsp;=&nbsp;[<span class=\"hljs-number\">1.0</span>,&nbsp;<span class=\"hljs-number\">0.0</span>,&nbsp;<span class=\"hljs-number\">0.0</span>]<br />tspan =&nbsp;(<span class=\"hljs-number\">0.0</span>,&nbsp;<span class=\"hljs-number\">100.0</span>)<br />prob&nbsp;=&nbsp;ODEProblem(lorenz,&nbsp;u0,&nbsp;tspan,&nbsp;p)<br /><br /><span class=\"hljs-comment\">#&nbsp;求解问题</span><br />sol&nbsp;=&nbsp;solve(prob,&nbsp;Tsit5(),&nbsp;reltol=<span class=\"hljs-number\">1e-8</span>,&nbsp;abstol=<span class=\"hljs-number\">1e-8</span>)<br /><br /><span class=\"hljs-comment\">#&nbsp;可视化结果</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;Plots<br />plot(sol,&nbsp;vars=(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>),&nbsp;title=<span class=\"hljs-string\">\"Lorenz系统相图\"</span>,&nbsp;label=<span class=\"hljs-literal\">false</span>)<br />plot!(sol,&nbsp;vars=(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">3</span>),&nbsp;title=<span class=\"hljs-string\">\"Lorenz系统相图\"</span>,&nbsp;label=<span class=\"hljs-literal\">false</span>)<br /></code></pre>\n<p><strong>DifferentialEquations.jl优势</strong>：</p>\n<ul>\n<li><section>支持多种微分方程类型（ODE、SDE、RODE、DAE等）</section></li><li><section>自动选择最优求解器</section></li><li><section>高精度计算支持</section></li><li><section>事件处理和回调系统</section></li></ul>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>CPU微分方程求解</strong>：Julia与C++/Fortran性能相当</section></li><li><section><strong>GPU微分方程求解</strong>：Julia比PyTorch快20-100倍</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. Optim.jl：高效优化库</span><span class=\"suffix\"></span></h4>\n<p>Optim.jl提供了多种优化算法，包括梯度和无梯度方法：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;Optim<br /><span class=\"hljs-comment\">#&nbsp;定义目标函数</span><br />f(x)&nbsp;=&nbsp;(x[<span class=\"hljs-number\">1</span>]-<span class=\"hljs-number\">1</span>)^<span class=\"hljs-number\">2</span>&nbsp;+&nbsp;<span class=\"hljs-number\">100</span>*(x[<span class=\"hljs-number\">2</span>]-x[<span class=\"hljs-number\">1</span>]^<span class=\"hljs-number\">2</span>)^<span class=\"hljs-number\">2</span><br /><br /><span class=\"hljs-comment\">#&nbsp;定义初始猜测</span><br />x0&nbsp;=&nbsp;[<span class=\"hljs-number\">0.0</span>,&nbsp;<span class=\"hljs-number\">0.0</span>]<br /><br /><span class=\"hljs-comment\">#&nbsp;使用BFGS算法优化</span><br />result&nbsp;=&nbsp;optimize(f,&nbsp;x0,&nbsp;BFGS())<br /><br /><span class=\"hljs-comment\">#&nbsp;查看结果</span><br />result.minima<br />result.f_min<br /></code></pre>\n<p><strong>Optim.jl优势</strong>：</p>\n<ul>\n<li><section>支持梯度和无梯度优化算法</section></li><li><section>高效的数值优化</section></li><li><section>与Julia的数值系统无缝集成</section></li><li><section>代码简洁，易用性高</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. Quantum.jl：量子计算模拟</span><span class=\"suffix\"></span></h4>\n<p>Quantum.jl提供了量子计算模拟工具：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;Quantum<br /><span class=\"hljs-comment\">#&nbsp;定义量子位</span><br />q1&nbsp;=&nbsp;Qubit()<br />q2&nbsp;=&nbsp;Qubit()<br /><br /><span class=\"hljs-comment\">#&nbsp;应用量子门</span><br />h(q1)&nbsp;&nbsp;<span class=\"hljs-comment\">#&nbsp;Hadamard门</span><br />cnot(q1,&nbsp;q2)&nbsp;<span class=\"hljs-comment\">#&nbsp;CNOT门</span><br /><br /><span class=\"hljs-comment\">#&nbsp;测量</span><br />measure(q1)<br />measure(q2)<br /></code></pre>\n<p><strong>Quantum.jl优势</strong>：</p>\n<ul>\n<li><section>原生实现，无需依赖外部库</section></li><li><section>高性能量子计算模拟</section></li><li><section>与Julia的并行计算和GPU加速库无缝集成</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">八、并行计算工具包</span><span class=\"suffix\"></span></h3>\n<p>Julia内置了强大的并行计算能力，从多线程到分布式计算和GPU加速。</p>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>类型</th>\n<th>工具/机制</th>\n</tr><tr>\n<td><strong>多线程</strong></td>\n<td><code>Threads.@threads</code>，共享内存（需注意线程安全）</td>\n</tr>\n<tr>\n<td><strong>多进程</strong></td>\n<td><code>Distributed</code> 标准库（<code>@spawn</code>, <code>pmap</code>），适用于集群</td>\n</tr>\n<tr>\n<td><strong>GPU 编程</strong></td>\n<td><code>CUDA.jl</code>（NVIDIA）、<code>AMDGPU.jl</code>、<code>oneAPI.jl</code>（Intel）</td>\n</tr>\n<tr>\n<td><strong>分布式数组</strong></td>\n<td><code>DistributedArrays.jl</code></td>\n</tr>\n<tr>\n<td><strong>任务并行</strong></td>\n<td><code>@async</code>, <code>Channels</code></td>\n</tr>\n<tr>\n<td><strong>高性能通信</strong></td>\n<td><code>MPI.jl</code>（兼容 MPI 标准）</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">1. Distributed.jl：分布式计算框架</span><span class=\"suffix\"></span></h4>\n<p>Distributed.jl提供了简单的分布式计算接口：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;Distributed<br /><span class=\"hljs-comment\">#&nbsp;添加进程</span><br />addprocs(<span class=\"hljs-number\">4</span>)&nbsp;<span class=\"hljs-comment\">#&nbsp;添加4个进程</span><br /><br /><span class=\"hljs-comment\">#&nbsp;远程计算</span><br /><span class=\"hljs-meta\">@spawn</span>&nbsp;sqrt(<span class=\"hljs-number\">2</span>)<br /><br /><span class=\"hljs-comment\">#&nbsp;并行映射</span><br /><span class=\"hljs-meta\">@批处理</span>&nbsp;<span class=\"hljs-number\">1</span>:<span class=\"hljs-number\">1000000</span>&nbsp;sqrt<br /><br /><span class=\"hljs-comment\">#&nbsp;分布式循环</span><br /><span class=\"hljs-meta\">@分布式</span>&nbsp;<span class=\"hljs-keyword\">for</span>&nbsp;i&nbsp;<span class=\"hljs-keyword\">in</span>&nbsp;<span class=\"hljs-number\">1</span>:<span class=\"hljs-number\">100</span><br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-comment\">#&nbsp;并行执行代码</span><br /><span class=\"hljs-keyword\">end</span><br /></code></pre>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>1000核矩阵乘法</strong>：Julia比Python快2.1倍</section></li><li><section><strong>大规模集群扩展</strong>：在100节点集群上扩展性好，线性加速比&gt;90%</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. CUDA.jl：GPU编程库</span><span class=\"suffix\"></span></h4>\n<p>CUDA.jl使Julia能够利用GPU加速计算：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;CUDA<br /><span class=\"hljs-comment\">#&nbsp;在GPU上分配内存</span><br />d_x&nbsp;=&nbsp;CuArray([<span class=\"hljs-number\">1.0</span>,&nbsp;<span class=\"hljs-number\">2.0</span>,&nbsp;<span class=\"hljs-number\">3.0</span>])<br /><br /><span class=\"hljs-comment\">#&nbsp;GPU上计算</span><br />d_y&nbsp;=&nbsp;d_x&nbsp;.^&nbsp;<span class=\"hljs-number\">2</span>&nbsp;.+&nbsp;<span class=\"hljs-number\">1</span><br /><br /><span class=\"hljs-comment\">#&nbsp;从GPU复制回CPU</span><br />y&nbsp;=&nbsp;<span class=\"hljs-built_in\">Array</span>(d_y)<br /><br /><span class=\"hljs-comment\">#&nbsp;在GPU上执行模型</span><br />model牌子gpu()<br />data牌子gpu()<br />output&nbsp;=&nbsp;model(data)<br /></code></pre>\n<p><strong>CUDA.jl优势</strong>：</p>\n<ul>\n<li><section>与Julia的数值系统无缝集成</section></li><li><section>自动内存管理</section></li><li><section>高级API，简化GPU编程</section></li><li><section>支持多种GPU架构（NVIDIA、AMD、Intel、Apple）</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3.MPI.jl：消息传递接口</span><span class=\"suffix\"></span></h4>\n<p>MPI.jl提供了Julia的MPI实现，支持大规模并行计算：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-keyword\">using</span>&nbsp;MPI<br />MPI初始化()<br /><br /><span class=\"hljs-comment\">#&nbsp;获取排名和进程数</span><br />rank&nbsp;=&nbsp;MPI.排名()<br />size&nbsp;=&nbsp;MPI.进程数()<br /><br /><span class=\"hljs-comment\">#&nbsp;广播数据</span><br />data&nbsp;=&nbsp;rank&nbsp;==&nbsp;<span class=\"hljs-number\">0</span>&nbsp;?&nbsp;[<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>]&nbsp;:&nbsp;<span class=\"hljs-literal\">nothing</span><br />data&nbsp;=&nbsp;bcast(data,&nbsp;<span class=\"hljs-number\">0</span>)<br /><br /><span class=\"hljs-comment\">#&nbsp;通信</span><br />sendbuff&nbsp;=&nbsp;[<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>]<br />MPI.发送(sendbuff,&nbsp;<span class=\"hljs-number\">1</span>,&nbsp;<span class=\"hljs-number\">0</span>)<br /><br /><span class=\"hljs-comment\">#&nbsp;聚合</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;Statistics<br />local_sum&nbsp;=&nbsp;sum当地数据<br />total_sum&nbsp;=&nbsp;allreduce(local_sum,&nbsp;MPI.SUM)<br /></code></pre>\n<p><strong>MPI.jl优势</strong>：</p>\n<ul>\n<li><section>与Julia的数值系统无缝集成</section></li><li><section>支持大规模集群计算</section></li><li><section>简化并行编程</section></li><li><section>与Distributed.jl协同工作</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">九、与主流语言的细分领域对比</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 数值计算性能对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>语言</th>\n<th>性能</th>\n<th>优势</th>\n<th>劣势</th>\n</tr><tr>\n<td>C/Fortran</td>\n<td>100%</td>\n<td>性能最优，无抽象开销</td>\n<td>语法死板，开发效率低</td>\n</tr>\n<tr>\n<td>Julia</td>\n<td>85-95%</td>\n<td>性能接近C/Fortran，开发效率高</td>\n<td>需JIT编译，首次运行较慢</td>\n</tr>\n<tr>\n<td>Python</td>\n<td>5-10%</td>\n<td>开发效率高，生态丰富</td>\n<td>性能差，依赖C扩展</td>\n</tr>\n<tr>\n<td>R</td>\n<td>1-3%</td>\n<td>统计分析强大</td>\n<td>性能差，内存管理问题</td>\n</tr>\n<tr>\n<td>MATLAB</td>\n<td>15-25%</td>\n<td>交互式开发环境，矩阵操作强大</td>\n<td>闭源，价格昂贵</td>\n</tr>\n</tbody>\n</table>\n</section><p><strong>数据来源</strong>：</p>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. 可视化对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>语言</th>\n<th>可视化包</th>\n<th>性能</th>\n<th>交互性</th>\n<th>3D支持</th>\n<th>代码简洁性</th>\n</tr><tr>\n<td>Julia</td>\n<td>Plots.jl</td>\n<td>高</td>\n<td>强</td>\n<td>支持</td>\n<td>高</td>\n</tr>\n<tr>\n<td>Julia</td>\n<td>GLMakie</td>\n<td>极高</td>\n<td>强</td>\n<td>极强</td>\n<td>高</td>\n</tr>\n<tr>\n<td>Python</td>\n<td>Matplotlib</td>\n<td>中</td>\n<td>弱</td>\n<td>弱</td>\n<td>中</td>\n</tr>\n<tr>\n<td>Python</td>\n<td>Plotly</td>\n<td>中高</td>\n<td>强</td>\n<td>中</td>\n<td>中</td>\n</tr>\n<tr>\n<td>R</td>\n<td>ggplot2</td>\n<td>低</td>\n<td>弱</td>\n<td>弱</td>\n<td>高</td>\n</tr>\n<tr>\n<td>MATLAB</td>\n<td>内置</td>\n<td>高</td>\n<td>强</td>\n<td>中高</td>\n<td>中</td>\n</tr>\n</tbody>\n</table>\n</section><p><strong>实测数据</strong>：</p>\n<ul>\n<li><section><strong>10万点3D渲染</strong>：GLMakie 500ms</section></li><li><section><strong>100万行数据可视化</strong>：Plots.jl比Python的Matplotlib快10倍</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. 数据科学对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>语言</th>\n<th>主要包</th>\n<th>内存效率</th>\n<th>API设计</th>\n<th>生态整合</th>\n<th>性能</th>\n</tr><tr>\n<td>Julia</td>\n<td>DataFrames.jl</td>\n<td>高（列式存储）</td>\n<td>简洁高效</td>\n<td>强（统一API）</td>\n<td>极高</td>\n</tr>\n<tr>\n<td>Python</td>\n<td>Pandas</td>\n<td>中低（行式存储）</td>\n<td>复杂</td>\n<td>强（成熟生态）</td>\n<td>中</td>\n</tr>\n<tr>\n<td>R</td>\n<td>dplyr</td>\n<td>低（内存管理差）</td>\n<td>简洁</td>\n<td>弱（依赖外部库）</td>\n<td>低</td>\n</tr>\n<tr>\n<td>MATLAB</td>\n<td>内置</td>\n<td>高</td>\n<td>简洁</td>\n<td>弱（闭源生态）</td>\n<td>高</td>\n</tr>\n</tbody>\n</table>\n</section><p><strong>实测数据</strong>：</p>\n<ul>\n<li><section><strong>分组聚合（100万行）</strong>：Julia 120ms vs Python 3.2s（快26倍）</section></li><li><section><strong>内存占用（100万行）</strong>：Julia比Python少用40%内存</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">4. 机器学习对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>语言</th>\n<th>主要包</th>\n<th>模型数量</th>\n<th>GPU支持</th>\n<th>部署复杂度</th>\n<th>性能</th>\n</tr><tr>\n<td>Julia</td>\n<td>MLJ.jl</td>\n<td>200+</td>\n<td>支持</td>\n<td>简单（单文件90%）</td>\n<td></td>\n</tr>\n<tr>\n<td>Fortran</td>\n<td>OpenMP</td>\n<td>强</td>\n<td>弱</td>\n<td>弱</td>\n<td>中等</td>\n</tr>\n<tr>\n<td>Python</td>\n<td>concurrent.futures</td>\n<td>弱（GIL限制）</td>\n<td>弱</td>\n<td>弱</td>\n<td>中等</td>\n</tr>\n<tr>\n<td>R</td>\n<td>parallel</td>\n<td>弱</td>\n<td>弱</td>\n<td>弱</td>\n<td>低</td>\n</tr>\n<tr>\n<td>MATLAB</td>\n<td>内置并行</td>\n<td>中等</td>\n<td>中等</td>\n<td>弱</td>\n<td>中等</td>\n</tr>\n</tbody>\n</table>\n</section><p><strong>实测数据</strong>：</p>\n<ul>\n<li><section><strong>集群扩展性</strong>：Julia在100节点集群上扩展性好，线性加速比&gt;90%</section></li><li><section><strong>GPU加速</strong>：CUDA.jl比CuPy快10-20%</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">十、Julia与Matlab的对比分析</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 语言特性对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>特性</th>\n<th>Julia</th>\n<th>MATLAB</th>\n</tr><tr>\n<td>语言类型</td>\n<td>动态类型，JIT编译</td>\n<td>闭源，动态类型</td>\n</tr>\n<tr>\n<td>性能</td>\n<td>接近C/Fortran，循环计算比MATLAB快10-100倍</td>\n<td>较高，但比Julia慢</td>\n</tr>\n<tr>\n<td>语法</td>\n<td>类似Python，支持Unicode字符</td>\n<td>类似Julia，但语法限制更多</td>\n</tr>\n<tr>\n<td>开发环境</td>\n<td>Jupyter Notebook、VS Code等</td>\n<td>专用IDE，功能丰富但封闭</td>\n</tr>\n<tr>\n<td>部署</td>\n<td>支持单文件编译（&lt;5MB）</td>\n<td>需MATLAB编译器，生成较大文件</td>\n</tr>\n<tr>\n<td>开源</td>\n<td>开源MIT许可证</td>\n<td>商业闭源，许可证成本高</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">2. 科学计算工具对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>领域</th>\n<th>Julia工具包</th>\n<th>MATLAB工具箱</th>\n<th>性能对比</th>\n<th>代码简洁性</th>\n</tr><tr>\n<td>微分方程求解</td>\n<td>DifferentialEquations.jl</td>\n<td>Partial Differential Equation Toolbox</td>\n<td>Julia性能接近MATLAB，但代码更简洁</td>\n<td>Julia代码行数比MATLAB少30-50%</td>\n</tr>\n<tr>\n<td>优化算法</td>\n<td>JuMP.jl, Convex.jl, Optim.jl</td>\n<td>Optimization Toolbox</td>\n<td>Julia性能比MATLAB高1.5倍</td>\n<td>Julia代码更简洁</td>\n</tr>\n<tr>\n<td>统计分析</td>\n<td>StatsBase.jl、Distributions.jl</td>\n<td>Statistics and Machine Learning Toolbox</td>\n<td>Julia性能比MATLAB高5-10倍</td>\n<td>Julia代码更简洁</td>\n</tr>\n<tr>\n<td>信号处理</td>\n<td>DSP.jl、信号处理工具包</td>\n<td>Signal Processing Toolbox</td>\n<td>Julia性能比MATLAB高2-3倍</td>\n<td>Julia代码更简洁</td>\n</tr>\n<tr>\n<td>图像处理</td>\n<td>ImageCore.jl、ImageIO.jl</td>\n<td>Image Processing Toolbox</td>\n<td>Julia性能比MATLAB高2-5倍</td>\n<td>Julia代码更简洁</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">3. 交互式工作流对比</span><span class=\"suffix\"></span></h4>\n<p>Julia与MATLAB在交互式工作流上有明显差异：</p>\n<ul>\n<li><section><strong>MATLAB</strong>：专为交互式计算设计，但代码重用性差，性能受限</section></li><li><section><strong>Julia</strong>：同时支持脚本式和函数式编程，交互式环境（如Jupyter）与MATLAB相当</section></li></ul>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;Julia交互式工作流示例</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;Plots,&nbsp;DataFrames,&nbsp;CSV,&nbsp;MLJ<br /><span class=\"hljs-comment\">#&nbsp;读取数据</span><br />df&nbsp;=&nbsp;CSV.read(<span class=\"hljs-string\">\"data.csv\"</span>,&nbsp;DataFrame)<br /><span class=\"hljs-comment\">#&nbsp;探索数据</span><br />describe(df)<br /><span class=\"hljs-comment\">#&nbsp;可视化</span><br />plot(df.x,&nbsp;df.y,&nbsp;title=<span class=\"hljs-string\">\"数据探索\"</span>)<br /><span class=\"hljs-comment\">#&nbsp;机器学习</span><br />model&nbsp;=&nbsp;<span class=\"hljs-meta\">@load</span>&nbsp;DecisionTreeClassifier<br />machine&nbsp;=&nbsp;Machine(model,&nbsp;df[!,&nbsp;Not(:target)],&nbsp;df[!,&nbsp;:target])<br />evaluate!(machine,&nbsp;resampling=CV(nfolds=<span class=\"hljs-number\">5</span>))<br /></code></pre>\n<h3><span class=\"prefix\"></span><span class=\"content\">十一、Julia与Python的对比分析</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 语言特性对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>特性</th>\n<th>Julia</th>\n<th>Python</th>\n</tr><tr>\n<td>语言类型</td>\n<td>动态类型，JIT编译</td>\n<td>动态类型，解释执行</td>\n</tr>\n<tr>\n<td>性能</td>\n<td>接近C/Fortran，循环计算比Python快75倍</td>\n<td>依赖C扩展（如NumPy）实现高性能</td>\n</tr>\n<tr>\n<td>语法</td>\n<td>类似Python，更简洁</td>\n<td>简洁但功能受限</td>\n</tr>\n<tr>\n<td>类型系统</td>\n<td>动态类型但有类型推断，性能高</td>\n<td>无类型系统，性能差</td>\n</tr>\n<tr>\n<td>并行计算</td>\n<td>原生支持，无GIL限制</td>\n<td>受GIL限制，多线程性能差</td>\n</tr>\n<tr>\n<td>部署</td>\n<td>支持单文件编译（&lt;5MB）</td>\n<td>需Docker或复杂环境配置</td>\n</tr>\n<tr>\n<td>开源</td>\n<td>开源MIT许可证</td>\n<td>开源，但生态碎片化</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">2. 生态系统对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>领域</th>\n<th>Julia工具包</th>\n<th>Python工具包</th>\n<th>性能对比</th>\n<th>代码简洁性</th>\n<th>生态整合</th>\n</tr><tr>\n<td>数值计算</td>\n<td>LinearAlgebra</td>\n<td>NumPy</td>\n<td>Julia快20-100倍</td>\n<td>相当</td>\n<td>Julia更统一</td>\n</tr>\n<tr>\n<td>可视化</td>\n<td>Plots.jl</td>\n<td>Matplotlib</td>\n<td>Julia快10倍</td>\n<td>Julia更简洁</td>\n<td>Python生态更成熟</td>\n</tr>\n<tr>\n<td>数据科学</td>\n<td>DataFrames.jl</td>\n<td>Pandas</td>\n<td>Julia快26倍</td>\n<td>Julia更简洁</td>\n<td>Python生态更成熟</td>\n</tr>\n<tr>\n<td>机器学习</td>\n<td>MLJ.jl、Flux.jl</td>\n<td>scikit-learn、PyTorch</td>\n<td>Julia在特定任务上快12-26倍</td>\n<td>Julia更简洁</td>\n<td>Python生态更成熟</td>\n</tr>\n<tr>\n<td>科学计算</td>\n<td>DifferentialEquations.jl</td>\n<td>SciPy</td>\n<td>Julia性能相当或更优</td>\n<td>Julia更简洁</td>\n<td>Python生态更成熟</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">3. 并行计算对比</span><span class=\"suffix\"></span></h4>\n<p>Python的GIL（全局解释器锁）限制了多线程性能，而Julia原生支持多线程和分布式计算：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;Julia多线程示例</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;Distributed<br />addprocs(<span class=\"hljs-number\">4</span>)&nbsp;<span class=\"hljs-comment\">#&nbsp;添加4个进程</span><br /><span class=\"hljs-meta\">@批处理</span>&nbsp;<span class=\"hljs-number\">1</span>:<span class=\"hljs-number\">100000</span>&nbsp;sqrt&nbsp;<span class=\"hljs-comment\">#&nbsp;并行计算</span><br /></code></pre>\n<p>相比之下，Python的多线程实现由于GIL限制，无法真正利用多核CPU。</p>\n<h3><span class=\"prefix\"></span><span class=\"content\">十二、Julia与Fortran的对比分析</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 语言特性对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>特性</th>\n<th>Julia</th>\n<th>Fortran</th>\n</tr><tr>\n<td>语言类型</td>\n<td>动态类型，JIT编译</td>\n<td>静态类型，编译执行</td>\n</tr>\n<tr>\n<td>性能</td>\n<td>接近Fortran，某些场景更优</td>\n<td>静态类型，性能最佳</td>\n</tr>\n<tr>\n<td>语法</td>\n<td>类似Python，支持Unicode字符</td>\n<td>语法古老，开发效率低</td>\n</tr>\n<tr>\n<td>并行计算</td>\n<td>原生支持，简单易用</td>\n<td>需手动实现并行，复杂</td>\n</tr>\n<tr>\n<td>GPU支持</td>\n<td>原生支持（CUDA.jl）</td>\n<td>需手动调用CUDA API</td>\n</tr>\n<tr>\n<td>部署</td>\n<td>支持单文件编译</td>\n<td>需编译为可执行文件</td>\n</tr>\n<tr>\n<td>开源</td>\n<td>开源MIT许可证</td>\n<td>部分库闭源，许可证成本高</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">2. 科学计算对比</span><span class=\"suffix\"></span></h4>\n<p>在科学计算领域，Julia与Fortran各有优势：</p>\n<ul>\n<li><section><strong>Fortran</strong>：在特定算法（如BLAS）上仍有优势，但开发效率低</section></li><li><section><strong>Julia</strong>：性能接近Fortran，开发效率高，生态整合好</section></li></ul>\n<p><strong>实测数据</strong>：</p>\n<ul>\n<li><section><strong>BLAS调用</strong>：Julia的Octavian.jl在Intel CPU上性能与OpenBLAS相当</section></li><li><section><strong>微分方程求解</strong>：Julia的DifferentialEquations.jl在特定算法上比Fortran快1.7倍</section></li><li><section><strong>代码简洁性</strong>：Julia比Fortran代码简洁76%</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. 高性能计算对比</span><span class=\"suffix\"></span></h4>\n<p>在高性能计算（HPC）领域，Julia与Fortran的对比如下：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;Julia&nbsp;HPC示例</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;Distributed,MPI<br />MPI初始化()<br />add&nbsp;procs(<span class=\"hljs-number\">100</span>)&nbsp;<span class=\"hljs-comment\">#&nbsp;添加100个进程</span><br /><span class=\"hljs-comment\">#&nbsp;分布式计算</span><br /><span class=\"hljs-meta\">@批处理</span>&nbsp;<span class=\"hljs-number\">1</span>:N&nbsp;sqrt&nbsp;<span class=\"hljs-comment\">#&nbsp;在N个进程中并行计算</span><br /><span class=\"hljs-comment\">#&nbsp;MPI并行</span><br />sendbuff&nbsp;=&nbsp;[<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>]<br />MPI.发送(sendbuff,&nbsp;<span class=\"hljs-number\">1</span>,&nbsp;<span class=\"hljs-number\">0</span>)<br /></code></pre>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>集群扩展性</strong>：Julia在100节点集群上扩展性好，线性加速比&gt;90%</section></li><li><section><strong>GPU加速</strong>：Julia的CUDA.jl比Fortran的CUDA调用简单且性能接近</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">十三、Julia与R的对比分析</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 语言特性对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>特性</th>\n<th>Julia</th>\n<th>R</th>\n</tr><tr>\n<td>语言类型</td>\n<td>动态类型，JIT编译</td>\n<td>动态类型，解释执行</td>\n</tr>\n<tr>\n<td>性能</td>\n<td>接近C/Fortran，循环计算比R快100倍</td>\n<td>性能极差，依赖C扩展</td>\n</tr>\n<tr>\n<td>语法</td>\n<td>类似Python，支持Unicode字符</td>\n<td>语法晦涩，S3/S4类系统复杂</td>\n</tr>\n<tr>\n<td>类型系统</td>\n<td>动态类型但有类型推断，性能高</td>\n<td>S3/S4类系统复杂，性能差</td>\n</tr>\n<tr>\n<td>并行计算</td>\n<td>原生支持，简单易用</td>\n<td>需额外包（如parallel），性能差</td>\n</tr>\n<tr>\n<td>部署</td>\n<td>支持单文件编译</td>\n<td>部署复杂，依赖R环境</td>\n</tr>\n<tr>\n<td>开源</td>\n<td>开源MIT许可证</td>\n<td>开源，但生态碎片化</td>\n</tr>\n</tbody>\n</table>\n</section><h4><span class=\"prefix\"></span><span class=\"content\">2. 统计计算对比</span><span class=\"suffix\"></span></h4>\n<p>R是统计计算的黄金标准，但Julia在性能和开发效率上有显著优势：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;Julia统计计算示例</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;Distributions,&nbsp;HypothesisTests<br /><span class=\"hljs-comment\">#&nbsp;定义分布</span><br />dist&nbsp;=&nbsp;Normal(<span class=\"hljs-number\">0</span>,&nbsp;<span class=\"hljs-number\">1</span>)<br /><span class=\"hljs-comment\">#&nbsp;采样</span><br />x&nbsp;=&nbsp;rand(dist,&nbsp;<span class=\"hljs-number\">1000</span>)<br /><span class=\"hljs-comment\">#&nbsp;统计检验</span><br />ttest(x,&nbsp;y)<br /><span class=\"hljs-comment\">#&nbsp;线性回归</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;GLM<br />ols&nbsp;=&nbsp;fit(LinearModel,&nbsp;<span class=\"hljs-meta\">@formula</span>(Y&nbsp;~&nbsp;X),&nbsp;df)<br /></code></pre>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>线性回归</strong>：Julia比R快10-20倍</section></li><li><section><strong>矩阵运算</strong>：Julia比R快5-10倍</section></li><li><section><strong>循环计算</strong>：Julia比R快100倍</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. 数据科学对比</span><span class=\"suffix\"></span></h4>\n<p>在数据科学领域，Julia的DataFrames.jl比R的dplyr有显著优势：</p>\n<ul>\n<li><section><strong>内存效率</strong>：DataFrames.jl比dplyr更高效</section></li><li><section><strong>性能</strong>：DataFrames.jl比dplyr快10倍</section></li><li><section><strong>API设计</strong>：DataFrames.jl比dplyr更简洁</section></li></ul>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>维度</th>\n<th>Julia</th>\n<th>Python</th>\n<th>MATLAB</th>\n<th>R</th>\n<th>Fortran</th>\n</tr><tr>\n<td><strong>性能</strong></td>\n<td>⭐⭐⭐⭐⭐（接近 C）</td>\n<td>⭐⭐（需 NumPy/Cython 加速）</td>\n<td>⭐⭐⭐（JIT 有限）</td>\n<td>⭐（向量化快，循环慢）</td>\n<td>⭐⭐⭐⭐⭐（HPC 黄金标准）</td>\n</tr>\n<tr>\n<td><strong>语法易用性</strong></td>\n<td>⭐⭐⭐⭐（数学友好）</td>\n<td>⭐⭐⭐⭐⭐</td>\n<td>⭐⭐⭐⭐</td>\n<td>⭐⭐⭐</td>\n<td>⭐（冗长，现代 Fortran 改善）</td>\n</tr>\n<tr>\n<td><strong>数值计算</strong></td>\n<td>⭐⭐⭐⭐⭐（原生支持）</td>\n<td>⭐⭐⭐⭐（NumPy/SciPy）</td>\n<td>⭐⭐⭐⭐⭐（矩阵为中心）</td>\n<td>⭐⭐⭐（stats 为主）</td>\n<td>⭐⭐⭐⭐⭐（数组操作强）</td>\n</tr>\n<tr>\n<td><strong>可视化</strong></td>\n<td>⭐⭐⭐⭐（Makie/Plots）</td>\n<td>⭐⭐⭐⭐⭐（Matplotlib/Seaborn/Plotly）</td>\n<td>⭐⭐⭐⭐⭐（内置强大）</td>\n<td>⭐⭐⭐⭐⭐（ggplot2）</td>\n<td>⭐（依赖外部库）</td>\n</tr>\n<tr>\n<td><strong>数据科学</strong></td>\n<td>⭐⭐⭐⭐（DataFrames.jl 成熟）</td>\n<td>⭐⭐⭐⭐⭐（pandas 主导）</td>\n<td>⭐⭐⭐（Table 支持一般）</td>\n<td>⭐⭐⭐⭐⭐（tidyverse）</td>\n<td>⭐</td>\n</tr>\n<tr>\n<td><strong>机器学习</strong></td>\n<td>⭐⭐⭐（MLJ/Flux 发展中）</td>\n<td>⭐⭐⭐⭐⭐（scikit-learn/TensorFlow/PyTorch）</td>\n<td>⭐⭐⭐（Statistics and ML Toolbox）</td>\n<td>⭐⭐⭐（caret/tidymodels）</td>\n<td>⭐</td>\n</tr>\n<tr>\n<td><strong>深度学习</strong></td>\n<td>⭐⭐⭐（Flux/Lux 快速发展）</td>\n<td>⭐⭐⭐⭐⭐（PyTorch/TensorFlow）</td>\n<td>⭐⭐（Deep Learning Toolbox）</td>\n<td>⭐</td>\n<td>⭐</td>\n</tr>\n<tr>\n<td><strong>微分方程/科学计算</strong></td>\n<td>⭐⭐⭐⭐⭐（DifferentialEquations.jl）</td>\n<td>⭐⭐⭐（SciPy）</td>\n<td>⭐⭐⭐⭐</td>\n<td>⭐⭐</td>\n<td>⭐⭐⭐⭐（如 PETSc 接口）</td>\n</tr>\n<tr>\n<td><strong>并行/GPU</strong></td>\n<td>⭐⭐⭐⭐⭐（原生多级并行）</td>\n<td>⭐⭐⭐（multiprocessing, CuPy）</td>\n<td>⭐⭐⭐（Parallel Computing Toolbox）</td>\n<td>⭐⭐（future/parallel）</td>\n<td>⭐⭐⭐⭐（OpenMP/MPI）</td>\n</tr>\n<tr>\n<td><strong>社区与生态</strong></td>\n<td>⭐⭐⭐（快速增长）</td>\n<td>⭐⭐⭐⭐⭐</td>\n<td>⭐⭐⭐（商业闭源限制）</td>\n<td>⭐⭐⭐⭐</td>\n<td>⭐⭐（学术/HPC 圈）</td>\n</tr>\n<tr>\n<td><strong>开源免费</strong></td>\n<td>✅（MIT）</td>\n<td>✅</td>\n<td>❌（商业许可）</td>\n<td>✅</td>\n<td>✅（现代编译器如 gfortran）</td>\n</tr>\n</tbody>\n</table>\n</section><h3><span class=\"prefix\"></span><span class=\"content\">十四、实际应用案例</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 气象模拟应用</span><span class=\"suffix\"></span></h4>\n<p>Julia正在气象模拟领域取得突破，如WRF模型的Julia实现：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;Julia气象模拟示例</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;WRF<br /><span class=\"hljs-comment\">#&nbsp;设置模拟参数</span><br />params&nbsp;=&nbsp;WRFParams(<br />&nbsp;&nbsp;&nbsp;&nbsp;nx&nbsp;=&nbsp;<span class=\"hljs-number\">200</span>,<br />&nbsp;&nbsp;&nbsp;&nbsp;ny&nbsp;=&nbsp;<span class=\"hljs-number\">200</span>,<br />&nbsp;&nbsp;&nbsp;&nbsp;nz&nbsp;=&nbsp;<span class=\"hljs-number\">50</span>,<br />&nbsp;&nbsp;&nbsp;&nbsp;dt&nbsp;=&nbsp;<span class=\"hljs-number\">30</span>,<br />&nbsp;&nbsp;&nbsp;&nbsp;<span class=\"hljs-comment\">#&nbsp;其他参数...</span><br />)<br /><br /><span class=\"hljs-comment\">#&nbsp;初始化模型</span><br />model&nbsp;=&nbsp;WRFModel(params)<br /><br /><span class=\"hljs-comment\">#&nbsp;运行模拟</span><br />solve(model,&nbsp;tspan=(<span class=\"hljs-number\">0</span>,&nbsp;<span class=\"hljs-number\">24</span>*<span class=\"hljs-number\">3600</span>))<br /><br /><span class=\"hljs-comment\">#&nbsp;可视化结果</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;GLMakie<br />contourf(model压力场,&nbsp;title=<span class=\"hljs-string\">\"海平面气压场\"</span>)<br /></code></pre>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>1000万网格点模拟</strong>：Julia比传统Fortran实现快2-3倍</section></li><li><section><strong>代码简洁性</strong>：Julia代码比Fortran少50-70%</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. 机器学习应用</span><span class=\"suffix\"></span></h4>\n<p>Julia的Flux.jl和MLJ.jl在机器学习领域有广泛应用：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;Julia机器学习示例</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;Flux<br /><span class=\"hljs-comment\">#&nbsp;定义深度学习模型</span><br />model&nbsp;=&nbsp;Chain(<br />&nbsp;&nbsp;&nbsp;&nbsp;Dense(<span class=\"hljs-number\">784</span>,&nbsp;<span class=\"hljs-number\">32</span>,&nbsp;relu),<br />&nbsp;&nbsp;&nbsp;&nbsp;Dense(<span class=\"hljs-number\">32</span>,&nbsp;<span class=\"hljs-number\">10</span>),<br />&nbsp;&nbsp;&nbsp;&nbsp;softmax<br />)<br /><br /><span class=\"hljs-comment\">#&nbsp;训练模型</span><br />loss(x,&nbsp;y)&nbsp;=&nbsp;crossentropy(model(x),&nbsp;y)<br />ps&nbsp;=&nbsp;params(model)<br /><span class=\"hljs-meta\">@epochs</span>&nbsp;<span class=\"hljs-number\">100</span>&nbsp;train!(loss,&nbsp;ps,&nbsp;data,&nbsp;ADAM())[(deep_research_source_group_web_54)]<br /><br /><span class=\"hljs-comment\">#&nbsp;使用MLJ.jl进行机器学习</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;MLJ<br /><span class=\"hljs-comment\">#&nbsp;加载模型</span><br />model&nbsp;=&nbsp;<span class=\"hljs-meta\">@load</span>&nbsp;RandomForestClassifier<br /><span class=\"hljs-comment\">#&nbsp;创建管道</span><br />pipeline&nbsp;=&nbsp;<span class=\"hljs-meta\">@pipeline</span>(<br />&nbsp;&nbsp;&nbsp;&nbsp;Standardizer(),<br />&nbsp;&nbsp;&nbsp;&nbsp;model,<br />&nbsp;&nbsp;&nbsp;&nbsp;Imputer()<br />)<br /><span class=\"hljs-comment\">#&nbsp;训练和评估</span><br />evaluate(pipeline,&nbsp;X,&nbsp;y,&nbsp;measure=r²)[(deep_research_source_group_web_55)]<br /></code></pre>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>ResNet50训练</strong>：Julia比Python快12%</section></li><li><section><strong>随机森林训练</strong>：Julia比Python快2.5倍</section></li><li><section><strong>代码简洁性</strong>：Julia代码比Python简洁30-50%</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. 科学计算应用</span><span class=\"suffix\"></span></h4>\n<p>DifferentialEquations.jl在微分方程求解领域有广泛应用：</p>\n<pre class=\"custom\"><code class=\"hljs\"><span class=\"hljs-comment\">#&nbsp;Julia微分方程求解示例</span><br /><span class=\"hljs-keyword\">using</span>&nbsp;DifferentialEquations,&nbsp;Plots<br /><span class=\"hljs-comment\">#&nbsp;定义Lorenz系统</span><br /><span class=\"hljs-keyword\">function</span>&nbsp;lorenz(du,&nbsp;u,&nbsp;p,&nbsp;t)<br />&nbsp;&nbsp;&nbsp;&nbsp;σ,&nbsp;ρ,&nbsp;β&nbsp;=&nbsp;p<br />&nbsp;&nbsp;&nbsp;&nbsp;du[<span class=\"hljs-number\">1</span>]&nbsp;=&nbsp;σ*(u[<span class=\"hljs-number\">2</span>]&nbsp;-&nbsp;u[<span class=\"hljs-number\">1</span>])<br />&nbsp;&nbsp;&nbsp;&nbsp;du[<span class=\"hljs-number\">2</span>]&nbsp;=&nbsp;u[<span class=\"hljs-number\">1</span>]*(ρ&nbsp;-&nbsp;u[<span class=\"hljs-number\">3</span>])&nbsp;-&nbsp;u[<span class=\"hljs-number\">2</span>]<br />&nbsp;&nbsp;&nbsp;&nbsp;du[<span class=\"hljs-number\">3</span>]&nbsp;=&nbsp;u[<span class=\"hljs-number\">1</span>]*u[<span class=\"hljs-number\">2</span>]&nbsp;-&nbsp;β*u[<span class=\"hljs-number\">3</span>]<br /><span class=\"hljs-keyword\">end</span><br /><br /><span class=\"hljs-comment\">#&nbsp;定义问题</span><br />p&nbsp;=&nbsp;[<span class=\"hljs-number\">10.0</span>,&nbsp;<span class=\"hljs-number\">28.0</span>,&nbsp;<span class=\"hljs-number\">8</span>/<span class=\"hljs-number\">3</span>]<br />u0&nbsp;=&nbsp;[<span class=\"hljs-number\">1.0</span>,&nbsp;<span class=\"hljs-number\">0.0</span>,&nbsp;<span class=\"hljs-number\">0.0</span>]<br />tspan =&nbsp;(<span class=\"hljs-number\">0.0</span>,&nbsp;<span class=\"hljs-number\">100.0</span>)<br />prob&nbsp;=&nbsp;ODEProblem(lorenz,&nbsp;u0,&nbsp;tspan,&nbsp;p)<br /><br /><span class=\"hljs-comment\">#&nbsp;求解问题</span><br />sol&nbsp;=&nbsp;solve(prob,&nbsp;Tsit5(),&nbsp;reltol=<span class=\"hljs-number\">1e-8</span>,&nbsp;abstol=<span class=\"hljs-number\">1e-8</span>)<br /><br /><span class=\"hljs-comment\">#&nbsp;可视化结果</span><br />plot(sol,&nbsp;vars=(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>),&nbsp;title=<span class=\"hljs-string\">\"Lorenz系统相图\"</span>,&nbsp;label=<span class=\"hljs-literal\">false</span>)<br />plot!(sol,&nbsp;vars=(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">3</span>),&nbsp;title=<span class=\"hljs-string\">\"Lorenz系统相图\"</span>,&nbsp;label=<span class=\"hljs-literal\">false</span>)<br /></code></pre>\n<p><strong>性能对比</strong>：</p>\n<ul>\n<li><section><strong>CPU求解</strong>：Julia性能与C++/Fortran相当</section></li><li><section><strong>GPU求解</strong>：Julia比PyTorch快20-100倍</section></li><li><section><strong>代码简洁性</strong>：Julia代码比Fortran简洁76%</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">细分领域对比总结：</span><span class=\"suffix\"></span></h3>\n<ul>\n<li><section><strong>数值模拟 &amp; HPC</strong>：Julia ≈ Fortran &gt; MATLAB &gt; Python &gt; R<br />\n（Julia 在易用性和性能间取得最佳平衡）</section></li><li><section><strong>数据探索 &amp; 统计分析</strong>：R ≈ Python &gt; Julia &gt; MATLAB &gt; Fortran</section></li><li><section><strong>深度学习研究</strong>：Python &gt;&gt; Julia &gt; MATLAB &gt; R ≈ Fortran</section></li><li><section><strong>微分方程求解</strong>：Julia &gt; MATLAB ≈ Python &gt; R &gt; Fortran（除非手写）</section></li><li><section><strong>教学与快速原型</strong>：Python ≈ MATLAB &gt; Julia &gt; R &gt; Fortran</section></li><li><section><strong>生产部署</strong>：Python &gt; Julia（正在追赶）&gt; MATLAB（许可证问题）&gt; R &gt; Fortran</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">十五、学习曲线与社区支持</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. 学习曲线对比</span><span class=\"suffix\"></span></h4>\n<section class=\"table-container\"><table>\n<tbody>\n<tr>\n<th>语言</th>\n<th>学习难度</th>\n<th>上手时间</th>\n<th>主要学习资源</th>\n</tr><tr>\n<td>Julia</td>\n<td>中等</td>\n<td>1-2周</td>\n<td>官方文档、Julia学院、GitHub仓库</td>\n</tr>\n<tr>\n<td>MATLAB</td>\n<td>低</td>\n<td>1周</td>\n<td>官方教程、大量在线资源</td>\n</tr>\n<tr>\n<td>Python</td>\n<td>低</td>\n<td>1-2周</td>\n<td>官方文档、大量在线教程</td>\n</tr>\n<tr>\n<td>Fortran</td>\n<td>高</td>\n<td>2-3个月</td>\n<td>官方文档、专业书籍</td>\n</tr>\n<tr>\n<td>R</td>\n<td>中等</td>\n<td>2-3周</td>\n<td>官方文档、大量统计教程</td>\n</tr>\n</tbody>\n</table>\n</section><p><strong>学习曲线分析</strong>：</p>\n<ul>\n<li><section><strong>MATLAB用户</strong>：可快速上手Julia，语法相似</section></li><li><section><strong>Python用户</strong>：学习曲线平缓，语法相似</section></li><li><section><strong>R用户</strong>：可快速上手Julia，语法更简洁</section></li><li><section><strong>Fortran/C++用户</strong>：需适应动态类型和JIT编译，但性能接近</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. 社区与支持</span><span class=\"suffix\"></span></h4>\n<p>Julia社区正在快速增长，提供丰富的支持资源：</p>\n<ul>\n<li><section><strong>GitHub项目</strong>：超过20,000个Julia项目</section></li><li><section><strong>活跃度</strong>：社区年增长40%+</section></li><li><section><strong>中文社区</strong>：非常活跃，有大量中文资料</section></li><li><section><strong>文档资源</strong>：官方文档完善，包文档丰富</section></li><li><section><strong>论坛支持</strong>：Discourse论坛活跃，问题解决率高</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">十六、总结与展望</span><span class=\"suffix\"></span></h3>\n<h4><span class=\"prefix\"></span><span class=\"content\">1. Julia的核心优势总结</span><span class=\"suffix\"></span></h4>\n<ul>\n<li><section><strong>高性能</strong>：JIT编译，接近C/Fortran性能</section></li><li><section><strong>易用性</strong>：语法简洁，类似Python/MATLAB</section></li><li><section><strong>全栈统一</strong>：一个语言完成从数据处理到部署的全流程</section></li><li><section><strong>生态整合</strong>：包之间无缝集成，API统一</section></li><li><section><strong>开源社区</strong>：活跃社区，快速增长</section></li><li><section><strong>类型系统</strong>：动态类型但有类型推断，性能高</section></li><li><section><strong>多分派机制</strong>：代码更灵活，性能更优</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">2. 适用场景与用户群体</span><span class=\"suffix\"></span></h4>\n<p>Julia特别适合以下场景和用户群体：</p>\n<ul>\n<li><section><strong>科学计算</strong>：物理、化学、生物等领域的数值模拟</section></li><li><section><strong>数据科学</strong>：大规模数据分析、统计建模</section></li><li><section><strong>机器学习</strong>：高性能深度学习和传统机器学习</section></li><li><section><strong>可视化</strong>：交互式数据可视化、科学数据展示</section></li><li><section><strong>并行计算</strong>：高性能计算、分布式系统</section></li><li><section><strong>用户群体</strong>：科学家、工程师、数据分析师、机器学习研究者</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">3. 未来发展趋势</span><span class=\"suffix\"></span></h4>\n<p>Julia的未来发展趋势包括：</p>\n<ul>\n<li><section><strong>性能优化</strong>：继续提升JIT编译效率，缩小与C/Fortran的差距</section></li><li><section><strong>生态扩展</strong>：继续扩展包生态系统，覆盖更多领域</section></li><li><section><strong>工具链完善</strong>：完善IDE支持、调试工具等开发体验</section></li><li><section><strong>部署优化</strong>：简化模型和应用部署流程</section></li><li><section><strong>并行计算</strong>：继续提升分布式计算和GPU加速能力</section></li><li><section><strong>社区增长</strong>：吸引更多用户和开发者加入社区</section></li></ul>\n<h4><span class=\"prefix\"></span><span class=\"content\">4. 与主流语言的互补性</span><span class=\"suffix\"></span></h4>\n<p>Julia与主流语言不是完全替代关系，而是<strong>互补关系</strong>：</p>\n<ul>\n<li><section><strong>与Python对比</strong>：Julia在性能上有优势，但Python在生态成熟度上领先</section></li><li><section><strong>与MATLAB对比</strong>：Julia在性能和开源性上有优势，但MATLAB在交互式环境上更成熟</section></li><li><section><strong>与Fortran对比</strong>：Julia在开发效率和生态整合上有优势，但Fortran在特定科学计算领域仍有性能优势</section></li><li><section><strong>与R对比</strong>：Julia在性能和代码简洁性上有优势，但R在统计分析领域有更丰富的工具</section></li></ul>\n<h3><span class=\"prefix\"></span><span class=\"content\">十七、给潜在用户的建议</span><span class=\"suffix\"></span></h3>\n<p>对于考虑使用Julia的用户，建议如下：</p>\n<ol>\n<li><section><strong>评估需求</strong>：确定您的主要计算需求是科学计算、数据科学还是机器学习</section></li><li><section><strong>学习路径</strong>：从基础语法开始，逐步学习类型系统和多分派机制</section></li><li><section><strong>工具选择</strong>：根据应用领域选择合适的工具包（如科学计算选DifferentialEquations.jl）</section></li><li><section><strong>性能调优</strong>：学习类型稳定性、避免类型不稳定性、使用@inbounds和@ threads等优化宏</section></li><li><section><strong>社区参与</strong>：加入Julia社区，参与讨论和贡献，获取最新支持</section></li><li><section><strong>混合编程</strong>：对于已有Python/R代码，可使用PyCall/RCall调用</section></li><li><section><strong>部署策略</strong>：对于生产环境，考虑使用PackageCompiler.jl编译为单文件</section></li></ol>\n<p><strong>最佳实践</strong>：</p>\n<ul>\n<li><section><strong>代码优化</strong>：保持类型稳定性，使用@ code _ warntype检查</section></li><li><section><strong>并行策略</strong>：对于大规模数据，优先使用多线程；对于集群计算，使用分布式计算</section></li><li><section><strong>GPU加速</strong>：对于大规模科学计算，考虑使用CUDA.jl加速</section></li><li><section><strong>可视化选择</strong>：对于基础可视化，使用Plots.jl；对于高性能3D可视化，使用GLMakie</section></li></ul>\n<p>Julia作为一门新兴的科学计算语言，以其独特的设计哲学——**\"一次编写，高效运行\"**，成功融合了动态语言的易用性和静态语言的高性能。</p>\n<p>Julia 是一门为“下一代科学计算”而生的语言，其核心优势在于：</p>\n<ul>\n<li><section><strong>性能与表达力的统一</strong></section></li><li><section><strong>统一的生态系统</strong>（从微分方程到深度学习）</section></li><li><section><strong>前沿的自动微分与可微分编程支持</strong></section></li><li><section><strong>原生并行与 GPU 支持</strong></section></li></ul>\n<p>在数值系统、类型系统、可视化、数据科学、机器学习、科学计算和并行计算等核心领域，Julia都展现出显著的技术优势。</p>\n<blockquote class=\"custom-blockquote multiquote-1\">\n<p>Julia与MATLAB、Python、Fortran和R等主流语言相比仍有差距，特别是在生态成熟度和用户基数方面，但其快速发展的社区和日益完善的工具链正迅速缩小这些差距。</p>\n</blockquote>\n<p>虽然在某些领域（如深度学习框架成熟度、数据科学社区规模）仍落后于 Python，但 Julia 正在快速填补这些空白，尤其在需要<strong>高性能、可组合、可微分</strong>的科学计算场景中，已成为不可忽视的选择。</p>\n<blockquote class=\"custom-blockquote multiquote-1\">\n<p>对于新项目，尤其是涉及<strong>数值模拟、优化、微分方程、可微分建模</strong>的研究或工程任务，且追求高性能、易用性和全栈统一的科研人员和工程师来说，<strong>Julia是一个极具潜力的选择</strong>。</p>\n</blockquote>\n<p>随着Julia生态系统的不断完善和性能的持续优化，它有望在未来几年内成为科学计算领域的主流语言之一，为科研和工程计算带来新的可能性。</p>\n<hr />\n<p>公众号<a href=\"https://www.cardopt.cn/api/images/1_1763724698_ce344a.png\" rel=\"noopener nofollow\">《博優旮旯-BOYOGALA》</a>，致力于让大家<strong>更专业、更完整和更系统</strong>地获取与了解数学（<code>运筹与优化、数值分析</code>）等相关数学知识分享！</p>\n<blockquote class=\"custom-blockquote multiquote-1\">\n<p>🎯公众号ID:boyogala,\n🌐网址: www.boyogala.us.kg,\n💬微信:&nbsp;boyougala,\n📧邮箱: boyogala@qq.com.</p>\n</blockquote>\n<p>说明文档：<a href=\"https://mp.weixin.qq.com/s/OJpmAIeQnAcxmSrIXsWrWQ\" rel=\"noopener nofollow\"><strong>公众号《博優旮旯-boyogala》的使用指南</strong></a>，以下罗列代表作可供查阅.</p>\n<p><code>优化求解器</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/UBRt0_-A0_5n85Vj6bwlGw\" rel=\"noopener nofollow\">优化求解器类型总结线性二次和非线性求解器</a>,<a href=\"https://mp.weixin.qq.com/s/MNzKEo_q06xWiH1xBCxQtQ\" rel=\"noopener nofollow\">Ipopt开源免费的非线性求解器</a>,<a href=\"https://mp.weixin.qq.com/s/pFwaMIGk86MnsG8w6Vjm2Q\" rel=\"noopener nofollow\">HiGHS开源免费整数线性求解器</a>,<a href=\"https://mp.weixin.qq.com/s/_AZ5-bdwZ1Blu3vpA98ccQ\" rel=\"noopener nofollow\">SCIP开源免费的优化求解器</a>,<a href=\"https://mp.weixin.qq.com/s/9FnUAHav-mfeYzfWZLwyGw\" rel=\"noopener nofollow\">Gurobi商业收费全局优化求解器</a>,<a href=\"https://mp.weixin.qq.com/s/LO5WH5be975ryjUczeAgTw\" rel=\"noopener nofollow\">CPLEX商业收费整数优化求解器</a>,<a href=\"https://mp.weixin.qq.com/s/xXxGCJlTaek6AymvQHf99A\" rel=\"noopener nofollow\">MOSEK商业收费的优化求解器</a>,<a href=\"https://mp.weixin.qq.com/s/g9Nu5ZzLZwGrIW6zgck1dw\" rel=\"noopener nofollow\">BARON商业收费的全局优化求解器</a>,<a href=\"https://mp.weixin.qq.com/s/BI92_k-JoCH-Eoj3lh5IsA\" rel=\"noopener nofollow\">LindoAPI商业收费的全局优化求解器</a>,<a href=\"https://mp.weixin.qq.com/s/58fIjguklH2R7v0o0aBkpA\" rel=\"noopener nofollow\">COPT国产自研的优化求解器</a></p>\n<p><code>三大数学软件</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/N9KDUwkf8xhGD-Kt0-SzSA\" rel=\"noopener nofollow\">MATLAB工程师的科学计算软件</a>,<a href=\"https://mp.weixin.qq.com/s/aV696xDAF1Fa-YYahWWLXw\" rel=\"noopener nofollow\">MATHEMATICA物理的计算软件</a>,<a href=\"https://mp.weixin.qq.com/s/KbyK_6iHSY9stpWBLFywCQ\" rel=\"noopener nofollow\">MAPLE数学家的数学软件</a></p>\n<p><code>嵌入式、无人机和机器人</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/_i7dolzdYlYeHoEBI7FZyw\" rel=\"noopener nofollow\">OSQP二次规划求解器</a></p>\n<p><code>线性方程组的求解软件</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/Tcv6H4PbEqws7ivrETctFQ\" rel=\"noopener nofollow\">PARDISO并行直接求解器</a>,<a href=\"https://mp.weixin.qq.com/s/SrJXaN5whCBAYlbTHIxdIA\" rel=\"noopener nofollow\">MUMPS高性能并行求解器</a>,<a href=\"https://mp.weixin.qq.com/s/-8e69XW-2m4oNe9TBmhYnQ\" rel=\"noopener nofollow\">SuitSparse稀疏矩阵软件包</a>,<a href=\"https://mp.weixin.qq.com/s/kG6784iGW9bCn_yygx86OA\" rel=\"noopener nofollow\">SuperLU非对称直接法求解器</a></p>\n<p><code>基于MATLAB的优化建模工具</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/UsUxTSQ_AtVlrXAe7HFPeQ\" rel=\"noopener nofollow\">CVX免费凸优化建模工具</a>,<a href=\"https://mp.weixin.qq.com/s/SP1ou_DyuLb1TgzI0JH2PA\" rel=\"noopener nofollow\">Yalmip免费的优化建模工具</a>,<a href=\"https://mp.weixin.qq.com/s/A8p7H7BquSsIBbMh3eMLGg\" rel=\"noopener nofollow\">CasADi开源最优化控制工具</a></p>\n<p><code>基于Python的优化建模工具</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/A8p7H7BquSsIBbMh3eMLGg\" rel=\"noopener nofollow\">CasADi非线性优化和最优控制</a>,<a href=\"https://mp.weixin.qq.com/s/L687fP8uaGdmMVmqVY1QKA\" rel=\"noopener nofollow\">Gekko数值优化和动态系统建模</a>,<a href=\"https://mp.weixin.qq.com/s/fnd3-Hu2OkEnIXJkW-ObfA\" rel=\"noopener nofollow\">Pyomo面向对象代数建模语言</a></p>\n<p><code>科学计算软件</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/37BW460fBosmIzZU9_a9hQ\" rel=\"noopener nofollow\">oneAPI统一的异构编程模型</a>,<a href=\"https://mp.weixin.qq.com/s/WUk6KdAQwE-GfOPQSBBG2w\" rel=\"noopener nofollow\">CUDA人工智能时代的基石</a>,<a href=\"https://mp.weixin.qq.com/s/ZB--8yvT_khcB8ZpTcQUGQ\" rel=\"noopener nofollow\">OpenFOAM开源的CFD软件</a>,<a href=\"https://mp.weixin.qq.com/s/OtHkmLUkaREP-ktpcLLajQ\" rel=\"noopener nofollow\">COMSOL业界多物理场仿真软件</a></p>\n<p><code>全球优化建模平台</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/mXjzRlPe2-v2ow7MQynChQ\" rel=\"noopener nofollow\">AMPL数学规划建模语言</a>,<a href=\"https://mp.weixin.qq.com/s/LjOTgVMij56jufQxHltsXQ\" rel=\"noopener nofollow\">AIMMS快速优化建模工具</a>,<a href=\"https://mp.weixin.qq.com/s/bAvCxUWyNmFgGYZQlQeOeA\" rel=\"noopener nofollow\">GAMS通用代数建模系统</a>,<a href=\"https://mp.weixin.qq.com/s/J8pur1XjjUXL52Jiv71H3Q\" rel=\"noopener nofollow\">JuMP数学优化建模语言（学习中…）</a></p>\n<p><code>人类在思考</code> — <strong>代表作：</strong><a href=\"https://mp.weixin.qq.com/s/n7vfkfiEz5t8Wq2YJvMMWA\" rel=\"noopener nofollow\">公众号排版数学公式的经验</a>,<a href=\"https://mp.weixin.qq.com/s/J5g9gGaCc0aAOkrjQ-fxtg\" rel=\"noopener nofollow\">200篇论文🆚1个优化求解器</a>,<a href=\"https://mp.weixin.qq.com/s/N1F7lVLi_YsNHNnvYQHTKQ\" rel=\"noopener nofollow\">盗版Windows系统🆚破解版LINGO18</a></p>\n<p><code>数学是第三世界</code> — <strong>代表作</strong>：\n<a href=\"https://mp.weixin.qq.com/s/PHjmBFYPJlBagcqJjX9udg\" rel=\"noopener nofollow\">数学研究需要师徒传承吗？</a>,<a href=\"https://mp.weixin.qq.com/s/uluBnkrO4Q59nUJ9o3QEIA\" rel=\"noopener nofollow\">数学的三次数学危机</a>,<a href=\"https://mp.weixin.qq.com/s/6MMMjFMCaAD4SmZXrD7x4Q\" rel=\"noopener nofollow\">矩阵空间的特殊矩阵</a>,<a href=\"https://mp.weixin.qq.com/s/z3jGTYjFhjWVx1OUyOzlpQ\" rel=\"noopener nofollow\">函数梯度的可视化</a></p>\n</section>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-23 12:34</span>&nbsp;\n<a href=\"https://www.cnblogs.com/boyogala\">博優旮旯</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "还有比ollama更傻瓜式的大模型本地部署方式吗 ？",
      "link": "https://www.cnblogs.com/JulianHuang/p/19520341",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/JulianHuang/p/19520341\" id=\"cb_post_title_url\" title=\"发布于 2026-01-23 09:08\">\n    <span>还有比ollama更傻瓜式的大模型本地部署方式吗 ？</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>LLM的狂风已经吹了几年， 所有人都耳濡目染的会飚上几句行话/名词。切好你自己有台4070的机器，恰好你有时间倒腾， 那就让我们回顾一遍名词，验证狂风吹过的技术车辙。</p>\n<p>恰好最近有台4070(12g显存)机器，于是尝试使用ollama部署大模型。</p>\n<blockquote>\n<p>RTX 4070 擅长训练中小型模型；凭借其 184 个 Tensor Core，它可以高效处理矩阵乘法等运算，这对于深度学习任务至关重要。<br />\nRTX 4070 适用于实时推理应用，提供快速的推理速度，使其成为聊天机器人和推荐系统等交互式 AI 应用的理想选择。</p>\n</blockquote>\n<p>本次会用到3个名词</p>\n<ol>\n<li><a href=\"https://www.baeldung.com/linux/nvidia-smi-full-gpu-details\" rel=\"noopener nofollow\" target=\"_blank\" title=\"nvidia-smi\">nvidia-smi</a> 英伟达设备管理工具</li>\n</ol>\n<p>基于nvidia managemant library（NVML）之上的命令行工具，用于管理和监控nvidia GPU设备， 随显示驱动一起分发。</p>\n<p>以下是未部署大模型时候的资源消耗快照， nvidia-smi  -l 可持续监控gpu使用。</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090138183-1418030168.png\" /></p>\n<ul>\n<li>第一行显示了nvidia-smi版本/驱动版本/<a href=\"https://zh.wikipedia.org/wiki/CUDA#%E7%B0%A1%E4%BB%8B\" rel=\"noopener nofollow\" target=\"_blank\" title=\"CUDA\">CUDA</a>版本</li>\n<li>0 表示nvidia 4070是第一张显卡， 索引的概念，下面的进程关联了这块显卡0</li>\n<li>Fan Temp Perf Pwr 分别显示GPU的当前风扇转速、温度、性能状态和功耗</li>\n<li>Memory-Usage  当前显存使用量和总显存(12g)</li>\n<li>GPU_Util 显示当前GPU计算能力的百分比</li>\n<li>Compute M 表示当前计算模型： compute</li>\n</ul>\n<ol start=\"2\">\n<li><a href=\"https://docs.ollama.com/\" rel=\"noopener nofollow\" target=\"_blank\" title=\"ollama\">ollama</a></li>\n</ol>\n<p>ollama是mata开源的，定位是<strong>模型管理器</strong>和<strong>推理框架</strong>，帮助用户傻瓜式在本地、k8s集群、虚拟机上部署开源大模型。</p>\n<p><code>ollama -h</code>提供了可用命令，也显示了可用的能力：创建模型、管控部署模型。</p>\n<p>ollama是服务端-客户端架构，有后台服务进程olllama.exe，提供了GUI终端和命令行工具可交互，另外提供sdk和restful api，可供各种程序或者语言操作ollama。</p>\n<pre><code> ollama  list\nNAME                      ID              SIZE      MODIFIED\nqwen3-embedding:latest    64b933495768    4.7 GB    15 hours ago\nqwen3:8b                  500a1f067a9f    5.2 GB    23 hours ago\n\n##  size 是预估的显存大小\n</code></pre>\n<ol start=\"3\">\n<li>qwen3:8b  vs   qwen3-embedding</li>\n</ol>\n<p>8b表示80亿参数（8 Billion parameters）</p>\n<p>除了chat模型， 还有嵌入模型embedding， 嵌入模型是将<strong>文本/image数据向量化</strong>的最新手段。</p>\n<hr />\n<p>下载完ollama， 选择<code>qwen3:8b</code>大模型，开始下载模型。</p>\n<h3 id=\"1--ollama-run-qwen38b\">1.  ollama run qwen3:8b</h3>\n<pre><code>$ ollama  run  qwen3:8b\n&gt;&gt;&gt; Send a message (/? for help)\n</code></pre>\n<p>运行千问大模型（未进行首次推理请求）， 显存和GPU使用率未发生变化；<br />\n<img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090213372-268817923.png\" /></p>\n<p>首次推理请求， 显存使用稳定在6g, gpu使用率上升，推理结束，显存使用率不会降， gpu使用率回落， 说明ollama是<strong>按需加载大模型</strong>。</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090232098-1659121708.png\" /></p>\n<h3 id=\"2-ollama-run--model--vs--ollama--serve\">2. ollama run  [model]  vs  ollama  serve</h3>\n<p>ollama serve意味着<strong>启动ollama作为web服务</strong>(不意味着当前启动了大模型)，默认在http://127.0.0.1:11434/监听，外部可使用restful api或者client sdk操作ollama、操作模型、与模型对话。</p>\n<pre><code>curl http://localhost:11434/api/chat -d '{\n  \"model\": \"qwen3:8b\",\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": \"Hello there!\"\n  }],\n  \"stream\": false\n}'\n{\"model\":\"qwen3:8b\",\"created_at\":\"2026-01-21T07:57:52.9621534Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Hello! How can I assist you today? 😊\",\"thinking\":\"Okay, the user greeted me with \\\"Hello there!\\\" So I need to respond in a friendly and welcoming manner. Let me make sure to acknowledge their greeting and offer assistance. I should keep it simple and positive. Maybe something like, \\\"Hello! How can I assist you today?\\\" That sounds good. Let me check if there's anything else I need to consider. No, that should cover it. Ready to respond.\\n\"},\"done\":true,\"done_reason\":\"stop\",\"total_duration\":1535352700,\"load_duration\":98539300,\"prompt_eval_count\":13,\"prompt_eval_duration\":208148700,\"eval_count\":101,\"eval_duration\":1223840500}\n</code></pre>\n<p>这个restful api启动了大模型，且是按需加载，  <code>stream: true</code> 就会按照streaming输出， 也就是走chunked transfer encoding watch机制。\"think\":默认为true表示输出推理思考。</p>\n<h3 id=\"3-ollama部署embeddings嵌入模型\">3. ollama部署embeddings嵌入模型</h3>\n<p>eembeddings嵌入模型不同于对话模型，用于将文本/imgae向量化，用于语义搜索、检索和RAG。这里有最新的<a href=\"https://huggingface.co/spaces/mteb/leaderboard\" rel=\"noopener nofollow\" target=\"_blank\" title=\"嵌入模型榜单\">嵌入模型榜单</a></p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090252139-202991087.png\" /></p>\n<p><strong>RAG 技术本质上是将Prompt增强</strong>技术， 本次利用ollama部署文本嵌入模型，提前将文本数据向量化后，存储在向量数据库。</p>\n<pre><code>curl -X POST http://localhost:11434/api/embed \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"qwen3-embedding\",\n    \"input\": \"The quick brown fox jumps over the lazy dog.\"\n  }'\n</code></pre>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090402066-2090586535.png\" /></p>\n<h2 id=\"4-function-calling\">4. function calling</h2>\n<p><code>what is  the temperature in the capital of china today?</code></p>\n<p>LLM是静态知识，他肯定不知道今天是几号？今天天气怎么样 ？<br />\n他的静态知识告诉他中国首部是北京(如果换首都，他也G了)。</p>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090424312-1965215025.png\" /></p>\n<p><strong>function calling允许模型调用外部工具并将其结果合并到对话响应中</strong></p>\n<blockquote>\n<p>无论是agentic开发，使用LLM APi， 理解function calling 都很重要，特别是底层的请求和响应payload工作方式。function calling需要结合应用能力一起来理解。</p>\n</blockquote>\n<p>function calling 使得LLM具备推理出你想要做的动作(以结构化json的方式给出), app据此执行动作，将调用的结果合并到LLM的输出应答里面。</p>\n<p>询问中国首都今天的气温？：</p>\n<p>LLM角度：</p>\n<ul>\n<li>先要要知道今天是几号， 提示应用去调用函数get_currentDate拿到时间</li>\n<li>然后提示应用拿city= beijing ,date=2016-01-22 去调用天气函数get_temperature(string city， string date)</li>\n<li>最后信息充足，对话中问类似的都能笑纳。</li>\n</ul>\n<p>应用角度：需要提供2个函数</p>\n<ul>\n<li>get_temperature(string city， string date)： Get the current temperature for a city</li>\n<li>get_currentDate()： Get the current date</li>\n</ul>\n<hr />\n<h4 id=\"-llm发起第一次function-calling-tools配置节携带了应用提供的2个外部调用函数\">① LLM发起第一次function calling, <code>tools</code>配置节携带了应用提供的2个外部调用函数。</h4>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090454039-1314904393.png\" /></p>\n<p>LLM推理提示应用去调用<code>get_currentDate</code>函数。</p>\n<h4 id=\"-应用拿这个推理信息去调用准备好的get_currentdate函数假设结果是2026-01-22\">② 应用拿这个推理信息，去调用准备好的get_currentDate函数，假设结果是<code>2026-01-22</code>。</h4>\n<h4 id=\"-llm发起第二次function-calling此时message配置节要附带第一次外部调用的信息\">③ LLM发起第二次function calling，此时<code>message</code>配置节要附带第一次外部调用的信息</h4>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090514188-995218064.png\" /></p>\n<p>LLM推理提示应用去调用<code>get_temperature</code>函数，此时参数都已经就绪了：beijing是推理已知，2026-01-22是外部调用注入信息。</p>\n<h4 id=\"-应用拿到推理信息去调用get_temperature函数-假设拿到结果是22\">④ 应用拿到推理信息，去调用<code>get_temperature</code>函数， 假设拿到结果是22°。</h4>\n<h4 id=\"-向llm发起最终天气对话喂给他所有信息\">⑤ 向LLM发起最终天气对话，喂给他所有信息</h4>\n<pre><code>  curl -s http://localhost:11434/api/chat -H \"Content-Type: application/json\" -d '{\n  \"model\": \"qwen3:8b\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"what is the temperature in the capital of china today?\"},\n    {\"role\": \"assistant\",\"tool_calls\":[{\"type\": \"function\",\"function\":{\"index\":0,\"name\":\"get_temperature\",\"arguments\":{\n             \"city\": \"Beijing\",\n              \"date\": \"2026-01-22\" }}}]},\n    {\"role\": \"tool\", \"tool_name\": \"get_temperature\", \"content\": \"22°C\"}\n  ],\n  \"stream\": false,\n  \"think\": false\n}'    // 注入的信息依旧放在`message`配置节。\n</code></pre>\n<p><img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/587720/202601/587720-20260123090556259-1793205593.png\" /></p>\n<p>模型就收到的外部调用的信息，并集成到自己推理系统。</p>\n<p>所以一次复杂的对话可能经过几次function calling， 于此同时也需要应用的配合。</p>\n\n</div>\n<div id=\"MySignature\">\n    <hr color=\"#987cb9\" size=\"3\" width=\"80%\" />\n<div style=\"text-align: center;\">\n<p>本文来自博客园，作者：{有态度的马甲}，转载请注明原文链接：<a href=\"https://www.cnblogs.com/JulianHuang/p/19520341\" target=\"_blank\">https://www.cnblogs.com/JulianHuang/p/19520341</a></p>\n<strong style=\"color: red;\">欢迎关注我的原创技术、职场公众号， 加好友谈天说地，一起进化</strong>\n<div><img src=\"https://blog-static.cnblogs.com/files/JulianHuang/QR.gif\" style=\"width: 250px; height: 250px;\" /> </div>\n\n</div>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-23 09:08</span>&nbsp;\n<a href=\"https://www.cnblogs.com/JulianHuang\">码甲哥不卷</a>&nbsp;\n阅读(<span id=\"post_view_count\">196</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": ".NET 10了，HttpClient还是不能用using吗？我做了一个实验",
      "link": "https://www.cnblogs.com/sdcb/p/19500792/20260119-using-httpclient",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/sdcb/p/19500792/20260119-using-httpclient\" id=\"cb_post_title_url\" title=\"发布于 2026-01-23 08:45\">\n    <span>.NET 10了，HttpClient还是不能用using吗？我做了一个实验</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"引言这个最佳实践到底过时了吗\">引言：这个“最佳实践”到底过时了吗？</h2>\n<p>每隔一段时间，就会看到类似问题反复出现：</p>\n<blockquote>\n<p>“都 .NET 10 了，<code>HttpClient</code> 还不能 <code>using</code> 吗？我每次请求 <code>new HttpClient()</code>，用完 <code>Dispose()</code>，不是很合理？”</p>\n</blockquote>\n<p>这类问题之所以经久不衰，是因为它<strong>在低并发下几乎永远跑得通</strong>；但一旦进入“高并发 + 短连接密集创建”的场景，就会突然变成玄学：有的人能跑，有的人会炸，有人说这是一个这是一个“bug”，在某某版本中会修复（其实并没有），有人说这是一个feature，设计就是如此……</p>\n<p>所以我决定做一个实验，来重现一下10年前就有的现象，看这些现象是否有任何不同。</p>\n<p>本文用一组可复现的压测（同机 server/client，Windows，requests=20000，parallel=200）对比：</p>\n<ul>\n<li>每请求 <code>new HttpClient</code> + <code>Dispose()</code>（也就是大家常说的“using 写法”）</li>\n<li>复用一个 <code>HttpClient</code>（静态/单例）</li>\n<li>使用 <code>IHttpClientFactory</code></li>\n</ul>\n<p>并观察关键指标：<strong>TIME_WAIT 数量</strong>，以及是否出现经典的端口耗尽错误：</p>\n<blockquote>\n<p>通常每个套接字地址(协议/网络地址/端口)只允许使用一次。</p>\n</blockquote>\n<hr />\n<h2 id=\"实验目标\">实验目标</h2>\n<p>验证“每请求 new HttpClient 并 using 释放”在高并发下会导致 TIME_WAIT 激增，并对比复用 HttpClient / IHttpClientFactory 的表现。</p>\n<hr />\n<h2 id=\"实验环境与参数\">实验环境与参数</h2>\n<ul>\n<li>OS: Windows</li>\n<li>SDK: .NET SDK 10.0.102</li>\n<li>服务器: HttpLeakServer（target net6，通过 roll-forward 运行）</li>\n<li>客户端: net48 / net6 / net8 / net10</li>\n<li>压测参数: requests=20000, parallel=200, timeoutSeconds=5</li>\n<li>TIME_WAIT 统计: <code>netstat -an</code> 过滤端口 5055</li>\n<li>隔离策略: 每轮结束后等待 TIME_WAIT &lt;= baseline + 200（每 10 秒检查，最长 300 秒）</li>\n</ul>\n<hr />\n<h2 id=\"如何运行可复现\">如何运行（可复现）</h2>\n<p>完整项目我放在 GitHub：<code>https://github.com/sdcb/http-client-exp</code></p>\n<h3 id=\"1启动服务端单独窗口\">1）启动服务端（单独窗口）</h3>\n<pre><code class=\"language-powershell\">dotnet run --project Server/HttpLeakServer/HttpLeakServer.csproj\n</code></pre>\n<h3 id=\"2运行实验脚本另一个窗口\">2）运行实验脚本（另一个窗口）</h3>\n<pre><code class=\"language-powershell\">scripts/run-experiment-external-server.ps1\n</code></pre>\n<p>本次日志目录：<code>logs/run-20260119-095017</code></p>\n<hr />\n<h2 id=\"解读为什么using-httpclient会把你推向端口耗尽\">解读：为什么“using HttpClient”会把你推向端口耗尽？</h2>\n<p>很多人直觉会觉得：<code>HttpClient</code> 是托管对象，用完 <code>Dispose()</code>，不就释放资源了吗？</p>\n<p>但这里有两个关键点经常被忽略：</p>\n<ol>\n<li><strong><code>HttpClient</code> 并不是“请求一次就关一次连接”的简单模型</strong>。HTTP Keep-Alive + 连接池的存在，意味着正确姿势应该是复用底层连接（或至少复用 handler 的连接池），让大量请求复用少量 TCP 连接。</li>\n<li>你频繁 <code>new HttpClient</code> + <code>Dispose()</code>，等价于<strong>频繁建立 TCP 连接并快速关闭</strong>。而 TCP 连接的关闭会进入 <strong>TIME_WAIT</strong>（具体哪一端进入 TIME_WAIT 与关闭时序有关），TIME_WAIT 存在的意义是保护网络不被“旧连接的残留包”污染。</li>\n</ol>\n<p>在“200 并发 + 2 万次请求”这种参数下，如果你让每个请求都创建新连接，那么很容易短时间制造大量 TIME_WAIT；一旦本机可用的临时端口范围被 TIME_WAIT 占满（或接近占满），新连接就会开始失败，典型异常就是：</p>\n<blockquote>\n<p>通常每个套接字地址(协议/网络地址/端口)只允许使用一次。</p>\n</blockquote>\n<p>这也是为什么同样的代码：</p>\n<ul>\n<li>在低并发下“看起来完全没问题”</li>\n<li>在压力一上来就开始“玄学报错”</li>\n</ul>\n<hr />\n<h2 id=\"那到底怎么写才对\">那到底怎么写才对？</h2>\n<p>本文不展开“所有场景的最佳实践”，只给两条最能落地的结论：</p>\n<ol>\n<li><strong>业务代码不要每次请求 new HttpClient</strong>。要么复用单例/静态 <code>HttpClient</code>，要么使用 <code>IHttpClientFactory</code>。</li>\n<li><strong>可以 using 的是 <code>HttpResponseMessage</code> / <code>HttpContent</code></strong>（它们确实应该及时释放），而不是“每个请求一个 HttpClient”。</li>\n</ol>\n<p>下面我把这次实验的<strong>完整代码</strong>和<strong>原始日志</strong>全部贴出来，方便你自己复跑/改参数/做二次验证。</p>\n<hr />\n<h2 id=\"完整代码精简版一份源码--条件编译\">完整代码（精简版：一份源码 + 条件编译）</h2>\n<p>完整项目地址：<code>https://github.com/sdcb/http-client-exp</code></p>\n<p>为了避免同样的代码贴四遍，这里把 net48/net6/net8/net10 的客户端合并成<strong>一份</strong>，用 <code>#if / #elif</code> 表示差异（实验输出与原始日志仍按“多份结果”原样保留在后文）。</p>\n<h3 id=\"简单服务端httpleakserver\">简单服务端：HttpLeakServer</h3>\n<h4 id=\"serverhttpleakserverhttpleakservercsproj\"><code>Server/HttpLeakServer/HttpLeakServer.csproj</code></h4>\n<pre><code class=\"language-xml\">&lt;Project Sdk=\"Microsoft.NET.Sdk.Web\"&gt;\n  &lt;PropertyGroup&gt;\n    &lt;TargetFramework&gt;net6.0&lt;/TargetFramework&gt;\n    &lt;Nullable&gt;enable&lt;/Nullable&gt;\n    &lt;ImplicitUsings&gt;enable&lt;/ImplicitUsings&gt;\n  &lt;/PropertyGroup&gt;\n&lt;/Project&gt;\n</code></pre>\n<h4 id=\"serverhttpleakserverprogramcs\"><code>Server/HttpLeakServer/Program.cs</code></h4>\n<pre><code class=\"language-csharp\">using Microsoft.AspNetCore.Builder;\nusing Microsoft.Extensions.Hosting;\nusing Microsoft.Extensions.Logging;\n\nvar builder = WebApplication.CreateBuilder(args);\n\nbuilder.Logging.ClearProviders();\nbuilder.Logging.AddSimpleConsole(options =&gt;\n{\n    options.SingleLine = true;\n    options.TimestampFormat = \"HH:mm:ss \";\n});\n\nvar app = builder.Build();\n\napp.MapGet(\"/\", () =&gt; Results.Text(\"ok\"));\napp.MapGet(\"/ping\", () =&gt; Results.Text(\"ok\"));\napp.MapGet(\"/slow\", async () =&gt;\n{\n    await Task.Delay(50);\n    return Results.Text(\"ok\");\n});\n\nvar url = Environment.GetEnvironmentVariable(\"HTTPLEAK_URL\") ?? \"http://localhost:5055\";\napp.Urls.Add(url);\n\napp.Lifetime.ApplicationStarted.Register(() =&gt;\n{\n    Console.WriteLine($\"Listening on {url}\");\n});\n\nawait app.RunAsync();\n</code></pre>\n<h3 id=\"客户端httpleakclientnet48net6net8net10-共用一份\">客户端：HttpLeakClient（net48/net6/net8/net10 共用一份）</h3>\n<h4 id=\"clientshttpleakclienthttpleakclientcsproj示意多目标--条件依赖\"><code>Clients/HttpLeakClient/HttpLeakClient.csproj</code>（示意：多目标 + 条件依赖）</h4>\n<pre><code class=\"language-xml\">&lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt;\n  &lt;PropertyGroup&gt;\n    &lt;OutputType&gt;Exe&lt;/OutputType&gt;\n    &lt;TargetFrameworks&gt;net48;net6.0;net8.0;net10.0&lt;/TargetFrameworks&gt;\n    &lt;LangVersion&gt;latest&lt;/LangVersion&gt;\n    &lt;Nullable&gt;enable&lt;/Nullable&gt;\n    &lt;ImplicitUsings&gt;enable&lt;/ImplicitUsings&gt;\n  &lt;/PropertyGroup&gt;\n\n  &lt;ItemGroup Condition=\"'$(TargetFramework)' == 'net48'\"&gt;\n    &lt;Reference Include=\"System.Net.Http\" /&gt;\n  &lt;/ItemGroup&gt;\n\n  &lt;ItemGroup Condition=\"'$(TargetFramework)' == 'net10.0'\"&gt;\n    &lt;PackageReference Include=\"Microsoft.Extensions.Http\" Version=\"10.0.0\" /&gt;\n  &lt;/ItemGroup&gt;\n&lt;/Project&gt;\n</code></pre>\n<h4 id=\"clientshttpleakclientprogramcs用-if-表示差异\"><code>Clients/HttpLeakClient/Program.cs</code>（用 <code>#if</code> 表示差异）</h4>\n<pre><code class=\"language-csharp\">using System.Diagnostics;\nusing System.Net.Http;\n\n#if NET48\nusing System.Net;\n#endif\n\n#if NET10_0_OR_GREATER\nusing Microsoft.Extensions.DependencyInjection;\n#endif\n\nstatic string? GetArg(string[] args, string name)\n{\n    for (var i = 0; i &lt; args.Length - 1; i++)\n    {\n        if (string.Equals(args[i], name, StringComparison.OrdinalIgnoreCase))\n        {\n            return args[i + 1];\n        }\n    }\n\n    return null;\n}\n\nstatic int GetArgInt(string[] args, string name, int defaultValue)\n{\n    var value = GetArg(args, name);\n    return int.TryParse(value, out var parsed) ? parsed : defaultValue;\n}\n\nvar url = GetArg(args, \"--url\") ?? \"http://localhost:5055/ping\";\nvar requests = GetArgInt(args, \"--requests\", 20000);\nvar parallel = GetArgInt(args, \"--parallel\", 200);\nvar logEvery = GetArgInt(args, \"--logEvery\", 1000);\nvar timeoutSeconds = GetArgInt(args, \"--timeoutSeconds\", 5);\n\n#if NET10_0_OR_GREATER\nvar mode = GetArg(args, \"--mode\") ?? \"new\"; // new | static | factory\n#endif\n\nConsole.WriteLine($\"url={url}\");\n#if NET10_0_OR_GREATER\nConsole.WriteLine($\"requests={requests}, parallel={parallel}, timeoutSeconds={timeoutSeconds}, mode={mode}\");\n#else\nConsole.WriteLine($\"requests={requests}, parallel={parallel}, timeoutSeconds={timeoutSeconds}\");\n#endif\n\n#if NET48\nServicePointManager.DefaultConnectionLimit = 1000;\nServicePointManager.Expect100Continue = false;\n#endif\n\nvar throttler = new SemaphoreSlim(parallel);\nvar tasks = new List&lt;Task&gt;(requests);\nvar sw = Stopwatch.StartNew();\nvar success = 0;\nvar failed = 0;\n\n#if NET10_0_OR_GREATER\nHttpClient? staticClient = null;\nif (string.Equals(mode, \"static\", StringComparison.OrdinalIgnoreCase))\n{\n    staticClient = new HttpClient\n    {\n        Timeout = TimeSpan.FromSeconds(timeoutSeconds)\n    };\n}\n\nIHttpClientFactory? httpClientFactory = null;\nServiceProvider? serviceProvider = null;\nif (string.Equals(mode, \"factory\", StringComparison.OrdinalIgnoreCase))\n{\n    var services = new ServiceCollection();\n    services.AddHttpClient();\n    serviceProvider = services.BuildServiceProvider();\n    httpClientFactory = serviceProvider.GetRequiredService&lt;IHttpClientFactory&gt;();\n}\n#endif\n\nfor (var i = 0; i &lt; requests; i++)\n{\n    await throttler.WaitAsync();\n    var index = i + 1;\n    tasks.Add(Task.Run(async () =&gt;\n    {\n        try\n        {\n#if NET10_0_OR_GREATER\n            HttpClient client;\n            if (staticClient != null)\n            {\n                client = staticClient;\n            }\n            else if (httpClientFactory != null)\n            {\n                client = httpClientFactory.CreateClient();\n                client.Timeout = TimeSpan.FromSeconds(timeoutSeconds);\n            }\n            else\n            {\n                client = new HttpClient();\n                client.Timeout = TimeSpan.FromSeconds(timeoutSeconds);\n            }\n\n            using var response = await client.GetAsync(url);\n            response.EnsureSuccessStatusCode();\n            Interlocked.Increment(ref success);\n\n            if (staticClient == null &amp;&amp; httpClientFactory == null)\n            {\n                client.Dispose();\n            }\n#else\n            using var client = new HttpClient();\n            client.Timeout = TimeSpan.FromSeconds(timeoutSeconds);\n            using var response = await client.GetAsync(url);\n            response.EnsureSuccessStatusCode();\n            Interlocked.Increment(ref success);\n#endif\n        }\n        catch (Exception ex)\n        {\n            var fail = Interlocked.Increment(ref failed);\n            if (fail &lt;= 5)\n            {\n                Console.WriteLine($\"ERR#{fail}: {ex.GetType().Name} {ex.Message}\");\n            }\n        }\n        finally\n        {\n            throttler.Release();\n        }\n    }));\n\n    if (index % logEvery == 0)\n    {\n        Console.WriteLine($\"queued: {index}/{requests}, success: {Volatile.Read(ref success)}, failed: {Volatile.Read(ref failed)}, elapsed: {sw.Elapsed}\");\n    }\n}\n\nawait Task.WhenAll(tasks);\nConsole.WriteLine($\"done: success={success}, failed={failed}, elapsed={sw.Elapsed}\");\n\n#if NET10_0_OR_GREATER\nstaticClient?.Dispose();\nserviceProvider?.Dispose();\n#endif\n</code></pre>\n<h3 id=\"powershell-脚本\">PowerShell 脚本</h3>\n<p>脚本我就不在文章里全文贴了：</p>\n<ul>\n<li>外部启动 Server 版本：<code>https://github.com/sdcb/http-client-exp/blob/main/scripts/run-experiment-external-server.ps1</code></li>\n<li>脚本内启动 Server 版本：<code>https://github.com/sdcb/http-client-exp/blob/main/scripts/run-experiment.ps1</code></li>\n</ul>\n<hr />\n<h2 id=\"原始实验结果日志完整贴出\">原始实验结果（日志，完整贴出）</h2>\n<blockquote>\n<p>日志目录：<code>logs/run-20260119-095017</code></p>\n</blockquote>\n<h3 id=\"experiment-summarylog\"><code>experiment-summary.log</code></h3>\n<pre><code class=\"language-text\">[2026-01-19T09:51:13.7180857+08:00] net48 ExitCode=0 OutLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net48.out.log ErrLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net48.err.log\n[2026-01-19T09:53:33.0362523+08:00] net6 ExitCode=0 OutLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net6.out.log ErrLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net6.err.log\n[2026-01-19T09:56:00.4574527+08:00] net8 ExitCode=0 OutLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net8.out.log ErrLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net8.err.log\n[2026-01-19T09:58:27.2618139+08:00] net10-new ExitCode=0 OutLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net10-new.out.log ErrLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net10-new.err.log\n[2026-01-19T10:00:53.0490472+08:00] net10-static ExitCode=0 OutLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net10-static.out.log ErrLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net10-static.err.log\n[2026-01-19T10:01:10.2096164+08:00] net10-factory ExitCode=0 OutLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net10-factory.out.log ErrLog=C:\\Users\\ZhouJie\\source\\repos\\http-client-exp\\logs\\\\run-20260119-095017\\net10-factory.err.log\n</code></pre>\n<h3 id=\"netstatlog\"><code>netstat.log</code></h3>\n<pre><code class=\"language-text\">[2026-01-19T09:50:55.6627455+08:00] before-net48 TIME_WAIT=2 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T09:51:15.4686603+08:00] after-net48 TIME_WAIT=20002 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T09:53:13.9647485+08:00] before-net6 TIME_WAIT=0 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T09:53:33.8112294+08:00] after-net6 TIME_WAIT=20000 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T09:55:42.6435015+08:00] before-net8 TIME_WAIT=0 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T09:56:01.1900000+08:00] after-net8 TIME_WAIT=18361 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T09:58:08.8603129+08:00] before-net10-new TIME_WAIT=0 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T09:58:28.1500737+08:00] after-net10-new TIME_WAIT=18860 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T10:00:36.1038757+08:00] before-net10-static TIME_WAIT=0 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T10:00:53.1007259+08:00] after-net10-static TIME_WAIT=200 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T10:00:53.2458798+08:00] before-net10-factory TIME_WAIT=200 ESTABLISHED=0 CLOSE_WAIT=0\n[2026-01-19T10:01:10.2574678+08:00] after-net10-factory TIME_WAIT=200 ESTABLISHED=0 CLOSE_WAIT=0\n</code></pre>\n<h3 id=\"cooldownlog\"><code>cooldown.log</code></h3>\n<pre><code class=\"language-text\">[2026-01-19T09:51:16.2687666+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:51:27.0427805+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:51:37.8184382+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:51:48.5404492+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:51:59.2846968+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:52:10.1562079+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:52:21.0588622+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:52:31.7777704+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:52:42.5664350+08:00] cooldown-net48 TIME_WAIT=20002 baseline=2 limit=202\n[2026-01-19T09:52:53.3299614+08:00] cooldown-net48 TIME_WAIT=20001 baseline=2 limit=202\n[2026-01-19T09:53:03.8109734+08:00] cooldown-net48 TIME_WAIT=10752 baseline=2 limit=202\n[2026-01-19T09:53:13.8560831+08:00] cooldown-net48 TIME_WAIT=0 baseline=2 limit=202\n[2026-01-19T09:53:34.6370711+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:53:45.5166974+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:53:56.2899736+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:54:07.0165428+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:54:17.8872137+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:54:28.7673529+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:54:39.5200091+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:54:50.3090659+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:55:01.0638920+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:55:11.7529528+08:00] cooldown-net6 TIME_WAIT=20000 baseline=0 limit=200\n[2026-01-19T09:55:22.3886445+08:00] cooldown-net6 TIME_WAIT=12540 baseline=0 limit=200\n[2026-01-19T09:55:32.4658786+08:00] cooldown-net6 TIME_WAIT=435 baseline=0 limit=200\n[2026-01-19T09:55:42.5354878+08:00] cooldown-net6 TIME_WAIT=0 baseline=0 limit=200\n[2026-01-19T09:56:01.8876493+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:56:12.6341541+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:56:23.2806604+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:56:33.9425873+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:56:44.6277561+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:56:55.3013674+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:57:06.0278128+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:57:16.7315578+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:57:27.3887925+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:57:38.0442453+08:00] cooldown-net8 TIME_WAIT=18361 baseline=0 limit=200\n[2026-01-19T09:57:48.6566839+08:00] cooldown-net8 TIME_WAIT=12989 baseline=0 limit=200\n[2026-01-19T09:57:58.7347337+08:00] cooldown-net8 TIME_WAIT=1069 baseline=0 limit=200\n[2026-01-19T09:58:08.7780660+08:00] cooldown-net8 TIME_WAIT=0 baseline=0 limit=200\n[2026-01-19T09:58:29.0261291+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T09:58:39.7491043+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T09:58:50.4613796+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T09:59:01.2064545+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T09:59:11.8710153+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T09:59:22.5648486+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T09:59:33.2755958+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T09:59:43.9631547+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T09:59:54.6979243+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T10:00:05.3961397+08:00] cooldown-net10-new TIME_WAIT=18860 baseline=0 limit=200\n[2026-01-19T10:00:15.8607378+08:00] cooldown-net10-new TIME_WAIT=11840 baseline=0 limit=200\n[2026-01-19T10:00:25.9471407+08:00] cooldown-net10-new TIME_WAIT=614 baseline=0 limit=200\n[2026-01-19T10:00:36.0111701+08:00] cooldown-net10-new TIME_WAIT=0 baseline=0 limit=200\n[2026-01-19T10:00:53.1510649+08:00] cooldown-net10-static TIME_WAIT=200 baseline=0 limit=200\n[2026-01-19T10:01:10.3057785+08:00] cooldown-net10-factory TIME_WAIT=200 baseline=200 limit=400\n</code></pre>\n<h3 id=\"net48outlog\"><code>net48.out.log</code></h3>\n<pre><code class=\"language-text\">url=http://localhost:5055/ping\nrequests=20000, parallel=200, timeoutSeconds=5\nqueued: 1000/20000, success: 803, failed: 0, elapsed: 00:00:00.7170409\nqueued: 2000/20000, success: 1803, failed: 0, elapsed: 00:00:01.5455465\nqueued: 3000/20000, success: 2800, failed: 0, elapsed: 00:00:02.3559601\nqueued: 4000/20000, success: 3801, failed: 0, elapsed: 00:00:03.1925172\nqueued: 5000/20000, success: 4802, failed: 0, elapsed: 00:00:04.0184093\nqueued: 6000/20000, success: 5802, failed: 0, elapsed: 00:00:04.8348862\nqueued: 7000/20000, success: 6800, failed: 0, elapsed: 00:00:05.6775563\nqueued: 8000/20000, success: 7801, failed: 0, elapsed: 00:00:06.5458377\nqueued: 9000/20000, success: 8802, failed: 0, elapsed: 00:00:07.4397727\nqueued: 10000/20000, success: 9803, failed: 0, elapsed: 00:00:08.3348468\nqueued: 11000/20000, success: 10802, failed: 0, elapsed: 00:00:09.2035416\nqueued: 12000/20000, success: 11804, failed: 0, elapsed: 00:00:10.0393223\nqueued: 13000/20000, success: 12806, failed: 0, elapsed: 00:00:10.8262272\nqueued: 14000/20000, success: 13804, failed: 0, elapsed: 00:00:11.6636597\nqueued: 15000/20000, success: 14802, failed: 0, elapsed: 00:00:12.4799716\nqueued: 16000/20000, success: 15819, failed: 0, elapsed: 00:00:13.4697834\nqueued: 17000/20000, success: 16822, failed: 0, elapsed: 00:00:14.5332049\nqueued: 18000/20000, success: 17818, failed: 0, elapsed: 00:00:15.5317397\nqueued: 19000/20000, success: 18816, failed: 0, elapsed: 00:00:16.5405963\nqueued: 20000/20000, success: 19819, failed: 0, elapsed: 00:00:17.6652081\ndone: success=20000, failed=0, elapsed=00:00:17.7541839\n</code></pre>\n<h3 id=\"net6outlog\"><code>net6.out.log</code></h3>\n<pre><code class=\"language-text\">url=http://localhost:5055/ping\nrequests=20000, parallel=200, timeoutSeconds=5\nqueued: 1000/20000, success: 803, failed: 0, elapsed: 00:00:00.7159800\nqueued: 2000/20000, success: 1803, failed: 0, elapsed: 00:00:01.6028433\nqueued: 3000/20000, success: 2802, failed: 0, elapsed: 00:00:02.4876237\nqueued: 4000/20000, success: 3802, failed: 0, elapsed: 00:00:03.3549543\nqueued: 5000/20000, success: 4802, failed: 0, elapsed: 00:00:04.2710795\nqueued: 6000/20000, success: 5801, failed: 0, elapsed: 00:00:05.1637653\nqueued: 7000/20000, success: 6802, failed: 0, elapsed: 00:00:06.0729777\nqueued: 8000/20000, success: 7802, failed: 0, elapsed: 00:00:07.0697555\nqueued: 9000/20000, success: 8801, failed: 0, elapsed: 00:00:08.0143162\nqueued: 10000/20000, success: 9800, failed: 0, elapsed: 00:00:08.9633860\nqueued: 11000/20000, success: 10802, failed: 0, elapsed: 00:00:09.9344239\nqueued: 12000/20000, success: 11802, failed: 0, elapsed: 00:00:10.8379783\nqueued: 13000/20000, success: 12801, failed: 0, elapsed: 00:00:11.6810601\nqueued: 14000/20000, success: 13801, failed: 0, elapsed: 00:00:12.5122642\nqueued: 15000/20000, success: 14800, failed: 0, elapsed: 00:00:13.3692282\nqueued: 16000/20000, success: 15803, failed: 0, elapsed: 00:00:14.2782829\nqueued: 17000/20000, success: 16801, failed: 0, elapsed: 00:00:15.2187642\nqueued: 18000/20000, success: 17802, failed: 0, elapsed: 00:00:16.0811154\nqueued: 19000/20000, success: 18810, failed: 0, elapsed: 00:00:16.9798536\nqueued: 20000/20000, success: 19817, failed: 0, elapsed: 00:00:17.8952478\ndone: success=20000, failed=0, elapsed=00:00:18.0175351\n</code></pre>\n<h3 id=\"net8outlog\"><code>net8.out.log</code></h3>\n<pre><code class=\"language-text\">url=http://localhost:5055/ping\nrequests=20000, parallel=200, timeoutSeconds=5\nqueued: 1000/20000, success: 800, failed: 0, elapsed: 00:00:00.8405997\nqueued: 2000/20000, success: 1802, failed: 0, elapsed: 00:00:01.6913251\nqueued: 3000/20000, success: 2800, failed: 0, elapsed: 00:00:02.6176324\nqueued: 4000/20000, success: 3802, failed: 0, elapsed: 00:00:03.4744051\nqueued: 5000/20000, success: 4801, failed: 0, elapsed: 00:00:04.2873345\nqueued: 6000/20000, success: 5800, failed: 0, elapsed: 00:00:05.0960137\nqueued: 7000/20000, success: 6803, failed: 0, elapsed: 00:00:05.8940500\nqueued: 8000/20000, success: 7802, failed: 0, elapsed: 00:00:06.7207568\nqueued: 9000/20000, success: 8803, failed: 0, elapsed: 00:00:07.5346954\nqueued: 10000/20000, success: 9800, failed: 0, elapsed: 00:00:08.3507870\nqueued: 11000/20000, success: 10802, failed: 0, elapsed: 00:00:09.1985154\nqueued: 12000/20000, success: 11803, failed: 0, elapsed: 00:00:10.0035216\nqueued: 13000/20000, success: 12803, failed: 0, elapsed: 00:00:10.7763067\nqueued: 14000/20000, success: 13802, failed: 0, elapsed: 00:00:11.6251384\nqueued: 15000/20000, success: 14802, failed: 0, elapsed: 00:00:12.4436652\nqueued: 16000/20000, success: 15807, failed: 0, elapsed: 00:00:13.3164599\nqueued: 17000/20000, success: 16801, failed: 0, elapsed: 00:00:14.2231530\nERR#1: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nERR#2: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nERR#3: HttpRequestException 通常每个套接���地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nERR#5: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nERR#4: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nqueued: 18000/20000, success: 17605, failed: 201, elapsed: 00:00:15.0862152\nqueued: 19000/20000, success: 17711, failed: 1090, elapsed: 00:00:15.6168905\nqueued: 20000/20000, success: 18165, failed: 1639, elapsed: 00:00:16.3332350\ndone: success=18361, failed=1639, elapsed=00:00:16.4268168\n</code></pre>\n<h3 id=\"net10-newoutlog\"><code>net10-new.out.log</code></h3>\n<pre><code class=\"language-text\">url=http://localhost:5055/ping\nrequests=20000, parallel=200, timeoutSeconds=5, mode=new\nqueued: 1000/20000, success: 805, failed: 0, elapsed: 00:00:00.7099803\nqueued: 2000/20000, success: 1800, failed: 0, elapsed: 00:00:01.5324361\nqueued: 3000/20000, success: 2800, failed: 0, elapsed: 00:00:02.3573877\nqueued: 4000/20000, success: 3800, failed: 0, elapsed: 00:00:03.2069000\nqueued: 5000/20000, success: 4800, failed: 0, elapsed: 00:00:04.0313423\nqueued: 6000/20000, success: 5802, failed: 0, elapsed: 00:00:04.8687039\nqueued: 7000/20000, success: 6801, failed: 0, elapsed: 00:00:05.7252572\nqueued: 8000/20000, success: 7800, failed: 0, elapsed: 00:00:06.5624078\nqueued: 9000/20000, success: 8800, failed: 0, elapsed: 00:00:07.4244971\nqueued: 10000/20000, success: 9800, failed: 0, elapsed: 00:00:08.2911306\nqueued: 11000/20000, success: 10800, failed: 0, elapsed: 00:00:09.1755667\nqueued: 12000/20000, success: 11801, failed: 0, elapsed: 00:00:10.1160925\nqueued: 13000/20000, success: 12802, failed: 0, elapsed: 00:00:11.0165038\nqueued: 14000/20000, success: 13801, failed: 0, elapsed: 00:00:11.9382103\nqueued: 15000/20000, success: 14802, failed: 0, elapsed: 00:00:12.8002543\nqueued: 16000/20000, success: 15801, failed: 0, elapsed: 00:00:13.7185127\nERR#3: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nERR#2: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nERR#1: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nERR#5: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nERR#4: HttpRequestException 通常每个套接字地址(协议/网络地址/端口)只允许使用一次。 (localhost:5055)\nqueued: 17000/20000, success: 16682, failed: 118, elapsed: 00:00:14.7578539\nqueued: 18000/20000, success: 16684, failed: 1118, elapsed: 00:00:15.4297844\nqueued: 19000/20000, success: 17661, failed: 1140, elapsed: 00:00:16.2796589\nqueued: 20000/20000, success: 18661, failed: 1140, elapsed: 00:00:17.3258406\ndone: success=18860, failed=1140, elapsed=00:00:17.3996862\n</code></pre>\n<h3 id=\"net10-staticoutlog\"><code>net10-static.out.log</code></h3>\n<pre><code class=\"language-text\">url=http://localhost:5055/ping\nrequests=20000, parallel=200, timeoutSeconds=5, mode=static\nqueued: 1000/20000, success: 801, failed: 0, elapsed: 00:00:00.6551470\nqueued: 2000/20000, success: 1800, failed: 0, elapsed: 00:00:01.5355394\nqueued: 3000/20000, success: 2800, failed: 0, elapsed: 00:00:02.3480456\nqueued: 4000/20000, success: 3800, failed: 0, elapsed: 00:00:03.0999050\nqueued: 5000/20000, success: 4800, failed: 0, elapsed: 00:00:03.8470820\nqueued: 6000/20000, success: 5800, failed: 0, elapsed: 00:00:04.5759884\nqueued: 7000/20000, success: 6800, failed: 0, elapsed: 00:00:05.3461919\nqueued: 8000/20000, success: 7801, failed: 0, elapsed: 00:00:06.0916621\nqueued: 9000/20000, success: 8802, failed: 0, elapsed: 00:00:06.9085343\nqueued: 10000/20000, success: 9800, failed: 0, elapsed: 00:00:07.8274125\nqueued: 11000/20000, success: 10800, failed: 0, elapsed: 00:00:08.6757231\nqueued: 12000/20000, success: 11800, failed: 0, elapsed: 00:00:09.5154490\nqueued: 13000/20000, success: 12800, failed: 0, elapsed: 00:00:10.3306765\nqueued: 14000/20000, success: 13800, failed: 0, elapsed: 00:00:11.1493724\nqueued: 15000/20000, success: 14800, failed: 0, elapsed: 00:00:11.9658212\nqueued: 16000/20000, success: 15801, failed: 0, elapsed: 00:00:12.7510706\nqueued: 17000/20000, success: 16800, failed: 0, elapsed: 00:00:13.4735304\nqueued: 18000/20000, success: 17800, failed: 0, elapsed: 00:00:14.2777953\nqueued: 19000/20000, success: 18800, failed: 0, elapsed: 00:00:15.0219907\nqueued: 20000/20000, success: 19801, failed: 0, elapsed: 00:00:15.7699702\ndone: success=20000, failed=0, elapsed=00:00:15.8507052\n</code></pre>\n<h3 id=\"net10-factoryoutlog\"><code>net10-factory.out.log</code></h3>\n<pre><code class=\"language-text\">url=http://localhost:5055/ping\nrequests=20000, parallel=200, timeoutSeconds=5, mode=factory\nqueued: 1000/20000, success: 801, failed: 0, elapsed: 00:00:00.7094395\nqueued: 2000/20000, success: 1801, failed: 0, elapsed: 00:00:01.5072383\nqueued: 3000/20000, success: 2800, failed: 0, elapsed: 00:00:02.3047647\nqueued: 4000/20000, success: 3800, failed: 0, elapsed: 00:00:03.0607252\nqueued: 5000/20000, success: 4800, failed: 0, elapsed: 00:00:03.8370598\nqueued: 6000/20000, success: 5800, failed: 0, elapsed: 00:00:04.6621606\nqueued: 7000/20000, success: 6800, failed: 0, elapsed: 00:00:05.4589104\nqueued: 8000/20000, success: 7800, failed: 0, elapsed: 00:00:06.2913588\nqueued: 9000/20000, success: 8800, failed: 0, elapsed: 00:00:07.0629536\nqueued: 10000/20000, success: 9800, failed: 0, elapsed: 00:00:07.8438472\nqueued: 11000/20000, success: 10800, failed: 0, elapsed: 00:00:08.5796209\nqueued: 12000/20000, success: 11800, failed: 0, elapsed: 00:00:09.3663975\nqueued: 13000/20000, success: 12801, failed: 0, elapsed: 00:00:10.1984757\nqueued: 14000/20000, success: 13800, failed: 0, elapsed: 00:00:11.0474925\nqueued: 15000/20000, success: 14800, failed: 0, elapsed: 00:00:11.9175753\nqueued: 16000/20000, success: 15800, failed: 0, elapsed: 00:00:12.7356429\nqueued: 17000/20000, success: 16801, failed: 0, elapsed: 00:00:13.4991148\nqueued: 18000/20000, success: 17800, failed: 0, elapsed: 00:00:14.2912941\nqueued: 19000/20000, success: 18800, failed: 0, elapsed: 00:00:15.0844828\nqueued: 20000/20000, success: 19800, failed: 0, elapsed: 00:00:15.8439387\ndone: success=20000, failed=0, elapsed=00:00:15.9485251\n</code></pre>\n<hr />\n<h2 id=\"局限与备注别把结论用错地方\">局限与备注（别把结论用错地方）</h2>\n<ol>\n<li><strong>客户端与服务器同机</strong>：TIME_WAIT 统计包含两端连接，不能完全归因于“客户端端口耗尽”，但它足以说明“短时间制造大量短连接”这件事本身的风险。</li>\n<li><strong>net48 设置了 <code>ServicePointManager.DefaultConnectionLimit = 1000</code></strong>，与 net6+/net10 的连接管理策略存在差异。</li>\n</ol>\n<hr />\n<h2 id=\"关键结果汇总\">关键结果汇总</h2>\n<h3 id=\"time_wait-统计端口-5055\">TIME_WAIT 统计（端口 5055）</h3>\n<table>\n<thead>\n<tr>\n<th>运行</th>\n<th style=\"text-align: right;\">before TIME_WAIT</th>\n<th style=\"text-align: right;\">after TIME_WAIT</th>\n<th style=\"text-align: right;\">耗时（秒）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>net48</td>\n<td style=\"text-align: right;\">2</td>\n<td style=\"text-align: right;\">20002</td>\n<td style=\"text-align: right;\">17.754</td>\n</tr>\n<tr>\n<td>net6</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">20000</td>\n<td style=\"text-align: right;\">18.018</td>\n</tr>\n<tr>\n<td>net8</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">18361</td>\n<td style=\"text-align: right;\">16.427</td>\n</tr>\n<tr>\n<td>net10</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">18860</td>\n<td style=\"text-align: right;\">17.400</td>\n</tr>\n<tr>\n<td>net10-static</td>\n<td style=\"text-align: right;\">0</td>\n<td style=\"text-align: right;\">200</td>\n<td style=\"text-align: right;\">15.851</td>\n</tr>\n<tr>\n<td>net10-factory</td>\n<td style=\"text-align: right;\">200</td>\n<td style=\"text-align: right;\">200</td>\n<td style=\"text-align: right;\">15.949</td>\n</tr>\n</tbody>\n</table>\n<p><img alt=\"elapsed_seconds\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260119112313722-548103397.png\" /><br />\n<img alt=\"time_wait_delta\" src=\"https://img2024.cnblogs.com/blog/233608/202601/233608-20260119112309621-529653637.png\" /></p>\n<p>来源：<code>logs/run-20260119-095017/netstat.log</code></p>\n<h3 id=\"客户端执行结果摘要\">客户端执行结果（摘要）</h3>\n<ul>\n<li>net48: success=20000, failed=0</li>\n<li>net6: success=20000, failed=0</li>\n<li>net8: success=18361, failed=1639（报错：“通常每个套接字地址只允许使用一次”）</li>\n<li>net10-new: success=18860, failed=1140（报错：“通常每个套接字地址只允许使用一次”）</li>\n<li>net10-static: success=20000, failed=0</li>\n<li>net10-factory: success=20000, failed=0</li>\n</ul>\n<p>来源：<code>logs/run-20260119-095017/*.out.log</code></p>\n<hr />\n<h2 id=\"结论\">结论</h2>\n<p>“.NET 10 了，HttpClient 还能不能 using？”——答案依然是：<strong>别把 HttpClient 当成一次性对象</strong>。</p>\n<p>你可以 <code>using</code> 的是请求/响应相关的对象（例如 <code>HttpResponseMessage</code>），但 <code>HttpClient</code> 本身更像一个“连接池的门面”：它越复用，越稳定，越不容易把你推向 TIME_WAIT 地狱。</p>\n<p>另外，从本次压测耗时来看，<strong>不做“每次请求都 using/new HttpClient”</strong> 的写法，速度其实还会稍微快一丢丢（当然差距很小）。</p>\n<p>感谢阅读！如果你觉得这些实验分析有意思，或者对 .NET 高性能编程感兴趣，欢迎在评论区留言交流，也欢迎加入我的 <strong>.NET骚操作 QQ群：495782587</strong>，我们一起探索更多技术硬核玩法。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-23 08:45</span>&nbsp;\n<a href=\"https://www.cnblogs.com/sdcb\">.NET骚操作</a>&nbsp;\n阅读(<span id=\"post_view_count\">730</span>)&nbsp;\n评论(<span id=\"post_comment_count\">10</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "火爆全网的Agent Skills，普通人到底该怎么用？-- 详细教程",
      "link": "https://www.cnblogs.com/jinjiangongzuoshi/p/19520101",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jinjiangongzuoshi/p/19520101\" id=\"cb_post_title_url\" title=\"发布于 2026-01-23 08:38\">\n    <span>火爆全网的Agent Skills，普通人到底该怎么用？-- 详细教程</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>最近，AI Agent Skills 简直火到出圈，说一句 “全网刷屏” 都毫不夸张！</p>\n<p>如今，越来越多 AI 产品都已接入 Skills 功能 —— 从 Claude Code、OpenCode 到 CodeX、Antigravity，再到最近刚升级的 Coze 2.0，几乎成了主流 AI 工具的 “标配”。</p>\n<p>但问题来了：<strong>火爆全网的 Skills，普通人到底该怎么用？</strong></p>\n<p>你会发现，除了 Coze 2.0 之外，绝大多数支持 Skills 的工具，本质都是 AI 编程工具 —— 不是 CLI 命令行，就是 IDE 开发环境。这对新手小白、非技术人员来说，门槛实在太高了！</p>\n<p>对普通人而言，不管是部署还是使用，都堪称 “拦路虎”：一旦涉及本地配置、命令行操作，就直接把大多数人挡在了门外。</p>\n<p>毕竟，真没多少人愿意花时间去修改、编写 Skills 相关文件 —— 这东西说复杂不算极致复杂，但对非专业人士来说，光是理解文件结构就够头疼。现在愿意在这些工具里折腾 Skills 的，基本都是偏极客、偏专业的技术玩家。</p>\n<p>而对更多朋友来说，困惑简直无处不在：</p>\n<ul>\n<li>安装步骤看不懂，半天装不上；</li>\n<li>终端是什么？完全没概念；</li>\n<li>Skills 的文件夹藏在哪，翻遍电脑找不到；</li>\n<li>Github 上的开源项目全是代码，根本看不懂怎么改成 Skills；</li>\n<li>想自己创建一个 Skill，却连入门的门都摸不到……</li>\n</ul>\n<p>所以今天，我就来分享一种 <strong>最简单、最便捷的 Skill 构建方法 —— 直接用嘴说，就能搞定！</strong> 全程不用碰代码、不用搞配置，特别适合新手小白和非技术人员。</p>\n<p>如果有对Skills还不了解的朋友，可以去看我的这篇文章：<a href=\"https://mp.weixin.qq.com/s/CFjpnEo9eOoaLYx2OqQ0kg\" rel=\"noopener nofollow\" target=\"_blank\">最近很火爆的Claude Skills到底是个啥？解决什么问题？怎么用！</a></p>\n<h2 id=\"具体操作\">具体操作</h2>\n<p>1、以Claude Code为例，其它AI工具也类似，先安装并进入到<code>Claude Code</code> 交互终端，如果还不知道怎么安装Claude Code，可以看这篇：<a href=\"https://mp.weixin.qq.com/s/PyqhVgbkNBzd7grKN5IxgQ\" rel=\"noopener nofollow\" target=\"_blank\">99% 的人都不知道的 Claude Code 使用技巧！</a><br />\n<img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121153305205.png\" /></p>\n<p>2、在CC交互中，输入提示词：</p>\n<pre><code>帮我安装skill-creator\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121153811834.png\" /></p>\n<p>3、比如我想开发一个图片转换、压缩的工具，希望把这个能力封装成一个可复用的skill技能，只需要在CC中输入提示词：</p>\n<pre><code>请帮我把开源项目https://github.com/lmageMagick/lmageMagick,打包成一个Skill，方便我以后直接调用它来对我的图片素材进行处理，可以比如格式转换，分辨率修改、压缩等等\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121160320245.png\" /></p>\n<p>4、大概3~4分钟左右后，Claude Code就帮我自动创建好了一个： <code>ImageMagick 图片处理 Skill</code><br />\n<img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121160733521.png\" /></p>\n<p>5、本地上传一张图片，测试一下</p>\n<pre><code>请将附件图片格式转化JPG，图片大小压缩一下，并保存到原始图片同目录下\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121170000979.png\" /></p>\n<p>可以看到，已经可以成功调用刚刚新构建的skill对图片格式转换以及图片大小压缩了<br />\n<img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121183417063.png\" /></p>\n<p>可以看到，真的只用了4步，就构建了一个全新的图片处理skill技能。</p>\n<h2 id=\"任何可复用的技能都建议skill化\">任何可复用的技能，都建议Skill化</h2>\n<p>这是我非常强调并同意的观点，任何会重复3次且以上的的，或者任何可以复用的能力，都建议抽象成技能，一定要Skill化。</p>\n<p>之前，要借助外力的，都是各种找工具，下载、安装工具，有了Skill之后，你可以很轻松的将其转化为AI的Skill技能。</p>\n<p>比如之前，大家会用到的各种格式工厂、图片压缩、视频下载之类的工具。这些工具，绝大多数都是把一些大佬的开源工具，封装一下包装成单独的产品。没有AI之前，每次就是去Google、百度各种搜。现在我们有了Skill后，我们可以结合开源项目，很容易就能做出相对应的skill了。</p>\n<p>Github上有很多优秀的开源项目，但对于普通人来讲，如何知道该用什么特定的开源项目来处理特定的事情呢？比如上面我们构建的图片压缩工具，只有你事先知道了有<code>ImageMagick</code>这个项目且知道它的开源地址（<code>https://github.com/lmageMagick/lmageMagick</code>），你才能把它喂给AI封装成Skill，但大多数普通人，有些可能连github是什么都不知道，那怎么封装呢？</p>\n<p>其实针对这个问题，你同样可以问AI，比如向AI描述你的需求，且问AI，在Github上有什么比较好的开源项目能帮我处理这类需求，然后给出项目链接。然后我们，再根据AI回复的结果，每个进去稍微看一下star数和内容是否符合自己需求就可以了。</p>\n<p><strong>举一个例子：</strong></p>\n<p>我相信大家经常都有一个需求，就是去各种视频网站上下载视频，比如视频号、抖音、小红书、Youtube、B站等。</p>\n<p>然后直接提出你的问题：</p>\n<blockquote>\n<p>在Github上有没有那种针对各种视频网站上的视频进行视频下载比较好的项目，比如支持视频号、抖音、小红书、Youtube、B站等视频下载，请给出项目名称，项目地址，点赞数等</p>\n</blockquote>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121194423030.png\" /></p>\n<p>不到一分钟的时间，就帮我自动整理出来10个推荐的开源项目<br />\n<img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121194454299.png\" /></p>\n<p>排名第一，也是最为推荐的项目当属<code>yt-dlp</code>，全平台通用，也是功能最强大的视频下载工具，支持平台: YouTube, B站, 抖音, 快手, 优酷, 腾讯视频, 爱奇艺等 1000+ 网站<br />\n<img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121194729405.png\" /></p>\n<p>为什么说<code>yt-dlp</code>最为推荐？不讲其它的，光Github上<code>143k</code>的star，就能充分证明这点，说这个项目是Github上最伟大的项目之一，也不为过。</p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121194640967.png\" /></p>\n<p>项目地址: <code> https://github.com/yt-dlp/yt-dlp</code></p>\n<p>你要相信，你的需求，永远不是这个世界上第一个提出这个需求的人，也绝对不是最后一个。</p>\n<p>然后把下面这段提示词，发给你装好了skill-creator的Claude Code：</p>\n<pre><code>帮我把这个开源工具https://github.com/yt-dlp/yt-dlp打包成一个Skill，只要我后续给出视频链接，就可以帮我下载视频。\n</code></pre>\n<p>接着，你只需要喝杯咖啡，等它完成就可以了<br />\n<img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121195246346.png\" /></p>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121200457340.png\" /></p>\n<p>接下来，我们随便找一个视频，测试验证一下</p>\n<p>就在小红书找一个韩立结婴的视频（哈哈，各位道友应该都看过这集吧）<br />\n<img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121200802848.png\" /></p>\n<p>选择第一个视频，复制链接，输入下面的提示词：</p>\n<pre><code>帮我下载这个视频:\nhttps://www.xiaohongshu.com/discovery/item/692ac48b000000001e031b6c?source=webshare&amp;xhsshare=pc_web&amp;xsec_token=ABb9ZAeT2LidNaRo8XrZpegRyrQ7X4mSQTDbOcCDSGBXI=&amp;xsec_source=pc_share\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121201006927.png\" /></p>\n<p>只花了15秒钟的时间，就把视频下载到了我的电脑桌面，打开看了一下，视频可以正常播入，且清晰度还不错，完美！</p>\n<p>最后，我们还要在Claude Code中查看一下本地已经安装的skills</p>\n<pre><code>/skills\n</code></pre>\n<p><img alt=\"\" src=\"https://image.kjdaohang.com/PicGo/20260121201304364.png\" /></p>\n<h2 id=\"最后说两句\">最后说两句</h2>\n<p>通过上面两个简单的实操案例，相信大家已经真切感受到：Skill 带来的绝不仅是「便捷」，更是一种「效率革命」。</p>\n<p>用一句话总结 Skill 的创建全流程，核心就四个字：<strong>需求→落地</strong>：</p>\n<blockquote>\n<p>锚定一个具体需求，借助 AI 挖掘 GitHub 上的优质开源项目，再通过 AI 将这些项目「Skill 化」封装，最终沉淀为一个个即拿即用、稳定可靠的 AI 技能。</p>\n</blockquote>\n<p>这远不止能解决图片处理、视频下载这类单一需求 —— 你甚至可以把这些超实用的开源项目整合打包，打造一个「万能 Skill 工具箱」。从此，不必再为各类格式转换、功能调用折腾五花八门的工具，一个 Skill 就能一站式搞定所有场景！</p>\n<p>无论是批量处理文件、自动化爬取数据，还是智能分析报表、快速搭建轻量应用…… 这些场景全都能实现 Skill 化，全都能无缝接入你的 AI Agent 体系，成为你专属的「核心技能包」，更是你应对各类需求时，最硬核、最可靠的「弹药库」。</p>\n<p>而我今天提到的这些，还仅仅是 GitHub 海量开源项目中的冰山一角。</p>\n<p>正因 Skill 的诞生，正因 AI Agent 的强大赋能，如今，每一个普通人都能站在「巨人的肩膀」上 —— 人类过去数十年积累的技术成果、开源智慧，不再是少数开发者的专属，只要你想，就能一键调用，为你所用。</p>\n\n</div>\n<div id=\"MySignature\">\n    技术改变世界！\n         --狂诗绝剑\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-23 08:38</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jinjiangongzuoshi\">狂师</a>&nbsp;\n阅读(<span id=\"post_view_count\">208</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Langchain 快速入门(一)",
      "link": "https://www.cnblogs.com/ClownLMe/p/19519224",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ClownLMe/p/19519224\" id=\"cb_post_title_url\" title=\"发布于 2026-01-22 22:51\">\n    <span>Langchain 快速入门(一)</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"简介\">简介</h1>\n<p>langchain专门用于构建LLM大语言模型，其中提供了大量的prompt模板，和组件，通过chain(链)的方式将流程连接起来，操作简单，开发便捷。</p>\n<h1 id=\"环境配置\">环境配置</h1>\n<p><strong>安装langchain框架</strong></p>\n<pre><code class=\"language-bash\">pip install langchain langchain-community\n</code></pre>\n<p>其中langchain可以提供了各种大模型语言库选择，（这里只列举几个）例如：</p>\n<pre><code class=\"language-bash\">#chatgpt\npip install langchain-openai\n#hugging face\npip install langchain-huggingface\n#千问\npip install langchain-qwq\n</code></pre>\n<h1 id=\"1-让模型跑起来\">1. 让模型跑起来</h1>\n<p>如何让你llm跑起来，这里用的是千问，来演示</p>\n<h3 id=\"案例\">案例</h3>\n<pre><code class=\"language-python\">import os\nfrom langchain_community.chat_models.tongyi import ChatTongyi\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n#这里是你的千问apikey\nos.environ[\"DASHSCOPE_API_KEY\"] = \"apikey\"\n\nmodel = ChatTongyi(model=\"qwen-plus\")\n\nprompt = ChatPromptTemplate.from_messages([\n&nbsp; &nbsp; (\"system\", \"你是一个精通{topic}的资深技术专家。\"),\n&nbsp; &nbsp; (\"user\", \"请用三句话解释一下什么是{concept}。\")\n])\n\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\n#文本输出\nresponse = chain.invoke({\"topic\": \"Python\", \"concept\": \"列表\"})\nprint(response)\n\n#分割\nprint(\"=\"*30)\n\n#流式输出\nfor chunk in chain.stream({\"topic\": \"人工智能\", \"concept\": \"神经网络\"}):\n&nbsp; &nbsp; print(chunk, end=\"\", flush=True)\n</code></pre>\n<h3 id=\"代码解释\">代码解释</h3>\n<p>整个代码的流程如下：<br />\n<strong>创建模型-&gt;构建提示词-&gt;构建chain链-&gt;使用大模型</strong></p>\n<h5 id=\"创建模型\">创建模型</h5>\n<p>这一步用不同的模型可能会不同<br />\n这里利用langchain的千问库创建模型，可能会不同</p>\n<pre><code class=\"language-python\">model = ChatTongyi(model=\"qwen-plus\")\n\n#例如用chatgpt\nllm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n</code></pre>\n<h5 id=\"构建提示词\">构建提示词</h5>\n<p>这一步构建利用了langchain库提供提示词模板：<br />\n其中用<code>{}</code>阔起来的在调用时可以动态用字典替换</p>\n<pre><code class=\"language-python\">prompt = ChatPromptTemplate.from_messages([\n&nbsp; &nbsp; (\"system\", \"你是一个精通{topic}的资深技术专家。\"),\n&nbsp; &nbsp; (\"user\", \"请用三句话解释一下什么是{concept}。\")\n])\n</code></pre>\n<p>各个角色功能如下：</p>\n<table>\n<thead>\n<tr>\n<th><strong>角色名称 (Role)</strong></th>\n<th><strong>对应的类</strong></th>\n<th><strong>作用说明</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>system</strong></td>\n<td><code>SystemMessage</code></td>\n<td><strong>系统提示词</strong>。用于设定 AI 的“人格”、专业背景、行为准则或约束条件。它通常优先级最高，决定了后续对话的基调。</td>\n</tr>\n<tr>\n<td><strong>user</strong></td>\n<td><code>HumanMessage</code></td>\n<td><strong>用户消息</strong>。代表人类发送的内容。这是模型需要直接回答或处理的问题。</td>\n</tr>\n<tr>\n<td><strong>ai</strong></td>\n<td><code>AIMessage</code></td>\n<td><strong>AI 消息</strong>。代表模型之前的回复。在构建多轮对话（带记忆）时，需要把模型之前的回复传回去。</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"构建chain链\">构建chain链</h5>\n<p>这个是langchain的灵魂，这里简单说明，后面会发更详细的教学文章<br />\nchain链的运行流程如下：<br />\n<strong>将输入填充prompt-&gt;将完整prompt喂给LLM-&gt;直接解析返回文本</strong></p>\n<p><code>StrOutputParser()</code>这个是langchain提供的文本解析器，用于将上面的结果解析为文本</p>\n<pre><code class=\"language-python\">output_parser = StrOutputParser()\nchain = prompt | model | output_parser\n</code></pre>\n<h5 id=\"使用大模型\">使用大模型</h5>\n<p>这里有两种方式：</p>\n<ol>\n<li>直接输出完整的文本</li>\n</ol>\n<pre><code class=\"language-python\">response = chain.invoke({\"topic\": \"Python\", \"concept\": \"列表\"})\nprint(response)\n</code></pre>\n<ol start=\"2\">\n<li>流文本输出（打字机）</li>\n</ol>\n<pre><code class=\"language-python\">for chunk in chain.stream({\"topic\": \"人工智能\", \"concept\": \"神经网络\"}):\n&nbsp; &nbsp; print(chunk, end=\"\", flush=True)\n</code></pre>\n<p><strong>如果❤喜欢❤本系列教程，就点个关注吧，后续不定期更新~</strong></p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-22 22:51</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ClownLMe\">ClownLMe</a>&nbsp;\n阅读(<span id=\"post_view_count\">119</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Nginx多服务静态资源路径冲突解决方案",
      "link": "https://www.cnblogs.com/yudaxia/p/19519201",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/yudaxia/p/19519201\" id=\"cb_post_title_url\" title=\"发布于 2026-01-22 22:34\">\n    <span>Nginx多服务静态资源路径冲突解决方案</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>在使用Nginx反向代理多个Flask应用时，遇到了一个棘手的问题：不同服务的静态资源（CSS/JS）会互相干扰。本文记录了问题的分析过程和解决方案。</p>\n<p><strong>关键词</strong>：Nginx反向代理、Flask静态资源、location匹配、proxy_pass</p>\n</blockquote>\n<h2 id=\"问题描述\">问题描述</h2>\n<p>在Nginx反向代理多个Flask服务时，不同服务的静态资源路径会发生冲突，导致服务A的页面加载了服务B的CSS/JS文件，或者找不到静态资源返回404错误。</p>\n<h3 id=\"问题场景\">问题场景</h3>\n<h4 id=\"部署架构\">部署架构</h4>\n<pre><code>域名: mathcoding.top\n├── 主服务 (端口5000) → 路径前缀: /\n└── 限流服务 (端口5001) → 路径前缀: /numberLimit\n</code></pre>\n<h4 id=\"初始nginx配置\">初始Nginx配置</h4>\n<pre><code class=\"language-nginx\"># 限流服务\nlocation /numberLimit {\n&nbsp; &nbsp; proxy_pass http://127.0.0.1:5001/;\n&nbsp; &nbsp; proxy_set_header Host $host;\n&nbsp; &nbsp; proxy_set_header X-Real-IP $remote_addr;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-Proto $scheme;\n}\n# 主服务（兜底规则）\nlocation / {\n&nbsp; &nbsp; proxy_pass http://127.0.0.1:5000;\n&nbsp; &nbsp; proxy_set_header Host $host;\n&nbsp; &nbsp; proxy_set_header X-Real-IP $remote_addr;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-Proto $scheme;\n}\n</code></pre>\n<h4 id=\"flask模板代码\">Flask模板代码</h4>\n<pre><code class=\"language-html\">&lt;!-- 5001端口的限流服务的模板 --&gt;\n&lt;link rel=\"stylesheet\" href=\"{{url_for('static', filename='css/style.css')}}\" /&gt;\n</code></pre>\n<h3 id=\"错误表现详解\">错误表现详解</h3>\n<p><strong>期望行为</strong>：</p>\n<ol>\n<li>访问 <code>https://mathcoding.top/numberLimit/</code> 加载限流服务的页面</li>\n<li>页面中的CSS链接应该请求限流服务(5001端口)的静态资源</li>\n<li>浏览器应该能正确获取到限流服务的 <code>static/css/style.css</code> 文件<br />\n<strong>实际行为</strong>：</li>\n<li>访问 <code>https://mathcoding.top/numberLimit/</code> ✅ 正确加载页面HTML</li>\n<li>Flask的 <code>url_for('static')</code> 生成路径：<code>/static/css/style.css</code></li>\n<li>浏览器发起请求：<code>https://mathcoding.top/static/css/style.css</code></li>\n<li>Nginx匹配到 <code>location /</code>（因为 <code>/static/...</code> 匹配不到 <code>/numberLimit</code>）</li>\n<li>请求被转发到主服务5000端口 ❌ <strong>错误的服务！</strong></li>\n<li>结果：加载了主服务的CSS（样式错误）或返回404（主服务没有这个文件）</li>\n</ol>\n<h3 id=\"问题的视觉表现\">问题的视觉表现</h3>\n<p>打开浏览器开发者工具Network标签会看到：</p>\n<pre><code>请求URL: https://mathcoding.top/static/css/style.css\n状态码: 200 或 404\n来源页面: https://mathcoding.top/numberLimit/\n问题: 这个CSS文件来自5000端口的主服务，不是5001端口的限流服务\n</code></pre>\n<p>页面表现：</p>\n<ul>\n<li>CSS样式不正确或完全没有样式</li>\n<li>控制台可能出现MIME类型错误</li>\n<li>如果主服务没有同名文件，则显示404错误</li>\n</ul>\n<h2 id=\"问题根源\">问题根源</h2>\n<h3 id=\"底层原理\">底层原理</h3>\n<ol>\n<li><strong>Flask URL生成机制</strong>：<code>url_for('static')</code> 生成的是绝对路径，默认为 <code>/static/...</code>，不包含服务的挂载前缀</li>\n<li><strong>Nginx location匹配规则</strong>：采用最长前缀匹配，<code>/static/...</code> 不匹配 <code>/numberLimit</code>，因此被 <code>location /</code> 捕获</li>\n<li><strong>路径命名空间冲突</strong>：多个服务共享同一个URL路径空间，都使用 <code>/static/...</code> 作为静态资源路径</li>\n</ol>\n<h3 id=\"请求流程分析\">请求流程分析</h3>\n<pre><code>Flask渲染模板\n&nbsp; &nbsp; ↓\nurl_for('static', filename='css/style.css')\n&nbsp; &nbsp; ↓\n生成HTML: &lt;link href=\"/static/css/style.css\"&gt;\n&nbsp; &nbsp; ↓\n浏览器解析HTML并发起请求: GET /static/css/style.css\n&nbsp; &nbsp; ↓\nNginx匹配规则:\n&nbsp; - /numberLimit? 不匹配 (请求路径是/static/..., 不是/numberLimit/...)\n&nbsp; - /? 匹配! (最长前缀匹配的兜底规则)\n&nbsp; &nbsp; ↓\nproxy_pass转发到: http://127.0.0.1:5000/static/css/style.css\n&nbsp; &nbsp; ↓\n错误: 5001服务的静态资源被错误地路由到5000服务\n</code></pre>\n<h3 id=\"为什么flask不生成-numberlimitstatic\">为什么Flask不生成 <code>/numberLimit/static/...</code>？</h3>\n<p>Flask应用本身不知道它被部署在什么路径下。从Flask的视角：</p>\n<ul>\n<li>它收到的请求路径是 <code>/</code>（因为 <code>proxy_pass http://127.0.0.1:5001/</code> 末尾有斜杠，会剥离前缀）</li>\n<li>它认为自己的根路径就是 <code>/</code></li>\n<li>所以 <code>url_for('static')</code> 生成 <code>/static/...</code> 而不是 <code>/numberLimit/static/...</code><br />\n这就是为什么需要在Flask端配置 <code>static_url_path</code>，或者在Nginx端做路径重写。</li>\n</ul>\n<h2 id=\"解决方案\">解决方案</h2>\n<h3 id=\"方案选择独立静态资源路径前缀\">方案选择：独立静态资源路径前缀</h3>\n<p>为每个服务配置独立的静态资源URL前缀，避免路径冲突。这种方案：</p>\n<ul>\n<li>服务代码改动最小（只改一个配置参数）</li>\n<li>不需要复杂的URL重写规则</li>\n<li>易于理解和维护</li>\n<li>符合微服务的命名空间隔离原则</li>\n</ul>\n<h3 id=\"flask配置\">Flask配置</h3>\n<pre><code class=\"language-python\"># 设置独立的静态资源URL路径\napp = Flask(__name__, static_url_path=\"/numberLimit-static\")\n</code></pre>\n<p><strong>参数说明</strong>：</p>\n<ul>\n<li><code>static_url_path</code>: 控制URL生成，影响 <code>url_for('static')</code> 的输出</li>\n<li><code>static_folder</code>: 控制文件系统路径（默认为'static'，不需要改）<br />\n<strong>效果</strong>：</li>\n</ul>\n<pre><code class=\"language-python\"># 修改前\nurl_for('static', filename='css/style.css') &nbsp;# → /static/css/style.css\n# 修改后\nurl_for('static', filename='css/style.css') &nbsp;# → /numberLimit-static/css/style.css\n</code></pre>\n<h3 id=\"nginx配置\">Nginx配置</h3>\n<pre><code class=\"language-nginx\"># 静态资源location（优先级高，放在前面）\nlocation /numberLimit-static/ {\n&nbsp; &nbsp; proxy_pass http://127.0.0.1:5001/numberLimit-static/;\n&nbsp; &nbsp; proxy_set_header Host $host;\n&nbsp; &nbsp; proxy_set_header X-Real-IP $remote_addr;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-Proto $scheme;\n}\n# 服务主路径\nlocation /numberLimit {\n&nbsp; &nbsp; proxy_pass http://127.0.0.1:5001/;\n&nbsp; &nbsp; proxy_set_header Host $host;\n&nbsp; &nbsp; proxy_set_header X-Real-IP $remote_addr;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-Proto $scheme;\n}\n# 主服务（放在最后）\nlocation / {\n&nbsp; &nbsp; proxy_pass http://127.0.0.1:5000;\n&nbsp; &nbsp; proxy_set_header Host $host;\n&nbsp; &nbsp; proxy_set_header X-Real-IP $remote_addr;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n&nbsp; &nbsp; proxy_set_header X-Forwarded-Proto $scheme;\n}\n</code></pre>\n<h3 id=\"工作流程\">工作流程</h3>\n<pre><code>Flask渲染模板\n&nbsp; &nbsp; ↓\nurl_for('static', filename='css/style.css')\n&nbsp; &nbsp; ↓\n生成HTML: &lt;link href=\"/numberLimit-static/css/style.css\"&gt;\n&nbsp; &nbsp; ↓\n浏览器请求: GET https://mathcoding.top/numberLimit-static/css/style.css\n&nbsp; &nbsp; ↓\nNginx匹配规则:\n&nbsp; - /numberLimit-static/? 匹配! (最长前缀匹配)\n&nbsp; &nbsp; ↓\nproxy_pass转发: http://127.0.0.1:5001/numberLimit-static/css/style.css\n&nbsp; &nbsp; ↓\nFlask处理:\n&nbsp; - 路由 /numberLimit-static/* 由 static_url_path 处理\n&nbsp; - 映射到文件系统: static/css/style.css\n&nbsp; &nbsp; ↓\n返回正确的CSS文件 ✅\n</code></pre>\n<h2 id=\"关键技术细节\">关键技术细节</h2>\n<h3 id=\"proxy_pass尾斜杠的作用\">proxy_pass尾斜杠的作用</h3>\n<pre><code class=\"language-nginx\"># ✅ 正确：带尾斜杠，进行路径替换\nproxy_pass http://127.0.0.1:5001/numberLimit-static/;\n# 请求 /numberLimit-static/css/style.css\n# 转发 http://127.0.0.1:5001/numberLimit-static/css/style.css\n# ❌ 错误：不带尾斜杠，拼接完整路径\nproxy_pass http://127.0.0.1:5001/numberLimit-static;\n# 请求 /numberLimit-static/css/style.css\n# 转发 http://127.0.0.1:5001/numberLimit-static/numberLimit-static/css/style.css\n</code></pre>\n<p><strong>原理</strong>：</p>\n<ul>\n<li>有尾斜杠：Nginx会用 <code>proxy_pass</code> 的路径<strong>替换</strong> <code>location</code> 匹配的部分</li>\n<li>无尾斜杠：Nginx会直接<strong>拼接</strong>完整的请求URI</li>\n</ul>\n<h3 id=\"location匹配优先级\">location匹配优先级</h3>\n<p>Nginx的location匹配规则（按优先级从高到低）：</p>\n<ol>\n<li>精确匹配 <code>location = /path</code></li>\n<li>正则匹配 <code>location ~ /pattern</code> 或 <code>location ~* /pattern</code></li>\n<li>前缀匹配（最长优先）<code>location /path</code><br />\n在本方案中：</li>\n</ol>\n<ul>\n<li><code>/numberLimit-static/</code> 长度19，比 <code>/</code> 更具体，优先匹配</li>\n<li><code>/numberLimit</code> 长度13，比 <code>/</code> 更具体，优先匹配</li>\n<li><code>/</code> 长度1，作为兜底，匹配所有其他请求<br />\n<strong>验证方法</strong>：</li>\n</ul>\n<pre><code class=\"language-bash\"># 测试Nginx配置\nnginx -t\n# 查看实际匹配的location（需要开启debug日志）\ntail -f /var/log/nginx/error.log | grep location\n</code></pre>\n<h2 id=\"更好的长期方案子域名\">更好的长期方案：子域名</h2>\n<p>当前的 <code>static_url_path</code> 方案是路径前缀部署下的权宜之计。<strong>最佳实践是为每个服务分配独立的子域名</strong>，这样可以从根本上解决路径冲突问题。</p>\n<h3 id=\"子域名方案示例\">子域名方案示例</h3>\n<pre><code class=\"language-nginx\"># 限流服务 - 独立子域名\nserver {\n&nbsp; &nbsp; server_name numberlimit.mathcoding.top;\n&nbsp; &nbsp;\n&nbsp; &nbsp; location / {\n&nbsp; &nbsp; &nbsp; &nbsp; proxy_pass http://127.0.0.1:5001;\n&nbsp; &nbsp; &nbsp; &nbsp; # proxy配置...\n&nbsp; &nbsp; }\n}\n# 主服务\nserver {\n&nbsp; &nbsp; server_name mathcoding.top www.mathcoding.top;\n&nbsp; &nbsp;\n&nbsp; &nbsp; location / {\n&nbsp; &nbsp; &nbsp; &nbsp; proxy_pass http://127.0.0.1:5000;\n&nbsp; &nbsp; &nbsp; &nbsp; # proxy配置...\n&nbsp; &nbsp; }\n}\n</code></pre>\n<p>Flask恢复默认配置：</p>\n<pre><code class=\"language-python\">app = Flask(__name__) &nbsp;# 无需设置static_url_path\n</code></pre>\n<p><strong>优势</strong>：</p>\n<ul>\n<li>每个服务有完全独立的URL路径空间</li>\n<li>无需任何特殊的静态资源配置</li>\n<li>更符合微服务架构理念</li>\n<li>便于服务独立扩展和迁移</li>\n</ul>\n<h2 id=\"总结\">总结</h2>\n<h3 id=\"问题本质\">问题本质</h3>\n<p>多个服务共享同一个URL路径空间，Flask生成的静态资源路径是绝对路径（<code>/static/...</code>），导致不同服务的静态资源被路由到错误的后端服务。</p>\n<h3 id=\"解决方案核心\">解决方案核心</h3>\n<p>为每个服务分配独立的静态资源URL前缀，通过Flask的 <code>static_url_path</code> 参数配合Nginx的location路由实现路径隔离。</p>\n<h3 id=\"关键配置\">关键配置</h3>\n<ol>\n<li><strong>Flask侧</strong>：<code>app = Flask(__name__, static_url_path=\"/服务名-static\")</code></li>\n<li><strong>Nginx侧</strong>：添加对应的 <code>location /服务名-static/</code> 规则</li>\n<li><strong>注意点</strong>：<code>proxy_pass</code> 末尾的斜杠会影响路径转换</li>\n</ol>\n<h3 id=\"适用场景\">适用场景</h3>\n<ul>\n<li>多个Web应用共享一个域名</li>\n<li>使用路径前缀区分不同服务（如 <code>/app1</code>、<code>/app2</code>）</li>\n<li>需要快速部署，暂时无法使用子域名</li>\n</ul>\n<h3 id=\"长期建议\">长期建议</h3>\n<p>当业务稳定后，建议迁移到子域名方案（如 <code>app1.example.com</code>、<code>app2.example.com</code>），从架构上彻底解决路径冲突问题。</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-22 22:34</span>&nbsp;\n<a href=\"https://www.cnblogs.com/yudaxia\">yupenglei</a>&nbsp;\n阅读(<span id=\"post_view_count\">53</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "吴恩达深度学习课程五：自然语言处理  第二周：词嵌入（四）分层 softmax 和负采样",
      "link": "https://www.cnblogs.com/Goblinscholar/p/19519181",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Goblinscholar/p/19519181\" id=\"cb_post_title_url\" title=\"发布于 2026-01-22 22:28\">\n    <span>吴恩达深度学习课程五：自然语言处理  第二周：词嵌入（四）分层 softmax 和负采样</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>此分类用于记录吴恩达深度学习课程的学习笔记。<br />\n课程相关信息链接如下：</p>\n<ol>\n<li>原课程视频链接：<a href=\"https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&amp;from_spmid=playlist.playlist-detail.0.0&amp;is_story_h5=false&amp;mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&amp;plat_id=116&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&amp;share_source=COPY&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1713085655&amp;unique_k=DfBgvFW&amp;up_id=8654113&amp;vd_source=e035e9878d32f414b4354b839a4c31a4\" rel=\"noopener nofollow\" target=\"_blank\">[双语字幕]吴恩达深度学习deeplearning.ai</a></li>\n<li>github课程资料，含课件与笔记:<a href=\"https://github.com/robbertliu/deeplearning.ai-andrewNG\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达深度学习教学资料</a></li>\n<li>课程配套练习（中英）与答案：<a href=\"https://blog.csdn.net/u013733326/article/details/79827273\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达深度学习课后习题与答案</a></li>\n</ol>\n<p>本篇为第五课的第二周内容，<a href=\"https://www.bilibili.com/video/BV1FT4y1E74V?spm_id_from=333.788.videopod.episodes&amp;vd_source=e035e9878d32f414b4354b839a4c31a4&amp;p=169\" rel=\"noopener nofollow\" target=\"_blank\">2.7</a>的内容以及一些相关知识的补充。</p>\n<hr />\n<p>本周为第五课的第二周内容，与 CV 相对应的，这一课所有内容的中心只有一个：<strong>自然语言处理（Natural Language Processing，NLP）</strong>。<br />\n应用在深度学习里，它是专门用来进行<strong>文本与序列信息建模</strong>的模型和技术，本质上是在全连接网络与统计语言模型基础上的一次“结构化特化”，也是人工智能中<strong>最贴近人类思维表达方式</strong>的重要研究方向之一。<br />\n<strong>这一整节课同样涉及大量需要反复消化的内容，横跨机器学习、概率统计、线性代数以及语言学直觉。</strong><br />\n语言不像图像那样“直观可见”，更多是抽象符号与上下文关系的组合，因此<strong>理解门槛反而更高</strong>。<br />\n因此，我同样会尽量补足必要的背景知识，尽可能用比喻和实例降低理解难度。<br />\n本周的内容关于词嵌入，是一种<strong>相对于独热编码，更能保留语义信息的文本编码方式</strong>。通过词嵌入，模型不再只是“记住”词本身，而是能够<strong>基于语义关系进行泛化</strong>，在一定程度上实现类似“<strong>举一反三</strong>”的效果。词嵌入是 NLP 领域中最重要的基础技术之一。</p>\n<p>本篇的内容关于<strong>分层 softmax 和负采样</strong>，是用来提升词嵌入模型训练效率，节省计算开销的技术。</p>\n<h1 id=\"1-分层-softmax\">1. 分层 softmax</h1>\n<p>在上一篇介绍 <a href=\"https://www.cnblogs.com/Goblinscholar/p/19504549\" target=\"_blank\">Word2Vec</a> 的时候，我们默认使用的是<strong>标准 softmax</strong> 来计算输出层的概率分布。但如果稍微停下来仔细想一想，就会立刻意识到一个问题：<strong>在大词表场景下，标准 softmax 几乎是不可用的。</strong></p>\n<p>下面我们分点来展开标准 softmax 在词嵌入中的局限和分层 softmax 的思想：</p>\n<h2 id=\"11-标准-softmax-的计算开销\">1.1 标准 softmax 的计算开销</h2>\n<p>在之前的示例中，我们反复强调过一点：<strong>输出层的神经元个数与词典大小完全一致</strong>。<br />\n这是因为，在 Word2Vec 的建模视角下，预测过程本质上可以被看作一个<strong>单标签分类问题</strong>：</p>\n<ul>\n<li>目标词对应“正确类别”</li>\n<li>词表中其余所有词都对应“错误类别”</li>\n</ul>\n<p>因此，我们需要通过 softmax 将模型输出映射为一个<strong>对整个词表的概率分布</strong>，并在反向传播阶段不断<strong>强化正确预测、抑制错误预测</strong>。</p>\n<p>从建模逻辑上看，这一过程是完全合理的，但真正的问题，并不出在“对不对”，而是“能不能实现”。</p>\n<p>我们知道，在 NLP 任务中，如果希望模型具备更强的泛化能力，第一步往往就是<strong>扩大语料规模</strong>，进而构建更大的词典。<br />\n而当我们希望模型在现实场景中“真正好用”时，其性能指标往往需要不断逼近，甚至尝试超越我们之前介绍过的<a href=\"https://www.cnblogs.com/Goblinscholar/p/19274215\" target=\"_blank\">贝叶斯最优错误率</a>。<br />\n在这个意义上，词典规模并不是一个可有可无的工程参数，而是模型能力的<strong>硬上限</strong>。</p>\n<p>一个直观的类比就是：<strong>人类的大脑究竟“记住”了多少词？</strong> 显然，这绝不是一个可以用“几千”或“几万”来描述的数量级。</p>\n<p>问题也正是在这里开始显现的：对于一次标准 softmax 计算而言，如果词表大小为 <span class=\"math inline\">\\(|V|\\)</span>，那么模型在输出层需要完成的操作包括：</p>\n<ol>\n<li>对 <strong><span class=\"math inline\">\\(|V|\\)</span> 个词向量</strong> 分别计算内积。</li>\n<li>对 <strong><span class=\"math inline\">\\(|V|\\)</span> 个得分</strong> 进行指数运算。</li>\n<li>对所有结果求和并完成归一化。</li>\n</ol>\n<p>其计算形式可以写成：</p>\n<p></p><div class=\"math display\">\\[P(w_o \\mid w_c)=\n\n\\frac{\\exp(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_c})}  \n{\\sum_{w \\in V} \\exp(\\mathbf{u}_w^\\top \\mathbf{v}_{w_c})}  \n\\]</div><p></p><p>注意，这里省略了 softmax 输出层的偏置项，在 Word2Vec 的实际建模与实现中，该偏置对词向量语义结构的影响通常可以忽略，这是很简单的道理：<strong>我们需要刻画向量间的距离关系，而偏置带来的整体平移显然是没有意义的。</strong></p>\n<p>回到正题，也就是说，<strong>哪怕我们只关心一个目标词的概率</strong>， 模型仍然必须对<strong>整个词表中的所有词</strong>各计算一遍打分。</p>\n<p>因此，每一个训练样本在输出层的<strong>计算复杂度都是：<span class=\"math inline\">\\(O(|V|)\\)</span>。</strong><br />\n当 <span class=\"math inline\">\\(|V|\\)</span> 只有几千时，这个代价尚可接受，但一旦词典规模达到几十万、甚至上百万级别，这一步计算就会迅速成为训练过程中的主要瓶颈。<br />\n换句话说，<strong>标准 softmax 的计算成本与词表规模线性相关</strong>，这一性质在大词表场景下是无法回避的。<br />\n用我们之前的内容来类比一下：<strong>你见过几百万类别的分类模型吗？</strong><br />\n也正是在这样的背景下，我们不得不思考新的问题： <strong>可不可以在不显式遍历整个词表的前提下，完成对目标词的有效建模？</strong><br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260122222725445-514225875.png\" /><br />\n于是，Word2Vec 提出了第一种加速策略：<strong>分层 softmax</strong>。</p>\n<h2 id=\"12-哈夫曼树\">1.2 哈夫曼树</h2>\n<p>哈夫曼是其实是数据结构里的内容，我们看看它在分层softmax中实现的效果。</p>\n<p>哈夫曼树最早被用于<strong>无损数据压缩</strong>，其核心思想可以概括为一句话：<strong>出现频率越高的符号，编码越短；出现频率越低的符号，编码越长。</strong> 通过这种方式，<strong>高频符号拥有更短的路径长度</strong>，从而在整体意义下最小化编码的期望长度。</p>\n<p>在 分层softmax 中，并没有使用其压缩编码的逻辑，而是重点利用哈夫曼树中<strong>高频符号拥有更短的路径长度</strong>的特点来组织词表并优化传播逻辑，它的过程如下：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260122222708870-226555105.png\" /></p>\n<p>再具体来说，Word2Vec 的分层 softmax 将词表组织成哈夫曼树的步骤如下：</p>\n<ol>\n<li><strong>统计词频</strong>：首先对语料库中的每个词计算出现频率，频率越高的词表示越常见，也就越可能在训练中被访问多次。</li>\n<li><strong>构建哈夫曼树</strong>：\n<ul>\n<li>将每个词作为叶子节点，节点权重 = 词频；</li>\n<li>从最小权重的两个节点开始合并，生成新的父节点，权重为子节点权重之和；</li>\n<li>重复此过程，直到只剩下根节点。<br />\n通过这种方式，高频词自然靠近根节点，低频词靠近叶子，从而保证<strong>频繁访问的词路径短</strong>。</li>\n</ul>\n</li>\n<li><strong>路径作为概率计算序列</strong>：\n<ul>\n<li>每条从根到叶子的路径对应一次概率计算的序列：每个节点上的选择可以看作一次二分类（是否沿左/右分支走）。</li>\n<li>目标词的概率 = 路径上所有二分类概率的乘积。</li>\n</ul>\n</li>\n<li><strong>降低训练开销</strong>：\n<ul>\n<li>高频词路径短 → 更新梯度的节点少 → 每次训练样本计算量小。</li>\n<li>低频词路径长 → 虽然节点多，但训练中出现频率低，对整体训练开销影响小。</li>\n<li>整体而言，<strong>平均计算复杂度从 <span class=\"math inline\">\\(O(|V|)\\)</span> 降到 <span class=\"math inline\">\\(O(\\log |V|)\\)</span></strong>，极大提升了大词表场景下的训练效率。</li>\n</ul>\n</li>\n</ol>\n<p>只看到这里，可能还是有些模糊，我们以 CBOW 为例，来看看使用分层 softmax 的整体网络结构和传播过程。</p>\n<h2 id=\"13--cbow-中的分层-softmax\">1.3  CBOW 中的分层 softmax</h2>\n<p><img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260122222724875-338795043.png\" /><br />\n这样，最终效果是：一<strong>个原本包含几百万类别的输出问题，被分解成了一系列二分类问题。</strong><br />\n在每次训练时，网络只需更新目标词路径上的相关节点参数，而不必触及整个输出层，从而大幅节省计算开销。<br />\n同时，由于哈夫曼树天然按照词频组织，高频词位于靠近根节点的路径上，使得这些词的更新路径更短、访问更快。这不仅加速了高频词的学习，也让整体训练过程更加高效。</p>\n<p>这便是分层 softmax 的原理，如果从实际应用上来说，你也可以叫它”堆叠 sigmoid“，而 CBOW 便常常和分层 softmax  搭配使用。</p>\n<p>下面我们便来看看另一种加速策略：负采样。</p>\n<h1 id=\"2-负采样negative-sampling\">2. 负采样（Negative Sampling）</h1>\n<p>如果说分层 softmax 是一种<strong>结构层面的加速</strong>，那么负采样更像是一种<strong>从目标函数层面“改问题”的方法</strong>。<br />\n它的出发点非常直接：<strong>我们真的有必要在训练时区分“目标词”和“所有非目标词”吗？</strong><br />\n答案是：<strong>不需要</strong>。<br />\n相较于分层 softmax，负采样的思想更加朴素、实现也更加简单。在实际工程中，它几乎可以看作是 <strong>Skip-gram 的默认搭档</strong>，也是 Word2Vec 最常被使用的训练方式之一。</p>\n<p>负采样的核心思想可以概括为一句话：<strong>只挑选少量负样本进行训练，而非遍历整个词表</strong>。换句话说，我们只关心“正确词 + 一些随机挑选的错误词”，其他的全体词不参与计算。<br />\n这样，计算量从 <span class=\"math inline\">\\(O(|V|)\\)</span> 直接降到了 <span class=\"math inline\">\\(O(k)\\)</span>，其中 <span class=\"math inline\">\\(k\\)</span> 是负样本的数量（通常 <span class=\"math inline\">\\(5 \\sim 20\\)</span>）。</p>\n<p>我们以 Skip-gram 为例来演示负采样的原理：</p>\n<h2 id=\"21-确定正负样本\">2.1 确定正负样本</h2>\n<p>使用负采样的第一步是确定正负样本，这一过程同样容易理解，来看看：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260122222401494-1883957489.png\" /><br />\n其中：</p>\n<ul>\n<li><strong>正样本</strong>：由真实语料中出现的上下文词构成；</li>\n<li><strong>负样本</strong>：从词表中随机抽取，但并不出现在当前上下文中的词。</li>\n</ul>\n<p>这里展开一点，对于负样本的采样，我们常常使用下面这个公式：</p>\n<p></p><div class=\"math display\">\\[P(w) \\propto U(w)^{3/4}\n\\]</div><p></p><ul>\n<li><span class=\"math inline\">\\(P(w)\\)</span>：表示<strong>在负采样中抽到词 <span class=\"math inline\">\\(w\\)</span> 的概率</strong>。</li>\n<li><span class=\"math inline\">\\(U(w)\\)</span>：表示词 <span class=\"math inline\">\\(w\\)</span> 在整个语料库中的<strong>出现频率</strong>。</li>\n<li><span class=\"math inline\">\\(\\propto\\)</span>：表示“<strong>与……成正比</strong>”，意思是我们先按这个规则给每个词一个权重，再归一化成概率。</li>\n</ul>\n<p>语言描述就是：先算每个词的词频的四分之三次方，然后把它们除以所有词的词频的四分之三次方 之和，就得到最终抽样概率 <span class=\"math inline\">\\(P(w)\\)</span>：</p>\n<p></p><div class=\"math display\">\\[P(w) = \\frac{U(w)^{3/4}}{\\sum_{w' \\in V} U(w')^{3/4}}  \n\\]</div><p></p><p>这个公式实际上做的是这样的工作：</p>\n<ol>\n<li>对高频词做了<strong>降权</strong>（比原始词频低一些），减少它们在负样本中出现的概率。</li>\n<li>对低频词做了<strong>相对提升</strong>（比直接按词频高一些），让它们有机会被采样到。</li>\n</ol>\n<p>这种采样策略在实践中被证明可以提高词向量训练的稳定性和语义表达能力。<br />\n完成了数据准备后，现在，就来看看 Skip-gram  中的负采样。</p>\n<h2 id=\"22-skip-gram-中的负采样\">2.2 Skip-gram 中的负采样</h2>\n<p>在 Skip-gram + 负采样 框架下，一次训练的传播过程可以概括为下图所示：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202601/3708248-20260122222708411-1201982967.png\" /></p>\n<p>这种思路其实很容易理解：<strong>在一次传播中，我不再一次更新输出层的所有参数，而是只计算我提前挑选好的一些样本，只更新它们相关的参数，来节省计算开销，剩下的等下次选中再更新。</strong></p>\n<p>最终，模型仅更新正样本和 <span class=\"math inline\">\\(k\\)</span> 个负样本对应的输出层参数以及输入词向量，其余词的向量完全不参与更新，从而节省了大量计算，实现 <span class=\"math inline\">\\(O(k)\\)</span> 的复杂度，远小于 <span class=\"math inline\">\\(O(|V|)\\)</span>。<br />\n这种“只更新被选中参数”的训练方式，使得<strong>负采样在大规模语料与超大词表场景下，具备极高的计算效率。</strong></p>\n<p>你会发现，负采样和分层 softmax 的一点共同逻辑就是<strong>把一次多分类拆成了多次二分类</strong>，这同样是我们可以学习的优化思路。</p>\n<h1 id=\"3总结\">3.总结</h1>\n<table>\n<thead>\n<tr>\n<th>概念</th>\n<th>原理</th>\n<th>比喻</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>softmax大词表瓶颈</td>\n<td>softmax 的计算与反向传播都必须遍历整个词表，词表越大，训练越慢，成为主要性能瓶颈</td>\n<td>在问路时，必须<strong>问完全国所有居民</strong></td>\n</tr>\n<tr>\n<td>分层 softmax</td>\n<td>用哈夫曼树将多分类问题拆成<strong>一条从根到叶的二分类决策路径</strong>，目标词概率等于路径上各二分类概率的乘积</td>\n<td>在<strong>每个路口选择方向</strong>，而不是一次问遍所有人。</td>\n</tr>\n<tr>\n<td>分层 softmax +CBOW</td>\n<td>每次训练只更新目标词路径上的节点参数，其余词完全不参与计算</td>\n<td>只维修<strong>你真正经过的路口</strong>，而不是重修整座城市。</td>\n</tr>\n<tr>\n<td>负采样（Negative Sampling）</td>\n<td>不再逼模型区分“目标词 vs 全词表”，而是区分“目标词 vs 少量噪声词”，复杂度为 <span class=\"math inline\">\\(O(k)\\)</span></td>\n<td>不用认清所有陌生人，只要确认<strong>朋友和几名路人</strong></td>\n</tr>\n<tr>\n<td>正负样本构造</td>\n<td>正样本来自真实上下文；负样本从词表随机抽取但不在上下文中</td>\n<td>真朋友 vs <strong>随机拉来的假熟人</strong></td>\n</tr>\n<tr>\n<td><span class=\"math inline\">\\(U(w)^{3/4}\\)</span> 采样分布</td>\n<td>对高频词降权、对低频词相对提升，使负样本更有信息量</td>\n<td>热门明星<strong>少出现点</strong>，路人<strong>多给点镜头</strong></td>\n</tr>\n<tr>\n<td>Skip-gram + 负采样</td>\n<td>每次仅更新中心词、正样本词和 <span class=\"math inline\">\\(k\\)</span> 个负样本的向量，其余参数不动</td>\n<td>只训练<strong>被点名的几个人</strong>，其他人下次再说</td>\n</tr>\n</tbody>\n</table>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-22 22:28</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Goblinscholar\">哥布林学者</a>&nbsp;\n阅读(<span id=\"post_view_count\">47</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "从DEM到等高线：手撕矢量与栅格两种地形表达",
      "link": "https://www.cnblogs.com/charlee44/p/19519065",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/charlee44/p/19519065\" id=\"cb_post_title_url\" title=\"发布于 2026-01-22 21:19\">\n    <span>从DEM到等高线：手撕矢量与栅格两种地形表达</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"从DEM到等高线：手撕矢量与栅格两种地形表达\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/1000410/202601/1000410-20260122212101111-1429511358.png\" />\n        深入解析等高线地形图的矢量与栅格两种生成原理，并通过 C++ 代码从零实现 DEM 到等高线的完整流程，揭示 GIS 地形表达的核心逻辑。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>本文节选自新书<a href=\"https://item.jd.com/14603137.html\" rel=\"noopener nofollow\" target=\"_blank\">《GIS基础原理与技术实践》</a>第6章。很多人会用 gdal_contour 一键生成等高线，但你知道它背后是如何通过三角网或格网 DEM 计算交线的吗？本文带你从零实现矢量等高线提取与栅格分层设色图生成，真正理解地形表达的本质。</p>\n</blockquote>\n<p><img alt=\"GIS基础原理与技术实践\" class=\"lazyload\" /></p>\n<h1 id=\"66-等高线地形图\">6.6 等高线地形图</h1>\n<p>地形的表达除了前文介绍的基于栅格形式的格网DEM和基于矢量形式的不规则三角网DEM之外，还有一种表达方式：等高线地形图，简称等高线图。等高线图的历史非常悠久，是一种非常经典的地形表达方式，在高中地理中就有详细的介绍和考察（有趣的是在国内的课程设置中，高中地理属于文科，本科的地理信息系统却属于理科，相信在进入大学后才接触到等高线图的人不止笔者一个）。</p>\n<h2 id=\"661-等高线图的基础理解\">6.6.1 等高线图的基础理解</h2>\n<p>所谓等高线，就是把地面上海拔高度相等的点相连，垂直投影到水平面上，并按照一定的比例缩放绘制到图纸所得到的闭合曲线。如果我们按照一定高度差（等高距），从低到高依次绘制等高线，就得到了一张等高线图。如下图6.15所示：</p>\n<p><img alt=\"图6.15 等高线图\" class=\"lazyload\" /></p>\n<p>等高线图虽然有不易识别的特点，但是其最大的优点就在于通过较少的信息量，就能够轻易识别出多种地形地貌，从而帮助我们做出地理空间相关的决策。一些典型的地貌包括：</p>\n<h3 id=\"1-陡坡和缓坡\">1. 陡坡和缓坡</h3>\n<p>等高线密集的地方，坡度较陡，如图6.16（1）所示。而图6.16（2）所示的坡度较缓，因为其等高线较为稀疏，坡度较缓。如果我们进行爬山活动，应选择等高线稀疏的地方。</p>\n<p><img alt=\"图6.16 陡坡和缓坡\" class=\"lazyload\" /></p>\n<h3 id=\"2-山头和洼地\">2. 山头和洼地</h3>\n<p>下图6.17中等高线（a）表示山头，等高线（b）为表示洼地，它们投影到平面上都是简单的闭合多边形曲线。两者的区别在于山头的内圈高程大于外圈，而洼地则相反。</p>\n<p><img alt=\"图6.17 山头和洼地\" class=\"lazyload\" /></p>\n<h3 id=\"3-山脊山谷和鞍部\">3. 山脊、山谷和鞍部</h3>\n<p>山脊位于等高线弯曲的地方，其高程值沿着凸向从高到底，如下图6.18（a）所示。山谷则相反，等高线弯曲处从高程值低往高程值高的地方凸出，如下图6.18（b）所示。山脊弯曲处相连的线被称为山脊线，附近雨水在降落到这条线上时会分别流向山脊的两侧，因此山脊线被称为分水线。山谷弯曲处相连的线被称为山谷线，雨水会从两侧上坡流向谷底，容易发育河流，因此山谷线被称为集水线。分水线、集水线是土木工程中需要重点关注的问题。</p>\n<p>鞍部是位于两个山头之间呈马鞍形的低凹部位，如下图6.18所示。鞍部是修建山区道路的关节点，可以考虑在鞍部修建越岭道路。</p>\n<p><img alt=\"图6.18 山脊、山谷和鞍部\" class=\"lazyload\" /></p>\n<h3 id=\"4-绝壁和悬崖\">4. 绝壁和悬崖</h3>\n<p>绝壁是坡度在70°以上的陡峭崖壁，此时多条等高线的一部分会重叠，将这部分用锯齿状的符号表示绝壁，如下图6.19（a）所示。悬崖是上部突出，下部凹进的绝壁，这种地貌的等高线出现相交。这时隐蔽的等高线用虚线表示，如下图6.19（c）所示。</p>\n<p><img alt=\"图6.19 绝壁和悬崖\" class=\"lazyload\" /></p>\n<p>等高线图的另一个特点的是其并不完全是定性的，由于每一条等高线都会标注高程信息，很多情况下可以进行大致定量运算，比如估算等高线图中两个点的相对高差、坡度等。所以等高线图确实是一种非常简洁有力的地形表达，应用非常广泛。</p>\n<h2 id=\"662-等高线地形图矢量形式\">6.6.2 等高线地形图矢量形式</h2>\n<p>正如图6.16~图6.19所示，最为简洁的等高线图是基于要素特征的，是由一组闭合的曲线组成的。因此不难想象，为了从DEM中生成矢量形式的等高线地形图，关键在于使用DEM的空间要素进行立体几何计算。其实在GDAL自带的工具中已经有提取等高线图的工具，但是正如前面笔者所说，结果并不重要，重要的是其中的原理。这里笔者自己的实现如下例6.10所示。</p>\n<pre><code class=\"language-cpp\">//例6.10 生成等高线图的矢量形式\n#include &lt;gdal_priv.h&gt;\n#include &lt;ogrsf_frmts.h&gt;\n\n#include &lt;Eigen/Eigen&gt;\n#include &lt;fstream&gt;\n#include &lt;iostream&gt;\n#include &lt;sstream&gt;\n\nusing namespace std;\nusing namespace Eigen;\n\nstruct TrigonVertexIndex {\n  size_t index[3];\n};\n\ndouble startHeight = 550;\ndouble endHeight = 2815;\ndouble heightInterval = 500;\n\nsize_t nV;                                       //点的个数\nstd::vector&lt;Vector3d&gt; vertexXyz;                 //点集\nsize_t nF;                                       //面的个数.\nstd::vector&lt;TrigonVertexIndex&gt; faceVertexIndex;  //面在点集中的序号\n\n//根据空截断字符串\nvoid ChopStringWithSpace(string line, vector&lt;string&gt;&amp; substring) {\n  std::stringstream linestream(line);\n  string sub;\n\n  while (linestream &gt;&gt; sub) {\n    substring.push_back(sub);\n  }\n}\n\nbool ReadTin(const char* szModelPath) {\n  ifstream infile(szModelPath, ios::binary);\n  if (!infile) {\n    printf(\"Can't Load %s\\n\", szModelPath);\n    return false;\n  }\n\n  string line;\n  while (line != string(\"end_header\")) {\n    getline(infile, line);\n    vector&lt;string&gt; substring;\n    ChopStringWithSpace(line, substring);\n\n    if (substring.size() == 3 &amp;&amp; substring[0] == \"element\") {\n      if (substring[1] == \"vertex\") {\n        nV = stoul(substring[2]);\n      } else if (substring[1] == \"face\") {\n        nF = stoul(substring[2]);\n      }\n    }\n  }\n\n  vertexXyz.resize(nV);\n  vertexXyz.shrink_to_fit();\n\n  uint8_t propertyNum = 3;\n  double* vertexTmp = new double[propertyNum * nV];\n  infile.read((char*)(vertexTmp),\n              static_cast&lt;int64_t&gt;(propertyNum * nV * sizeof(double)));\n  for (size_t i = 0; i &lt; nV; i++) {\n    vertexXyz[i].x() = vertexTmp[i * propertyNum];\n    vertexXyz[i].y() = vertexTmp[i * propertyNum + 1];\n    vertexXyz[i].z() = vertexTmp[i * propertyNum + 2];\n  }\n\n  delete[] vertexTmp;\n  vertexTmp = nullptr;\n\n  faceVertexIndex.resize(nF);\n  faceVertexIndex.shrink_to_fit();\n\n  for (size_t i = 0; i &lt; nF; i++) {\n    uint8_t type;\n    infile.read((char*)(&amp;type), 1);\n\n    if (type != 3) {\n      printf(\"Format Incompatible Or Non Trigon!\\n\");\n      return false;\n    }\n\n    for (unsigned int j = 0; j &lt; type; j++) {\n      int id;\n      infile.read((char*)(&amp;id), sizeof(int));\n      faceVertexIndex[i].index[j] = static_cast&lt;size_t&gt;(id);\n    }\n  }\n\n  infile.close();\n\n  return true;\n}\n\n//判断几种可能的相交情况\nint CalTriangleType(TrigonVertexIndex trigonVID,\n                    std::vector&lt;bool&gt;&amp; vertexFlag) {\n  bool triVertexFlag[3] = {false, false, false};\n  for (int vi = 0; vi &lt; 3; vi++) {\n    size_t vid = trigonVID.index[vi];\n    triVertexFlag[vi] = vertexFlag[vid];\n  }\n\n  int type = 0;\n  if (!triVertexFlag[0] &amp;&amp; !triVertexFlag[1] &amp;&amp; !triVertexFlag[2]) {\n    type = 0;\n  } else if (!triVertexFlag[0] &amp;&amp; !triVertexFlag[1] &amp;&amp; triVertexFlag[2]) {\n    type = 1;\n  } else if (triVertexFlag[0] &amp;&amp; !triVertexFlag[1] &amp;&amp; !triVertexFlag[2]) {\n    type = 2;\n  } else if (!triVertexFlag[0] &amp;&amp; triVertexFlag[1] &amp;&amp; !triVertexFlag[2]) {\n    type = 3;\n  } else if (triVertexFlag[0] &amp;&amp; triVertexFlag[1] &amp;&amp; triVertexFlag[2]) {\n    type = 4;\n  } else if (triVertexFlag[0] &amp;&amp; triVertexFlag[1] &amp;&amp; !triVertexFlag[2]) {\n    type = 5;\n  } else if (!triVertexFlag[0] &amp;&amp; triVertexFlag[1] &amp;&amp; triVertexFlag[2]) {\n    type = 6;\n  } else if (triVertexFlag[0] &amp;&amp; !triVertexFlag[1] &amp;&amp; triVertexFlag[2]) {\n    type = 7;\n  }\n\n  return type;\n}\n\n//计算空间线段已知Z值的点的坐标\nbool CalPointOfSegmentLineWithZ(Vector3d O, Vector3d E, double z, Vector3d&amp; P) {\n  if (E.z() &lt; O.z()) {\n    Vector3d tmp = O;\n    O = E;\n    E = tmp;\n  }\n\n  double t = (z - O.z()) / (E.z() - O.z());\n  if (t &lt; 0 &amp;&amp; t &gt; 1) {\n    return false;\n  }\n\n  Vector3d D = E - O;\n  P = O + D * t;\n\n  return true;\n}\n\n//计算空间中三角形与直线相交\nvoid CalTriangleIntersectingLine(TrigonVertexIndex trigonVID, int cornerId,\n                                 Vector3d&amp; start, Vector3d&amp; end, double z) {\n  vector&lt;Vector3d&gt; xyzList(3);\n  for (size_t vi = 0; vi &lt; 3; vi++) {\n    size_t vid = trigonVID.index[vi];\n    xyzList[vi] = vertexXyz[vid];\n  }\n\n  if (cornerId == 0) {\n    CalPointOfSegmentLineWithZ(xyzList[0], xyzList[1], z, start);\n    CalPointOfSegmentLineWithZ(xyzList[0], xyzList[2], z, end);\n  } else if (cornerId == 1) {\n    CalPointOfSegmentLineWithZ(xyzList[1], xyzList[0], z, start);\n    CalPointOfSegmentLineWithZ(xyzList[1], xyzList[2], z, end);\n  } else if (cornerId == 2) {\n    CalPointOfSegmentLineWithZ(xyzList[2], xyzList[1], z, start);\n    CalPointOfSegmentLineWithZ(xyzList[2], xyzList[0], z, end);\n  }\n}\n\nbool CalIsoHeightLine(TrigonVertexIndex trigonVID, int type, Vector3d&amp; start,Vector3d&amp; end, double height) {\n  bool flag = false;\n  switch (type) {\n    case 1:\n    case 5: {\n      CalTriangleIntersectingLine(trigonVID, 2, start, end, height);\n      flag = true;\n      break;\n    }\n    case 2:\n    case 6: {\n      CalTriangleIntersectingLine(trigonVID, 0, start, end, height);\n      flag = true;\n      break;\n    }\n    case 3:\n    case 7: {\n      CalTriangleIntersectingLine(trigonVID, 1, start, end, height);\n      flag = true;\n      break;\n    }\n    case 0:\n    case 4:\n    default:\n      break;\n  }\n\n  return flag;\n}\n\nint main() {\n  GDALAllRegister();  // GDAL所有操作都需要先注册格式\n\n  vector&lt;double&gt; heightThresholdList;\n  {\n    double heightThreshold = startHeight;\n    while (heightThreshold &lt; endHeight) {\n      heightThresholdList.push_back(heightThreshold);\n      heightThreshold = heightThreshold + heightInterval;\n    }\n  }\n\n  string workDir = getenv(\"GISBasic\");\n  string outShpFile = workDir + \"/../Data/Terrain/dst.shp\";\n\n  string tinPath = workDir + \"/../Data/Terrain/terrain.ply\";\n  if (!ReadTin(tinPath.c_str())) {\n    return 1;\n  }\n\n  //创建\n  GDALDriver* driver =\n      GetGDALDriverManager()-&gt;GetDriverByName(\"ESRI Shapefile\");\n  if (!driver) {\n    printf(\"Get Driver ESRI Shapefile Error！\\n\");\n    return 1;\n  }\n\n  GDALDataset* dataset =\n      driver-&gt;Create(outShpFile.c_str(), 0, 0, 0, GDT_Unknown, nullptr);\n  OGRLayer* poLayer = dataset-&gt;CreateLayer(\"IsoHeightline\", nullptr,\n                                           wkbMultiLineStringZM, nullptr);\n\n  OGRFeature* poFeature = new OGRFeature(poLayer-&gt;GetLayerDefn());\n  OGRMultiLineString multiLineString;\n\n  for (size_t i = 0; i &lt; heightThresholdList.size(); i++) {\n    double heightThreshold = heightThresholdList[i];\n\n    std::vector&lt;bool&gt; vertexFlag(vertexXyz.size(), false);\n    for (size_t i = 0; i &lt; vertexXyz.size(); i++) {\n      if (vertexXyz[i].z() &gt;= heightThreshold) {\n        vertexFlag[i] = true;\n      }\n    }\n\n    for (size_t fi = 0; fi &lt; faceVertexIndex.size(); fi++) {\n      int type = CalTriangleType(faceVertexIndex[fi], vertexFlag);\n\n      Vector3d start;\n      Vector3d end;\n      if (CalIsoHeightLine(faceVertexIndex[fi], type, start, end,\n                           heightThreshold)) {\n        OGRLinearRing ogrring;\n        ogrring.setPoint(0, start.x(), start.y(), start.z());\n        ogrring.setPoint(1, end.x(), end.y(), end.z());\n        multiLineString.addGeometry(&amp;ogrring);\n      }\n    }\n  }\n\n  poFeature-&gt;SetGeometry(&amp;multiLineString);\n  if (poLayer-&gt;CreateFeature(poFeature) != OGRERR_NONE) {\n    printf(\"Failed to create feature in shapefile.\\n\");\n    return 1;\n  }\n\n  //释放\n  GDALClose(dataset);\n  dataset = nullptr;\n\n  return 0;\n}\n</code></pre>\n<p>在本例中笔者使用的DEM是不规则三角网形式的DEM，不过如果使用规则格网也差不多，都需要先获取一组立体空间三角形。要获取等高线，我们可以设想某一固定的高程面与这一组立体空间三角形相交，那么必然可以得到相交的线段，这个线段也就是等高线上的线段。</p>\n<p>某一固定的高程面与这一组立体空间三角形相交的算法也不是使用计算几何算法硬算，其实原理非常简单，如果高程面与立体空间三角形相交，那么空间三角形就会有一个角或者两个角在高程面上方。换句话说，高程面与立体空间三角形相交，比如有一个角在高程面上方，或者在高程面下方。只要求取这个角，就可以获取到两条相交的三角形边。最后，求两个相交的三角形边上固定高程的点，将两点相连就是等高线上的线段。</p>\n<p>其实上述原理也体现了笔者在前面的论述，DEM其实只是个2.5维的数据，这里确实也没有用到真正意义上的三维立体空间运算，而是很快根据高度值做出高程面与立体空间三角形相交的判定。这种降维的思想在GIS中是非常有用的，我们应该充分利用它。最后得到的结果如下图6.20所示：</p>\n<p><img alt=\"图6.20 根据DEM生成等高线图\" class=\"lazyload\" /></p>\n<h2 id=\"663-等高线地形图栅格形式\">6.6.3 等高线地形图栅格形式</h2>\n<p>等高线地形图的矢量形式虽然比较简洁，但是确实不够直观。我们可以仿照热力图的表达，将其栅格化，并根据不同的高度区间赋予不同的颜色，就得到了分层设色的等高线地形图。这种栅格形式的等高线地形图更为直接美观，我们可以很容易根据颜色区分那些地区属于平原、丘陵、盆地、高原或者山地，也方便直接输出图纸。</p>\n<p>一个思路是将例6.10所得到的结果栅格化，不过这并不是最佳的方案。由于格网DEM数据本身就是栅格化的，我们可以直接在格网DEM上生成分层设色等高线地形图，如下例6.11所示：</p>\n<pre><code class=\"language-cpp\">//例6.11 生成等高线图的栅格形式\n#include &lt;gdal_priv.h&gt;\n\n#include &lt;array&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nusing namespace std;\n\nusing F_RGB = std::array&lt;double, 3&gt;;\n\nint demWidth;\nint demHeight;\n\ndouble geoTransform[6] = {0};\ndouble startX;  //左上角点坐标X\ndouble dx;      // X方向的分辨率\ndouble startY;  //左上角点坐标Y\ndouble dy;      // Y方向的分辨率\n\nvector&lt;float&gt; demBuf;\n\nint dstBandNum = 4;\nvector&lt;uint8_t&gt; dstBuf;\n\ndouble startHeight = 550;\ndouble endHeight = 2815;\ndouble heightInterval = 500;\n\nvector&lt;F_RGB&gt; tableRGB(256);         //颜色映射表\nvector&lt;double&gt; heightThresholdList;  //高度区间\nvector&lt;F_RGB&gt; heightRGBList;         //高度区间对应的颜色\n\n//生成渐变色\nvoid Gradient(F_RGB &amp;start, F_RGB &amp;end, vector&lt;F_RGB&gt; &amp;RGBList) {\n  F_RGB d;\n  for (int i = 0; i &lt; 3; i++) {\n    d[i] = (end[i] - start[i]) / RGBList.size();\n  }\n\n  for (size_t i = 0; i &lt; RGBList.size(); i++) {\n    for (int j = 0; j &lt; 3; j++) {\n      RGBList[i][j] = start[j] + d[j] * i;\n    }\n  }\n}\n\n//初始化颜色查找表\nvoid InitColorTable() {\n  F_RGB blue({17, 60, 235});   //蓝色\n  F_RGB green({17, 235, 86});  //绿色\n  vector&lt;F_RGB&gt; RGBList(60);\n  Gradient(blue, green, RGBList);\n  for (int i = 0; i &lt; 60; i++) {\n    tableRGB[i] = RGBList[i];\n  }\n\n  F_RGB yellow({235, 173, 17});  //黄色\n  RGBList.clear();\n  RGBList.resize(60);\n  Gradient(green, yellow, RGBList);\n  for (int i = 0; i &lt; 60; i++) {\n    tableRGB[i + 60] = RGBList[i];\n  }\n\n  F_RGB red({235, 60, 17});  //红色\n  RGBList.clear();\n  RGBList.resize(60);\n  Gradient(yellow, red, RGBList);\n  for (int i = 0; i &lt; 60; i++) {\n    tableRGB[i + 120] = RGBList[i];\n  }\n\n  F_RGB white({235, 17, 235});  //紫色\n  RGBList.clear();\n  RGBList.resize(76);\n  Gradient(red, white, RGBList);\n  for (int i = 0; i &lt; 76; i++) {\n    tableRGB[i + 180] = RGBList[i];\n  }\n}\n\nvoid ReadDem() {\n  string workDir = getenv(\"GISBasic\");\n  string demPath = workDir + \"/../Data/Terrain/dem.tif\";\n\n  GDALDataset *dem = (GDALDataset *)GDALOpen(demPath.c_str(), GA_ReadOnly);\n  if (!dem) {\n    cout &lt;&lt; \"Can't Open Image!\" &lt;&lt; endl;\n    return;\n  }\n\n  demWidth = dem-&gt;GetRasterXSize();\n  demHeight = dem-&gt;GetRasterYSize();\n\n  dem-&gt;GetGeoTransform(geoTransform);\n  startX = geoTransform[0];  //左上角点坐标X\n  dx = geoTransform[1];      // X方向的分辨率\n  startY = geoTransform[3];  //左上角点坐标Y\n  dy = geoTransform[5];      // Y方向的分辨率\n\n  // noValue = dem-&gt;GetRasterBand(1)-&gt;GetNoDataValue();\n\n  size_t demBufNum = (size_t)demWidth * demHeight;\n  demBuf.resize(demBufNum, 0);\n\n  int depth = sizeof(float);\n  dem-&gt;GetRasterBand(1)-&gt;RasterIO(GF_Read, 0, 0, demWidth, demHeight,\n                                  demBuf.data(), demWidth, demHeight,\n                                  GDT_Float32, depth, demWidth * depth);\n\n  GDALClose(dem);\n  dem = nullptr;\n}\n\nvoid HandleDem() {\n  size_t dstBufNum = (size_t)demWidth * demHeight * dstBandNum;\n  dstBuf.resize(dstBufNum, 255);\n\n  for (size_t i = 0; i &lt; heightThresholdList.size(); i++) {\n    double heightThreshold = heightThresholdList[i];\n    F_RGB thresholdRgb = heightRGBList[i];\n\n    for (int yi = 0; yi &lt; demHeight; yi++) {\n      for (int xi = 0; xi &lt; demWidth; xi++) {\n        size_t m = (size_t)demWidth * yi + xi;\n\n        if (demBuf[m] &gt; heightThreshold) {\n          size_t n = (size_t)demWidth * dstBandNum * yi + dstBandNum * xi;\n          for (int bi = 0; bi &lt; 3; bi++) {\n            dstBuf[n + bi] = (uint8_t)thresholdRgb[bi];\n          }\n        }\n      }\n    }\n  }\n}\n\nvoid WriteDst() {\n  string workDir = getenv(\"GISBasic\");\n  string demPath = workDir + \"/../Data/Terrain/dst.tif\";\n\n  GDALDriver *pDriver =\n      GetGDALDriverManager()-&gt;GetDriverByName(\"GTIFF\");  //图像驱动\n  char **ppszOptions = NULL;\n  ppszOptions =\n      CSLSetNameValue(ppszOptions, \"BIGTIFF\", \"IF_NEEDED\");  //配置图像信息\n  GDALDataset *dst = pDriver-&gt;Create(demPath.c_str(), demWidth, demHeight, 4,\n                                     GDT_Byte, ppszOptions);\n  if (!dst) {\n    printf(\"Can't Write Image!\");\n    return;\n  }\n\n  dst-&gt;SetGeoTransform(geoTransform);\n\n  int depth = sizeof(uint8_t);\n  dst-&gt;RasterIO(GF_Write, 0, 0, demWidth, demHeight, dstBuf.data(), demWidth,\n                demHeight, GDT_Byte, dstBandNum, nullptr, dstBandNum * depth,\n                demWidth * dstBandNum * depth, depth);\n\n  GDALClose(dst);\n  dst = nullptr;\n}\n\nint main() {\n  GDALAllRegister();  // GDAL所有操作都需要先注册格式\n\n  //设置Proj数据\n  std::string projDataPath = getenv(\"GISBasic\");\n  projDataPath += \"/share/proj\";\n  CPLSetConfigOption(\"PROJ_LIB\", projDataPath.c_str());\n\n  ReadDem();\n\n  InitColorTable();\n\n  double heightThreshold = startHeight;\n  while (heightThreshold &lt; endHeight) {\n    heightThresholdList.push_back(heightThreshold);\n    heightThreshold = heightThreshold + heightInterval;\n  }\n\n  if (heightThresholdList.size() == 1) {\n    heightRGBList.push_back(tableRGB[0]);\n  } else {\n    size_t step = tableRGB.size() / (heightThresholdList.size() - 1);\n    size_t index = 0;\n    for (size_t i = 0; i &lt; heightThresholdList.size() - 1; i++) {\n      heightRGBList.push_back(tableRGB[index]);\n      index = index + step;\n    }\n    heightRGBList.push_back(tableRGB[tableRGB.size() - 1]);\n  }\n\n  HandleDem();\n\n  WriteDst();\n\n  return 0;\n}\n</code></pre>\n<p>与基于矢量要素的几何运算不同，基于栅格的运算更多的是基于图像处理的思想。我们并不知道每一条具体的等高线在哪里，但是我们可以向栅格中插值。具体来说，就是如果该栅格所代表的点的高程大于高程区间的临界值，那么就向其填充合适的颜色；按照高程区间格式填充多次，直到所有高程区间都填充完成。这样，等高线就由不同的颜色区间体现出来了。最终生成的等高线图如下图6.21所示。</p>\n<p><img alt=\"图6.21 根据DEM生成等高线图\" class=\"lazyload\" /></p>\n<h1 id=\"结语\">结语</h1>\n<p>在本章中，我们详细论述了一种综合了矢量特性与栅格特性的地理空间数据——地形。因此，如果我们前面对矢量和栅格掌握的比较熟练，掌握地形相关的知识也不是太难。此外，我们还介绍了一些地形数据的基本处理方法，地形内插算法，晕渲图与等高线图的制作。其实地形相关的知识非常之丰富，远不是本章有限的内容所能涵盖的。而且，地形数据有其数据敏感性，普通从业者想获取高精度的数据进行深入研究也十分不易。不过还是那句话，示例的结果不重要，重要的是要了解其底层的原理，建立一个相对系统而全面的认知，在遇到更为复杂的难题时才能心中不慌。</p>\n<hr />\n<p>本文节选自作者新书《GIS基础原理与技术实践》第6章。书中系统讲解 GIS 核心理论与多语言实战，适合开发者与高校师生。</p>\n<p>📚 <strong>配套资源开源</strong>：<a href=\"https://github.com/fafa1899/GISBasic\" rel=\"noopener nofollow\" target=\"_blank\">GitHub</a> | <a href=\"https://gitcode.com/charlee44/GISBasic\" rel=\"noopener nofollow\" target=\"_blank\">GitCode</a></p>\n<p>🛒 <strong>支持正版</strong>：<a href=\"https://item.jd.com/14603137.html\" rel=\"noopener nofollow\" target=\"_blank\">京东</a>｜<a href=\"https://product.dangdang.com/29988568.html\" rel=\"noopener nofollow\" target=\"_blank\">当当</a></p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-22 21:19</span>&nbsp;\n<a href=\"https://www.cnblogs.com/charlee44\">charlee44</a>&nbsp;\n阅读(<span id=\"post_view_count\">31</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "一天一个Python库：requests - 简单好用的HTTP请求库",
      "link": "https://www.cnblogs.com/min2k/p/19519059",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/min2k/p/19519059\" id=\"cb_post_title_url\" title=\"发布于 2026-01-22 21:16\">\n    <span>一天一个Python库：requests - 简单好用的HTTP请求库</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"requests---简单好用的http请求库\">requests - 简单好用的HTTP请求库</h1>\n<h2 id=\"一什么是requests\">一、什么是requests？</h2>\n<p><strong>requests</strong> 是一个用于发送HTTP请求的 Python 库。<br />\n它可以帮助你：</p>\n<ul>\n<li>轻松发送GET、POST、PUT、DELETE等请求</li>\n<li>处理Cookie、会话等复杂性</li>\n<li>自动解压缩内容</li>\n<li>处理国际化域名和URL</li>\n</ul>\n<h2 id=\"二应用场景\">二、应用场景</h2>\n<p><strong>requests</strong> 广泛应用于以下实际场景：</p>\n<ul>\n<li><strong>Web爬虫</strong>: 抓取网页内容，分析数据。</li>\n<li><strong>API交互</strong>: 与各种Web服务（如天气API、社交媒体API）进行通信，获取或提交数据。</li>\n<li><strong>自动化测试</strong>: 自动化测试Web应用的接口功能。</li>\n</ul>\n<h2 id=\"三如何安装\">三、如何安装</h2>\n<ol>\n<li>使用 pip 安装</li>\n</ol>\n<pre><code class=\"language-bash\">pip install requests\n\n# 如果安装慢的话，推荐使用国内镜像源\npip install requests -i https://pypi.tuna.tsinghua.edu.cn/simple/\n</code></pre>\n<ol start=\"2\">\n<li>使用 <a href=\"https://www.min2k.com/tools/python-run/\" rel=\"noopener nofollow\" target=\"_blank\">PythonRun</a> 在线运行代码（无需本地安装）</li>\n</ol>\n<h2 id=\"四示例代码\">四、示例代码</h2>\n<p>发送一个GET请求并检查状态码</p>\n<pre><code class=\"language-python\">import requests\n\n# 定义要请求的URL\nurl = \"https://www.example.com\"\n\n# 发送GET请求\nresponse = requests.get(url)\n\n# 检查HTTP状态码\nif response.status_code == 200:\n    print(f\"请求成功！状态码: {response.status_code}\")\n    # 打印响应内容的开头部分\n    print(\"响应内容前100个字符:\", response.text[:100])\nelse:\n    print(f\"请求失败！状态码: {response.status_code}\")\n</code></pre>\n<p>使用 <a href=\"https://www.min2k.com/tools/python-run/?code=import%20requests%0A%0A%23%20%E5%AE%9A%E4%B9%89%E8%A6%81%E8%AF%B7%E6%B1%82%E7%9A%84URL%0Aurl%20%3D%20%22https%3A%2F%2Fwww.example.com%22%0A%0A%23%20%E5%8F%91%E9%80%81GET%E8%AF%B7%E6%B1%82%0Aresponse%20%3D%20requests.get%28url%29%0A%0A%23%20%E6%A3%80%E6%9F%A5HTTP%E7%8A%B6%E6%80%81%E7%A0%81%0Aif%20response.status_code%20%3D%3D%20200%3A%0A%20%20%20%20print%28f%22%E8%AF%B7%E6%B1%82%E6%88%90%E5%8A%9F%EF%BC%81%E7%8A%B6%E6%80%81%E7%A0%81%3A%20%7Bresponse.status_code%7D%22%29%0A%20%20%20%20%23%20%E6%89%93%E5%8D%B0%E5%93%8D%E5%BA%94%E5%86%85%E5%AE%B9%E7%9A%84%E5%BC%80%E5%A4%B4%E9%83%A8%E5%88%86%0A%20%20%20%20print%28%22%E5%93%8D%E5%BA%94%E5%86%85%E5%AE%B9%E5%89%8D100%E4%B8%AA%E5%AD%97%E7%AC%A6%3A%22%2C%20response.text%5B%3A100%5D%29%0Aelse%3A%0A%20%20%20%20print%28f%22%E8%AF%B7%E6%B1%82%E5%A4%B1%E8%B4%A5%EF%BC%81%E7%8A%B6%E6%80%81%E7%A0%81%3A%20%7Bresponse.status_code%7D%22%29\" rel=\"noopener nofollow\" target=\"_blank\">PythonRun</a> 在线运行这段代码，结果如下：</p>\n<pre><code class=\"language-text\">请求成功！状态码: 200\n响应内容前100个字符: &lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;title&gt;Example Domain&lt;/title&gt;&lt;meta name=\"viewport\" content=\"wid\n</code></pre>\n<p>使用 <a href=\"https://www.min2k.com/tools/mermaid/?code=flowchart%20TB%0A%20%20A%5B%E5%BC%80%E5%A7%8B%5D%20--%3E%20B%7B%E5%AE%9A%E4%B9%89URL%7D%3B%0A%20%20B%20--%3E%20C%5B%E5%8F%91%E9%80%81GET%E8%AF%B7%E6%B1%82%5D%3B%0A%20%20C%20--%3E%20D%7B%E6%A3%80%E6%9F%A5%E7%8A%B6%E6%80%81%E7%A0%81%20%3D%3D%20200%3F%7D%3B%0A%20%20D%20--%20%E6%98%AF%20--%3E%20E%5B%E6%89%93%E5%8D%B0%E8%AF%B7%E6%B1%82%E6%88%90%E5%8A%9F%E4%BF%A1%E6%81%AF%E5%92%8C%E5%93%8D%E5%BA%94%E5%86%85%E5%AE%B9%5D%3B%0A%20%20D%20--%20%E5%90%A6%20--%3E%20F%5B%E6%89%93%E5%8D%B0%E8%AF%B7%E6%B1%82%E5%A4%B1%E8%B4%A5%E4%BF%A1%E6%81%AF%5D%3B%0A%20%20E%20--%3E%20G%5B%E7%BB%93%E6%9D%9F%5D%3B%0A%20%20F%20--%3E%20G%3B\" rel=\"noopener nofollow\" target=\"_blank\">MermaidGo</a> 绘制示例代码的流程图，结果如下：</p>\n<p><img alt=\"mermaid-20260122_210458\" class=\"lazyload\" /></p>\n<h2 id=\"五学习资源\">五、学习资源</h2>\n<ol>\n<li>开源项目：<a href=\"https://github.com/psf/requests\" rel=\"noopener nofollow\" target=\"_blank\">requests</a></li>\n<li>中文自述：<a href=\"https://www.python64.cn/readme/requests/\" rel=\"noopener nofollow\" target=\"_blank\">REMDME</a></li>\n<li>在线运行：<a href=\"https://www.min2k.com/tools/python-run/\" rel=\"noopener nofollow\" target=\"_blank\">PythonRun</a></li>\n</ol>\n<blockquote>\n<p>如果这篇文章对你有帮助，欢迎点赞、收藏、转发！<br />\n学习过程中有任何问题，欢迎在评论区留言交流～</p>\n</blockquote>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-22 21:16</span>&nbsp;\n<a href=\"https://www.cnblogs.com/min2k\">敏编程</a>&nbsp;\n阅读(<span id=\"post_view_count\">69</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI开发-python-langchain框架（1-4动态少样本提示）",
      "link": "https://www.cnblogs.com/yclh/p/19518845",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/yclh/p/19518845\" id=\"cb_post_title_url\" title=\"发布于 2026-01-22 19:31\">\n    <span>AI开发-python-langchain框架（1-4动态少样本提示）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p class=\"qwen-markdown-paragraph\"><span class=\"qwen-markdown-text\">这个代码的核心功能是：<strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">基于输入词的长度动态选择反义词示例，并调用大模型生成反义词</span></strong><span class=\"qwen-markdown-text\">，体现了 <strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">“动态少样本提示（Dynamic Few-Shot Prompting）”</span></strong><span class=\"qwen-markdown-text\"> 与 <strong class=\"qwen-markdown-strong\"><span class=\"qwen-markdown-text\">“上下文长度感知的示例选择”</span></strong><span class=\"qwen-markdown-text\"> 的能力。</span></span></span></span></p>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:python;gutter:true;\">from langchain.prompts import FewShotPromptTemplate, PromptTemplate\nfrom langchain.prompts.example_selector import LengthBasedExampleSelector\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\nimport os\nfrom dotenv import load_dotenv\n\n\n# 定义反义词任务的示例数据集（few-shot examples）\n# 每个示例包含一个输入词（input）和对应的反义词（output）\nexamples = [\n    {\"input\": \"开心\", \"output\": \"伤心\"},\n    {\"input\": \"高\", \"output\": \"矮\"},\n    {\"input\": \"精力充沛\", \"output\": \"没精打采\"},\n    {\"input\": \"粗\", \"output\": \"细\"},\n]\n\n# 定义单个示例的格式模板\n# 使用 {input} 和 {output} 作为占位符，用于后续填充具体值\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],  # 声明模板中使用的变量名\n    template=\"Input: {input}\\nOutput: {output}\",  # 示例的文本格式\n)\n\n# 创建一个基于长度的示例选择器（LengthBasedExampleSelector）\n# 作用：根据输入提示的总长度动态选择最合适的示例数量，避免超出模型上下文限制\nexample_selector = LengthBasedExampleSelector(\n    examples=examples,  # 提供所有候选示例\n    example_prompt=example_prompt,  # 用于格式化每个示例的模板\n    max_length=25,  # 设定整个 prompt（含前缀、示例、后缀）的最大 token 长度（此处为字符数近似）\n    # 注意：LengthBasedExampleSelector 默认使用 len(text) 计算长度（非精确 token 数），适用于简单场景\n)\n\n# 构建动态少样本提示模板（FewShotPromptTemplate）\n# 它会根据输入内容的长度，自动从 examples 中选择合适数量的示例插入到 prompt 中\ndynamic_prompt = FewShotPromptTemplate(\n    example_selector=example_selector,  # 使用上面定义的动态选择器（而非固定示例列表）\n    example_prompt=example_prompt,      # 单个示例的格式\n    prefix=\"给出每个输入的反义词\",       # 提示的开头部分（任务指令）\n    suffix=\"Input: {adjective}\\nOutput:\",  # 提示的结尾部分，包含待预测的输入占位符\n    input_variables=[\"adjective\"],      # 声明最终用户输入的变量名（与 suffix 中的 {adjective} 对应）\n)\n\n# === 测试 1：输入较短，应选择多个示例 ===\nprint(\"【测试1】输入较短，选择多个示例：\")\nprint(dynamic_prompt.format(adjective=\"big\"))\n\nprint('------------')\n\n# === 测试 2：输入很长，应只选择少量或一个示例以控制总长度 ===\nlong_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\nprint(\"【测试2】输入很长，仅选择一个示例：\")\nprint(dynamic_prompt.format(adjective=long_string))\n\nprint('------------')\n\n# === 测试 3：动态添加新示例 ===\n# 向示例选择器中新增一个示例（\"胖\" -&gt; \"瘦\"）\nnew_example = {\"input\": \"胖\", \"output\": \"瘦\"}\ndynamic_prompt.example_selector.add_example(new_example)\nprint(\"【测试3】添加新示例后，查询'热情'：\")\nprint(dynamic_prompt.format(adjective=\"热情\"))\n\nprint('------------')\n\n# === 配置并调用 DeepSeek 大语言模型 ===\n\nllm = ChatOpenAI(\n    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n    base_url=os.getenv(\"DEEP_URL\"),  # Deepseek 的 API 基础地址\n    model=\"deepseek-v3:671b\",  # Deepseek 对话模型（可选：deepseek-chat-pro 等高级模型）\n    temperature=0.7,  # 温度参数（0-1，越低越稳定）\n    max_tokens=1024  # 最大生成 tokens\n)\n\n# 创建字符串输出解析器，用于将模型返回的 AIMessage 转换为纯文本\noutput_parser = StrOutputParser()\n\n# 构建处理链（Chain）：prompt → LLM → output parser\n# 使用 LangChain 的管道操作符 `|` 连接各组件\nchain = dynamic_prompt | llm | output_parser\n\n# 调用链，传入输入变量 {\"adjective\": \"热情\"}\n# 注意：chain.invoke() 内部已包含 llm 调用和 output_parser 解析，无需再手动调用 output_parser\nmessage = chain.invoke({\"adjective\": \"热情\"})\n\n# ⚠️ 注意：上一行 `chain.invoke()` 已经返回了字符串（因为最后是 StrOutputParser）\n# 所以下面这行是多余的，甚至会导致错误（因为 message 已是 str，不能再次 invoke）\n# result = output_parser.invoke(message)  # ❌ 错误：message 是 str，不是 AIMessage\n\n# 正确做法：直接使用 message 作为结果\nresult = message\n\nprint('###############')\nprint(\"【模型输出】\")\nprint(result)\n</pre>\n</div>\n<h1>输出结果：</h1>\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:python;gutter:true;\">【测试1】输入较短，选择多个示例：\n给出每个输入的反义词\n\nInput: 开心\nOutput: 伤心\n\nInput: 高\nOutput: 矮\n\nInput: 精力充沛\nOutput: 没精打采\n\nInput: 粗\nOutput: 细\n\nInput: big\nOutput:\n------------\n【测试2】输入很长，仅选择一个示例：\n给出每个输入的反义词\n\nInput: 开心\nOutput: 伤心\n\nInput: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\nOutput:\n------------\n【测试3】添加新示例后，查询'热情'：\n给出每个输入的反义词\n\nInput: 开心\nOutput: 伤心\n\nInput: 高\nOutput: 矮\n\nInput: 精力充沛\nOutput: 没精打采\n\nInput: 粗\nOutput: 细\n\nInput: 胖\nOutput: 瘦\n\nInput: 热情\nOutput:\n------------\n###############\n【模型输出】\n冷淡\n</pre>\n</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h1>&nbsp;</h1>\n<h1>核心要点总结</h1>\n<div>&nbsp;</div>\n<div>这段代码是基于 LangChain 框架对接 DeepSeek 大模型，实现「动态少样本（Few-Shot）反义词生成」的完整案例，核心解决「固定示例易超出模型上下文长度」的问题，通过动态示例选择器适配不同长度输入，同时结合 LangChain 链式调用简化模型调用流程，先明确整体定位，再拆解核心重点：</div>\n<div>&nbsp;</div>\n<h2>一、整体流程概览（核心逻辑链）</h2>\n<div>&nbsp;</div>\n<div>代码遵循 LangChain 「示例准备 → 动态提示构建 → 模型调用 → 结果解析」的少样本学习经典流程，整体可概括为：</div>\n<div>\n<div dir=\"ltr\">\n<div>\n<pre><code>1. 定义反义词任务的固定示例数据集，为模型提供参考案例\n2. 配置基于长度的示例选择器，根据输入文本长度动态筛选示例数量\n3. 构建动态少样本提示模板，自动适配输入长度生成合规 Prompt\n4. 初始化 DeepSeek 模型客户端，配置核心调用参数\n5. 构建「提示模板 → 大模型 → 输出解析」的链式调用流程\n6. 调用链条完成反义词生成，并输出纯文本结果\n</code></pre>\n</div>\n<div>&nbsp;</div>\n</div>\n</div>\n<div>&nbsp;</div>\n<h2>二、核心重点拆解（必掌握）</h2>\n<div>&nbsp;</div>\n<h3>1. 少样本提示（Few-Shot Prompt）核心组件</h3>\n<div>&nbsp;</div>\n<div>这是实现「模型参考示例生成结果」的基础，也是 LangChain 提示工程的核心用法：</div>\n<div>&nbsp;</div>\n<div>&nbsp; &nbsp;<code>- 示例数据集（examples）：以键值对形式存储「输入-输出」示例，为模型提供任务参考（如\"开心\"→\"伤心\"）；</code></div>\n<div><code> - 单示例模板（example_prompt）：定义单个示例的文本格式（Input/Output 固定样式），统一示例展示形式；</code></div>\n<div>\n<div dir=\"ltr\">\n<div>\n<pre><code>- 动态少样本模板（FewShotPromptTemplate）：整合示例选择器、单示例模板、前缀/后缀，生成最终发给模型的完整 Prompt；\n  - prefix：任务指令（\"给出每个输入的反义词\"），明确模型要执行的任务；\n  - suffix：待填充的用户输入占位符，承接动态输入内容。\n</code></pre>\n</div>\n</div>\n</div>\n<h3>2. 动态示例选择器（LengthBasedExampleSelector）</h3>\n<div>&nbsp;</div>\n<div>这是代码的核心亮点，解决「固定示例数量易超上下文长度」的问题：</div>\n<div>\n<div dir=\"ltr\">\n<div>\n<pre><code>核心作用：根据输入文本的长度，自动计算并选择合适数量的示例（输入越长，选的示例越少），避免 Prompt 总长度超出模型上下文限制；\n关键参数：\n  - examples：候选示例列表；\n  - example_prompt：示例格式化模板（用于计算单示例长度）；\n  - max_length：Prompt 允许的最大长度（此处为字符数近似值）。\n</code></pre>\n</div>\n<div>&nbsp;</div>\n</div>\n</div>\n<div>&nbsp;</div>\n<h3>3. LangChain 链式调用（| 操作符）</h3>\n<div>&nbsp;</div>\n<div>简化多组件协作流程，是 LangChain 核心设计理念：</div>\n<div>\n<div dir=\"ltr\">\n<div>\n<pre><code>- 链条构成：dynamic_prompt（生成 Prompt） | llm（调用模型） | output_parser（解析结果）；\n- 核心优势：无需手动分步调用（先格式化 Prompt、再调用模型、最后解析结果），一行代码完成全流程；\n- 调用方式：chain.invoke({\"adjective\": \"热情\"}) 传入输入变量，直接返回解析后的纯文本结果。\n</code></pre>\n</div>\n<div>&nbsp;</div>\n</div>\n</div>\n<div>&nbsp;</div>\n<h3>4. 输出解析器（StrOutputParser）</h3>\n<div>&nbsp;</div>\n<div>解决「模型返回 AIMessage 对象→提取纯文本」的问题：</div>\n<div>\n<div dir=\"ltr\">\n<div>\n<pre><code>核心作用：将 LangChain 模型返回的 AIMessage 类型（含 content/metadata 等字段）转换为纯字符串，简化结果使用；\n关键注意点：链式调用中已包含解析步骤，无需手动再次调用 output_parser.invoke()（否则会报错）。</code></pre>\n</div>\n</div>\n</div>\n<p>&nbsp;</p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-22 19:31</span>&nbsp;\n<a href=\"https://www.cnblogs.com/yclh\">万笑佛</a>&nbsp;\n阅读(<span id=\"post_view_count\">47</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}