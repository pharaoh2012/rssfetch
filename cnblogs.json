{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "32岁程序员猝死：让我想起了我曾经的加班经历，庆幸自己还活着",
      "link": "https://www.cnblogs.com/StephenYoung/p/19539895",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/StephenYoung/p/19539895\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 20:41\">\n    <span>32岁程序员猝死：让我想起了我曾经的加班经历，庆幸自己还活着</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>最近，看到32岁程序员猝死的新闻刷爆全网。</p>\n<p>我瞬间想起了自己曾经的加班经历，心底只剩一句庆幸——还好，我还活着。</p>\n<p>曾经，我负责全球著名A客户的项目设备软件开发，那段时间常年辗转于国内各大代工厂，脚步从未停歇。最难忘的一次，是连续加班整整90天，这三个月里，我几乎连轴转、无一天休息，其中还有好几天熬了通宵。因为工厂是24小时的，设备是24小时的。新产品试产，任何一个环节的任何一个小问题，都是大问题。必须重视，必须解决。对于质量的把控，近乎变态，否则，这家A客户的产品也不可能独步全球了。</p>\n<p>印象最深的一次，头天早上7点准时起床，匆匆洗漱后就钻进工厂车间投入工作，直到第二天上午八九点，才拖着灌了铅似的身体走出厂区。本以为能短暂歇口气、眯上一会儿，可当天下午3点，一个紧急电话突然打来，我又不得不打起精神，风风火火地再次冲进车间，投入到设备调试中。</p>\n<p>车间里调试代码，必须站着操作电脑，我平均一天站立超过10小时，下班后双脚肿得连鞋子都难脱下。一段时间后，痔疮也犯了，彼时出差在陌生城市，我只能独自抽2个小时去附近医院拿药，即便顶着疼痛，第二天依旧要站着调试代码。当时我只有一个念头：忙完这阵，一定要换个环境，别真的没命挣钱、没命花。</p>\n<p>和我有同样感受的，还有我们在同一个项目组的合作厂商的一个H哥们儿，H哥和我同龄，只是人家正经985毕业的，而我不是。H哥也曾多次跟我抱怨：“真不想做A客户的项目了，太累了，吃不消。每天连饭都不能按时吃上，最近都饿得胃疼。跟客户开会是英文，我必须吃饱了才能听懂老外说话”。确实，可常常到下午七八点因为客户还在跟我们开会，或者check我们的项目进度，安排工作任务。经常都是没吃晚饭，那会儿脑子发懵，别说听英语了，我连汉语都听不懂了。我回答H哥说：哈哈，你这是电池没电了嘛，当然听不懂了。我有电也听不太懂多少。确实，现在我也是也饿得不行，这绝非长久之计呀。</p>\n<p>果然，项目的下一个阶段开始时，我没再见到他的身影，微信询问后才得知，他申请调到了其他项目，不用再饿肚子加班了，哈哈。偶尔的加班也能接受，语气里满是久违的朝气与平和。</p>\n<p>半年后，我也抓住一次偶然的机会离职，离开了原东家，彻底告别了A客户的项目，不用再忍受饿肚子、连轴转的加班日子。</p>\n<p>如今回想那段日子，依旧心有余悸。</p>\n<p>感叹：那些没日没夜的加班、身体承受的痛苦，都成了难忘的过往。工作是生活的一部分，挣钱是为了滋养生活，而非消耗生活。加班或许无法完全避免，挣钱也依然是我们前行的动力，但我们不能被这份执念裹挟，忽略了生活质量，透支了身体健康。</p>\n<p>真正的成熟，从来不是一味拼命加班、盲目挣钱，而是懂得平衡加班与生活，兼顾挣钱与健康。工作再重要，也不能替代三餐四季的安稳；薪资再诱人，也不及身体健康的珍贵。愿每一个为生活奔波、为挣钱努力的人，都能守住平衡，不辜负工作，也不辜负自己，不透支健康，也不浪费时光——毕竟，能好好工作、好好生活、好好活着，才是最难得的幸福。</p>\n<p>今年，2026年，我依然庆幸自己还活着。<br />\n<img alt=\"公众号动漫+微信官方码\" src=\"https://img2024.cnblogs.com/blog/2213992/202602/2213992-20260205204048587-607747837.png\" /></p>\n<p>更多精彩关注微信公众号：</p>\n<p>轻松做出漂亮的LabVIEW界面-FlateUI2.0(源码分享)</p>\n<p>15年技术人+3年创业失败者的感悟：工控圈里，技术的话语权为啥这么弱？</p>\n<p>LabVIEW记录XY轴运动轨迹并绘图的编程方法(源码附件)</p>\n<p>C#重难点2：事件</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 20:41</span>&nbsp;\n<a href=\"https://www.cnblogs.com/StephenYoung\">Stephen_Young</a>&nbsp;\n阅读(<span id=\"post_view_count\">14</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "利用自定义html元素实现支持实时修改的高亮代码块",
      "link": "https://www.cnblogs.com/Fan-iX/p/-/shadow-dom-code-highlight",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Fan-iX/p/-/shadow-dom-code-highlight\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 19:39\">\n    <span>利用自定义html元素实现支持实时修改的高亮代码块</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"利用自定义html元素实现支持实时修改的高亮代码块\">利用自定义html元素实现支持实时修改的高亮代码块</h1>\n<p>代码块高亮是前端开发中常见的需求，尤其是在展示代码片段的博客、文档等场景中。市面上有很多成熟的代码高亮库，比如<code>Highlight.js</code>、<code>Prism.js</code>等，它们都能很好地实现代码高亮功能。</p>\n<p>通常的高亮代码块是“静态”的，修改代码内容后需要对DOM元素重新应用高亮样式。由于涉及DOM操作，在Vue等前端框架中使用必须谨慎处理，否则会出现DOM树和虚拟DOM不一致的问题，造成很多麻烦。</p>\n<p>那么有没有办法让代码高亮不改变DOM结构呢？答案是有的，我们可以利用自定义HTML元素和Shadow DOM来实现这一点。</p>\n<h3 id=\"shadow-dom和自定义html元素\">Shadow DOM和自定义HTML元素</h3>\n<p>Shadow DOM允许我们创建封闭的DOM树，Shadow DOM内可以使用自己的样式，并封装复杂的逻辑，而不会影响到外部的DOM结构。现代浏览器的<code>&lt;input&gt;</code>（特别是<code>&lt;input type=\"range\"&gt;</code>、<code>&lt;input type=\"date\"&gt;</code>等复杂控件）元素就是利用Shadow DOM实现的。</p>\n<p>要想使用Shadow DOM，我们需要创建一个自定义HTML元素，并在其中通过<code>attachShadow</code>方法创建Shadow DOM。</p>\n<pre><code class=\"language-js\">class MyElement extends HTMLElement {\n    constructor() {\n        super()\n        const shadow = this.attachShadow({ mode: 'open' })\n        shadow.innerHTML = `&lt;p&gt;Hello, Shadow DOM!&lt;/p&gt;`\n    }\n}\ncustomElements.define('my-element', MyElement)\n</code></pre>\n<p>之后，我们就可以在HTML中使用<code>&lt;my-element&gt;&lt;/my-element&gt;</code>来插入这个自定义元素。</p>\n<pre><code class=\"language-html\">&lt;my-element&gt;&lt;/my-element&gt;\n</code></pre>\n<p>在DevTools中，我们可以看到<code>&lt;my-element&gt;</code>的渲染结果，其中包括元素内部的Shadow DOM：</p>\n<pre><code class=\"language-html\">&lt;my-element&gt;\n  #shadow-root (open)\n    &lt;p&gt;Hello, Shadow DOM!&lt;/p&gt;\n&lt;/my-element&gt;\n</code></pre>\n<h3 id=\"在自定义元素中获取内容\">在自定义元素中获取内容</h3>\n<p>我们希望在自定义元素中获取标签之间的内容。这可以通过插槽（slot）机制实现。插槽机制允许我们在自定义元素中定义占位符，外部传入的内容会被插入到这些占位符中。</p>\n<p>为了使用插槽，我们需要在Shadow DOM中添加一个<code>&lt;slot&gt;</code>元素：</p>\n<pre><code class=\"language-js\">class MyElement extends HTMLElement {\n    constructor() {\n        super()\n        const shadow = this.attachShadow({ mode: 'open' })\n        shadow.innerHTML = `&lt;slot&gt;&lt;/slot&gt;`\n        const slot = shadow.querySelector('slot')\n        slot.addEventListener('slotchange', this.handleSlotChange.bind(this))\n    }\n    handleSlotChange(event) {\n        const slot = event.target\n        console.log('Slot content changed:', slot.assignedNodes({ flatten: true }))\n    }\n}\ncustomElements.define('my-element', MyElement)\n</code></pre>\n<p>对于HTML片段</p>\n<pre><code class=\"language-html\">&lt;my-element id=\"my-el\"&gt;&lt;p&gt;This is slotted content.&lt;/p&gt;&lt;/my-element&gt;\n</code></pre>\n<p>当页面第一次加载时，控制台会显示</p>\n<pre><code>Slot content changed:&nbsp;[p]\n</code></pre>\n<p>其中<code>p</code>就是元素内部的<code>&lt;p&gt;</code>节点。</p>\n<p>如果我们动态修改<code>&lt;my-element&gt;</code>内的内容，比如通过JavaScript：</p>\n<pre><code class=\"language-js\">document.getElementById('my-el').innerHTML = '&lt;pre&gt;New slotted content1.&lt;/pre&gt;&lt;pre&gt;New slotted content2.&lt;/pre&gt;'\n</code></pre>\n<p>控制台会显示</p>\n<pre><code>Slot content changed: (2)&nbsp;[pre, pre]\n</code></pre>\n<p>两个<code>pre</code>节点就是我们新修改的内容。</p>\n<p>通过这种方法，我们可以在自定义元素中实时获取内容的变化。</p>\n<h3 id=\"利用自定义元素实现高亮代码块\">利用自定义元素实现高亮代码块</h3>\n<p>结合前面的内容，我们可以创建一个自定义元素<code>&lt;pre-highlight&gt;</code>，用于实现高亮代码块的功能。只需要监听插槽内容的变化，将内容传递给高亮库进行处理，然后将处理后的结果显示出来即可。</p>\n<pre><code class=\"language-js\">class PreHighlightElement extends HTMLElement {\n    constructor() {\n        super()\n        const shadow = this.attachShadow({ mode: 'open' })\n        shadow.innerHTML = `\n&lt;link rel=\"stylesheet\" href=\"https://unpkg.com/@highlightjs/cdn-assets/styles/github.min.css\"&gt;\n&lt;pre id=\"code\"&gt;&lt;/pre&gt;\n&lt;pre hidden&gt;&lt;slot&gt;&lt;/slot&gt;&lt;/pre&gt;\n`\n        this.__code = this.shadowRoot.querySelector('#code')\n        this.__slot = this.shadowRoot.querySelector('slot')\n        this.__slot.addEventListener('slotchange', this.highlightContent.bind(this))\n    }\n\n    highlightContent() {\n        if (typeof hljs === 'undefined') return\n\n        let text = this.__slot.assignedNodes({ flatten: true }).map(n =&gt; n.textContent).join(\"\")\n        const code = document.createElement('code')\n\n        const result = hljs.highlightAuto(text)\n        code.innerHTML = result.value\n        if (result.language) code.classList.add(`language-${result.language}`)\n        this.__code.replaceChildren(code)\n    }\n}\n\ncustomElements.define('pre-highlight', PreHighlightElement)\n</code></pre>\n<p>使用方法：</p>\n<pre><code class=\"language-html\">&lt;pre-highlight id=\"my-el\"&gt;\nfunction helloWorld() {\n    console.log(\"Hello, world!\")\n}\n&lt;/pre-highlight&gt;\n</code></pre>\n<p>渲染结果为</p>\n<pre><code class=\"language-html\">&lt;pre-highlight id=\"my-el\"&gt;\n  #shadow-root (open)\n    &lt;link rel=\"stylesheet\" href=\"https://unpkg.com/@highlightjs/cdn-assets/styles/github.min.css\"&gt;\n    &lt;pre id=\"code\"&gt;\n      &lt;code class=\"language-javascript\"&gt;\n        &lt;span class=\"hljs-keyword\"&gt;function&lt;/span&gt;\n        &lt;span class=\"hljs-title function_\"&gt;helloWorld&lt;/span&gt;\n        \"(\"\n        &lt;span class=\"hljs-params\"&gt;&lt;/span&gt;)\n        \"{\"\n        &lt;span class=\"hljs-variable language_\"&gt;console&lt;/span&gt;\n        \".\"\n        &lt;span class=\"hljs-title function_\"&gt;log&lt;/span&gt;\n        \"(\"\n        &lt;span class=\"hljs-string\"&gt;\"Hello, world!\"&lt;/span&gt;\n        \") }\"\n      &lt;/code&gt;\n    &lt;/pre&gt;\n    &lt;pre hidden=\"\"&gt;\n      &lt;slot&gt;\n        #text\n      &lt;/slot&gt;\n    &lt;/pre&gt;\n  \" function helloWorld() { console.log(\"Hello, world!\") } \"\n&lt;/pre-highlight&gt;\n</code></pre>\n<p>修改<code>&lt;pre-highlight&gt;</code>内的内容后，高亮效果会自动更新。</p>\n<pre><code class=\"language-js\">document.getElementById('my-el').textContent = `void helloWorld(void) {\n    printf(\"Hello, World!\");\n}`\n</code></pre>\n<p>渲染结果为</p>\n<pre><code class=\"language-html\">&lt;pre-highlight id=\"my-el\"&gt;\n  #shadow-root (open)\n    &lt;link rel=\"stylesheet\" href=\"https://unpkg.com/@highlightjs/cdn-assets/styles/github.min.css\"&gt;\n    &lt;pre id=\"code\"&gt;\n      &lt;code class=\"language-cpp\"&gt;\n        &lt;span class=\"hljs-function\"&gt;\n          &lt;span class=\"hljs-type\"&gt;void&lt;/span&gt;\n          &lt;span class=\"hljs-title\"&gt;helloWorld&lt;/span&gt;\n          &lt;span class=\"hljs-params\"&gt;\n            \"(\"\n            &lt;span class=\"hljs-type\"&gt;void&lt;/span&gt;\n            \")\"\n          &lt;/span&gt;\n        &lt;/span&gt;\n        \"{\"\n        &lt;span class=\"hljs-built_in\"&gt;printf&lt;/span&gt;\n        \"(\"\n        &lt;span class=\"hljs-string\"&gt;\"Hello, World!\"&lt;/span&gt;\n        \"); }\"\n      &lt;/code&gt;\n    &lt;/pre&gt;\n    &lt;pre hidden=\"\"&gt;\n      &lt;slot&gt;\n        #text\n      &lt;/slot&gt;\n    &lt;/pre&gt;\n  \"void helloWorld(void) { printf(\"Hello, World!\"); }\"\n&lt;/pre-highlight&gt;\n</code></pre>\n<h3 id=\"一些改进\">一些改进</h3>\n<p>为了避免高亮库加载和高亮处理过程中的闪烁，我们可以在Shadow DOM中使用两个<code>&lt;pre&gt;</code>元素：一个用于显示原始内容，另一个用于显示高亮后的内容。初始时只显示原始内容，高亮处理完成后再切换显示。</p>\n<p>此外，我们还可以添加一个<code>lang</code>属性，允许用户指定代码语言，以提高高亮的准确性。</p>\n<p>最终结果如下：</p>\n<pre><code class=\"language-js\">class PreHighlightElement extends HTMLElement {\n    constructor() {\n        super()\n        const shadow = this.attachShadow({ mode: 'open' })\n        shadow.innerHTML = `\n&lt;link rel=\"stylesheet\" href=\"https://unpkg.com/@highlightjs/cdn-assets/styles/github.min.css\"&gt;\n&lt;pre id=\"raw\"&gt;&lt;slot&gt;&lt;/slot&gt;&lt;/pre&gt;\n&lt;pre id=\"cooked\" hidden&gt;&lt;/pre&gt;\n`\n        this.__raw = this.shadowRoot.querySelector('#raw')\n        this.__cooked = this.shadowRoot.querySelector('#cooked')\n        this.__slot = this.shadowRoot.querySelector('slot')\n        this.__slot.addEventListener('slotchange', this.highlightContent.bind(this))\n    }\n\n    highlightContent() {\n        this.__raw.hidden = false\n        this.__cooked.hidden = true\n        if (typeof hljs === 'undefined') return\n\n        let text = this.__slot.assignedNodes({ flatten: true }).map(n =&gt; n.textContent).join(\"\")\n        const lang = this.getAttribute('lang')\n        const code = document.createElement('code')\n\n        if (lang) {\n            const result = hljs.highlight(text, { language: lang, ignoreIllegals: true })\n            code.innerHTML = result.value\n            code.classList.add(`language-${lang}`)\n        } else {\n            const result = hljs.highlightAuto(text)\n            code.innerHTML = result.value\n            if (result.language) code.classList.add(`language-${result.language}`)\n        }\n        this.__cooked.replaceChildren(code)\n        this.__raw.hidden = true\n        this.__cooked.hidden = false\n    }\n}\n\ncustomElements.define('pre-highlight', PreHighlightElement)\n</code></pre>\n<p>用例：</p>\n<pre><code class=\"language-html\">&lt;pre-highlight id=\"code\" lang=\"html\"&gt;&lt;/pre-highlight&gt;\n&lt;input type=\"range\" id=\"input\" value=\"10\" /&gt;\n&lt;script&gt;\n    const input = document.getElementById('input')\n    const preHighlight = document.getElementById('code')\n\n    input.oninput = function(e) {\n        preHighlight.textContent = `&lt;textarea rows=\"${this.value}\" cols=\"50\"&gt;\n    Hello, world!\n&lt;/textarea&gt;`\n    }\n    input.oninput()\n&lt;/script&gt;\n</code></pre>\n<p>在这个例子中，我们创建了一个滑动条，可以动态修改<code>&lt;pre-highlight&gt;</code>内的代码内容，内容修改后会实时显示高亮效果。</p>\n<h3 id=\"在vue中使用pre-highlight\">在Vue中使用<code>&lt;pre-highlight&gt;</code></h3>\n<p>通过自定义元素的方法，我们可以轻松地在Vue项目中使用高亮代码块，而无需担心DOM和虚拟DOM的不一致问题。</p>\n<p>为了避免自定义元素和Vue组件名冲突，我们需要在配置中制定<code>isCustomElement</code>选项：</p>\n<pre><code class=\"language-js\">// vite.config.js\nexport default defineConfig({\n  plugins: [\n    vue({\n      template: {\n        compilerOptions: {\n          // 将所有含\"-\"的标签视为自定义元素\n          // Vue3中通常使用帕斯卡命名法（单词首字母大写）作为组件标签\n          isCustomElement: (tag) =&gt; tag.includes('-')\n        }\n      }\n    })\n  ]\n})\n</code></pre>\n<p>之后就可以在组件或页面中直接使用<code>&lt;pre-highlight&gt;</code>元素，内部可以使用Vue的数据绑定而不用担心虚拟DOM冲突的问题：</p>\n<pre><code class=\"language-html\">&lt;template&gt;\n    &lt;pre-highlight lang=\"javascript\"&gt;\nfunction greet({{arg}}) {\n    console.log(\"Hello, \" + {{arg}} + \"!\")\n}\n    &lt;/pre-highlight&gt;\n&lt;/template&gt;\n</code></pre>\n<h3 id=\"附完整的单页html演示代码\">附：完整的单页html演示代码</h3>\n<details>\n原生html\n<pre><code class=\"language-html\">&lt;html&gt;\n\n&lt;head&gt;\n    &lt;script src=\"https://unpkg.com/@highlightjs/cdn-assets/highlight.min.js\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n        class PreHighlightElement extends HTMLElement {\n            constructor() {\n                super()\n                const shadow = this.attachShadow({ mode: 'open' })\n                shadow.innerHTML = `\n&lt;link rel=\"stylesheet\" href=\"https://unpkg.com/@highlightjs/cdn-assets/styles/github.min.css\"&gt;\n&lt;pre id=\"raw\"&gt;&lt;slot&gt;&lt;/slot&gt;&lt;/pre&gt;\n&lt;pre id=\"cooked\" hidden&gt;&lt;/pre&gt;\n`\n                this.__raw = this.shadowRoot.querySelector('#raw')\n                this.__cooked = this.shadowRoot.querySelector('#cooked')\n                this.__slot = this.shadowRoot.querySelector('slot')\n                this.__slot.addEventListener('slotchange', this.highlightContent.bind(this))\n            }\n\n            highlightContent() {\n                this.__raw.hidden = false\n                this.__cooked.hidden = true\n                if (typeof hljs === 'undefined') return\n\n                let text = this.__slot.assignedNodes({ flatten: true }).map(n =&gt; n.textContent).join(\"\")\n                const lang = this.getAttribute('lang')\n                const code = document.createElement('code')\n\n                if (lang) {\n                    const result = hljs.highlight(text, { language: lang, ignoreIllegals: true })\n                    code.innerHTML = result.value\n                    code.classList.add(`language-${lang}`)\n                } else {\n                    const result = hljs.highlightAuto(text)\n                    code.innerHTML = result.value\n                    if (result.language) code.classList.add(`language-${result.language}`)\n                }\n                this.__cooked.replaceChildren(code)\n                this.__raw.hidden = true\n                this.__cooked.hidden = false\n            }\n        }\n\n        customElements.define('pre-highlight', PreHighlightElement)\n    &lt;/script&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;pre-highlight id=\"code\" lang=\"html\"&gt;&lt;/pre-highlight&gt;\n    &lt;input type=\"range\" id=\"input\" value=\"10\" /&gt;\n    &lt;script&gt;\n        const input = document.getElementById('input')\n        const preHighlight = document.getElementById('code')\n\n        input.oninput = function (e) {\n            preHighlight.textContent = `&lt;textarea rows=\"${this.value}\" cols=\"50\"&gt;\n    Hello, world!\n&lt;/textarea&gt;`\n        }\n        input.oninput()\n    &lt;/script&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre>\n</details>\n<details>\n使用Vue\n<pre><code class=\"language-html\">&lt;html&gt;\n\n&lt;head&gt;\n    &lt;script src=\"https://unpkg.com/vue@3/dist/vue.global.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://unpkg.com/@highlightjs/cdn-assets/highlight.min.js\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n        class PreHighlightElement extends HTMLElement {\n            constructor() {\n                super()\n                const shadow = this.attachShadow({ mode: 'open' })\n                shadow.innerHTML = `\n&lt;link rel=\"stylesheet\" href=\"https://unpkg.com/@highlightjs/cdn-assets/styles/github.min.css\"&gt;\n&lt;pre id=\"raw\"&gt;&lt;slot&gt;&lt;/slot&gt;&lt;/pre&gt;\n&lt;pre id=\"cooked\" hidden&gt;&lt;/pre&gt;\n`\n                this.__raw = this.shadowRoot.querySelector('#raw')\n                this.__cooked = this.shadowRoot.querySelector('#cooked')\n                this.__slot = this.shadowRoot.querySelector('slot')\n                this.__slot.addEventListener('slotchange', this.highlightContent.bind(this))\n            }\n\n            highlightContent() {\n                this.__raw.hidden = false\n                this.__cooked.hidden = true\n                if (typeof hljs === 'undefined') return\n\n                let text = this.__slot.assignedNodes({ flatten: true }).map(n =&gt; n.textContent).join(\"\")\n                const lang = this.getAttribute('lang')\n                const code = document.createElement('code')\n\n                if (lang) {\n                    const result = hljs.highlight(text, { language: lang, ignoreIllegals: true })\n                    code.innerHTML = result.value\n                    code.classList.add(`language-${lang}`)\n                } else {\n                    const result = hljs.highlightAuto(text)\n                    code.innerHTML = result.value\n                    if (result.language) code.classList.add(`language-${result.language}`)\n                }\n                this.__cooked.replaceChildren(code)\n                this.__raw.hidden = true\n                this.__cooked.hidden = false\n            }\n        }\n\n        customElements.define('pre-highlight', PreHighlightElement)\n    &lt;/script&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;div id=\"app\"&gt;&lt;/div&gt;\n    &lt;script&gt;\n        const { createApp, ref } = Vue\n        let app = createApp({\n            data() {\n                return { a: ref(10) }\n            },\n            template: `&lt;pre-highlight id=\"code\" lang=\"javascript\"&gt;let a = \\{\\{a\\}\\}&lt;/pre-highlight&gt;\n            &lt;input type=\"range\" v-model=\"a\" /&gt;`\n        })\n        app.config.compilerOptions.isCustomElement = (tag) =&gt; tag.includes('-')\n        app.mount('#app')\n    &lt;/script&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre>\n</details>\n<p>渲染效果：</p>\n<p><img alt=\"动画\" class=\"lazyload\" /></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 19:39</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Fan-iX\">Fan-iX</a>&nbsp;\n阅读(<span id=\"post_view_count\">32</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战",
      "link": "https://www.cnblogs.com/algieba/p/19581327",
      "published": "",
      "description": "<div class=\"postcontent\">\n\t\t\t    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3169973/202602/3169973-20260205193318638-616281954.png\" />\n        会跑代码还不够，我们要懂原理。本文从 Transformer 的底层视角出发，图解从位置编码到注意力机制的全流程；并基于 Kaggle 平台，深入拆解 HuggingFace Transformers 库的“铁三角”组件与生成参数的玄机。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"大模型实战-03-拆解-transformers从原理图解到-huggingface-transformers-实战\">[大模型实战 03] 拆解 Transformers：从原理图解到 HuggingFace Transformers 实战</h1>\n<blockquote>\n<p><strong>核心摘要 (TL;DR)</strong></p>\n<ul>\n<li><strong>原理</strong>：图解 Transformer 是如何通过“注意力机制”和“位置编码”来理解人类语言的。</li>\n<li><strong>实战</strong>：在 <strong>Kaggle (双 T4 GPU)</strong> 环境下，拆解 HuggingFace 代码的“铁三角”（Config, Tokenizer, Model）。</li>\n<li><strong>技巧</strong>：掌握 <code>Temperature</code> 和 <code>Top_p</code>，学会控制 AI 的“创造力”。</li>\n</ul>\n</blockquote>\n<h2 id=\"前言\">前言</h2>\n<p>各位友人们，大家好，这里是阿尔。在上一节的“炼丹”环境搭建中，咱们成功地将Qwen2.5模型运行了起来，跑通了。 但是相信大家对运行的时候的那些参数都代表着什么，都还是懵的。 这篇博客就是为此准备的，我们打算先快速大概地了解一下当前的大模型的底层原理，再结合在一起介绍通用的transformers库, 去看看代码和如何对应这些理论的。</p>\n<h2 id=\"1-transformer-极简原理大模型是怎么思考的\">1. Transformer 极简原理：大模型是怎么思考的？</h2>\n<p>大模型和普通的机器学习模型一样，本质也是一个函数，不同的是，传统机器学习可能输入的是一些整理好的数据，比如房子的尺寸，地段，购买时长，和市中心的距离等等数据，输出一个预测的放假，而大模型输入的是我们的问题，拿到的是大模型给出的回答。但，咱们究其根本，它们都可以看作是一个函数，我们输入一些东西，经过运算之后，输出给我们一些东西。</p>\n<p>对于大模型，它的本质就是一个<strong>下一个字符预测器</strong>，我们输入一些文字，它只负责根据它所训练的海量数据，输出最符合，最有可能的下一个字符，就像一个<strong>文字接龙机器</strong>，其核心任务只有一个：<strong>根据上文，猜下一个字是什么。</strong></p>\n<p>为了实现这个目标，Transformer 架构经历了一个精密的流水线：<br />\n<img alt=\"包含Transformer的5个步骤的总览图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/01_cover.png\" /><br />\n下面，我们会稍微详细一点地介绍一下各个步骤。</p>\n<h3 id=\"11-第一步token-化-tokenization--查字典\">1.1 第一步：Token 化 (Tokenization) —— 查字典</h3>\n<p>大模型既然是一个函数，那么肯定是针对数字进行处理的，所以，我们就需要一个法子，去将我们的文字字符（甚至是图片）变成大模型认识的数字（或者说张量，向量）。 这就是Token化，去做这一步操作的函数，或者模块就是<strong>分词器（Tokenizer）</strong>。</p>\n<p>那么<strong>Tokenizer</strong>具体是如何工作的？它实际上就是将一个个的字符，对应成一个个数字，或者说ID，就像ASCII码一样，不过它做得更高级。</p>\n<p>把每一个字都找一个数字对应，有点奢侈，特别是对于英语,act, acting,action,actor其实都有相同的词根，主要的语义来源于其词根act，所以分词器是按照词元（token）去拆分的，能把有些词拆成词根、前缀和后缀等等，当然具体如何拆，取决于字典如何定义，字典有多大。</p>\n<p>这里我们给一个简单的例子</p>\n<ul>\n<li><strong>输入</strong>：“我爱 AI”</li>\n<li><strong>动作</strong>：切分 -&gt; <code>[\"我\", \"爱\", \"AI\"]</code> -&gt; 查表 -&gt; <code>[2301, 452, 1083]</code><br />\n<img alt=\"Token化的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.1_tokenization.png\" /></li>\n</ul>\n<h3 id=\"12-第二步embedding--位置编码--赋予含义与顺序\">1.2 第二步：Embedding &amp; 位置编码 —— 赋予含义与顺序</h3>\n<p>有了token之后，我们想知道词和词的关系，我们想要通过一个可以量化的量去判断两个词是否是有关系的，关系多大。这就引入了我们的下一个模块，词嵌入模块（Embedding）。</p>\n<ul>\n<li><strong>Embedding (词向量)</strong>：把每个 ID 变成一个长长的向量（比如 4096 维的数组）。这个向量在模型训练之前是随机的，其后随着海量的训练数据洗礼，越相关的词向量越靠近，越不相关的词越远离。这个向量代表了词的<strong>含义</strong>。比如“猫”和“狗”的向量在空间里距离很近，“苹果”和“手机”在某种语境下也更近。</li>\n</ul>\n<p>光知道词和词的关系还不够，“我爱你”和“你爱我”的每一个词都是相同的，但是它们确实可以不相关的两个句子，“小狗咬了我”和“我咬了小狗”，也会被人视作“可以正常理解”和“这人好像不对劲”两种完全不同的理解。显而易见，词语在句中的位置是一个非常重要的信息，我们不能弄丢它，也需要将这一部分信息传递给模型训练时候去学习。</p>\n<ul>\n<li><strong>Positional Encoding (位置编码)</strong>：Transformer 是并行计算的（它一眼看完所有词），这导致它不知道“我爱你”和“你爱我”的区别。所以，我们需要给每个词贴上一个“座位号”，告诉模型谁在前面，谁在后面。<br />\n<img alt=\"Embedding &amp; 位置编码 —— 赋予含义与顺序示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.2_ebedding.png\" /></li>\n</ul>\n<h3 id=\"13-第三步self-attention-自注意力--寻找关系\">1.3 第三步：Self-Attention (自注意力) —— 寻找关系</h3>\n<p>接下来就是大模型的灵魂，我们想知道一句话里的每一个词元和其他词元的<strong>关联度</strong>，其实就是上下文的联系。当模型处理“苹果”这个词时，如果上下文里有“手机”、“发布会”，注意力机制会告诉模型：“嘿，这里的‘苹果’指的是科技公司，不是水果！”,自注意力机制，让模型能理解上下文。<br />\n<img alt=\"Self-Attention 示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.3-self-attention.png\" /></p>\n<h3 id=\"14-第四步mlp-前馈神经网络--消化吸收\">1.4 第四步：MLP (前馈神经网络) —— 消化吸收</h3>\n<p>如果说注意力机制是“看”，那么 MLP 就是“想”。它包含多层神经元，负责对提取到的信息进行复杂的非线性变换和逻辑推理,也就是传统的深度学习。<br />\n<img alt=\"MLP (前馈神经网络)示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.4-mlp.png\" /></p>\n<h3 id=\"15-第五步decoder--softmax--输出概率\">1.5 第五步：Decoder &amp; Softmax —— 输出概率</h3>\n<p>经过层层计算，模型最终会输出一个包含了所有词汇（比如 15 万个词）的<strong>概率列表</strong>。</p>\n<ul>\n<li><code>AI</code> (80%)</li>\n<li><code>吃</code> (10%)</li>\n<li><code>睡</code> (5%)<br />\n...<br />\n最后，我们根据这个概率列表，选择下一个词。<br />\n<img alt=\"输出概率的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/1.5-decoder&amp;softmax.png\" /></li>\n</ul>\n<hr />\n<p>这就是大模型词语接龙的原理了。</p>\n<h2 id=\"2-拆解模型文件夹下载下来的到底是什么\">2. 拆解模型文件夹：下载下来的到底是什么？</h2>\n<p>理论讲完了，我们来看看在 <strong>Kaggle</strong> 的文件系统里，这些理论变成了什么文件。</p>\n<p>当你下载一个模型（以 Qwen2.5-7B 为例）时，文件夹结构如下：</p>\n<p><img alt=\"safetensor格式的文件组织示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/safetensor.png\" /></p>\n<h3 id=\"21-核心架构configjson\">2.1 核心架构：config.json</h3>\n<p><strong>config.json</strong>: 是大模型的身份证，也可以说是体检表，它其中就是真正的我们的模型，具体由哪些层构成，是什么架构类型，隐藏层有多深，注意力头的数量有几个，词表的大小是多少。对于大模型而言，它就是模型本身，也是<strong>骨架</strong>，因为大模型重要的是训练完的参数，模型本身是很小的。<br />\n<img alt=\"config.json的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/config.json.png\" /></p>\n<h3 id=\"22-行为预设generation_configjson\">2.2 行为预设：generation_config.json</h3>\n<p><strong>generation_config.json</strong>:是模型的出厂默认设置。<br />\n<img alt=\"generation_config的示意图\" src=\"https://cdn.jsdelivr.net/gh/Algieba-dean/BlogImgs@master/blog-images/test_blog/llm03-know-more-about-transformer-lib/generation_config_.png\" /></p>\n<h3 id=\"23-大脑本身safetensors和indexjson\">2.3 大脑本身：*.safetensors和*.index.json</h3>\n<p>有了config.json中的躯体，我们再从<em>.safetensors和</em>.index.json载入灵魂，这才是我们能说会道的大模型。</p>\n<ul>\n<li><strong><code>model-xxxxx.safetensors</code></strong>：这里面存的是实打实的<strong>张量（Tensor）数据</strong>，即数十亿个参数的浮点数。为了方便存储和加载，通常会被切分成多个 2GB-5GB 的小文件（Shard）。</li>\n<li><strong><code>model.safetensors.index.json</code></strong>：这是一张<strong>藏宝图</strong>。因为权重被切分了，模型需要知道“第 5 层的权重”到底藏在哪个文件里。\n<ul>\n<li><strong>内部长这样</strong>：<pre><code class=\"language-json\">{\n  \"metadata\": { \"total_size\": 15423653888 },\n  \"weight_map\": {\n    \"model.layers.0.self_attn.q_proj.weight\": \"model-00001-of-00004.safetensors\",\n    \"model.layers.20.mlp.gate_proj.weight\": \"model-00003-of-00004.safetensors\"\n  }\n}\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"数字和文字的翻译官tokenizer相关文件\">数字和文字的翻译官：tokenizer相关文件</h3>\n<ul>\n<li><strong><code>vocab.json</code> / <code>merges.txt</code></strong>：这是最原始的<strong>生词表</strong>。记录了所有字、词根对应的 ID。</li>\n<li><strong><code>tokenizer.json</code></strong>：这是一个<strong>编译后</strong>的高效字典文件，包含了分词的所有逻辑（Pre-tokenization, Normalization 等），加载速度比读原始文本快得多。</li>\n<li><strong><code>tokenizer_config.json</code> (至关重要)</strong>：这是分词器的<strong>配置文件</strong>。\n<ul>\n<li>它定义了<strong>特殊符号</strong>（Special Tokens）：比如哪个 ID 代表“开始”，哪个代表“结束”。</li>\n<li>它包含了 <strong>Chat Template (聊天模板)</strong>：这是一段 Jinja2 代码，决定了 <code>apply_chat_template</code> 如何工作。</li>\n<li><strong>内部长这样</strong>：<pre><code class=\"language-json\">{\n  \"chat_template\": \"{% for message in messages %}...&lt;|im_start|&gt;...\",\n  \"eos_token\": \"&lt;|im_end|&gt;\",\n  \"pad_token\": \"&lt;|endoftext|&gt;\"\n}\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-transformers库实战代码中的铁三角\">3. Transformers库实战：代码中的“铁三角”</h2>\n<p>在Transforms库中，我们永远绕不开三个核心类。</p>\n<p><strong>环境准备：</strong><br />\n在 Kaggle 右侧设置中，确保 <strong>Internet: On</strong> 且 <strong>Accelerator: GPU T4 x2</strong>。</p>\n<pre><code class=\"language-python\">!pip install -U transformers accelerate bitsandbytes\n</code></pre>\n<h3 id=\"31-autotokenizer-翻译官\">3.1 AutoTokenizer (翻译官)</h3>\n<p>对应理论中的 <strong>Token 化</strong> 步骤,和模型文件中的tokenizer相关文件。</p>\n<pre><code class=\"language-python\">from transformers import AutoTokenizer\n\nmodel_path = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1/\"\n\n# 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n\ntext = \"Transformer is amazing\"\n# 1. 编码 (Encode): 文本 -&gt; 数字 ID\ninput_ids = tokenizer.encode(text)\nprint(f\"原文: {text}\")\nprint(f\"数字 ID: {input_ids}\")\n\n# 2. 解码 (Decode): 数字 ID -&gt; 文本\ndecoded_text = tokenizer.decode(input_ids)\nprint(f\"还原: {decoded_text}\")\n</code></pre>\n<p>得到的结果将会是这样</p>\n<pre><code class=\"language-bash\">原文: Transformer is amazing\n数字 ID: [46358, 374, 7897]\n还原: Transformer is amazing\n</code></pre>\n<p><strong>对话格式：Chat Template</strong><br />\n大模型需要特定的对话格式（Prompt）。来将模型的回答，和用户的问题做区分, 我们一般都可以通过载入语言模型对应模板（不同家的模型，可能模板会有不同），甚至去拼装历史记录。</p>\n<pre><code class=\"language-python\">messages = [\n    {\"role\": \"system\", \"content\": \"你是一个物理学家。\"},\n    {\"role\": \"user\", \"content\": \"用一句话解释相对论。\"}\n]\n\n# 自动应用聊天模板\nformatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nprint(\"模型实际看到的输入:\\n\", formatted_prompt)\n</code></pre>\n<p>运行结果如下</p>\n<pre><code class=\"language-bash\">模型实际看到的输入:\n &lt;|im_start|&gt;system\n你是一个物理学家。&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n用一句话解释相对论。&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></pre>\n<p>看到这里，我们也能更好地理解，为什么一个本质是词语接龙的模型，能够区分问题，然后做出回答。因为在训练的过程中，我们会让他知道&lt;|im_start&gt;表示一个message的开始，其中会告诉模型，这句话是谁说的，这句话在什么位置结束，它接龙的时候也会带上开始和结束符号，在推理模型中，甚至会带上思考的标签。</p>\n<h3 id=\"32-automodel-大脑本体\">3.2 AutoModel (大脑本体)</h3>\n<p>对应理论中的 <strong>Embedding -&gt; Attention -&gt; MLP</strong> 计算过程,通过从config.json中加载模型躯体，在加载上模型的safetensor灵魂数据以及generation_config.json的默认初始化,<br />\n在 Kaggle 上，我们拥有 <strong>双 T4 (15GB x 2)</strong>，一定要利用 <code>device_map=\"auto\"</code> 让库自动分配显存。</p>\n<pre><code class=\"language-python\">from transformers import AutoModelForCausalLM\nimport torch\n\n# 加载模型\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",      # 关键！自动将模型切分到两张 T4 显卡上\n    torch_dtype=torch.float16, # 使用半精度，节省显存\n    trust_remote_code=True\n)\n\nprint(f\"模型加载成功！显存分布: {model.hf_device_map}\")\n</code></pre>\n<p>输出结果为</p>\n<pre><code class=\"language-bash\">模型加载成功！显存分布: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n</code></pre>\n<h2 id=\"4-掌控生成的调节旋钮\">4. 掌控生成的“调节旋钮”</h2>\n<p>在模型输出的过程中，我们有一些参数可以对输出结果进行调节，对应于我们讲理论部分的中的 <strong>第五步 (Softmax 概率输出)</strong>, 我们有一堆下一个词元的概率分布了，但是我们应该如何去选择呢？</p>\n<h3 id=\"41-temperature-温度\">4.1 Temperature (温度)</h3>\n<p>我们可以设定的Temperature参数，我们在generation_config.json中也看见过它</p>\n<p>值越大，更热，更具创造性，更容易输出各种天马行空的词, 会缩小所有词元的差距，<strong>雨露均沾</strong>，以达到创造性，让低概率的词，也有机会被选中，当然,也更容易胡说八道，出现幻觉.<br />\n值越小，更冷，更严肃，在温度为0的时候甚至会固定输出最高的那个词，它会拉大高概率和低概率的差距，<strong>赢家通吃</strong>，让模型的回答更稳定，严谨。</p>\n<h3 id=\"42-top_p-核采样\">4.2 Top_p (核采样)</h3>\n<p>和温度不同，我们还有另一种方式，这种方式更类似于拉网，我们只要可能性前80%的词，在那些词里进行挑选，这个可能性就是P，这个选词（采样）方法又叫top_p(核采样).<br />\n简而言之：其只在累积概率达到 P (e.g., 0.9) 的前几个词里选。直接切掉尾部那些极低概率的离谱词。</p>\n<h3 id=\"43-实验一下\">4.3 实验一下</h3>\n<p>我们这里先定义一个函数，以温度和top_p为参数去测试不同的参数对回答的影响</p>\n<pre><code class=\"language-python\"># 定义一个测试函数\ndef test_generation(temp, top_p, prompt_text):\n    messages = [\n        {\"role\": \"system\", \"content\": \"你是一个前卫的科幻小说家。\"},\n        {\"role\": \"user\", \"content\": prompt_text}\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    print(f\"\\n======== 设置: Temperature={temp}, Top_p={top_p} ========\")\n\n    try:\n        generated_ids = model.generate(\n            **inputs,\n            max_new_tokens=100,  # 限制长度，方便快速看结果\n            temperature=temp,\n            top_p=top_p,\n            do_sample=True,      # 必须开启采样\n            pad_token_id=tokenizer.eos_token_id\n        )\n        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n        # 只打印回答部分\n        print(response.split('assistant')[-1].strip())\n    except Exception as e:\n        print(f\"生成出错: {e}\")\n</code></pre>\n<h3 id=\"温度实验\">温度实验</h3>\n<p>然后再其下新建code block去测试，看看效果</p>\n<pre><code class=\"language-python\">prompt = \"请用这三个词写一个微故事：量子、失恋、炒饭。\"\n\n# 1. 低温模式 (0.1)：严谨、死板\ntest_generation(temp=0.1, top_p=0.9, prompt_text=prompt)\n\n# 2. 适中模式 (0.7)：正常、流畅\ntest_generation(temp=0.7, top_p=0.9, prompt_text=prompt)\n\n# 3. 高温模式 (1.5)：疯狂、混乱\n# 注意：可能会输出乱码或完全不通顺的句子\ntest_generation(temp=1.5, top_p=0.9, prompt_text=prompt)\n</code></pre>\n<p>这是我的运行结果</p>\n<pre><code class=\"language-bash\">======== 设置: Temperature=0.1, Top_p=0.9 ========\n在量子世界里，时间与空间的概念变得模糊不清，而李明的世界也因为一段失败的恋情而变得一片混沌。他尝试着用量子纠缠理论来修复自己破碎的心灵，却意外地将自己从现实世界送入了一个平行宇宙。\n\n在这个平行宇宙中，李明发现了一家特别的餐馆，这里的厨师是一位曾经的恋人，她正在为一位顾客准备一道特别的炒饭。这道炒饭不仅色香味俱全，还\n\n======== 设置: Temperature=0.7, Top_p=0.9 ========\n在量子世界的边缘，李明独自一人坐在一家不起眼的小餐馆里，面前是一碗普通的炒饭。他和女朋友分手的原因，是因为她沉迷于虚拟现实中的量子世界，而他却对现实世界充满了留恋。\n\n就在几天前，他们最后一次争吵后，她告诉他：“我找到了真正的自我——一个穿梭在量子世界的探索者。”她留下了一盘未吃完的炒饭，然后消失在了虚拟现实中。\n\n李明望着那盘炒饭，\n\n======== 设置: Temperature=1.5, Top_p=0.9 ========\n标题：时空泡饭\n\n李明最近失恋了，每天只能煮一大锅泡饭来消磨时间。\n\n这天李明正在做饭，他忽然接收到一条量子信号。他惊奇地发现那是自己失恋前女友的坐标位置。想到能与自己相爱过的人共度时光是多么美妙的事情，他便将自己煮了一锅泡饭送进了坐标传送器。\n\n结果却是一锅炒饭。\n\n李明百思不解。\n</code></pre>\n<h3 id=\"top_p实验\">top_p实验</h3>\n<p>接下来是top_p</p>\n<pre><code class=\"language-python\">prompt_2 = \"请给一种不存在的颜色起个名字，并描述它的样子。\"\n\n# 1. 极窄采样 (0.01)：只选概率最高的那个词（近似贪婪搜索）\ntest_generation(temp=0.8, top_p=0.01, prompt_text=prompt_2)\n\n# 2. 宽广采样 (0.95)：允许罕见词出现\ntest_generation(temp=0.8, top_p=0.95, prompt_text=prompt_2)\n</code></pre>\n<p>输出结果为</p>\n<pre><code class=\"language-bash\">======== 设置: Temperature=0.8, Top_p=0.01 ========\n这种不存在的颜色我命名为“星尘紫”。它是一种梦幻般的颜色，介于紫色和银色之间，仿佛是宇宙中无数微小的星辰碎片在闪烁时所散发出的光芒。在不同的光线下，星尘紫会呈现出不同的色调，有时偏紫，有时偏银，有时又像是掺杂了点点星光的淡蓝色。它既神秘又优雅，仿佛能让人感受到宇宙的浩瀚与深邃。\n\n======== 设置: Temperature=0.8, Top_p=0.95 ========\n这种不存在的颜色我称之为“星际幻彩”（Stellar Mirage）。在视觉上，它并非单一色调，而是一种动态变化的色彩组合，像是无数微小的光点在眼前闪烁变幻，这些光点包含了所有可见光谱的颜色，同时又带着一种神秘的、不可名状的色彩。\n\n当观察者注视着“星际幻彩”的时候，他可以看到蓝、绿、紫等颜色快速地在眼前切换和混合，它们以一种几\n</code></pre>\n<h2 id=\"5-完整-transformers-代码实战\">5. 完整 Transformers 代码实战</h2>\n<p>把所有积木搭在一起，这就是一段标准的模型推理代码：</p>\n<pre><code class=\"language-python\">import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# 1. 设置模型 ID\nmodel_id = \"/kaggle/input/qwen2.5/transformers/7b-instruct/1/\"\n\n# 2. 加载翻译官\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n\n# 3. 加载大脑 (双卡模式)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\n\n# 4. 准备输入\nprompt = \"请用这三个词写一个微小说：Kaggle、深夜、爆显存\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是一个幽默的程序员。\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# 5. 生成 (调节参数！)\nprint(\" 正在生成...\")\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512,\n    temperature=0.8,  # 稍微有点创意\n    top_p=0.9,        # 剔除离谱词\n    do_sample=True    # 必须开启采样，温度才生效\n)\n\n# 6. 解码并输出\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(\"-\" * 20)\nprint(f\" 回答:\\n{response}\")\nprint(\"-\" * 20)\n</code></pre>\n<p>这是输出结果</p>\n<pre><code class=\"language-bash\">Loading checkpoint shards: 100%\n 4/4 [00:13&lt;00:00,  3.27s/it]\nThe module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\nWARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n 正在生成...\n--------------------\n 回答:\n在一个寒冷的深夜，李雷坐在他那间堆满咖啡罐和代码文件的房间里，屏幕上是Kaggle竞赛的数据集。他正尝试训练一个复杂的深度学习模型。然而，就在他认为胜利在望的时候，“嘶~”的一声，显示器瞬间变成了一片漆黑，伴随着一声悲壮的“爆显存了”。\n\n李雷揉了揉眼睛，看着眼前一片空白的屏幕，心中充满了挫败感，但他转念一想：“还好不是‘爆内存了’，否则我这台老旧电脑可能就要彻底退休了。”于是，他又开始调整参数，希望能在这个深夜里找到那个隐藏在数据海洋中的宝藏。\n--------------------\n</code></pre>\n<h3 id=\"hint\">Hint:</h3>\n<p><strong>所有的代码，都可以在 <a href=\"https://www.kaggle.com/code/thaodinhoio/llm03-transformers\" rel=\"noopener nofollow\" target=\"_blank\">这个笔记本</a>中直接获取运行哦</strong></p>\n<h2 id=\"6-常见问题-qa\">6. 常见问题 (Q&amp;A)</h2>\n<p><strong>Q: 在 Kaggle 上 <code>device_map=\"auto\"</code> 是必须的吗？</strong><br />\n<strong>A:</strong> 如果你使用单卡 T4 (15GB) 跑 7B 模型（约 14GB），勉强能塞进一张卡。但如果你开启了 Kaggle 的 <strong>T4 x2</strong>，为了利用全部 30GB 显存，<strong>必须</strong>加这个参数，否则模型只会塞进第一张卡，导致第一张爆满，第二张围观。</p>\n<p><strong>Q: 为什么生成的每一句话都不一样？</strong><br />\n<strong>A:</strong> 因为我们开启了 <code>do_sample=True</code> 并且设置了 <code>temperature &gt; 0</code>。模型在选择下一个词时是<strong>按概率随机抽取</strong>的。如果你想让结果每次都一样（比如做数学题），请设置 <code>do_sample=False</code>（此时温度失效，变为贪婪解码）。</p>\n<p><strong>Q: 什么是 Logits？</strong><br />\n<strong>A:</strong> 在代码深处，模型输出的那个“概率表”在变成百分比之前，叫 Logits（未归一化的数值）。Softmax 函数的作用就是把 Logits 变成概率。你可以把 Logits 理解为模型对每个词的“原始打分”。</p>\n<p><strong>Q: Token 和字是一一对应的吗？</strong><br />\n<strong>A:</strong> 不一定。</p>\n<ul>\n<li>英文：通常一个单词是一个 Token，长单词可能被切分。</li>\n<li>中文：通常一个汉字是一个 Token，但常见词（如“你好”）可能会合并为一个 Token。</li>\n<li>平均来说，<strong>0.75 个英文单词 ≈ 1 Token</strong>，<strong>1 个汉字 ≈ 1.5 - 2 Token</strong>（取决于分词器效率，Qwen 的中文压缩率很高）。</li>\n</ul>\n<hr />\n<p><strong>本文作者：</strong> Algieba<br />\n<strong>本文链接：</strong> <a href=\"https://blog.algieba12.cn/llm03-know-more-about-transformer-lib/\" rel=\"noopener nofollow\" target=\"_blank\">https://blog.algieba12.cn/llm03-know-more-about-transformer-lib/</a><br />\n<strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</p>\n<pre><code>\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"itemdesc\">\n\t\t\t发表于 \n<span id=\"post-date\">2026-02-05 19:33</span>&nbsp;\n<a href=\"https://www.cnblogs.com/algieba\">阿尔的代码屋</a>&nbsp;\n阅读(<span id=\"post_view_count\">33</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</div>"
    },
    {
      "title": "Trae IDE 隐藏玩法：接入即梦 AI，生成高质量大片！",
      "link": "https://www.cnblogs.com/daimajiangxin/p/19581317",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/daimajiangxin/p/19581317\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 19:30\">\n    <span>Trae IDE 隐藏玩法：接入即梦 AI，生成高质量大片！</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"Trae IDE 隐藏玩法：接入即梦 AI，生成高质量大片！\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3365149/202602/3365149-20260205192859421-189703184.png\" />\n        如何在 Trae IDE 中利用 jimeng-api 项目，快速搭建并使用即梦 (Jimeng) 的 AI 绘图能力，实现免费的高质量图像生成。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>想用 AI 生成电影级画质的美图，却被高昂的订阅费劝退？</p>\n<p>在 AI 绘图领域，字节跳动的 <strong>即梦 (Jimeng)</strong> 凭借其对中文的深度理解和惊艳的画面质感，迅速出圈。</p>\n<p>今天，我们将解锁 <strong>Trae IDE</strong> 的隐藏技能——结合开源神器 <code>jimeng-api</code>，<strong>从零打造</strong>一个专属的 AI 绘图技能。无需复杂的代码，只需简单的配置，你的 IDE 就能变身“神笔马良”，<strong>免费</strong>生成高质量大片！</p>\n<h2 id=\"️-一准备工作部署-api-服务\">🛠️ 一、准备工作：部署 API 服务</h2>\n<p>首先，我们需要搭建一个能调用即梦能力的桥梁。感谢开源社区，GitHub 上的 <a href=\"https://github.com/iptag/jimeng-api\" rel=\"noopener nofollow\" target=\"_blank\">jimeng-api</a> 项目完美解决了这个问题。</p>\n<h3 id=\"1-克隆项目\">1. 克隆项目</h3>\n<p>将项目源码下载到本地：</p>\n<pre><code class=\"language-bash\">git clone https://github.com/iptag/jimeng-api.git\n</code></pre>\n<h3 id=\"2-docker-部署\">2. Docker 部署</h3>\n<p>使用 Docker 部署最简单，无需关心环境依赖。</p>\n<p><strong>方式 A：使用 docker-compose</strong></p>\n<pre><code class=\"language-bash\">cd jimeng-api\ndocker-compose up -d\n</code></pre>\n<p><strong>方式 B：手动构建运行</strong></p>\n<pre><code class=\"language-bash\">cd jimeng-api\ndocker build -t jimeng-api .\n\ndocker run -d \\\n  --name jimeng-api \\\n  -p 5100:5100 \\\n  --restart unless-stopped \\\n  jimeng-api\n</code></pre>\n<blockquote>\n<p>💡 <strong>提示</strong>：服务启动后默认监听 <code>5100</code> 端口。</p>\n</blockquote>\n<h3 id=\"3-获取关键凭证-token\">3. 获取关键凭证 (Token)</h3>\n<p>你需要获取即梦账号的 <code>sessionid</code> 作为调用凭证：</p>\n<ol>\n<li>访问 <a href=\"https://jimeng.jianying.com\" rel=\"noopener nofollow\" target=\"_blank\">即梦官网 (jimeng.jianying.com)</a> 并登录。</li>\n<li>按 <code>F12</code> 打开浏览器开发者工具，切换到 <code>Application</code> -&gt; <code>Cookies</code>。</li>\n<li>找到 <code>sessionid</code> 的值，复制备用。</li>\n</ol>\n<p><img alt=\"获取 Session ID\" class=\"lazyload\" /></p>\n<h2 id=\"-二在-trae-ide-中装载绘图技能\">⚡ 二、在 Trae IDE 中装载“绘图技能”</h2>\n<p>现在，我们把部署好的 API 能力集成到 Trae 中。</p>\n<h3 id=\"1-植入技能文件\">1. 植入技能文件</h3>\n<p>将下载好的 <code>jimeng-api</code> 文件夹，完整复制到 Trae 的技能目录中。</p>\n<ul>\n<li><strong>全局生效</strong> (推荐，所有项目可用)：<br />\n复制到 <code>C:\\Users\\你的用户名\\.trae\\skills</code></li>\n<li><strong>项目生效</strong> (仅当前项目可用)：<br />\n复制到项目根目录下的 <code>.trae/skills</code></li>\n</ul>\n<h3 id=\"2-安装-python-依赖\">2. 安装 Python 依赖</h3>\n<p>Trae 运行该技能脚本需要 Python 环境支持，请确保安装了以下库：</p>\n<pre><code class=\"language-bash\">pip install requests Pillow\n</code></pre>\n<h2 id=\"-三进阶体验智能绘图\">🎨 三、进阶：体验智能绘图</h2>\n<p>一切就绪！现在 Trae 已经不仅仅是一个代码编辑器，它还是你的 <strong>AI 绘图助理</strong>。Trae 会自动识别你的绘图意图并调用技能。</p>\n<blockquote>\n<p><strong>💡 使用小贴士</strong><br />\n由于脚本需要验证身份，第一次使用时，请告诉 Trae 你的 <code>sessionid</code>。</p>\n</blockquote>\n<p><strong>实战演示：</strong></p>\n<blockquote>\n<p><strong>User</strong>: “我的 sessionid 是 xxxxx，使用即梦帮我生成一张 2K 分辨率的日落海滩图，画面要唯美。”</p>\n<p><strong>Trae</strong>: [收到！正在调用 jimeng 技能...生成图片...保存到 /pic 目录]</p>\n</blockquote>\n<p><strong>✨ 作品展示：</strong><br />\n执行成功后，高清大图会自动保存在项目的 <code>pic</code> 目录下（已自动转换为 PNG 格式）。看看这细节：</p>\n<p><img alt=\"\" class=\"lazyload\" /><br />\n<img alt=\"\" class=\"lazyload\" /><br />\n<img alt=\"\" class=\"lazyload\" /><br />\n<img alt=\"\" class=\"lazyload\" /></p>\n<h2 id=\"-四总结\">📝 四、总结</h2>\n<p>通过 <strong>Docker 部署 jimeng-api</strong> 配合 <strong>Trae IDE</strong> 的强大扩展能力，我们仅用了几分钟就搭建了一套低成本、高效的 AI 绘图工作流。</p>\n<p>相比于昂贵的商业 API，这种方案：</p>\n<ul>\n<li>✅ <strong>更灵活</strong>：本地控制，随心所欲。</li>\n<li>✅ <strong>更经济</strong>：直接利用现有账号权益。</li>\n<li>✅ <strong>更极客</strong>：将 AI 能力无缝融入开发环境。</li>\n</ul>\n<p>快去试试用代码画出你的梦境吧！🚀</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 19:30</span>&nbsp;\n<a href=\"https://www.cnblogs.com/daimajiangxin\">代码匠心</a>&nbsp;\n阅读(<span id=\"post_view_count\">38</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "一个小球的人生哲思：从3D绘制到碰壁反弹",
      "link": "https://www.cnblogs.com/lixingqiu/p/19580269",
      "published": "",
      "description": "<h1 class=\"postTitle\"><a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/lixingqiu/p/19580269\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 17:01\">\n    <span>一个小球的人生哲思：从3D绘制到碰壁反弹</span>\n    \n\n</a>\n</h1>\n\t<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p><span><img alt=\"谢谢你绘3D红球再滚动_赋予人\" class=\"lazyload\" height=\"156\" width=\"88\" /></span></p>\n<p>在数字世界的舞台上，总有一些神奇的代码能在短短数行之内，创造出令人惊叹的视觉奇观。今天，我们要聊的，就是这样一个采用C++精灵库编写的“魔法程序”。它仅用了12行核心代码，就操控着一个小球，在屏幕上完成了一次从3D绘制到碰壁反弹动画的精彩表演。这个过程，就像是一场浓缩的人生戏剧，充满了梦想、创造、成长与自我约束。让我们化身为这个小球的“造物主”，以一种拟人的、诙谐幽默的视角，深入剖析它的一生。</p>\n<p><strong>第一幕：诞生！从无到有的3D之梦</strong></p>\n<p><span>程序的主角，是一个名为 r 的小球。它的生命始于一片“黑色”的虚无(一开始它是隐藏的)之中。程序的第一行就宣告了它的存在：Sprite r;。它的舞台，是一个标题名为“绘3D红球再滚动”的屏幕，尺寸为 480x720。</span></p>\n<p><span>但在它能滚动之前，它必须首先有它自己的外形。于是，它的造物主开始了它的“创世工程”。这是一段仅有100次循环的代码，却构建了它最辉煌的时刻：</span></p>\n<pre class=\"code-snippet__js\"><span style=\"color: rgba(255, 0, 0, 1);\"><code><span class=\"code-snippet__attribute\">for(int i=<span class=\"code-snippet__number\">0;i&lt;<span class=\"code-snippet__number\">100;i++) r.penshade(i).dot(d-i).wait(<span class=\"code-snippet__number\">0.<span class=\"code-snippet__number\">05);&nbsp;</span></span></span></span></span></code></span></pre>\n<p><span>想象一下，造物主让小球静止在一个虚拟的“画布”中央。然后，它开始以一种“光影魔术”来塑造自己。它不断地改变画笔的颜色深浅（penshade(i)），然后用一种特殊的打圆点画笔（dot(d-i)）在屏幕上画出一个个点。每一次循环，颜色都会更红一些，点的大小也会更小一些。当循环结束时，一个拥有层次感、仿佛能折射光线的“3D”红球，就这样诞生了。这是一次从无到有的创造，充满了耐心和精妙的技巧。这告诉我们，任何伟大的成就，都始于一次次微小而持续的努力。</span></p>\n<p><strong>第二幕：铭记！自我认知与身份确立</strong></p>\n<p><span>有了外在的形体，这个小球还需要一个内在的“我”。它的造物主深知这一点，于是，一个充满个性的“8”字被写在了它的表面。</span></p>\n<pre class=\"code-snippet__js\"><span style=\"color: rgba(255, 0, 0, 1);\"><code><span class=\"code-snippet__attribute\">r.pencolor(<span class=\"code-snippet__string\">\"black\").write(<span class=\"code-snippet__string\">\"8\",<span class=\"code-snippet__number\">36); &nbsp;//画笔颜色设为黑并且写<span class=\"code-snippet__number\">8字</span></span></span></span></span></code></span></pre>\n<p><span>这个“8”字，不仅仅是一个装饰。它是小球的“灵魂烙印”，是它独一无二的身份标识。它告诉世界：“我是我，不是别人。” 然后，造物主为了让这份“身份”永不磨灭，做了一件大事——截图。</span></p>\n<pre class=\"code-snippet__js\"><span style=\"color: rgba(255, 0, 0, 1);\"><code><span class=\"code-snippet__attribute\">screen.savepng(filename, {-d/<span class=\"code-snippet__number\">2, d/<span class=\"code-snippet__number\">2, d,d},true);</span></span></span></code></span></pre>\n<p><span>它将这个独一无二的、带有“8”字的3D红球，永久地保存为一个名为 3dball.png 的图像文件。从此，无论它在未来的旅程中经历了什么，只要再次加载这个图像，它的“8”字灵魂就会回归。这就像我们每个人，在成长过程中，都会留下自己的印记，无论是作品、思想还是经历，这些都是我们身份的基石。</span></p>\n<p><strong>第三幕：启程！探索与成长的旅途</strong></p>\n<p><span>身份确立后，小球的造物主为它设置了一个简单而又充满希望的旅程。它被赋予了一个初始速度，并被设定为“永远滚动”：</span></p>\n<pre class=\"code-snippet__js\"><span style=\"color: rgba(255, 0, 0, 1);\"><code>r.<span class=\"code-snippet__title\">clear().<span class=\"code-snippet__title\">shape(filename).<span class=\"code-snippet__title\">show().<span class=\"code-snippet__title\">wait(<span class=\"code-snippet__number\">1); &nbsp;<span class=\"code-snippet__comment\">//清除所画图形把角色设为filename</span></span></span></span></span></span></code></span></pre>\n<p><span>它被赋予了一个初始速度，并被设定为“永远滚动”：</span></p>\n<pre class=\"code-snippet__js\"><span style=\"color: rgba(255, 0, 0, 1);\"><code><span class=\"code-snippet__built_in\">int&nbsp;k=&nbsp;<span class=\"code-snippet__number\">1;<br /></span></span></code><code><span class=\"code-snippet__keyword\">while(<span class=\"code-snippet__literal\">true){ /<span class=\"code-snippet__comment\">/不断重复，让角色滚动，</span></span></span></code><code>&nbsp; &nbsp;<br />  r.right(<span class=\"code-snippet__number\">1*k).addx(step*k).wait(<span class=\"code-snippet__number\">0.01);</span></span></code><code>&nbsp; <br />&nbsp;<span class=\"code-snippet__comment\">//... 碰撞检测 ...<br /></span></code><code>}</code></span></pre>\n<p><span>现在，小球真正开始了它的生命之旅。它以一种优雅的姿态，每旋转一度（right(1*k)），就向前移动一小步（addx(step*k)）。它的世界是一个 480x720 的屏幕，而它自己的大小是 d。它的旅程充满了未知和新奇，每一次移动都是一次探索，每一次旋转都是一次思考。这就像我们每个人的人生，从懵懂无知到开始探索世界，在不断的学习和实践中，一步一步地成长。</span></p>\n<p><strong>第四幕：束缚！边界与规则的指引</strong></p>\n<p><span>旅程并非一帆风顺。在小球的旅途中，造物主为它设置了一个残酷而又仁慈的“束缚”——边界。</span></p>\n<pre class=\"code-snippet__js\"><span style=\"color: rgba(255, 0, 0, 1);\"><code><span class=\"code-snippet__attribute\">if(r.xcor()+d/<span class=\"code-snippet__number\">2&gt;=<span class=\"code-snippet__number\">240&nbsp;|| r.xcor()-d/<span class=\"code-snippet__number\">2&lt;=-<span class=\"code-snippet__number\">240)k=-k;</span></span></span></span></span></code></span></pre>\n<p><span>这行代码是旅程的“刹车”和“方向盘”。它时刻监控着小球的位置：</span></p>\n<p><span>当小球的右边缘（xcor() + d/2）触碰到屏幕的最右侧（240）时，它会改变移动的方向。</span></p>\n<p><span>当小球的左边缘（xcor() - d/2）触碰到屏幕的最左侧（-240）时，它也会改变移动的方向。</span></p>\n<p><span>这个“碰壁反弹”的规则，看似是一种限制，却恰恰是小球能在有限空间内持续运动的根本保障。它教会小球学会自我约束。没有边界的世界是混沌的，有了边界，运动才有了方向和意义。这就像社会规则和道德准则，它们并非要束缚我们，而是为了保护我们，让我们在一个有序的环境中，能够更稳定、更长远地发展。不懂得约束的自由，终将把我们推向深渊。</span></p>\n<p><strong>第五幕：哲思！人生的隐喻与启示</strong></p>\n<p><span>回顾这个小球的一生，我们会发现，它的旅程正是我们每个人人生的缩影。</span></p>\n<p><span><span><strong><em>创造自我</em></strong>： 从无到有，我们通过学习和实践，不断塑造和完善自己的能力与品格，最终形成一个独一无二的“自我”。</span></span></p>\n<p><span><span><em><strong>确立身份</strong></em>： 我们的作品、成就和经历，构成了我们的社会身份和个人价值，这是我们存在的证明。</span></span></p>\n<p><span><span><strong><em>探索世界</em></strong>： 我们带着最初的热情和好奇，在人生的道路上不断前行，学习新事物，体验新感受。</span></span></p>\n<p><span><span><em><strong>遵守规则</strong></em>： 社会和生活的“边界”让我们学会了自律和尊重，它确保了我们的行为有章可循，从而避免了混乱和伤害。</span></span></p>\n<p><span>最终，这个小球在它的世界里，永不停歇地滚动着，时而前进，时而后退，始终保持着自己的节奏。它的人生，就是一场在规则与自由之间寻求平衡的旅程。</span></p>\n<p><span>结语：一段代码，一种人生</span></p>\n<p><span>这个仅有12行核心代码的程序，通过一个小球的视角，为我们展现了一个完整而深刻的“人生”故事。它告诉我们，人生始于创造，成于认知，行于探索，终于规则。它用一种诙谐幽默的方式，让我们看到了代码背后蕴含的智慧和哲理。下次当你看到一段简洁而强大的代码时，请记住，它可能不仅仅是指令的集合，更是一个微型世界的完整故事。</span></p>\n<p>这个程序的代码如下所示：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">本程序动画网址: </span><span style=\"color: rgba(0, 128, 0, 1); text-decoration: underline;\">https://www.douyin.com/video/7603283479270296867</span>\n#include <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">sprites.h</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">包含C++精灵库 </span>\n<span style=\"color: rgba(0, 0, 255, 1);\">using</span> <span style=\"color: rgba(0, 0, 255, 1);\">namespace</span><span style=\"color: rgba(0, 0, 0, 1);\"> std;\nScreen screen{</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">绘3D红球再滚动</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>,<span style=\"color: rgba(128, 0, 128, 1);\">480</span>,<span style=\"color: rgba(128, 0, 128, 1);\">720</span><span style=\"color: rgba(0, 0, 0, 1);\">};\nSprite r;      </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">建立角色叫r</span>\n\n<span style=\"color: rgba(0, 0, 255, 1);\">int</span> main(){        <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">主功能块</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">int</span> d= <span style=\"color: rgba(128, 0, 128, 1);\">100</span><span style=\"color: rgba(0, 0, 0, 1);\">; \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">float</span> step = <span style=\"color: rgba(128, 0, 128, 1);\">3.14159535897932</span>*d/<span style=\"color: rgba(128, 0, 128, 1);\">360.0</span>;  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">每次红球移动的距离</span>\n    r.bgcolor(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">black</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>).hide().speed(<span style=\"color: rgba(128, 0, 128, 1);\">0</span>).color(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">red</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>).pu(); <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">准备工作\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">重复100次画一个模拟的3D红球</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">for</span>(<span style=\"color: rgba(0, 0, 255, 1);\">int</span> i=<span style=\"color: rgba(128, 0, 128, 1);\">0</span>;i&lt;<span style=\"color: rgba(128, 0, 128, 1);\">100</span>;i++) r.penshade(i).dot(d-i).wait(<span style=\"color: rgba(128, 0, 128, 1);\">0.05</span><span style=\"color: rgba(0, 0, 0, 1);\">); \n    \n    r.pencolor(</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">black</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>).write(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">8</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>,<span style=\"color: rgba(128, 0, 128, 1);\">36</span>);  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">画笔颜色设为黑并且写8字</span>\n    \n    <span style=\"color: rgba(0, 0, 255, 1);\">string</span> filename = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">res/3dball.png</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>; <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">设立要保存的png文件名</span>\n    screen.savepng(filename, {-d/<span style=\"color: rgba(128, 0, 128, 1);\">2</span>, d/<span style=\"color: rgba(128, 0, 128, 1);\">2</span>, d,d},<span style=\"color: rgba(0, 0, 255, 1);\">true</span>);<span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">截取所画图形，保存</span>\n    r.clear().shape(filename).show().wait(<span style=\"color: rgba(128, 0, 128, 1);\">1</span>);  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">清除所画图形把角色设为filename</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">int</span> k= <span style=\"color: rgba(128, 0, 128, 1);\">1</span><span style=\"color: rgba(0, 0, 0, 1);\">;\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">while</span>(<span style=\"color: rgba(0, 0, 255, 1);\">true</span>){       <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">不断重复，让角色滚动，</span>\n       r.right(<span style=\"color: rgba(128, 0, 128, 1);\">1</span>*k).addx(step*k).wait(<span style=\"color: rgba(128, 0, 128, 1);\">0.01</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n       </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">下面是碰到边缘就反弹</span>\n       <span style=\"color: rgba(0, 0, 255, 1);\">if</span>(r.xcor()+d/<span style=\"color: rgba(128, 0, 128, 1);\">2</span>&gt;=<span style=\"color: rgba(128, 0, 128, 1);\">240</span> || r.xcor()-d/<span style=\"color: rgba(128, 0, 128, 1);\">2</span>&lt;=-<span style=\"color: rgba(128, 0, 128, 1);\">240</span>)k=-<span style=\"color: rgba(0, 0, 0, 1);\">k;\n    }\n   r.done();     </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">完成了</span>\n   <span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(128, 0, 128, 1);\">0</span>;    <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">返回0</span>\n}</pre>\n</div>\n<p>&nbsp;</p>\n\n</div>\n<div class=\"clear\"></div>\n\n\t<div class=\"postDesc\">posted on \n<span id=\"post-date\">2026-02-05 17:01</span>&nbsp;\n<a href=\"https://www.cnblogs.com/lixingqiu\">李兴球</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "吴恩达深度学习课程五：自然语言处理  第三周：序列模型与注意力机制 课后习题与代码实践",
      "link": "https://www.cnblogs.com/Goblinscholar/p/19579899",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/Goblinscholar/p/19579899\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 16:34\">\n    <span>吴恩达深度学习课程五：自然语言处理  第三周：序列模型与注意力机制 课后习题与代码实践</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>此分类用于记录吴恩达深度学习课程的学习笔记。<br />\n课程相关信息链接如下：</p>\n<ol>\n<li>原课程视频链接：<a href=\"https://www.bilibili.com/video/BV1FT4y1E74V?buvid=XU762317353676D786954061C192FE625463B&amp;from_spmid=playlist.playlist-detail.0.0&amp;is_story_h5=false&amp;mid=zqernykrmpf7XfIorMR%2FnA%3D%3D&amp;plat_id=116&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=ce0bc526-db69-428a-962e-c65ed8c267bc&amp;share_source=COPY&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1713085655&amp;unique_k=DfBgvFW&amp;up_id=8654113&amp;vd_source=e035e9878d32f414b4354b839a4c31a4\" rel=\"noopener nofollow\" target=\"_blank\">[双语字幕]吴恩达深度学习deeplearning.ai</a></li>\n<li>github课程资料，含课件与笔记:<a href=\"https://github.com/robbertliu/deeplearning.ai-andrewNG\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达深度学习教学资料</a></li>\n<li>课程配套练习（中英）与答案：<a href=\"https://blog.csdn.net/u013733326/article/details/79827273\" rel=\"noopener nofollow\" target=\"_blank\">吴恩达深度学习课后习题与答案</a></li>\n</ol>\n<p>本篇为第五课第三周的课后习题和代码实践部分。</p>\n<hr />\n<h1 id=\"1理论习题\">1.理论习题</h1>\n<p><a href=\"https://blog.csdn.net/u013733326/article/details/90144255\" rel=\"noopener nofollow\" target=\"_blank\">【中英】【吴恩达课后测验】Course 5 - 序列模型 - 第三周测验</a></p>\n<p>在这一周的习题内容中有一道题的相关内容需要展开，先看一下题面：</p>\n<blockquote>\n<p>网络通过学习注意力分数来决定「把注意力放在哪里」，这些值是由一个小神经网络计算得到的。<br />\n这里，我们<strong>不能</strong>把 <span class=\"math inline\">\\(s^{&lt;t-1&gt;}\\)</span> 替换成 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span> 作为这个神经网络的输入，<br />\n原因如下：因为 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span> 依赖于当前的注意力权重，而当前的注意力权重又依赖于注意力分数，因此在需要计算注意力分数的时刻，<span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span> <strong>尚未被计算出来</strong>， 此时只能使用前一时刻的隐藏状态 <span class=\"math inline\">\\(s^{&lt;t-1&gt;}\\)</span>，而无法使用当前的 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span>。<br />\n<strong>答案：正确</strong></p>\n</blockquote>\n<p>这道题一眼看下来是很迷惑的，这是<strong>因为这道题讲的是最早的注意力机制的逻辑，和我们在理论部分介绍的主流逻辑有所不同</strong>。<br />\n在<a href=\"https://www.cnblogs.com/Goblinscholar/p/19563950\" target=\"_blank\">理论部分</a>，我们介绍的注意力机制，学习注意力分数的网络的输入就是 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span> ，但在最早的注意力机制中，它的输入是 <span class=\"math inline\">\\(s^{&lt;t-1&gt;}\\)</span>。<br />\n其传播逻辑可以用一句话总结：<strong>在当前时间步中，原始注意力机制将由上一时刻解码状态计算得到的上下文向量 <span class=\"math inline\">\\(c^{&lt;t&gt;}\\)</span> 作为解码器输入的一部分，用于计算当前解码状态 <span class=\"math inline\">\\(s^{&lt;t&gt;}\\)</span>。</strong><br />\n我们展开如下：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162801516-704399919.png\" /><br />\n14 年的原论文使用的就是这种传播方式，具有开创性价值。但在现代，这种将上下文向量直接纳入状态递推的做法更多只具有历史意义，了解即可。</p>\n<h1 id=\"2代码实践\">2.代码实践</h1>\n<p><a href=\"https://blog.csdn.net/u013733326/article/details/97619187\" rel=\"noopener nofollow\" target=\"_blank\">【中文】【吴恩达课后编程作业】Course 5 - 序列模型 - 第三周作业</a><br />\n还是先摆链接，这篇里博主就机器翻译和触发词检测两部分内容，非常详细地演示了本周的内容，但还是要注意 Keras 的导包更新问题。<br />\n我们同样使用框架来演示这部分内容，主要内容如下：</p>\n<ol>\n<li><strong>使用编码解码框架进行日期格式翻译</strong></li>\n<li><strong>在编码解码框架下对比贪心解码和束搜索性能</strong></li>\n<li><strong>使用带注意力机制的编码解码框架进行日期格式翻译</strong></li>\n</ol>\n<h2 id=\"21-数据准备\">2.1 数据准备</h2>\n<p>需要说明的是，由于 <strong>seq2seq 模型自身的编码–解码结构</strong> 以及 <strong>序列数据的时序特性</strong>，这类任务相比许多常规监督学习任务，通常<strong>对计算资源有更高的要求</strong>。<br />\n以机器翻译任务为例，网络上存在大量公开数据集，如英法翻译、中英翻译等。由于这类数据获取相对容易，同时自然语言本身具有<strong>词汇规模大、表达形式多样</strong>的特点，模型在训练过程中往往需要同时维护<strong>源语言词典和目标语言词典</strong>，其规模通常至少达到 <strong>数万级别</strong>。这直接导致模型在参数规模、计算量以及显存占用等方面的成本显著提升。<br />\n因此，这次我们选用一种相对简化的机器翻译任务——<strong>日期格式翻译</strong>，该类任务的主要特点在于<strong>词典规模小、输入输出结构清晰、语义歧义极少</strong>，可以较高效率地演示本周内容。</p>\n<p>这次我们不使用网上公开的数据集，而是拓展吴恩达老师在编程作业里的逻辑，进行<strong>人工合成数据</strong>。<br />\n我们通过设计好的脚本，生成 <strong>10000 条</strong> 以下格式的相关数据和对应标签并保存为文件：</p>\n<pre><code class=\"language-python\">HUMAN_TEMPLATES = [  \n    \"{day} {month} {year}\",  \n    \"{month} {day}, {year}\",  \n    \"{day} of {month}, {year}\",  \n    \"{month} {day} {year}\",  \n    \"{day}/{month_num}/{year_short}\",  \n    \"{month_num}/{day}/{year_short}\",  \n    \"{weekday}, {day} {month_abbr} {year}\",  \n    \"{month_abbr} {day}th {year_short}\",  \n    \"{year}-{month_num}-{day}\",  \n    \"The {day}{day_suffix} of {month}, {year}\",  \n    \"{day}th {month_abbr}, {year}\",  \n    \"{month} the {day}{day_suffix}, {year}\",  \n    \"Date: {year}/{month_num}/{day}\",  \n    \"{day} in Roman: {roman_day} {month} {year}\",  \n    \"{month_abbr}. {day}, '{year_short}\",  \n]\n</code></pre>\n<p>完整代码放在附录，打印几条生成数据如下：</p>\n<pre><code>[1] The 28th of April, 1975  ==&gt;  1975-04-28\n[2] Date: 2017/05/5  ==&gt;  2017-05-05\n[3] Jan 10th 44  ==&gt;  1944-01-10\n[4] Tuesday, 6 Nov 2096  ==&gt;  2096-11-06\n[5] The 26th of August, 2089  ==&gt;  2089-08-26\n[6] Date: 2061/08/18  ==&gt;  2061-08-18\n[7] 08/4/35  ==&gt;  1935-08-04\n[8] 27 of May, 1948  ==&gt;  1948-05-27\n[9] 8 of April, 1907  ==&gt;  1907-04-08\n[10] November 11, 1937  ==&gt;  1937-11-11\n</code></pre>\n<p>准备好数据后，我们便可以进行下一步内容。</p>\n<h2 id=\"22-模型设计\">2.2 模型设计</h2>\n<p>在<a href=\"https://www.cnblogs.com/Goblinscholar/p/19547898\" target=\"_blank\">本周第一篇</a>内容里我们就提到过：编码解码框架下的网络结构实际上是分别设计的两个子网络，在代码逻辑中，就是<strong>先分别定义设计编码器和解码器，再在整体网络中调用</strong>。我们分点展开如下：</p>\n<h4 id=\"1编码器\">（1）编码器</h4>\n<pre><code class=\"language-python\">class Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim):\n        super().__init__()\n        # 嵌入层\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        # 使用双向 LSTM\n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n    def forward(self, x):\n        embedded = self.embedding(x) \n        outputs, (hidden, cell) = self.lstm(embedded)\n        # outputs: LSTM 对每个时间步的输出,用于注意力的相关计算。\n        # (hidden,cell): 最后时间步的隐藏状态和细胞状态，是解码器的初始输入。\n        return outputs, (hidden, cell)\n\n</code></pre>\n<h4 id=\"2解码器\">（2）解码器</h4>\n<pre><code class=\"language-python\">class Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, use_attention=False):\n        super().__init__()\n        # 设置参数，默认不使用注意力机制\n        self.use_attention = use_attention\n        # 嵌入层\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        # LSTM \n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n        if use_attention:\n            # 注意力评分层，将解码器 hidden 与编码器 outputs 拼接计算注意力分数\n            self.attn = nn.Linear(hidden_dim + hidden_dim * 2, 1)\n            # 注意力结合层：将解码器 hidden 与上下文向量融合为最终 LSTM 输出\n            self.attn_combine = nn.Linear(hidden_dim + hidden_dim * 2, hidden_dim)\n\n        # 输出层：将 LSTM 输出映射到词表维度，预测下一个 token\n        self.out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_step, hidden_cell, encoder_outputs):\n        embedded = self.embedding(input_step)\n        # LSTM 前向计算，得到当前时间步输出和新的 hidden/cell\n        lstm_out, hidden_cell = self.lstm(embedded, hidden_cell)\n        # 去掉时间维度，方便注意力计算\n        query = lstm_out.squeeze(1)\n\n        if self.use_attention:\n            # 扩展 query，实际上就是把当前解码隐藏状态复制到和编码隐藏状态数相同。\n            query_exp = query.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n            # 计算注意力分数\n            attn_scores = self.attn(torch.cat((query_exp, encoder_outputs), dim=-1)).squeeze(-1)\n            # 转为注意力权重\n            attn_weights = F.softmax(attn_scores, dim=-1).unsqueeze(1)\n            # 计算上下文向量 bmm:批量矩阵乘法\n            context = torch.bmm(attn_weights, encoder_outputs).squeeze(1)\n            # 将 query 与上下文向量融合\n            combined = torch.cat((query, context), dim=1)\n            output = torch.tanh(self.attn_combine(combined))\n        else:\n            # 不使用注意力时，直接用 LSTM 输出预测\n            output = query\n\n        # 输出层\n        output = self.out(output)\n        return output, hidden_cell\n</code></pre>\n<h4 id=\"3整体网络\">（3）整体网络</h4>\n<pre><code class=\"language-python\">class Seq2Seq(nn.Module):  \n    def __init__(self, encoder, decoder):  \n        super().__init__()  \n        # 编码器和解码器直接作为初始化参数，在主函数中创建传入\n        self.encoder = encoder  \n        self.decoder = decoder \n         \n    def forward(self, src, tgt):  \n        # src:样本 tgt:标签\n        # 编码器\n        enc_outputs, (hidden, cell) = self.encoder(src)\n\n        # 这里是合并双向 LSTM 的 hidden/cell\n        # 将两个方向的 hidden/cell 在层维度上分开，再求和或拼接作为解码器初始状态\n        hidden = hidden.view(2, -1, hidden.size(-1)).sum(dim=0, keepdim=True)\n        cell   = cell.view(2, -1, cell.size(-1)).sum(dim=0, keepdim=True)\n        dec_hidden = (hidden, cell)  # 解码器初始状态\n\n        outputs = []  # 存储每个时间步的输出\n\n        # 解码器\n        for t in range(tgt.size(1)):\n            # 训练逻辑：每步使用真实标签作为输入\n            out, dec_hidden = self.decoder(\n                tgt[:, t].unsqueeze(1), dec_hidden, enc_outputs\n            )\n            # tgt[:, t] 因为批次训练，每次要去取第 t 列\n            # out: 当前时间步预测的 token 概率分布 [batch_size, vocab_size]\n            # dec_hidden: 更新后的解码器隐藏状态\n            outputs.append(out)  # 保存当前输出\n\n        # 将所有时间步输出堆叠为最终结果。\n        return torch.stack(outputs, dim=1)\n\n</code></pre>\n<p>这样我们就完成了模型的设计，继续下一部分内容。</p>\n<h2 id=\"23-解码函数\">2.3 解码函数</h2>\n<p>还是理论部分提到的，在推理阶段，解码器的输出其实是一系列的概率分布，我们<strong>使用解码函数，就是设计相应的搜索策略来寻找最大的联合概率，得到最终输出。</strong><br />\n这里，我们定义完整的解码函数，<strong>主逻辑为贪心解码，补充束搜索的相关逻辑并通过参数选择是否使用</strong>：</p>\n<pre><code class=\"language-python\">def decode(model, input_tensor, use_beam_search=False, beam_width=3, max_len=20):\n    \"\"\"\n    参数：\n    - model: 已训练的 Seq2Seq 模型（包含 encoder 和 decoder）\n    - input_tensor: 输入序列的 tensor（一个样本）\n    - use_beam_search: 是否使用束搜索\n    - beam_width: 束搜索时的候选数量\n    - max_len: 解码最大长度\n    \"\"\"\n    model.eval()  # 评估模式\n    with torch.no_grad():  \n        input_tensor = input_tensor.unsqueeze(0) \n        # 编码器传播\n        enc_outputs, (hidden, cell) = model.encoder(input_tensor)\n        hidden = hidden.view(2, -1, hidden.size(-1)).sum(dim=0, keepdim=True)\n        cell   = cell.view(2, -1, cell.size(-1)).sum(dim=0, keepdim=True)\n        dec_hidden = (hidden, cell)  # 解码器初始状态\n        \n        # 贪心解码\n        if not use_beam_search:\n            # 初始解码输入：SOS：初始符，这里是简化为了 \\t\n            dec_input = torch.tensor([[target_token_index['\\t']]], device=device)\n            result = []\n            for _ in range(max_len):\n                # 解码器传播\n                out, dec_hidden = model.decoder(dec_input, dec_hidden, enc_outputs)\n                # 取当前时间步最大概率的 token\n                pred = out.argmax(dim=-1)\n                token = pred.item()\n                if token == target_token_index['\\n']:  # 遇到 EOS 停止，同样简化为\\n\n                    break\n                result.append(reverse_target_char_index[token])  # 保存字符\n                dec_input = pred.unsqueeze(0)  # 下一步输入为当前预测，即自回归。\n\n            return ''.join(result)  # 返回解码后的字符串\n\n       \n        # 束搜索\n        candidates = [([target_token_index['\\t']], 0.0, dec_hidden)]  # 初始化候选序列\n        for _ in range(max_len):\n            new_cands = []\n            for seq, score, h in candidates:\n                # 如果序列已遇到 EOS，保留候选\n                if seq[-1] == target_token_index['\\n']:\n                    new_cands.append((seq, score, h))\n                    continue\n                # 当前时间步输入\n                inp = torch.tensor([[seq[-1]]], device=device)\n                out, new_h = model.decoder(inp, h, enc_outputs)\n                # 计算 log 概率\n                log_probs = F.log_softmax(out, dim=-1).squeeze(0)\n                # 取 top-k 候选\n                top_vals, top_idx = log_probs.topk(beam_width)\n                for v, idx in zip(top_vals, top_idx):\n                    new_seq = seq + [idx.item()]  # 扩展序列\n                    new_score = score + v.item()  # 更新累积 log 概率\n                    new_cands.append((new_seq, new_score, new_h))\n            # 按累积概率排序，保留前 beam_width 个序列\n            candidates = sorted(new_cands, key=lambda x: x[1], reverse=True)[:beam_width]\n        # 最终选择概率最高的序列\n        best = candidates[0][0]\n        # 去掉控制符，转换为最终字符\n        return ''.join(reverse_target_char_index.get(i, '') \n                       for i in best[1:] if i != target_token_index['\\n'])\n\n</code></pre>\n<p>解码的逻辑也被使用在之后的评估环节中。</p>\n<h2 id=\"24-评估指标belu-和完全匹配率\">2.4 评估指标：BELU 和完全匹配率</h2>\n<p>自然，在完成训练后，我们要有相应的指标来进行评估，首先就是我们在理论部分介绍的 BELU ，它通过计算输出和标签在不同尺度上的匹配度来评估模型效果。<br />\n但是，我们说：BELU 这类语言指标使用的原因是因为存在”同义不同形“的情况。<br />\n而<strong>在日期翻译里，翻译结果是唯一的：错一个数字就是完全不同的一天。</strong><br />\n因此，我们再手工定义一个完全匹配率：输出和标签只有相同和不相同两种情况。<br />\n这样，<strong>我们使用 BELU 来评估序列模型拟合的大致效果，再用完全匹配率来观察在日期翻译任务中模型的真正性能。</strong></p>\n<pre><code class=\"language-python\">from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n# BLEU\ndef evaluate_bleu(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):\n    total_bleu = 0\n    # BLEU 平滑方法，避免短句导致分数过低\n    smoother = SmoothingFunction().method4\n\n    for i in range(len(test_enc_input)):\n        # 使用 decode 函数生成预测序列\n        pred = decode(model, test_enc_input[i], use_beam_search, beam_width)\n        # 去掉目标序列的起始符和结束符\n        true = test_targets[i][1:-1]\n        # BLEU 接受列表形式：ref 是参考序列列表，hyp 是预测序列\n        ref = [list(true)]\n        hyp = list(pred)\n        # 计算当前样本 BLEU 分数\n        bleu = sentence_bleu(ref, hyp, smoothing_function=smoother)\n        total_bleu += bleu\n\n    # 平均 BLEU 分数，并转换为百分比\n    avg_bleu = total_bleu / len(test_enc_input) * 100\n    print(f\"BLEU: {avg_bleu:.2f}\")\n    return avg_bleu\n\n# 完全匹配率\ndef evaluate_exact_match(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):\n    correct = 0\n    total = len(test_enc_input)\n    for i in range(total):\n        pred = decode(model, test_enc_input[i], use_beam_search, beam_width)\n        true = test_targets[i][1:-1]\n        # 判断预测与真实是否完全一致\n        if pred == true:\n            correct += 1\n    acc = correct / total * 100\n    print(f\"完全匹配率: {acc:.2f}%\")\n    return acc\n\n</code></pre>\n<p><strong>这里分开写两个指标方便演示和单独调用，实际上，把二者的前半部分逻辑合在一起是更高效的做法。</strong><br />\n到此，需要强调的部分就全部定义完毕，我们开始正式运行。</p>\n<h2 id=\"25-运行效果\">2.5 运行效果</h2>\n<p>我们设置参数如下：</p>\n<pre><code class=\"language-python\">EMB_DIM = 64  \nHIDDEN_DIM = 256  \nEPOCHS = 10  \nBATCH_SIZE = 128  \nLR = 0.001\n</code></pre>\n<p>下面，就来分情况看看效果：</p>\n<h4 id=\"1不使用注意力机制使用贪心解码\">（1）不使用注意力机制，使用贪心解码</h4>\n<p>在这个条件下，我们定义主函数如下：</p>\n<pre><code class=\"language-python\">if __name__ == \"__main__\":  \n\tEMB_DIM = 64  \n\tHIDDEN_DIM = 256  \n\tEPOCHS = 10  \n\tBATCH_SIZE = 128  \n\tLR = 0.001\n\t# 关键设置\n    USE_ATTENTION    = False  \n    USE_BEAM_SEARCH  = False  \n    BEAM_WIDTH       = 3  \n  \n    print(f\"\\n=== 配置 ===\")  \n    print(f\"使用注意力: {USE_ATTENTION}\")  \n    print(f\"使用束搜索: {USE_BEAM_SEARCH} (beam width = {BEAM_WIDTH if USE_BEAM_SEARCH else 'No'})\")  \n  \n    encoder = Encoder(num_encoder_tokens, EMB_DIM, HIDDEN_DIM).to(device)  \n    decoder = Decoder(num_decoder_tokens, EMB_DIM, HIDDEN_DIM, use_attention=USE_ATTENTION).to(device)  \n    model = Seq2Seq(encoder, decoder).to(device)  \n  \n    print(\"\\n开始训练...\")  \n    train_time = train(model, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)  \n  \n    print(\"\\n评估中...\")  \n    bleu = evaluate_bleu(model, test_encoder_input, test_targets,  \n                         use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n    exact_acc = evaluate_exact_match(model, test_encoder_input, test_targets,  \n                                     use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n  \n\tprint(\"\\n=== 抽取测试样本输出 ===\")  \n\tsample_indices = random.sample(range(len(test_encoder_input)), 10)  \n\tfor i in sample_indices:  \n\t    src = test_inputs[i]  \n\t    true = test_targets[i][1:-1]  \n\t    pred,_ = decode(model, test_encoder_input[i],  \n\t                  use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n\t    print(f\"Human  : {src} | True  : {true} | Predicted  : {pred}\")\n</code></pre>\n<p>结果如下：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162800621-1772145123.png\" /></p>\n<p>得益于任务本身和合成数据较为简单，可以发现指标还不错，我们不着急分析 ，继续下一步设置进行对比。</p>\n<h4 id=\"2不使用注意力机制使用束搜索\">（2）不使用注意力机制，使用束搜索</h4>\n<p>这次，设置主函数参数如下：</p>\n<pre><code class=\"language-python\">USE_ATTENTION    = False  \nUSE_BEAM_SEARCH  = True # 使用束搜索 \nBEAM_WIDTH       = 3 # 束宽\n</code></pre>\n<p>看看结果：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162801820-1581242608.png\" /></p>\n<p>指标好像有所提升，但也可能是训练中的偶然性，我们再增加束宽看看：</p>\n<pre><code class=\"language-python\">BEAM_WIDTH       = 10 \n</code></pre>\n<p><img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162800857-1336184228.png\" /><br />\n可以发现，<strong>随着束宽的增加，解码时间明显增加，而指标浮动不大</strong>，这说明限制指标的关键因素可能不是搜索策略。<br />\n由此，我们进入下一步：</p>\n<h4 id=\"3使用注意力机制\">（3）使用注意力机制</h4>\n<p>现在，设置参数如下：</p>\n<pre><code class=\"language-python\">USE_ATTENTION    = True # 使用注意力机制  \nUSE_BEAM_SEARCH  = True  \nBEAM_WIDTH       = 3 \n</code></pre>\n<p>看看结果：<br />\n<img alt=\"image.png\" src=\"https://img2024.cnblogs.com/blog/3708248/202602/3708248-20260205162800597-1996349670.png\" /></p>\n<p>好像也没什么用啊？ 你会发现：<strong>使用注意力机制后，训练时间因相关计算明显增加，但是指标并没有明显提升。</strong><br />\n实际上，这和我们的日期翻译任务本身的特点有关：</p>\n<ol>\n<li><strong>输入输出序列较短</strong>：每个样本通常只有 5~15 个字符左右，LSTM 很容易在没有注意力的情况下就捕捉到序列整体信息。</li>\n<li><strong>信息对齐较简单</strong>：日期翻译本质上是字符级的对齐映射，例如 “Jan 5, 2003” → “2003-01-05”，几乎不存在复杂的长距离依赖。</li>\n</ol>\n<p>注意力机制最明显的作用是在序列较长或输入输出关系复杂时，帮助解码器聚焦到相关的编码状态。而对于现在这种短序列、规则性强的任务，其计算成本增加，而带来的性能提升很有限。</p>\n<p>在这个任务中，想要指标继续增加，有一个很简单的方法就是<strong>在脚本中增加我们人工合成的数据量</strong>，就不再展示了。</p>\n<p><strong>至此，吴恩达深度学习课程五课十五周的内容就全部结束了，之后会出一个完整的目录。</strong></p>\n<h1 id=\"3-附录\">3. 附录</h1>\n<h2 id=\"31-数据合成\">3.1 数据合成</h2>\n<pre><code class=\"language-python\">import torch\nimport random\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nPAD_TOKEN = \"&lt;PAD&gt;\"\n\nMONTHS_FULL = [\n    \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n    \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n]\nMONTHS_ABBR = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n               \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n\nHUMAN_TEMPLATES = [\n    \"{day} {month} {year}\",\n    \"{month} {day}, {year}\",\n    \"{day} of {month}, {year}\",\n    \"{month} {day} {year}\",\n    \"{day}/{month_num}/{year_short}\",\n    \"{month_num}/{day}/{year_short}\",\n    \"{weekday}, {day} {month_abbr} {year}\",\n    \"{month_abbr} {day}th {year_short}\",\n    \"{year}-{month_num}-{day}\",\n    \"The {day}{day_suffix} of {month}, {year}\",\n    \"{day}th {month_abbr}, {year}\",\n    \"{month} the {day}{day_suffix}, {year}\",\n    \"Date: {year}/{month_num}/{day}\",\n    \"{day} in Roman: {roman_day} {month} {year}\",\n    \"{month_abbr}. {day}, '{year_short}\",\n]\n\ndef roman_numeral(n):\n    vals = (10, 9, 5, 4, 1)\n    nums = ('X', 'IX', 'V', 'IV', 'I')\n    result = \"\"\n    for v, r in zip(vals, nums):\n        while n &gt;= v:\n            result += r\n            n -= v\n    return result\n\ndef random_date(start_year=1900, end_year=2100):\n    start = datetime(start_year, 1, 1)\n    end = datetime(end_year, 12, 31)\n    delta = end - start\n    return start + timedelta(days=random.randint(0, delta.days))\n\ndef human_readable_date(date):\n    template = random.choice(HUMAN_TEMPLATES)\n    day = date.day\n    month_idx = date.month - 1\n    month_full = MONTHS_FULL[month_idx]\n    month_abbr = MONTHS_ABBR[month_idx]\n    year = date.year\n    month_num = str(date.month).zfill(2)\n    year_short = str(year % 100).zfill(2)\n    weekday = date.strftime(\"%A\")\n\n    day_suffix = \"th\"\n    if day % 10 == 1 and day != 11: day_suffix = \"st\"\n    elif day % 10 == 2 and day != 12: day_suffix = \"nd\"\n    elif day % 10 == 3 and day != 13: day_suffix = \"rd\"\n\n    day_str = str(day)\n    roman_day = roman_numeral(day) if random.random() &lt; 0.2 else day_str\n\n    return template.format(\n        day=day_str,\n        day_suffix=day_suffix,\n        month=month_full,\n        month_abbr=month_abbr,\n        year=year,\n        month_num=month_num,\n        year_short=year_short,\n        weekday=weekday,\n        roman_day=roman_day\n    )\n\ndef machine_readable_date(date):\n    return date.strftime(\"%Y-%m-%d\")\n\ndef generate_date_pair():\n    d = random_date()\n    return human_readable_date(d), machine_readable_date(d)\n\n\nNUM_SAMPLES = 10000\ninput_texts, target_texts = [], []\n\nfor _ in range(NUM_SAMPLES):\n    human, machine = generate_date_pair()\n    input_texts.append(human)\n    target_texts.append('\\t' + machine + '\\n')\n\ntrain_inputs, test_inputs, train_targets, test_targets = train_test_split(\n    input_texts, target_texts, test_size=0.2, random_state=42\n)\n\nall_input_chars = [PAD_TOKEN] + sorted(set(''.join(input_texts)))\nall_target_chars = [PAD_TOKEN] + sorted(set(''.join(target_texts)))\n\ninput_token_index = {c: i for i, c in enumerate(all_input_chars)}\ntarget_token_index = {c: i for i, c in enumerate(all_target_chars)}\n\nreverse_input_char_index = {i: c for c, i in input_token_index.items()}\nreverse_target_char_index = {i: c for c, i in target_token_index.items()}\n\nmax_encoder_len = max(len(txt) for txt in input_texts)\nmax_decoder_len = max(len(txt) for txt in target_texts)\n\nnum_encoder_tokens = len(all_input_chars)\nnum_decoder_tokens = len(all_target_chars)\n\nprint(f\"max enc len: {max_encoder_len}, max dec len: {max_decoder_len}\")\nprint(f\"input vocab size: {num_encoder_tokens}, target vocab size: {num_decoder_tokens}\")\n\ndef texts_to_tensor(texts, token_index, max_len):\n    pad_idx = token_index[PAD_TOKEN]\n    data = np.full((len(texts), max_len), pad_idx, dtype=np.int64)\n    for i, txt in enumerate(texts):\n        for t, char in enumerate(txt):\n            if t &lt; max_len:\n                data[i, t] = token_index[char]\n    return torch.tensor(data, dtype=torch.long, device=device)\n\ntrain_encoder_input = texts_to_tensor(train_inputs, input_token_index, max_encoder_len)\ntrain_decoder_input = texts_to_tensor(train_targets, target_token_index, max_decoder_len)\n\ntest_encoder_input = texts_to_tensor(test_inputs, input_token_index, max_encoder_len)\ntest_decoder_input = texts_to_tensor(test_targets, target_token_index, max_decoder_len)\n\n\ndataset_pt = {\n    'train_encoder_input': train_encoder_input,\n    'train_decoder_input': train_decoder_input,\n    'test_encoder_input': test_encoder_input,\n    'test_decoder_input': test_decoder_input,\n    'input_token_index': input_token_index,\n    'target_token_index': target_token_index,\n    'reverse_input_char_index': reverse_input_char_index,\n    'reverse_target_char_index': reverse_target_char_index,\n    'max_encoder_len': max_encoder_len,\n    'max_decoder_len': max_decoder_len,\n    'num_encoder_tokens': num_encoder_tokens,\n    'num_decoder_tokens': num_decoder_tokens\n}\n\ntorch.save(dataset_pt, \"date_dataset.pt\")\nprint(\"已保存：date_dataset.pt\")\n\n\n\ndef tensor_to_numpy(t):\n    return t.detach().cpu().numpy()\n\ndataset_npz = {\n    \"train_encoder_input\": tensor_to_numpy(train_encoder_input),\n    \"train_decoder_input\": tensor_to_numpy(train_decoder_input),\n    \"test_encoder_input\": tensor_to_numpy(test_encoder_input),\n    \"test_decoder_input\": tensor_to_numpy(test_decoder_input),\n\n    \"input_token_index\": np.array(input_token_index, dtype=object),\n    \"target_token_index\": np.array(target_token_index, dtype=object),\n    \"reverse_input_char_index\": np.array(reverse_input_char_index, dtype=object),\n    \"reverse_target_char_index\": np.array(reverse_target_char_index, dtype=object),\n\n    \"max_encoder_len\": max_encoder_len,\n    \"max_decoder_len\": max_decoder_len,\n    \"num_encoder_tokens\": num_encoder_tokens,\n    \"num_decoder_tokens\": num_decoder_tokens\n}\n\nnp.savez(\"date_dataset.npz\", **dataset_npz)\nprint(\"已保存：date_dataset.npz\\n\")\n\ndef decode(tensor_row, reverse_dict):\n    return ''.join(\n        reverse_dict[idx.item()]\n        for idx in tensor_row\n        if reverse_dict[idx.item()] != PAD_TOKEN\n    )\n\nprint(\"示例数据：\")\nfor i in range(20):\n    src = decode(train_encoder_input[i], reverse_input_char_index)\n    tgt = decode(train_decoder_input[i], reverse_target_char_index)\n    print(f\"[{i+1}] {src}  ==&gt;  {tgt.strip()}\")\n</code></pre>\n<h2 id=\"32-日期格式翻译-pytorch版\">3.2 日期格式翻译-PyTorch版</h2>\n<pre><code class=\"language-python\">import random  \nimport torch  \nimport torch.nn as nn  \nimport torch.nn.functional as F  \nfrom torch import optim  \nimport time  \nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n  \ndata = torch.load(\"date_dataset.pt\", map_location=device)  \n  \ntrain_encoder_input = data['train_encoder_input']  \ntrain_decoder_input = data['train_decoder_input']  \ntest_encoder_input  = data['test_encoder_input']  \ntest_decoder_input  = data['test_decoder_input']  \n  \ninput_token_index = data['input_token_index']  \ntarget_token_index = data['target_token_index']  \nreverse_target_char_index = data['reverse_target_char_index']  \n  \nnum_encoder_tokens = data['num_encoder_tokens']  \nnum_decoder_tokens = data['num_decoder_tokens']  \n  \nmax_encoder_len = data['max_encoder_len']  \nmax_decoder_len = data['max_decoder_len']  \n  \nPAD_TOKEN = \"&lt;PAD&gt;\"  \nreverse_input_char_index = {i: c for c, i in input_token_index.items()}  \n  \ndef tensor_to_text(tensor_row, reverse_dict):  \n    return ''.join(  \n        reverse_dict[idx.item()]  \n        for idx in tensor_row  \n        if reverse_dict[idx.item()] != PAD_TOKEN  \n    )  \n  \ntest_inputs = [tensor_to_text(test_encoder_input[i], reverse_input_char_index)  \n               for i in range(len(test_encoder_input))]  \n  \ntest_targets = [tensor_to_text(test_decoder_input[i], reverse_target_char_index)  \n                for i in range(len(test_decoder_input))]  \n  \nclass Encoder(nn.Module):  \n    def __init__(self, vocab_size, emb_dim, hidden_dim):  \n        super().__init__()  \n        self.embedding = nn.Embedding(vocab_size, emb_dim)  \n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)  \n  \n    def forward(self, x):  \n        embedded = self.embedding(x)  \n        outputs, (hidden, cell) = self.lstm(embedded)  \n        return outputs, (hidden, cell)  \n  \nclass Decoder(nn.Module):  \n    def __init__(self, vocab_size, emb_dim, hidden_dim, use_attention=False):  \n        super().__init__()  \n        self.use_attention = use_attention  \n        self.embedding = nn.Embedding(vocab_size, emb_dim)  \n        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)  \n  \n        if use_attention:  \n            self.attn = nn.Linear(hidden_dim + hidden_dim * 2, 1)  \n            self.attn_combine = nn.Linear(hidden_dim + hidden_dim * 2, hidden_dim)  \n  \n        self.out = nn.Linear(hidden_dim, vocab_size)  \n  \n    def forward(self, input_step, hidden_cell, encoder_outputs):  \n        embedded = self.embedding(input_step)  \n        lstm_out, hidden_cell = self.lstm(embedded, hidden_cell)  \n  \n        query = lstm_out.squeeze(1)  \n  \n        if self.use_attention:  \n            query_exp = query.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)  \n            attn_scores = self.attn(torch.cat((query_exp, encoder_outputs), dim=-1)).squeeze(-1)  \n            attn_weights = F.softmax(attn_scores, dim=-1).unsqueeze(1)  \n            context = torch.bmm(attn_weights, encoder_outputs).squeeze(1)  \n            combined = torch.cat((query, context), dim=1)  \n            output = torch.tanh(self.attn_combine(combined))  \n        else:  \n            output = query  \n  \n        output = self.out(output)  \n        return output, hidden_cell  \n  \nclass Seq2Seq(nn.Module):  \n    def __init__(self, encoder, decoder):  \n        super().__init__()  \n        self.encoder = encoder  \n        self.decoder = decoder  \n  \n    def forward(self, src, tgt):  \n        enc_outputs, (hidden, cell) = self.encoder(src)  \n        hidden = hidden.view(2, -1, hidden.size(-1)).sum(dim=0, keepdim=True)  \n        cell   = cell.view(2, -1, cell.size(-1)).sum(dim=0, keepdim=True)  \n        dec_hidden = (hidden, cell)  \n        outputs = []  \n  \n        for t in range(tgt.size(1)):  \n            out, dec_hidden = self.decoder(tgt[:, t].unsqueeze(1), dec_hidden, enc_outputs)  \n            outputs.append(out)  \n  \n        return torch.stack(outputs, dim=1)  \n  \ndef train(model, epochs=10, batch_size=128, lr=0.001):  \n    enc_optimizer = optim.Adam(model.encoder.parameters(), lr=lr)  \n    dec_optimizer = optim.Adam(model.decoder.parameters(), lr=lr)  \n    criterion = nn.CrossEntropyLoss(ignore_index=0)  \n  \n    start_time = time.time()  \n  \n    for epoch in range(epochs):  \n        total_loss = 0  \n        num_batches = 0  \n        perm = torch.randperm(len(train_encoder_input))  \n        for i in range(0, len(train_encoder_input), batch_size):  \n            idx = perm[i:i+batch_size]  \n            enc_in = train_encoder_input[idx]  \n            dec_in_full = train_decoder_input[idx]  \n            dec_in = dec_in_full[:, :-1]  \n            dec_target = dec_in_full[:, 1:]  \n  \n            enc_optimizer.zero_grad()  \n            dec_optimizer.zero_grad()  \n  \n            outputs = model(enc_in, dec_in)  \n  \n            loss = criterion(outputs.reshape(-1, num_decoder_tokens), dec_target.reshape(-1))  \n            loss.backward()  \n            enc_optimizer.step()  \n            dec_optimizer.step()  \n  \n            total_loss += loss.item()  \n            num_batches += 1  \n  \n        avg_loss = total_loss / num_batches  \n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")  \n  \n    train_time = time.time() - start_time  \n    print(f\"训练总时间: {train_time:.2f} 秒\")  \n    return train_time  \n  \nimport time  \n  \ndef decode(model, input_tensor, use_beam_search=False, beam_width=3, max_len=20):  \n    model.eval()  \n    start_time = time.time()    \n    with torch.no_grad():  \n        input_tensor = input_tensor.unsqueeze(0)  \n        enc_outputs, (hidden, cell) = model.encoder(input_tensor)  \n        hidden = hidden.view(2, -1, hidden.size(-1)).sum(dim=0, keepdim=True)  \n        cell   = cell.view(2, -1, cell.size(-1)).sum(dim=0, keepdim=True)  \n        dec_hidden = (hidden, cell)  \n  \n        if not use_beam_search:  \n            dec_input = torch.tensor([[target_token_index['\\t']]], device=device)  \n            result = []  \n            for _ in range(max_len):  \n                out, dec_hidden = model.decoder(dec_input, dec_hidden, enc_outputs)  \n                pred = out.argmax(dim=-1)  \n                token = pred.item()  \n                if token == target_token_index['\\n']:  \n                    break  \n                result.append(reverse_target_char_index[token])  \n                dec_input = pred.unsqueeze(0)  \n            decoded = ''.join(result)  \n  \n        else:  \n            candidates = [([target_token_index['\\t']], 0.0, dec_hidden)]  \n            for _ in range(max_len):  \n                new_cands = []  \n                for seq, score, h in candidates:  \n                    if seq[-1] == target_token_index['\\n']:  \n                        new_cands.append((seq, score, h))  \n                        continue  \n                    inp = torch.tensor([[seq[-1]]], device=device)  \n                    out, new_h = model.decoder(inp, h, enc_outputs)  \n                    log_probs = F.log_softmax(out, dim=-1).squeeze(0)  \n                    k = min(beam_width, log_probs.size(0))    \n                    top_vals, top_idx = log_probs.topk(k)  \n                    for v, idx in zip(top_vals, top_idx):  \n                        new_seq = seq + [idx.item()]  \n                        new_score = score + v.item()  \n                        new_cands.append((new_seq, new_score, new_h))  \n                candidates = sorted(new_cands, key=lambda x: x[1], reverse=True)[:beam_width]  \n  \n            best = candidates[0][0]  \n            decoded = ''.join(reverse_target_char_index.get(i, '') for i in best[1:] if i != target_token_index['\\n'])  \n  \n    decode_time = time.time() - start_time  \n    return decoded, decode_time  \n  \n  \ndef evaluate_bleu(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):  \n    total_bleu = 0  \n    total_time = 0  \n    smoother = SmoothingFunction().method4  \n  \n    for i in range(len(test_enc_input)):  \n        pred, dt = decode(model, test_enc_input[i], use_beam_search, beam_width)  \n        total_time += dt  \n        true = test_targets[i][1:-1]  \n        ref = [list(true)]  \n        hyp = list(pred)  \n        bleu = sentence_bleu(ref, hyp, smoothing_function=smoother)  \n        total_bleu += bleu  \n  \n    avg_bleu = total_bleu / len(test_enc_input) * 100  \n    avg_time = total_time / len(test_enc_input)  \n    print(f\"BLEU: {avg_bleu:.2f} | 平均解码时间: {avg_time:.4f} 秒/样本\")  \n    return avg_bleu, avg_time  \n  \n  \ndef evaluate_exact_match(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):  \n    correct = 0  \n    total = len(test_enc_input)  \n  \n    for i in range(total):  \n        pred, _ = decode(model, test_enc_input[i], use_beam_search, beam_width)    \n        true = test_targets[i][1:-1]    \n        if pred == true:  \n            correct += 1  \n  \n    acc = correct / total * 100  \n    print(f\"完全匹配率: {acc:.2f}%\")  \n    return acc  \n  \nif __name__ == \"__main__\":  \n    EMB_DIM = 64  \n    HIDDEN_DIM = 256  \n    EPOCHS = 10  \n    BATCH_SIZE = 128  \n    LR = 0.001  \n  \n    USE_ATTENTION    = True  \n    USE_BEAM_SEARCH  = True  \n    BEAM_WIDTH       = 3  \n  \n    print(f\"\\n=== 配置 ===\")  \n    print(f\"使用注意力: {USE_ATTENTION}\")  \n    print(f\"使用束搜索: {USE_BEAM_SEARCH} (beam width = {BEAM_WIDTH if USE_BEAM_SEARCH else 'No'})\")  \n  \n    encoder = Encoder(num_encoder_tokens, EMB_DIM, HIDDEN_DIM).to(device)  \n    decoder = Decoder(num_decoder_tokens, EMB_DIM, HIDDEN_DIM, use_attention=USE_ATTENTION).to(device)  \n    model = Seq2Seq(encoder, decoder).to(device)  \n  \n    print(\"\\n开始训练...\")  \n    train_time = train(model, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)  \n  \n    print(\"\\n评估中...\")  \n    bleu = evaluate_bleu(model, test_encoder_input, test_targets,  \n                         use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n    exact_acc = evaluate_exact_match(model, test_encoder_input, test_targets,  \n                                     use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n  \n    print(\"\\n=== 抽取测试样本输出 ===\")  \n    sample_indices = random.sample(range(len(test_encoder_input)), 10)  \n    for i in sample_indices:  \n        src = test_inputs[i]  \n        true = test_targets[i][1:-1]  \n        pred,_ = decode(model, test_encoder_input[i],  \n                      use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n        print(f\"Human  : {src} | True  : {true} | Predicted  : {pred}\")\n</code></pre>\n<h2 id=\"33-日期格式翻译-tf版\">3.3 日期格式翻译-TF版</h2>\n<pre><code class=\"language-python\">import numpy as np  \nimport tensorflow as tf  \nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional  \nimport random  \nimport time  \nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction  \n  \ndataset = np.load(\"date_dataset.npz\", allow_pickle=True)  \n  \ntrain_encoder_input = dataset['train_encoder_input']  \ntrain_decoder_input = dataset['train_decoder_input']  \ntest_encoder_input = dataset['test_encoder_input']  \ntest_decoder_input = dataset['test_decoder_input']  \n  \ninput_token_index = dataset['input_token_index'].item()  \ntarget_token_index = dataset['target_token_index'].item()  \nreverse_target_char_index = dataset['reverse_target_char_index'].item()  \n  \nnum_encoder_tokens = int(dataset['num_encoder_tokens'])  \nnum_decoder_tokens = int(dataset['num_decoder_tokens'])  \nmax_encoder_len = int(dataset['max_encoder_len'])  \nmax_decoder_len = int(dataset['max_decoder_len'])  \n  \nreverse_input_char_index = {i: c for c, i in input_token_index.items()}  \n  \nPAD_TOKEN = \"&lt;PAD&gt;\"    \n  \ndef tensor_to_text(tensor_row, reverse_dict):  \n    return ''.join(  \n        reverse_dict.get(int(idx), '')  \n        for idx in tensor_row  \n        if reverse_dict.get(int(idx), '') != PAD_TOKEN  \n    )  \n  \n  \ntest_inputs = [tensor_to_text(test_encoder_input[i], reverse_input_char_index)  \n               for i in range(len(test_encoder_input))]  \n  \ntest_targets = [tensor_to_text(test_decoder_input[i], reverse_target_char_index)  \n                for i in range(len(test_decoder_input))]  \n  \nclass Encoder(tf.keras.Model):  \n    def __init__(self, vocab_size, emb_dim, hidden_dim):  \n        super(Encoder, self).__init__()  \n        self.embedding = Embedding(vocab_size, emb_dim, mask_zero=True)  \n        self.lstm = Bidirectional(  \n            LSTM(hidden_dim, return_sequences=True, return_state=True)  \n        )  \n  \n    def call(self, x):  \n        embedded = self.embedding(x)  \n        outputs, fw_h, fw_c, bw_h, bw_c = self.lstm(embedded)  \n        hidden = tf.concat([fw_h, bw_h], axis=-1)  \n        cell = tf.concat([fw_c, bw_c], axis=-1)  \n        return outputs, (hidden, cell)  \n  \n  \nclass Decoder(tf.keras.layers.Layer):  \n    def __init__(self, vocab_size, emb_dim, hidden_dim, use_attention=False):  \n        super(Decoder, self).__init__()  \n        self.use_attention = use_attention  \n        self.embedding = Embedding(vocab_size, emb_dim, mask_zero=True)  \n        self.lstm = LSTM(hidden_dim, return_sequences=False, return_state=True)  \n        self.out = Dense(vocab_size)  \n  \n        if use_attention:  \n            self.attn_W1 = Dense(hidden_dim)                 \n            self.attn_W2 = Dense(hidden_dim)               \n            self.attn_V  = Dense(1)  \n            self.attn_combine = Dense(hidden_dim)  \n  \n    def call(self, input_step, hidden_cell, encoder_outputs):  \n   \n        embedded = self.embedding(input_step)            \n        lstm_out, h, c = self.lstm(embedded, initial_state=hidden_cell)  \n        query = lstm_out                                  \nif self.use_attention:  \n            score = self.attn_V(  \n                tf.tanh(  \n                    self.attn_W1(encoder_outputs) +  \n                    self.attn_W2(query[:, tf.newaxis, :])  \n                )  \n            )                                             \n  \n            attn_weights = tf.nn.softmax(score, axis=1)   \n  \n            context = tf.reduce_sum(encoder_outputs * attn_weights, axis=1)  \n            combined = tf.concat([query, context], axis=-1)  \n            output = tf.tanh(self.attn_combine(combined))  \n        else:  \n            output = query  \n  \n        logits = self.out(output)                         \n        return logits, (h, c)  \n  \n  \nclass Seq2Seq(tf.keras.Model):  \n    def __init__(self, encoder, decoder):  \n        super(Seq2Seq, self).__init__()  \n        self.encoder = encoder  \n        self.decoder = decoder  \n  \n    def call(self, src, tgt, training=True):  \n        enc_outputs, (hidden, cell) = self.encoder(src)  \n  \n        hidden_fw, hidden_bw = tf.split(hidden, num_or_size_splits=2, axis=-1)  \n        cell_fw, cell_bw = tf.split(cell, num_or_size_splits=2, axis=-1)  \n  \n        hidden = hidden_fw + hidden_bw  \n        cell = cell_fw + cell_bw  \n  \n        dec_hidden = (hidden, cell)  \n  \n        outputs = []  \n        for t in range(tgt.shape[1]):  \n            step_input = tgt[:, t:t + 1]  \n            logits, dec_hidden = self.decoder(  \n                step_input, dec_hidden, enc_outputs  \n            )  \n            outputs.append(logits)  \n  \n        return tf.stack(outputs, axis=1)  \n  \ndef train(model, epochs=10, batch_size=128, lr=0.001):  \n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)  \n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  \n  \n    start_time = time.time()  \n    n_samples = len(train_encoder_input)  \n  \n    for epoch in range(epochs):  \n        total_loss = 0.0  \n        num_batches = 0  \n  \n        indices = np.random.permutation(n_samples)  \n        enc = train_encoder_input[indices]  \n        dec = train_decoder_input[indices]  \n  \n        for i in range(0, n_samples, batch_size):  \n            enc_batch = enc[i:i + batch_size]  \n            dec_batch = dec[i:i + batch_size]  \n  \n            dec_in = dec_batch[:, :-1]  \n            dec_tgt = dec_batch[:, 1:]  \n  \n            with tf.GradientTape() as tape:  \n                logits = model(enc_batch, dec_in, training=True)  \n                loss_value = loss_fn(dec_tgt, logits)  \n                loss_value += tf.reduce_sum(model.losses)  \n  \n            grads = tape.gradient(loss_value, model.trainable_variables)  \n            optimizer.apply_gradients(zip(grads, model.trainable_variables))  \n  \n            total_loss += float(loss_value)  \n            num_batches += 1  \n  \n        avg_loss = total_loss / num_batches  \n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")  \n  \n    train_time = time.time() - start_time  \n    print(f\"训练用时: {train_time:.2f} 秒\")  \n    return train_time  \n  \n@tf.function  \ndef greedy_decode_step(decoder, dec_input, dec_hidden, enc_outputs):  \n    logits, dec_hidden = decoder(dec_input, dec_hidden, enc_outputs)  \n    return logits, dec_hidden  \n  \n  \ndef decode(model, input_tensor, use_beam_search=False, beam_width=3, max_len=20):  \n    dummy_src = tf.zeros((1, max_encoder_len), dtype=tf.int32)  \n    dummy_tgt = tf.zeros((1, 1), dtype=tf.int32)  \n    _ = model(dummy_src, dummy_tgt, training=False)  \n  \n    start_time = time.time()  \n  \n    input_tensor = tf.expand_dims(input_tensor, 0)  \n    enc_outputs, (hidden, cell) = model.encoder(input_tensor)  \n  \n    hidden_fw, hidden_bw = tf.split(hidden, num_or_size_splits=2, axis=-1)  \n    cell_fw, cell_bw = tf.split(cell, num_or_size_splits=2, axis=-1)  \n  \n    hidden = hidden_fw + hidden_bw  \n    cell = cell_fw + cell_bw  \n  \n    dec_hidden = (hidden, cell)  \n  \n    if not use_beam_search:  \n        dec_input = tf.constant([[target_token_index['\\t']]], dtype=tf.int32)  \n        result = []  \n        for _ in range(max_len):  \n            logits, dec_hidden = greedy_decode_step(  \n                model.decoder, dec_input, dec_hidden, enc_outputs  \n            )  \n            pred = tf.argmax(logits, axis=-1)  \n            token = int(pred[0])  \n            if token == target_token_index.get('\\n', -1):  \n                break  \n            result.append(reverse_target_char_index.get(token, ''))  \n            dec_input = pred[:, tf.newaxis]  \n  \n        decoded = ''.join(result)  \n  \n    else:  \n        candidates = [([target_token_index['\\t']], 0.0, dec_hidden)]  \n        for _ in range(max_len):  \n            new_cands = []  \n            for seq, score, h_c in candidates:  \n                if seq[-1] == target_token_index.get('\\n', -1):  \n                    new_cands.append((seq, score, h_c))  \n                    continue  \n                inp = tf.constant([[seq[-1]]], dtype=tf.int32)  \n                logits, new_h_c = model.decoder(inp, h_c, enc_outputs)  \n                log_probs = tf.nn.log_softmax(logits, axis=-1)[0]  \n                top_vals, top_idx = tf.math.top_k(log_probs, k=beam_width)  \n                for v, idx in zip(top_vals, top_idx):  \n                    new_seq = seq + [int(idx)]  \n                    new_score = score + float(v)  \n                    new_cands.append((new_seq, new_score, new_h_c))  \n  \n            candidates = sorted(new_cands, key=lambda x: x[1], reverse=True)[:beam_width]  \n  \n        best_seq = candidates[0][0]  \n        decoded = ''.join(  \n            reverse_target_char_index.get(i, '') for i in best_seq[1:] if i != target_token_index.get('\\n', -1))  \n  \n    decode_time = time.time() - start_time  \n    return decoded, decode_time  \n  \n  \ndef evaluate_bleu(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):  \n    total_bleu = 0  \n    total_time = 0  \n    smoother = SmoothingFunction().method4  \n  \n    for i in range(len(test_enc_input)):  \n        pred, dt = decode(model, test_enc_input[i], use_beam_search, beam_width)  \n        total_time += dt  \n        true = test_targets[i][1:-1]    \n        ref = [list(true)]  \n        hyp = list(pred)  \n        bleu = sentence_bleu(ref, hyp, smoothing_function=smoother)  \n        total_bleu += bleu  \n  \n    avg_bleu = total_bleu / len(test_enc_input) * 100  \n    avg_time = total_time / len(test_enc_input)  \n    print(f\"BLEU: {avg_bleu:.2f} | 平均解码时间: {avg_time:.4f} 秒/样本\")  \n    return avg_bleu, avg_time  \n  \n  \ndef evaluate_exact_match(model, test_enc_input, test_targets, use_beam_search=False, beam_width=3):  \n    correct = 0  \n    total = len(test_enc_input)  \n  \n    for i in range(total):  \n        pred, _ = decode(model, test_enc_input[i], use_beam_search, beam_width)  \n        true = test_targets[i][1:-1]  \n        if pred == true:  \n            correct += 1  \n  \n    acc = correct / total * 100  \n    print(f\"完全匹配率: {acc:.2f}%\")  \n    return acc  \n  \n  \nif __name__ == \"__main__\":  \n    EMB_DIM = 64  \n    HIDDEN_DIM = 256   \n    BATCH_SIZE = 128  \n    LR = 0.001  \n  \n    USE_ATTENTION = True  \n    USE_BEAM_SEARCH = True  \n    BEAM_WIDTH = 3  \n  \n    print(f\"\\n=== 配置 ===\")  \n    print(f\"使用注意力: {USE_ATTENTION}\")  \n    print(f\"使用束搜索: {USE_BEAM_SEARCH} (beam width = {BEAM_WIDTH if USE_BEAM_SEARCH else 'No'})\")  \n  \n    encoder = Encoder(num_encoder_tokens, EMB_DIM, HIDDEN_DIM)  \n    decoder = Decoder(num_decoder_tokens, EMB_DIM, HIDDEN_DIM, use_attention=USE_ATTENTION)  \n    model = Seq2Seq(encoder, decoder)  \n  \n    print(\"\\n开始训练...\")  \n    train_time = train(model, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR)  \n  \n    print(\"\\n评估中...\")  \n    bleu = evaluate_bleu(model, test_encoder_input, test_targets,  \n                         use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n    exact_acc = evaluate_exact_match(model, test_encoder_input, test_targets,  \n                                     use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n  \n    print(\"\\n=== 抽取测试样本输出 ===\")  \n    sample_indices = random.sample(range(len(test_encoder_input)), 10)  \n    for i in sample_indices:  \n        src = test_inputs[i]  \n        true = test_targets[i][1:-1]  \n        pred, _ = decode(model, test_encoder_input[i],  \n                         use_beam_search=USE_BEAM_SEARCH, beam_width=BEAM_WIDTH)  \n        print(f\"Human  : {src} | True  : {true} | Predicted  : {pred}\")\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 16:34</span>&nbsp;\n<a href=\"https://www.cnblogs.com/Goblinscholar\">哥布林学者</a>&nbsp;\n阅读(<span id=\"post_view_count\">17</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "鸿蒙应用开发UI基础第三节：UIAbility生命周期全解析",
      "link": "https://www.cnblogs.com/san-xiu/p/19579716",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/san-xiu/p/19579716\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 15:55\">\n    <span>鸿蒙应用开发UI基础第三节：UIAbility生命周期全解析</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"学习目标\">【学习目标】</h2>\n<ol>\n<li>掌握UIAbility核心生命周期方法的触发时机、系统行为及约束规则；</li>\n<li>理解生命周期与WindowStage的深度联动逻辑，明确页面加载、事件订阅的时机；</li>\n<li>掌握<code>onDestroy</code>回调的特殊触发规则（含API 13一键清理、调试模式、terminateSelf调用场景）；</li>\n<li>能通过实操验证生命周期执行顺序，掌握WindowStage事件监听的正确方式，避免新手常见混淆；</li>\n<li>区分“应用退后台”与“应用销毁”的核心差异，规避生命周期相关的开发误区。</li>\n</ol>\n<h2 id=\"本节重点\">【本节重点】</h2>\n<h3 id=\"1-核心问题导入\">1. 核心问题导入</h3>\n<ul>\n<li>应用启动、切后台、切前台、退出时，UIAbility会经历哪些状态变化？</li>\n<li>窗口的获焦/失焦、前台/后台状态该如何通过WindowStage事件监听？</li>\n<li>WindowStage的事件订阅与解绑，分别应在哪个生命周期方法中执行？</li>\n<li>不同关闭应用的方式（手动调用API、一键清理、调试模式移除任务）对<code>onDestroy</code>触发有何影响？</li>\n<li>为什么页面只能在<code>onWindowStageCreate</code>中加载，不能在<code>onCreate</code>中加载？</li>\n</ul>\n<h3 id=\"2-核心概念\">2. 核心概念</h3>\n<p>UIAbility生命周期是指UIAbility从创建到销毁的全流程，由系统统一调度，包含创建、窗口管理、前后台切换、销毁等核心阶段。通过重写生命周期方法，可在特定时机完成初始化、资源申请/释放、页面加载等操作；</p>\n<p>窗口的活动状态（获焦/失焦、前台/后台）需通过<code>WindowStage</code>事件监听实现，而非生命周期方法。<br />\n<code>onDestroy</code>作为生命周期最后一个方法，其触发与否受关闭应用的方式、API版本、应用类型（有无实况窗）、运行模式（调试/正式）等因素影响。</p>\n<blockquote>\n<p>本节内容延用<code>FirstApplication</code>工程,无新增文件。 使用默认启动模式singleton。</p>\n</blockquote>\n<h2 id=\"二uiability生命周期核心阶段与方法\">二、UIAbility生命周期核心阶段与方法</h2>\n<h3 id=\"1-生命周期可视化示意图\">1. 生命周期可视化示意图</h3>\n<p>生命周期的完整流转逻辑可通过以下示意图直观理解，涵盖启动、前后台切换、关闭全流程的方法触发顺序与WindowStage事件关联：</p>\n<p>示意图说明：左侧为UIAbility核心生命周期方法，右侧为关联的WindowStage事件，窗口的获焦/失焦、前台/后台状态需通过<code>windowStageEvent</code>事件监听实现；销毁阶段的<code>onDestroy</code>触发规则需结合关闭应用的方式判断。</p>\n<p><img alt=\"UIAbility的生命周期示意图\" class=\"lazyload\" /></p>\n<h3 id=\"2-核心方法\">2. 核心方法</h3>\n<p>以下是UIAbility核心生命周期方法的详细说明，聚焦各方法的触发时机、核心作用与执行次数：</p>\n<h4 id=\"oncreatewant-launchparam\"><code>onCreate(want, launchParam)</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility实例首次创建时触发；</li>\n<li><strong>核心作用</strong>：完成应用全局初始化工作，比如建立数据库连接、初始化全局配置参数、日志模块、网络配置、创建全局通用工具类等，这些资源会在<code>onDestroy</code>中对应释放；</li>\n<li><strong>执行次数</strong>：整个UIAbility实例生命周期内仅触发1次。</li>\n</ul>\n<h4 id=\"onwindowstagecreatewindowstage\"><code>onWindowStageCreate(windowStage)</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility实例创建完成后、应用进入前台前，且系统首次创建WindowStage（窗口容器）时触发；</li>\n<li><strong>核心作用</strong>：负责页面加载、订阅WindowStage相关事件（如窗口焦点变化、显示/隐藏事件）；</li>\n<li><strong>执行次数</strong>：单实例（singleton）模式下仅触发1次（多实例模式下每次创建新实例都会触发）。</li>\n</ul>\n<h4 id=\"onforeground\"><code>onForeground()</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility从后台切换至前台、界面即将可见之前触发；</li>\n<li><strong>核心作用</strong>：恢复前台运行所需资源，比如重启暂停的定时器、重新开启定位服务、恢复网络请求轮询等；</li>\n<li><strong>执行次数</strong>：可多次触发（每次切前台都会执行）。</li>\n</ul>\n<h4 id=\"onbackground\"><code>onBackground()</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility界面完全不可见（如按Home键切后台、打开其他应用覆盖当前界面）后触发；</li>\n<li><strong>核心作用</strong>：暂停前台资源以节省系统开销，比如停止定时器、关闭定位服务、保存用户操作数据（作为<code>onDestroy</code>未触发时的兜底方案）；</li>\n<li><strong>执行次数</strong>：可多次触发（每次切后台都会执行）。</li>\n</ul>\n<h4 id=\"onwindowstagewilldestroywindowstage\"><code>onWindowStageWillDestroy(windowStage)</code></h4>\n<ul>\n<li><strong>触发时机</strong>：WindowStage（窗口容器）即将被销毁前触发；</li>\n<li><strong>核心作用</strong>：解绑在<code>onWindowStageCreate</code>中订阅的WindowStage事件、清理窗口相关缓存资源，避免内存泄漏；</li>\n<li><strong>执行次数</strong>：仅在应用“优雅销毁”（如调用<code>terminateSelf()</code>、正常退出）时触发1次，一键清理等强制销毁场景不触发。</li>\n</ul>\n<h4 id=\"onwindowstagedestroy\"><code>onWindowStageDestroy()</code></h4>\n<ul>\n<li><strong>触发时机</strong>：WindowStage（窗口容器）销毁完成后触发；</li>\n<li><strong>核心作用</strong>：确认窗口相关资源已释放，做最终的窗口状态校验；</li>\n<li><strong>执行次数</strong>：仅在应用“优雅销毁”时触发1次，强制销毁场景不触发。</li>\n</ul>\n<h4 id=\"ondestroy\"><code>onDestroy()</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility实例即将被销毁前触发；</li>\n<li><strong>核心作用</strong>：释放<code>onCreate</code>中初始化的全局资源，比如关闭数据库连接、清理全局缓存、保存最终的应用状态数据；</li>\n<li><strong>执行次数</strong>：仅在“优雅销毁”时触发1次（API 13+中，无实况窗应用被一键清理、调试模式移除任务时，系统直接终止进程，该方法不触发）。</li>\n</ul>\n<h4 id=\"onnewwantwant-launchparam\"><code>onNewWant(want, launchParam)</code></h4>\n<ul>\n<li><strong>触发时机</strong>：UIAbility实例已启动（未销毁）、再次被外部调用（如其他页面/应用跳转）时触发；</li>\n<li><strong>核心作用</strong>：处理新的启动参数，比如接收跳转传参、更新页面展示内容；</li>\n<li><strong>执行次数</strong>：按需触发（每次复用实例调用都会执行）。</li>\n<li></li>\n</ul>\n<p><strong>注意</strong>：</p>\n<ul>\n<li><code>onDestroy</code>触发特殊规则：API 13及以上版本中，无实况窗应用被一键清理、调试模式下移除任务时，系统直接终止进程，该方法不会触发；仅<code>terminateSelf()</code>调用、正常返回退出、有实况窗应用被一键清理时触发。</li>\n</ul>\n<h3 id=\"3-windowstage事件详解监听窗口活动状态\">3. WindowStage事件详解（监听窗口活动状态）</h3>\n<p>窗口的所有活动状态均通过<code>windowStageEvent</code>事件监听，核心事件说明：</p>\n<ul>\n<li><code>SHOWN</code>：窗口从后台切换到前台（可见），代表应用切前台；</li>\n<li><code>HIDDEN</code>：窗口从前台切换到后台（不可见），代表应用切后台；</li>\n<li><code>ACTIVE</code>：窗口获得焦点（可接收点击/输入），处于可交互状态；</li>\n<li><code>INACTIVE</code>：窗口失去焦点（无法接收输入），处于不可交互状态；</li>\n<li><code>RESUMED</code>：窗口进入前台可交互状态，应用正常运行；</li>\n<li><code>PAUSED</code>：窗口进入前台不可交互状态。</li>\n</ul>\n<h2 id=\"三完整生命周期代码示例\">三、完整生命周期代码示例</h2>\n<pre><code class=\"language-javascript\">import { UIAbility, AbilityConstant, Want, common } from '@kit.AbilityKit';\nimport { window } from '@kit.ArkUI';\nimport { hilog } from '@kit.LogKit';\nimport { BusinessError } from '@ohos.base';\n\nconst TAG = 'UIAbility_Lifecycle';\nconst DOMAIN = 0x0000;\n\nexport default class EntryAbility extends UIAbility {\n  // 1. 创建阶段（仅1次）\n  onCreate(want: Want, launchParam: AbilityConstant.LaunchParam) {\n    hilog.info(DOMAIN, TAG, '--- onCreate 触发（全局初始化）---');\n    // 资源初始化：全局配置、数据库连接等（对应释放：onDestroy）\n  }\n\n  // 2. 窗口创建阶段（窗口首次创建/重建）\n  onWindowStageCreate(windowStage: window.WindowStage): void {\n    hilog.info(DOMAIN, TAG, '--- onWindowStageCreate 触发（加载页面）---');\n    // 订阅WindowStage事件\n    this.registerWindowStageEvent(windowStage);\n    // 加载页面（唯一合法时机）\n    windowStage.loadContent('pages/Index').then(()=&gt;{\n      hilog.info(DOMAIN, TAG, 'Index页面加载成功');\n    }).catch((err:BusinessError)=&gt;{\n      hilog.error(DOMAIN, TAG, `页面加载失败：code=${err.code}, message=${err.message}`);\n    })\n  }\n\n  // 3. 前台阶段（切前台时）\n  onForeground() {\n    hilog.info(DOMAIN, TAG, '--- onForeground 触发（恢复前台资源）---');\n    // 启动前台专属资源（定时器、定位等）\n  }\n\n  // 4. 后台阶段（切后台时）\n  onBackground() {\n    hilog.info(DOMAIN, TAG, '--- onBackground 触发（释放后台资源）---');\n    // 暂停前台资源，保存关键数据（兜底）\n  }\n\n  // 5. 窗口预销毁阶段（窗口即将销毁）\n  onWindowStageWillDestroy(windowStage: window.WindowStage) {\n    hilog.info(DOMAIN, TAG, '--- onWindowStageWillDestroy 触发（窗口预销毁）---');\n    // 解绑WindowStage事件，避免内存泄漏\n    this.unregisterWindowStageEvent(windowStage);\n  }\n\n  // 6. 窗口销毁阶段（窗口已销毁）\n  onWindowStageDestroy() {\n    hilog.info(DOMAIN, TAG, '--- onWindowStageDestroy 触发（销毁窗口）---');\n    // 窗口实例失效，无需额外操作\n  }\n\n  // 7. 销毁阶段（仅1次，存在不触发场景）\n  onDestroy() {\n    hilog.info(DOMAIN, TAG, '--- onDestroy 触发（销毁应用）---');\n    // 释放全局资源、保存最终数据\n  }\n\n  // 8. 新参数阶段（已启动后接收新请求）\n  onNewWant(want: Want, launchParam: AbilityConstant.LaunchParam) {\n    hilog.info(DOMAIN, TAG, `--- onNewWant 触发（新参数：${JSON.stringify(want)}）---`);\n    // 处理新的启动参数\n  }\n  /**\n   * 订阅WindowStage事件（监听窗口活动状态）\n   * @param windowStage - 当前窗口实例\n   */\n  private registerWindowStageEvent(windowStage: window.WindowStage): void {\n    try {\n      windowStage.on('windowStageEvent', (data) =&gt; {\n        const stageEventType: window.WindowStageEventType = data;\n        switch (stageEventType) {\n          case window.WindowStageEventType.SHOWN:\n            hilog.info(DOMAIN, TAG, `windowStage foreground`);\n            break;\n          case window.WindowStageEventType.ACTIVE:\n            hilog.info(DOMAIN, TAG, `windowStage active`);\n            break;\n          case window.WindowStageEventType.INACTIVE:\n            hilog.info(DOMAIN, TAG, `windowStage inactive`);\n            break;\n          case window.WindowStageEventType.HIDDEN:\n            hilog.info(DOMAIN, TAG, `windowStage background`);\n            break;\n          case window.WindowStageEventType.RESUMED:\n            hilog.info(DOMAIN, TAG, `windowStage resumed`);\n            break;\n          case window.WindowStageEventType.PAUSED:\n            hilog.info(DOMAIN, TAG, `windowStage paused.`);\n            break;\n          default:\n            break;\n        }\n      });\n      hilog.info(DOMAIN, TAG, 'WindowStage事件订阅成功');\n    } catch (exception) {\n      hilog.error(DOMAIN, TAG, `订阅窗口事件失败：${JSON.stringify(exception)}`);\n    }\n  }\n\n  /**\n   * 解绑WindowStage事件（资源释放）\n   */\n  private unregisterWindowStageEvent(windowStage: window.WindowStage): void {\n    try {\n      windowStage.off('windowStageEvent');\n      hilog.info(DOMAIN, TAG, '窗口事件解绑成功');\n    } catch (err) {\n      hilog.error(DOMAIN, TAG, `解绑窗口事件失败：code=${err.code}, message=${err.message}`);\n    }\n  }\n}\n\n/**\n * 手动停止当前UIAbility实例（触发onDestroy）\n */\nexport function stopCurrentAbility(context: common.UIAbilityContext): void {\n  context.terminateSelf().then(()=&gt;{\n    hilog.info(DOMAIN, TAG, 'terminateSelf调用成功，将触发onDestroy');\n  }).catch((err: BusinessError) =&gt; {\n    hilog.error(DOMAIN, TAG, `停止UIAbility失败：code=${err.code}, message=${err.message}`);\n  });\n}\n</code></pre>\n<h2 id=\"四新增手动关闭应用indexets\">四、新增手动关闭应用Index.ets</h2>\n<pre><code class=\"language-javascript\">import { common } from '@kit.AbilityKit';\nimport { stopCurrentAbility } from '../entryability/EntryAbility';\n\n@Entry\n@Component\nstruct Index {\n  @State message: string = '第一个应用';\n  private context: common.UIAbilityContext = this.getUIContext().getHostContext() as common.UIAbilityContext;\n\n  aboutToAppear(): void {}\n\n  build() {\n    Column() {\n      Text(this.message)\n        .fontSize($r('app.float.page_text_font_size'))\n        .fontWeight(FontWeight.Bold);\n\n      Button(\"关闭应用程序\")\n        .onClick(()=&gt;{\n          stopCurrentAbility(this.context);\n        });\n    }\n    .height('100%')\n    .width('100%');\n  }\n}\n</code></pre>\n<h2 id=\"五实战实操验证生命周期执行顺序\">五、实战实操：验证生命周期执行顺序</h2>\n<h3 id=\"目标\">目标</h3>\n<p>通过手动操作应用，在Logcat中观察生命周期方法和WindowStage事件的触发顺序，重点验证不同场景下<code>onDestroy</code>的触发情况。</p>\n<blockquote>\n<p>环境说明：开发工具最低支持API 13，无法测试API 12及以下版本，所有测试基于API 13 手机模拟器。<br />\n不同类型的设备监听窗口活动状态输出日志会有差异。</p>\n</blockquote>\n<h3 id=\"场景1启动应用--返回桌面--再次打开应用\">场景1：启动应用 → 返回桌面 → 再次打开应用</h3>\n<p><strong>日志输出</strong>：</p>\n<pre><code>&lt;!-- 启动应用 --&gt;\n--- onCreate 触发（全局初始化）---\n--- onWindowStageCreate 触发（加载页面）---\nWindowStage事件订阅成功\n--- onForeground 触发（恢复前台资源）---\nwindowStage foreground\nwindowStage active\nIndex页面加载成功\n\n&lt;!-- 返回桌面 --&gt;\nwindowStage paused.\nwindowStage inactive\n--- onBackground 触发（释放后台资源）---\nwindowStage background\n\n\n&lt;!-- 再次打开应用 --&gt;\n--- onNewWant 触发（新参数：{\"deviceId\":\"\",\"bundleName\":\"com.example.FirstApplication\",\"abilityName\":\"EntryAbility\",\"moduleName\":\"entry\",\"uri\":\"\",\"type\":\"\",\"flags\":0,\"action\":\"action.system.home\",\"parameters\":{\"debugApp\":true,\"moduleName\":\"entry\",\"ohos.aafwk.param.displayId\":0},\"fds\":{},\"entities\":[\"entity.system.home\"]}）---\n--- onForeground 触发（恢复前台资源）---\nwindowStage foreground\nwindowStage active\n</code></pre>\n<h3 id=\"场景2点击关闭应用程序按钮调用terminateself\">场景2：点击“关闭应用程序”按钮（调用terminateSelf）</h3>\n<p><strong>日志输出</strong>：</p>\n<pre><code>&lt;!-- terminateSelf调用成功，将触发onDestroy --&gt;\nwindowStage inactive\n--- onBackground 触发（释放后台资源）---\nwindowStage background\n--- onWindowStageWillDestroy 触发（窗口预销毁）---\n窗口事件解绑成功\n--- onWindowStageDestroy 触发（销毁窗口）---\n--- onDestroy 触发（销毁应用）---\n</code></pre>\n<h3 id=\"场景3启动应用--切后台--一键清理无实况窗应用\">场景3：启动应用 → 切后台 → 一键清理无实况窗应用</h3>\n<p><strong>日志输出</strong>：</p>\n<pre><code>--- onCreate 触发（全局初始化）---\n--- onWindowStageCreate 触发（加载页面）---\nWindowStage事件订阅成功\n--- onForeground 触发（恢复前台资源）---\nwindowStage foreground\nwindowStage active\nIndex页面加载成功\nwindowStage inactive\n--- onBackground 触发（释放后台资源）---\nwindowStage background\n</code></pre>\n<p><strong>关键结论</strong>：一键清理时系统直接终止进程，未触发<code>onWindowStageWillDestroy</code>/<code>onWindowStageDestroy</code>/<code>onDestroy</code>。</p>\n<h2 id=\"六内容总结\">六、内容总结</h2>\n<ol>\n<li><strong>生命周期分工</strong>：<code>onCreate</code>做全局初始化（仅1次），<code>onWindowStageCreate</code>负责页面加载/事件订阅（单实例模式下仅首次启动触发1次），<code>onDestroy</code>释放全局资源（API 13+一键清理无实况窗应用不触发）；</li>\n<li><strong>WindowStage核心规则</strong>：事件订阅/解绑必须成对出现在<code>onWindowStageCreate</code>/<code>onWindowStageWillDestroy</code>，避免内存泄漏；</li>\n<li><strong>数据安全兜底</strong>：因<code>onDestroy</code>存在不触发场景，关键数据需在<code>onBackground</code>中保存；</li>\n<li><strong>前后台切换链路</strong>：切后台先触发PAUSED/INACTIVE→<code>onBackground</code>→HIDDEN；切前台先触发SHOWN→<code>onForeground</code>→ACTIVE/RESUMED。</li>\n</ol>\n<h3 id=\"核心问题解答为什么页面只能在onwindowstagecreate中加载\">核心问题解答：为什么页面只能在<code>onWindowStageCreate</code>中加载？</h3>\n<p>页面加载的本质是将 UI 组件挂载到系统的窗口容器（WindowStage）上，两个生命周期阶段的核心差异决定了加载时机：</p>\n<ul>\n<li><code>onCreate</code>阶段：UIAbility 实例刚创建，系统尚未分配 WindowStage（无页面承载载体），此时调用<code>loadContent</code>会因无窗口容器而失败，甚至导致应用崩溃；</li>\n<li><code>onWindowStageCreate</code>阶段：系统已创建 WindowStage 并作为参数传入，此时拥有了页面渲染所需的窗口容器，是加载页面的时机。</li>\n</ul>\n<h3 id=\"新手避坑指南\">新手避坑指南</h3>\n<ul>\n<li>禁止在<code>onCreate</code>中加载页面（无WindowStage实例）；</li>\n<li>不要依赖<code>onDestroy</code>保存关键数据，优先在<code>onBackground</code>兜底；</li>\n<li>生命周期回调是在应用主线程执行，为了确保应用性能，建议在生命周期回调中，仅执行必要的轻量级操作。对于耗时任务，推荐采用异步处理或交由子线程执行，避免阻塞主线程。</li>\n<li>如果需要感知UIAbility生命周期变化，开发者可以使用ApplicationContext注册接口监听UIAbility生命周期变化。</li>\n</ul>\n<pre><code class=\"language-javascript\">  // 定义生命周期ID\n  private lifecycleId: number = -1;\n  \n  // 定义生命周期回调对象\n  let abilityLifecycleCallback: AbilityLifecycleCallback = {\n    // 各回调方法\n  }\n    // 获取应用上下文\n  let applicationContext = this.context.getApplicationContext();\n   // 注册应用内生命周期回调\n  this.lifecycleId = applicationContext.on('abilityLifecycle', abilityLifecycleCallback);\n  \n</code></pre>\n<h2 id=\"七代码仓库\">七、代码仓库</h2>\n<ul>\n<li>工程名称：FirstApplication</li>\n<li>仓库地址：<a href=\"https://gitee.com/HarmonyOS-UI-Basics/harmony-os-ui-basics.git\" rel=\"noopener nofollow\" target=\"_blank\">https://gitee.com/HarmonyOS-UI-Basics/harmony-os-ui-basics.git</a></li>\n</ul>\n<h2 id=\"八下节预告\">八、下节预告</h2>\n<p>下一节我们将系统学习UIAbility的全量启动模式，重点掌握：</p>\n<ol>\n<li>multiton（多实例）、singleton（单实例）、specified（指定实例）三种启动模式的核心差异与实例创建规则；</li>\n<li>不同启动模式的配置方法（module.json5配置+代码层面参数传递）和适用业务场景；</li>\n<li>多实例/指定实例模式与生命周期的联动关系（分析<code>onCreate</code>/<code>onNewWant</code>/<code>onWindowStageCreate</code>的差异化触发逻辑）；</li>\n<li>实操验证不同启动模式的效果，解决实例冲突、参数传递异常等开发常见问题。</li>\n</ol>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 15:55</span>&nbsp;\n<a href=\"https://www.cnblogs.com/san-xiu\">鸿蒙-散修</a>&nbsp;\n阅读(<span id=\"post_view_count\">10</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "MWGA 双线编译技术方案：一份代码，双端生成",
      "link": "https://www.cnblogs.com/xdesigner/p/19579270",
      "published": "",
      "description": "<h2>\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xdesigner/p/19579270\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 14:44\">\n    <span>MWGA 双线编译技术方案：一份代码，双端生成</span>\n    \n\n</a>\n\n\t\t</h2>\n\t\t<div class=\"postText\">    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        企业软件普遍面临桌面端深度应用与网页端便捷访问的双重需求。传统做法需要两套技术栈、两套代码库与两套研发团队，导致成本高、周期长、双端逻辑不一致。MWGA 凭借双线编译能力，仅需一份 C# 核心代码，即可同时生成桌面 EXE 与网页 WebAssembly 应用，实现双端代码复用、逻辑统一、低成本维护，为跨端开发提供全新解决方案。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;<span>一、引言</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>企业软件普遍面临<span>桌面端深度应用<span>与<span>网页端便捷访问<span>的双重需求。传统做法需要两套技术栈、两套代码库与两套研发团队，导致成本高、周期长、双端逻辑不一致。<span>MWGA 凭借双线编译能力<span>，仅需一份 C# 核心代码，即可同时生成<span>桌面 EXE<span> 与<span>网页 WebAssembly 应用<span>，实现双端代码复用、逻辑统一、低成本维护，为跨端开发提供全新解决方案。</span></span></span></span></span></span></span></span></span></span></span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>二、核心技术原理</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>MWGA 的双线编译基于<span>模块化架构<span>与<span>跨平台编译引擎<span>，实现「<span>一份代码，双向生成<span>」。</span></span></span></span></span></span></span></p>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>代码分层：<span> 将代码划分为<span>核心业务逻辑层<span>与<span>端侧 UI 适配层<span>。核心层包含数据模型、算法、权限校验等通用功能，纯 C# 编写且不依赖端侧 API，双端<span>完全复用<span>；UI 层根据桌面与网页特性分别做轻量化适配。</span></span></span></span></span></span></span></span></p>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>双线编译引擎：<span> 对核心代码进行双向转换——编译为 EXE 时整合桌面 UI，生成原生应用，支持本地高性能与硬件对接；编译为 WebAssembly 时，生成浏览器可直接运行的应用，实现零安装、跨平台访问。<span>核心逻辑无需修改<span>，仅通过配置即可切换输出形态，从底层保障代码复用与双端稳定运行。</span></span></span></span></p>\n</li>\n</ul>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</h1>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>三、工程实践核心优势</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><strong><span>（一）代码零重复，研发效率翻倍</span></strong></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>传统跨端开发中，60%–80% 的业务逻辑需重复编写；<span>MWGA 实现核心逻辑 100% 复用<span>。以进销存系统为例，成本计算、库存核算等功能仅需编写一次，双端直接调用，研发工作量减少 <span>50% 以上<span>，新功能上线周期缩短 <span>40%–60%<span>。</span></span></span></span></span></span></span></div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><strong><span>（二）逻辑绝对一致，规避业务风险</span></strong></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>财务、医疗、工业等行业对业务一致性要求极高，双端逻辑差异易引发数据错误与合规风险。<span>MWGA 双端调用同一核心代码<span>，计算结果、权限规则、业务流程 <span>100% 统一<span>，彻底杜绝逻辑冲突，降低合规与业务风险。</span></span></span></span></span></div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><strong><span>（三）维护成本大降，迭代效率提升</span></strong></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>软件生命周期中，维护成本占比超 70%。MWGA 实现「<span>一次修改，双端同步<span>」：Bug 修复、功能升级仅需改动核心代码，重新编译后双端同步生效，测试流程无需重复执行，<span>维护成本可降低 60% 以上<span>。</span></span></span></span></span></div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><strong><span>（四）精简技术栈，降低团队成本</span></strong></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>无需同时配备 C# 与前端团队，<span>原有 C# 开发团队即可承接双端开发<span>，无需学习 React/Vue 等前端框架，团队规模可精简 <span>30%–50%<span>，新人上手成本也大幅降低。</span></span></span></span></span></div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><strong><span>（五）老项目低成本 Web 化，保护历史资产</span></strong></h2>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>针对沉淀多年的 WinForms/WPF 老项目，MWGA <span>无需重写核心逻辑<span>，仅需构建网页 UI 层即可快速生成 Web 版，<span>代码复用率可达 90%<span>，研发成本降低 <span>约 80%<span>，有效延长老项目生命周期。</span></span></span></span></span></span></span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>四、商业价值赋能</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（一）提升产品竞争力，扩大市场覆盖</span></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>基于 MWGA 可快速推出「<span>桌面 + 网页<span>」双端产品：桌面 EXE 满足本地高性能、硬件对接需求，网页版适配零安装、远程访问场景，覆盖更多客户群体，<span>显著提升中标率<span>。</span></span></span></span></span></div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（二）灵活交付，适配多元需求</span></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>可根据客户需求<span>按需编译<span>：大型企业选本地 EXE 版，中小企业选网页版，集团企业选混合部署模式，<span>无需额外开发<span>，一套代码满足私有化、云端、内外网等多种部署需求。</span></span></span></span></span></div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（三）创新商业模式，拓展盈利空间</span></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>支持<span>阶梯定价<span>（基础 Web 版引流、专业 EXE 版收费）、<span>订阅制转型<span>（网页版适配云端订阅），同时基于复用代码快速提供定制服务，提升客单价与持续收入。</span></span></span></span></span></div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（四）延长产品生命周期，提升资产回报</span></h2>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>传统桌面软件通过 MWGA 快速 Web 化，无需重构核心代码，即可适配数字化转型需求，<span>产品生命周期可延长 3–5 年<span>，历史技术投入持续创造价值。</span></span></span></p>\n</div>\n<div class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>五、灵活部署：全功能 EXE 与轻量化 Web</span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>MWGA 支持「<span>EXE 全功能 + Web 部分功能<span>」的差异化部署：</span></span></span></p>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><strong>桌面版</strong><span><strong>：</strong>打包所有模块，包含硬件对接、系统管理等全量功能。</span></span></p>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><strong>网页版</strong><span><strong>：</strong>仅保留查询、报表等基础功能，剔除敏感与高性能消耗模块。</span></span></p>\n</li>\n</ul>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>这样既实现<span>安全隔离<span>、降低网页端安全风险，又<span>优化性能<span>——网页端加载速度可提升 <span>30% 以上<span>，精准适配管理员与普通员工的不同使用场景。</span></span></span></span></span></span></span></p>\n</div>\n<div class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>六、典型行业应用</span></h1>\n<div class=\"TableToolbar-container\">\n<table align=\"left\" border=\"0\">\n<tbody>\n<tr>\n<td style=\"text-align: center;\">\n<p><strong><span>行业</span></strong></p>\n</td>\n<td style=\"text-align: center;\">\n<p><strong><span>桌面EXE</span></strong></p>\n</td>\n<td style=\"text-align: center;\">\n<p><strong><span>网页端</span></strong></p>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">\n<p><span>工业软件</span></p>\n</td>\n<td style=\"text-align: center;\">\n<p><span>设备对接、全功能现场</span></p>\n</td>\n<td style=\"text-align: center;\">\n<p><span>远程监控、看板</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">\n<p><span>企业管理</span></p>\n</td>\n<td style=\"text-align: center;\">\n<p><span>核心业务、重流程</span></p>\n</td>\n<td style=\"text-align: center;\">\n<p><span>外勤查询、审批</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">\n<p><span>医疗软件</span></p>\n</td>\n<td style=\"text-align: center;\">\n<p><span>诊疗全流程、设备</span></p>\n</td>\n<td style=\"text-align: center;\">\n<p><span>远程会诊、查阅</span></p>\n</td>\n</tr>\n<tr>\n<td style=\"text-align: center;\">\n<p><span>政务软件</span></p>\n</td>\n<td style=\"text-align: center;\">\n<p><span>内网审批、敏感数据</span></p>\n</td>\n<td style=\"text-align: center;\">\n<p><span>外网便民查询、服务</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n<span><span>均实现「<span><span>一套代码，双端适配<span><span>」。</span></span></span></span></span></span></div>\n<p>&nbsp;</p>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><span>七、与传统方案对比</span></span></h1>\n<div class=\"TableToolbar-container\">\n<div>\n<table align=\"left\" class=\"Table FocusPlugin--unfocused\" style=\"height: 694px; width: 872px;\">\n<tbody>\n<tr class=\"Table-row\"><th class=\"Table-data Table-header\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>对比维度</span></div>\n<p>&nbsp;</p>\n</th><th class=\"Table-data Table-header\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>传统双端开发</span></div>\n<p>&nbsp;</p>\n</th><th class=\"Table-data Table-header\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>纯 Web 开发</span></div>\n<p>&nbsp;</p>\n</th><th class=\"Table-data Table-header\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>Electron/MAUI</span></div>\n<p>&nbsp;</p>\n</th><th class=\"Table-data Table-header\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>MWGA</span></div>\n<p>&nbsp;</p>\n</th></tr>\n<tr class=\"Table-row\">\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>代码复用率</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>低</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>中</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>中</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>高</span></div>\n<p>&nbsp;</p>\n</td>\n</tr>\n<tr class=\"Table-row\">\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>双端一致性</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>差</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>无</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>中</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>优</span></div>\n<p>&nbsp;</p>\n</td>\n</tr>\n<tr class=\"Table-row\">\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>研发维护成本</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>高</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>中</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>中</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>低</span></div>\n<p>&nbsp;</p>\n</td>\n</tr>\n<tr class=\"Table-row\">\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>桌面/网页性能</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>优/中</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>差/优</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>中/差</span></div>\n<p>&nbsp;</p>\n</td>\n<td class=\"Table-data\" style=\"text-align: center;\">\n<p class=\"Table-dataInputContainer\">&nbsp;</p>\n<div class=\"Table-dataInput\"><span>优/优</span></div>\n<p>&nbsp;</p>\n</td>\n</tr>\n</tbody>\n</table>\n<span><span>MWGA 在代码复用、一致性、成本、性能上<span><span>均优于传统方案<span><span>，是跨端开发的优选方案。</span></span></span></span></span></span></div>\n</div>\n<div class=\"Editable-divider FocusPlugin--unfocused\">&nbsp;</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><span>八、总结</span></span></h1>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><span><span>M</span><span>WGA 双线编译以「<span><span>一份代码、双端生成<span><span>」为核心，解决了传统跨端开发的核心痛点，既实现了研发与维护的<span><span>降本增效<span><span>，又为<span><span>产品竞争力<span><span>与<span><span>商业模式创新<span><span>提供了支撑。其差异化部署模式进一步提升了场景适配性，在工业、医疗、政务等行业具备广阔前景。随着技术迭代，MWGA 将适配更多端侧形态，成为企业数字化转型的<span><span>核心技术支撑<span><span>。</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">&nbsp;</p>\n</div>\n<div class=\"Editable-unstyled\">\n<p class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><em><span><span>本文用于说明 MWGA 双线编译能力的价值与优势，供技术选型与商业决策参考。</span></span></em></p>\n</div>\n\n</div>\n<div class=\"clear\"></div>\n</div>\n\t\t<p class=\"postfoot\">\n\t\t\tposted on \n<span id=\"post-date\">2026-02-05 14:44</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xdesigner\">袁永福 电子病历，医疗信息化</a>&nbsp;\n阅读(<span id=\"post_view_count\">146</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n\t\t</p>"
    },
    {
      "title": "告别沉闷的直方图：绘制高颜值的威尔金森图与麦穗图",
      "link": "https://www.cnblogs.com/wang_yb/p/19579048",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/wang_yb/p/19579048\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 14:13\">\n    <span>告别沉闷的直方图：绘制高颜值的威尔金森图与麦穗图</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在数据可视化世界中，我们经常用直方图来描述数据的分布情况，但今天我想介绍两种特别而优雅的点状图变体：<strong>威尔金森点状图</strong>和<strong>麦穗图</strong>。</p>\n<p>它们像数据世界的\"点彩派\"画家，用简单的点创造出丰富的信息层次。</p>\n<p>与<strong>直方图</strong>相比，这种点绘法不仅能够更直观地展示数据分布的细节，还能更好地揭示数据之间的关系和模式，使得观察者能够从更广阔的视角理解数据集的特点。</p>\n<h1 id=\"1-威尔金森点状图\">1. 威尔金森点状图</h1>\n<p>想象一下，你有一袋彩色弹珠，需要按颜色分类展示。如果只是简单地把所有弹珠倒出来，它们会杂乱无章。</p>\n<p>但如果你为每种颜色准备一个小盒子，把相同颜色的弹珠整齐地堆叠在里面，这就是<strong>威尔金森点状图</strong>的<strong>基本思想</strong>。</p>\n<p><strong>威尔金森点状图</strong>将数据点堆叠在对应的数值区域，形成类似直方图的分布展示，但保留了每个数据点的个体性。</p>\n<p>它不是用条形的高度表示频率，而是用实际的数据点数量来可视化分布。</p>\n<p>下面基于<code>matplotlib</code>库封装了一个绘制威尔金森点状图的函数。</p>\n<pre><code class=\"language-python\">def wilkinson_dot_plot(\n    data, bins=10, dot_size=40, dot_spacing=0.8, show_stats=True, random_seed=42\n):\n    \"\"\"\n    创建威尔金森点状图\n\n    参数:\n    data: 输入数据（一维数组）\n    bins: 分组数量或分组边界\n    dot_size: 点的大小\n    dot_spacing: 点之间的垂直间距\n    random_seed: 随机种子\n    \"\"\"\n    np.random.seed(random_seed)\n\n    # 创建图形\n    fig, ax = plt.subplots(figsize=(10, 7))\n\n    # 计算直方图数据\n    hist, bin_edges = np.histogram(data, bins=bins)\n\n    # 为每个分组创建点\n    max_count = 0  # 记录最大堆叠高度\n    bin_centers = []  # 保存每个分组的中心位置\n\n    # 省略...\n\n    plt.tight_layout()\n    return fig, ax, (bin_edges, hist)\n</code></pre>\n<p><strong>威尔金森点状图</strong>的核心算法可以分解为几个步骤：</p>\n<ol>\n<li><strong>数据分箱</strong>：将连续数据分成若干个等宽的区间</li>\n<li><strong>点位置计算</strong>：在每个区间内，将数据点垂直堆叠</li>\n<li><strong>避免重叠</strong>：通过调整点的垂直位置防止重叠，同时保持可读性</li>\n</ol>\n<p>使用起来也简单：</p>\n<pre><code class=\"language-python\"># 生成示例数据\nnp.random.seed(42)\n\n# 数据集：正态分布\ndata_normal = np.random.normal(100, 15, 100)\ndot_size = 200\n\n# 创建威尔金森点状图\nfig1, ax1, stats1 = wilkinson_dot_plot(data_normal, bins=12, dot_size=dot_size)\nplt.show()\n</code></pre>\n\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/83005/202602/83005-20260205141247619-795851871.png\" /></p>\n<h1 id=\"2-麦穗图\">2. 麦穗图</h1>\n<p>如果把<strong>威尔金森点状图</strong>比作整齐堆叠的弹珠，那么<strong>麦穗图</strong>就像是田野中的麦穗——每个数据点都像一颗麦粒，精确地生长在自己的位置上，展示其实际数值。</p>\n<p><strong>麦穗图</strong>是<strong>威尔金森点状图</strong>的一种变体，它将点放置在其实际数值位置，而不是分组中心。</p>\n<p>这保留了数据的精确性，同时通过堆叠避免了重叠。</p>\n<p><strong>麦穗图</strong>的实现与<strong>威尔金森点状图</strong>类似，但有一个<strong>关键区别</strong>：点沿x轴放置在实际数据值位置，而不是分组中心。</p>\n<pre><code class=\"language-python\"># 封装麦穗图函数\ndef strip_plot(\n    data,\n    bin_edges=None,\n    bins=10,\n    dot_size=40,\n    dot_spacing=0.8,\n    jitter_amount=0.2,\n    random_seed=42,\n):\n    \"\"\"\n    创建麦穗图（在威尔金森点状图基础上显示实际值）\n\n    参数:\n    data: 输入数据（一维数组）\n    bin_edges: 可选，使用预定义的分组边界\n    bins: 如果未提供bin_edges，则使用此参数创建分组\n    dot_size: 点的大小\n    dot_spacing: 点之间的垂直间距\n    jitter_amount: 水平抖动程度（避免重叠）\n    random_seed: 随机种子\n    \"\"\"\n    np.random.seed(random_seed)\n\n    # 创建图形\n    fig, ax = plt.subplots(figsize=(10, 7))\n\n    # 如果没有提供分组边界，则计算\n    if bin_edges is None:\n        hist, bin_edges = np.histogram(data, bins=bins)\n    else:\n        hist, bin_edges = np.histogram(data, bins=bin_edges)\n\n    # 对数据进行排序\n    sorted_data = np.sort(data)\n\n    # 将数据分配到对应的分组\n    data_by_bin = []\n    for i in range(len(bin_edges) - 1):\n        lower, upper = bin_edges[i], bin_edges[i + 1]\n        bin_data = sorted_data[(sorted_data &gt;= lower) &amp; (sorted_data &lt; upper)]\n        data_by_bin.append(bin_data)\n\n    # 处理最后一个分组（包含最大值）\n    if len(data) &gt; 0:\n        last_bin_data = sorted_data[sorted_data &gt;= bin_edges[-2]]\n        if len(data_by_bin) &gt; 0:\n            data_by_bin[-1] = last_bin_data\n\n    # 绘制麦穗图\n    max_points_in_bin = 0\n    \n    # 省略 ...\n\n    plt.tight_layout()\n    return fig, ax, (bin_edges, data_by_bin)\n</code></pre>\n<p>使用起来也简单：</p>\n<pre><code class=\"language-python\"># 生成示例数据\nnp.random.seed(42)\n\n# 数据集：正态分布\ndata_normal = np.random.normal(100, 15, 100)\ndot_size = 200\n\n# 创建的麦穗图\nbin_edges = 12\nfig2, ax2, stats2 = strip_plot(\n    data_normal, bin_edges=bin_edges, dot_size=dot_size, jitter_amount=0.15\n)\nplt.show()\n</code></pre>\n\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/83005/202602/83005-20260205141247657-1144653987.png\" /></p>\n<h1 id=\"3-两种图的应用场景\">3. 两种图的应用场景</h1>\n<p>下面模拟一个学生考试成绩分布的分析场景，看看上面两种点状图的应用。</p>\n<pre><code class=\"language-python\"># 示例：学生考试成绩分布分析\nprint(\"示例：学生考试成绩分布分析\")\nprint(\"-\" * 40)\n\n# 创建模拟的考试成绩数据\nnp.random.seed(42)\nexam_scores = np.concatenate(\n    [\n        np.random.normal(65, 8, 45),  # 中等水平学生\n        np.random.normal(85, 6, 30),  # 优秀学生\n        np.random.normal(45, 10, 24),  # 需要帮助的学生\n    ]\n)\n\n# 过滤掉不合理分数\nexam_scores = np.clip(exam_scores, 0, 100)\n\nprint(f\"学生总数: {len(exam_scores)}\")\nprint(f\"分数范围: {exam_scores.min():.1f} - {exam_scores.max():.1f}\")\nprint(f\"平均分: {exam_scores.mean():.1f}\")\nprint(f\"及格率: {(exam_scores &gt;= 60).sum() / len(exam_scores) * 100:.1f}%\\n\")\n\n# 使用威尔金森点状图\nprint(\"创建威尔金森点状图...\")\nfig1, ax1, stats1 = wilkinson_dot_plot(\n    exam_scores,\n    bins=[0, 40, 60, 70, 80, 90, 100],\n)\nplt.show()\n\n# 使用麦穗图\nprint(\"创建麦穗图...\")\nfig2, ax2, stats2 = strip_plot(\n    exam_scores,\n    bins=[0, 40, 60, 70, 80, 90, 100],\n    jitter_amount=0.15,\n)\nplt.show()\n\n## 输出结果：\n'''\n示例：学生考试成绩分布分析\n----------------------------------------\n学生总数: 99\n分数范围: 25.1 - 94.4\n平均分: 65.3\n及格率: 63.6%\n'''\n</code></pre>\n\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/83005/202602/83005-20260205141247651-13405857.png\" /></p>\n\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/83005/202602/83005-20260205141247624-1158297536.png\" /></p>\n<p>在学生考试成绩分析这个场景中：</p>\n<ul>\n<li><strong>威尔金森点状图</strong>：将分数分组成区间（如60-70分），所有该区间的学生点都堆叠在区间中心，清晰地展示了分数段的整体分布形态，类似直方图但能看到个体点。</li>\n<li><strong>麦穗图</strong>：点在实际分数位置堆叠（如65分、68分等），既显示了每个学生的具体分数，又通过垂直堆叠避免了重叠，保留了数据的精确性。</li>\n</ul>\n<p>总的来说，<strong>威尔金森点状图</strong>看分布形态（<strong>区间视角</strong>），<strong>麦穗图</strong>看具体数值（<strong>精确视角</strong>）。</p>\n<h1 id=\"4-总结\">4. 总结</h1>\n<p><strong>威尔金森点状图</strong>和<strong>麦穗图</strong>为数据可视化工具箱增添了优雅而实用的工具。</p>\n<p>它们填补了传统<strong>直方图</strong>和<strong>散点图</strong>之间的空白，提供了同时展示数据分布和个体数据点的独特方式。</p>\n<p>在数据可视化中，选择合适的图表类型就像选择正确的工具来完成工作。</p>\n<p><strong>威尔金森点状图</strong>和<strong>麦穗图</strong>提供了独特的视角，让我们能够同时看到森林和树木——既理解整体分布，又关注个体数据点。</p>\n<p>完整代码分享：<a href=\"https://url11.ctfile.com/f/45455611-8640860073-8a6639?p=6872\" rel=\"noopener nofollow\" target=\"_blank\">威尔金森与麦穗图.ipynb</a> (访问密码: 6872)</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 14:13</span>&nbsp;\n<a href=\"https://www.cnblogs.com/wang_yb\">wang_yb</a>&nbsp;\n阅读(<span id=\"post_view_count\">39</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "推荐 .NET 8.0 开源项目伪微服务框架",
      "link": "https://www.cnblogs.com/1312mn/p/18376570",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/1312mn/p/18376570\" id=\"cb_post_title_url\" title=\"发布于 2026-02-05 13:32\">\n    <span>推荐 .NET 8.0 开源项目伪微服务框架</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<h2><span class=\"prefix\"><span class=\"content\">前言</span></span></h2>\n<p>嘿，小伙伴们！最近看到了一个 .NET 8.0 的小项目伪微服务框架，非常适合想要快速搭建小型应用项目或是想要学习.NET 8.0及新的技术，但又不知道从哪里学起的朋友。这个框架可以帮助我们简化开发流程，同时还能适应不断变化的需求。</p>\n<p>它虽然简化了很多复杂的微服务特性，但仍保留了关键的微服务理念，让我们可以专注于业务逻辑而不是底层细节，并且达到实操效果。</p>\n<p>想要快速上手 .NET 8.0，不妨试试这个框架。项目已经在 GitHub 上开源了，欢迎大家前来围观、提供建议或贡献代码。希望这个框架能够帮助更好的帮助我们工作中遇到的问题。</p>\n<h2><span class=\"prefix\"><span class=\"content\">项目介绍</span></span></h2>\n<p>为什么说是伪微服务框架，常见微服务框架可能还包括服务容错、服务间的通信、服务追踪和监控、服务注册和发现等等，而这里为了在使用中的更简单，将很多东西进行了简化或者省略了。</p>\n<p><strong>简化微服务概念</strong>：作者虽然称为“伪微服务”，但本框架依然具备一些关键的微服务特性，如模块化设计和服务解耦。它通过减少复杂的服务间通信、容错机制和服务发现等功能，使开发更加高效且易于管理。</p>\n<p><strong>技术栈</strong>：集成了一些比较实用的新技术，包括EF Core、Redis、RabbitMQ和MySQL，确保应用高效又能应对高并发场景。</p>\n<p><strong>易用性</strong>：该框架的目标是尽可能地简化开发过程，提供完整的接口和文档，能够迅速上手，并将更多精力集中在业务逻辑上。</p>\n<p><strong>持续优化</strong>：尽管目前仍处于早期阶段，但作者承诺将持续改进和完善该项目。这包括但不限于引入更多高级特性、增强现有功能以及提升整体性能。</p>\n<h2><span class=\"prefix\"><span class=\"content\">项目核心</span></span></h2>\n<p><strong>对象映射</strong>：使用 <code>AutoMapper</code> 自动处理对象之间的映射，让你无需手动编写繁琐的转换代码。</p>\n<p><strong>查询封装</strong>：通过 <code>Ardalis.Specification</code> 和 <code>LinqKit.Core</code> 封装 EF Core 查询，让数据获取变得更简单直观。</p>\n<p><strong>数据库交互</strong>：在 EF Core 中重写了 <code>SaveChangesInterceptor</code>，实现了软删除功能，并自动管理创建时间和更新时间字段。</p>\n<p><strong>整体架构</strong>：采用了整洁的架构设计，便于理解和维护。</p>\n<p><strong>业务功能</strong>：已经预置了一些基本的业务功能，开箱即用。</p>\n<p><strong>依赖注入</strong>：内置了依赖注入容器，方便管理组件和服务。</p>\n<p><strong>认证与授权</strong>：使用双 token 实现了安全登录和无感知的前端 token 刷新。</p>\n<p><strong>分布式 ID 生成</strong>：集成了 <code>Snowflake</code> 分布式 ID 生成器，确保全局唯一标识符。</p>\n<p><strong>缓存与锁</strong>：通过 Redis 实现了分布式缓存和分布式锁，提高了系统的可用性和并发性能。</p>\n<p><strong>消息队列</strong>：利用 <code>RabbitMQ</code> 封装了异步任务处理机制，如文件上传和下载。</p>\n<p><strong>定时任务</strong>：结合 <code>Cronos</code> 和 <code>BackgroundService</code> 实现了秒级定时任务。</p>\n<p><strong>数据初始化</strong>：使用 <code>BackgroundService</code> 进行数据初始化，比如字典数据的加载。</p>\n<p><strong>日志记录</strong>：采用 <code>Serilog</code> 记录异常日志，并通过 Docker 部署实现日志的可视化监控。</p>\n<p><strong>操作日志</strong>：通过自定义过滤器和反射记录操作日志，便于追踪用户行为。</p>\n<p><strong>权限验证</strong>：实现了权限验证过滤器，确保用户只能访问被授权的资源。</p>\n<p><strong>统一响应格式</strong>：使用 <code>IAsyncResultFilter</code> 统一了返回给前端的数据格式。</p>\n<p><strong>Excel 操作</strong>：集成 <code>EPPlus</code> 库，支持 Excel 导入和导出。</p>\n<p><strong>一键部署</strong>：支持使用 <code>goploy</code> 快速部署前后端应用。</p>\n<p><strong>API 文档</strong>：通过 <code>Swagger</code> 自动生成 RESTful API 文档，方便前端和后端开发人员协作。</p>\n<p><strong>配置加载</strong>：自动加载 <code>appsettings.json</code> 文件中的配置信息。</p>\n<h2><span class=\"prefix\"><span class=\"content\">项目框架</span></span></h2>\n<p>通过Github下载项目源码，我们可以查看项目框架，具体如下图所示：</p>\n<p><img height=\"700\" src=\"https://img2024.cnblogs.com/blog/576536/202408/576536-20240823165609019-1106450563.png\" style=\"vertical-align: middle; display: block; margin-left: auto; margin-right: auto;\" width=\"619\" /></p>\n<img alt=\"\" />\n<p>1、Libraries</p>\n<p>包含各种外部类库，对其深加工使用在项目中</p>\n<p>2、Services/Basic</p>\n<p>微服务基础支撑子系统</p>\n<p>3、Services/NCDP</p>\n<p>微服务业务子系统</p>\n<p>4、Services/SystemService</p>\n<p>微服务系统服务（包括数据库的更新、定时任务、数据初始化、Swagger承载、RabbitMQ队列事件处理器等）</p>\n<p>5、sun.Core</p>\n<p>sun.Core作为了中转，其他外部或者自己封装的类库，在引用的时候都是在sun.Core中进行的引用。</p>\n<p>算是间接引用，来简化项目中的依赖关系。</p>\n<p>同时在sun.Core也封装了一些核心组件和服务。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/576536/202408/576536-20240823165855661-1876970856.png\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></p>\n<p>6、sun.Infrastructure</p>\n<p>主要封装一些通用的方法，以及基础设施组件，供外部使用。</p>\n<p><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/576536/202408/576536-20240823165907878-476029284.png\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></p>\n<h2><span class=\"prefix\"><span class=\"content\">项目说明</span></span></h2>\n<p>关于项目详细使用情况可以查看作者整理的文章链接 https://www.cnblogs.com/aehyok/p/18058032 ，希望能够帮助大家更好的理解项目和学习知识点，提升自己的技术能力，下面只展示了部分内容。</p>\n<h4><span class=\"prefix\"><span class=\"content\">1、业务功能</span></span></h4>\n<p>目前基本实现的功能包括：用户管理、角色管理、区域管理、查看日志（登录日志和操作日志）、菜单管理、权限控制、系统管理等功能。</p>\n<p><img height=\"380\" src=\"https://img2024.cnblogs.com/blog/576536/202408/576536-20240823165950242-344963990.png\" style=\"vertical-align: middle; display: block; margin-left: auto; margin-right: auto;\" width=\"690\" /></p>\n<h4><span class=\"prefix\"><span class=\"content\">2、生成文档工具</span></span></h4>\n<p>Swagger 生成REST APIs文档工具包含可以承载多个微服务项目，通过右上角进行切换，便可以查看当前微服务项目的接口文档，并可以进行测试</p>\n<p>测试接口直接可在swagger ui上进行</p>\n<p>统一添加接口中的Header参数</p>\n<p>通过对Swagger UI进行部分的自定义，使的更好的适配自己的项目，比如添加登录，这样接口便直接可以在swagger UI上面进行。</p>\n<p><img src=\"https://img2024.cnblogs.com/blog/576536/202408/576536-20240823170029758-1450910516.png\" style=\"vertical-align: middle; display: block; margin-left: auto; margin-right: auto;\" width=\"619\" /></p>\n<h4><span class=\"prefix\"><span class=\"content\">3、分布式雪花Id生成器</span></span></h4>\n<p>Snowflake 所使用的开源类库：https://github.com/stulzq/snowflake-net</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;summary&gt;</span>\n<span style=\"color: rgba(128, 128, 128, 1);\">///</span><span style=\"color: rgba(0, 128, 0, 1);\"> 分布式雪花Id生成器\n</span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;/summary&gt;</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> SnowFlake\n{\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span><span style=\"color: rgba(0, 128, 0, 1);\"> 通过静态类只实例化一次IdWorker 否则生成的Id会有重复\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;/summary&gt;</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">private</span> <span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">readonly</span> Lazy&lt;IdWorker&gt; _instance = <span style=\"color: rgba(0, 0, 255, 1);\">new</span>(() =&gt;<span style=\"color: rgba(0, 0, 0, 1);\">\n    {\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">var</span> commonOptions = App.Options&lt;CommonOptions&gt;<span style=\"color: rgba(0, 0, 0, 1);\">();\n\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> IdWorker(commonOptions.WorkerId, commonOptions.DatacenterId);\n    });\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">static</span> IdWorker Instance =<span style=\"color: rgba(0, 0, 0, 1);\"> _instance.Value;\n}</span></pre>\n</div>\n<p>其中 WorkerId和DatacenterId保持不同的话，例如两个微服务WorkerId一个为1一个为2,那么在同一毫秒数生成的Id肯定是不同的。</p>\n<p>同一个IdWorker在一个毫秒中可以生成4096个序列号 足够大型系统使用了，不怕重复的问题。</p>\n<h4><span class=\"prefix\"><span class=\"content\">4、分布式缓存和分布式锁</span></span></h4>\n<p>Redis 统一封装实现分布式缓存和分布式锁，所使用的开源类库：https://github.com/2881099/csredis</p>\n<p>目前主要封装了几个常用的接口方法</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">interface</span><span style=\"color: rgba(0, 0, 0, 1);\"> IRedisService\n{\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span><span style=\"color: rgba(0, 128, 0, 1);\"> 查看服务是否运行\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;/summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;returns&gt;&lt;/returns&gt;</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">bool</span><span style=\"color: rgba(0, 0, 0, 1);\"> PingAsync();\n\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span><span style=\"color: rgba(0, 128, 0, 1);\"> 根据key获取缓存\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;/summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"key\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;returns&gt;&lt;/returns&gt;</span>\n    Task&lt;T&gt; GetAsync&lt;T&gt;(<span style=\"color: rgba(0, 0, 255, 1);\">string</span><span style=\"color: rgba(0, 0, 0, 1);\"> key);\n\n\n\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span><span style=\"color: rgba(0, 128, 0, 1);\"> 设置指定key的缓存值(不过期)\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;/summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"key\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"value\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;returns&gt;&lt;/returns&gt;</span>\n    Task&lt;<span style=\"color: rgba(0, 0, 255, 1);\">bool</span>&gt; SetAsync(<span style=\"color: rgba(0, 0, 255, 1);\">string</span> key, <span style=\"color: rgba(0, 0, 255, 1);\">object</span><span style=\"color: rgba(0, 0, 0, 1);\"> value);\n\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span><span style=\"color: rgba(0, 128, 0, 1);\"> 设置指定key的缓存值(可设置过期时间和Nx、Xx)\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;/summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"key\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"value\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"expire\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"exists\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;returns&gt;&lt;/returns&gt;</span>\n    Task&lt;<span style=\"color: rgba(0, 0, 255, 1);\">bool</span>&gt; SetAsync(<span style=\"color: rgba(0, 0, 255, 1);\">string</span> key, <span style=\"color: rgba(0, 0, 255, 1);\">object</span> value, TimeSpan expire, RedisExistence? exists = <span style=\"color: rgba(0, 0, 255, 1);\">null</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span><span style=\"color: rgba(0, 128, 0, 1);\"> 设置指定key的缓存值(可设置过期秒数和Nx、Xx)\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;/summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"key\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"value\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"expireSeconds\"&gt;</span><span style=\"color: rgba(0, 128, 0, 1);\">过期时间单位为秒</span><span style=\"color: rgba(128, 128, 128, 1);\">&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"exists\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;returns&gt;&lt;/returns&gt;</span>\n    Task&lt;<span style=\"color: rgba(0, 0, 255, 1);\">bool</span>&gt; SetAsync(<span style=\"color: rgba(0, 0, 255, 1);\">string</span> key, <span style=\"color: rgba(0, 0, 255, 1);\">object</span> value, <span style=\"color: rgba(0, 0, 255, 1);\">int</span> expireSeconds = -<span style=\"color: rgba(128, 0, 128, 1);\">1</span>, RedisExistence? exists = <span style=\"color: rgba(0, 0, 255, 1);\">null</span><span style=\"color: rgba(0, 0, 0, 1);\">);\n\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span><span style=\"color: rgba(0, 128, 0, 1);\"> 删除Key\n    </span><span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;/summary&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;param name=\"key\"&gt;&lt;/param&gt;</span>\n    <span style=\"color: rgba(128, 128, 128, 1);\">///</span> <span style=\"color: rgba(128, 128, 128, 1);\">&lt;returns&gt;&lt;/returns&gt;</span>\n    Task&lt;<span style=\"color: rgba(0, 0, 255, 1);\">long</span>&gt; DeleteAsync(<span style=\"color: rgba(0, 0, 255, 1);\">string</span><span style=\"color: rgba(0, 0, 0, 1);\"> key);\n\n\n    Task</span>&lt;Dictionary&lt;<span style=\"color: rgba(0, 0, 255, 1);\">string</span>,<span style=\"color: rgba(0, 0, 255, 1);\">string</span>&gt;&gt;<span style=\"color: rgba(0, 0, 0, 1);\"> ScanAsync();\n}</span></pre>\n</div>\n<p>主要是为了保持与redis cli中的方法一致，选了这个类库，当然你也可以选择其他的类库 还是蛮多的。</p>\n<p>同时还封装了一个接口用于前端监测所有的key和value。</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">async</span> Task&lt;<span style=\"color: rgba(0, 0, 255, 1);\">dynamic</span>&gt;<span style=\"color: rgba(0, 0, 0, 1);\"> ScanAsync(PagedQueryModelBase model)\n{\n    List</span>&lt;<span style=\"color: rgba(0, 0, 255, 1);\">string</span>&gt; list = <span style=\"color: rgba(0, 0, 255, 1);\">new</span> List&lt;<span style=\"color: rgba(0, 0, 255, 1);\">string</span>&gt;<span style=\"color: rgba(0, 0, 0, 1);\">();\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\">根沐model.Keyword进行模糊匹配</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">var</span> scanResult = <span style=\"color: rgba(0, 0, 255, 1);\">await</span> RedisHelper.ScanAsync(model.Page, $<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">*{model.Keyword}*</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">, model.Limit);\n    list.AddRange(scanResult.Items);\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">var</span> values = <span style=\"color: rgba(0, 0, 255, 1);\">await</span><span style=\"color: rgba(0, 0, 0, 1);\"> RedisHelper.MGetAsync(list.ToArray());\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">var</span> resultDictionary = list.Zip(values, (key, value) =&gt; <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> { key, value })\n                                    .ToDictionary(item </span>=&gt; item.key, item =&gt;<span style=\"color: rgba(0, 0, 0, 1);\"> item.value);\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">dynamic</span> result = <span style=\"color: rgba(0, 0, 255, 1);\">new</span><span style=\"color: rgba(0, 0, 0, 1);\"> ExpandoObject();\n    result.Items </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> resultDictionary;\n    result.Cursor </span>= scanResult.Cursor;  <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> 下一次要通过这个Cursor获取下一页的keys</span>\n   <span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> result;\n}</span></pre>\n</div>\n<h4><span class=\"prefix\"><span class=\"content\">5、自动化部署</span></span></h4>\n<p>通过google/zx使用nodejs开发了一个脚本，用于自动化部署</p>\n<p>可以参考Github的地址：https://github.com/aehyok/zx-deploy</p>\n<p>主要是用于开发环境，通过</p>\n<div class=\"cnblogs_code\">\n<pre>pnpm sun-<span style=\"color: rgba(0, 0, 0, 1);\">baisc\npnpm sun</span>-<span style=\"color: rgba(0, 0, 0, 1);\">ncdp\npnpm sun</span>-systemserivce</pre>\n</div>\n<p>当然你还可以通过组合命令进行部署，例如想一起部署三个服务</p>\n<div class=\"cnblogs_code\">\n<pre>pnpm sun-all</pre>\n</div>\n<pre class=\"custom\"><code class=\"hljs\">其实就是&nbsp;&nbsp;<br /></code></pre>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">pnpm sun-ncdp &amp;&amp; </span>\npnpm sun-basic &amp;&amp;<span style=\"color: rgba(0, 0, 0, 1);\"> \npnpm sun</span>-systemservice<span style=\"color: rgba(128, 0, 0, 1);\">\"</span></pre>\n</div>\n<p>这里我用的<code>&amp;&amp;</code>相当于上面三个命令串行执行，先执行sun-ncdp,再执行sun-basic,最后执行sun-systemservice。</p>\n<p>如果你的电脑或者服务器性能足够好，可以使用<code>&amp;</code>符号，这样就是并行执行，三个服务同时启动，这样可以节省时间。</p>\n<p>以上仅展示了项目的部分内容，还有许多其他的技术应用和功能需要大家去发现。</p>\n<p>然后我们可以下载源码参考文档并进行实际操作，以便全面了解整个项目的技术应用和特性。</p>\n<h2><span class=\"prefix\"><span class=\"content\">项目地址</span></span></h2>\n<p>Github：https://github.com/aehyok/.NET8.0</p>\n<h2 class=\"md-end-block md-heading\"><span class=\"md-plain\">最后</span></h2>\n<p><span class=\"md-plain\"><span class=\"md-plain md-expand\">如果你觉得这篇文章对你有帮助，不妨点个赞支持一下！你的支持是我继续分享知识的动力。如果有任何疑问或需要进一步的帮助，欢迎随时留言。也可以加入微信公众号&nbsp;<span class=\"md-pair-s \"><strong>[DotNet技术匠]</strong><span class=\"md-plain md-expand\">&nbsp;社区，与其他热爱技术的同行一起交流心得，共同成长！</span></span></span></span></p>\n<p><span class=\"md-plain\"><img alt=\"\" src=\"https://img2024.cnblogs.com/blog/576536/202408/576536-20240813102419584-1596250541.png\" style=\"display: block; margin-left: auto; margin-right: auto;\" /></span></p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-05 13:32</span>&nbsp;\n<a href=\"https://www.cnblogs.com/1312mn\">小码编匠</a>&nbsp;\n阅读(<span id=\"post_view_count\">342</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}