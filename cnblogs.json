{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "这两个网站，一个可以当时间胶囊，一个充满了赛博菩萨。",
      "link": "https://www.cnblogs.com/thisiswhy/p/19474147",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/thisiswhy/p/19474147\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 20:27\">\n    <span>这两个网站，一个可以当时间胶囊，一个充满了赛博菩萨。</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<section id=\"nice\"><p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">你好呀，我是歪歪。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">前两天不是发了这篇<a href=\"https://mp.weixin.qq.com/s/YIRyn5iyT3yFDgHTjzvA8Q\" rel=\"noopener nofollow\" style=\"color: rgba(239, 112, 96, 1); font-weight: bold; border-radius: 0; margin: 0; padding: 0; text-decoration: none;\">《可怕，看到一个如此冷血的算法。》</a>嘛。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">文章中有这样的一个链接：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110151208.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">我当时放这个链接的目的是为了方便大家直达吃瓜现场。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">但是，由于这个帖子最终被证实是假的，所以被官方给“夹”了：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110151912.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">幸好，原文本来就不长，所以我在我的文章中把原文全部给截下来了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">也算是以另外一种形式保留了吃瓜现场。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">如果这个“爆料”的帖子再长一点，按照我的习惯，我可能就不会把整个帖子搬运过来了，只会留取我认为关键的部分。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">但是这种“我认为关键的部分”是非常主观的，有的人就是想看原贴长什么样，但是原贴又被删除了，怎么办？</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">我教你一招，老好用了。</p>\n<h2><span class=\"prefix\" style=\"display: none;\"></span><span class=\"content\">时间胶囊</span><span class=\"suffix\" style=\"display: none;\"></span><span> </span></h2>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">在万能的互联网上，有这样一个仿佛是时间胶囊一般存在的神奇的网站：</p>\n<blockquote class=\"custom-blockquote multiquote-1\"><span style=\"display: none; color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.5em; letter-spacing: 0; text-align: left; font-weight: normal;\"></span>\n<p style=\"text-indent: 0; padding: 8px 0; color: rgba(0, 0, 0, 1); font-size: 15px; line-height: 1.8em; letter-spacing: 0; text-align: left; font-weight: normal; margin: 0;\">https://archive.org/</p>\n</blockquote>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110152655.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这个网站是叫做\"互联网档案馆\"（Internet Archive），于 1996 年成立的非营利组织维护的网站。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">自 1996 年以来，互联网档案库与世界各地的图书馆和合作伙伴合作，建立了一个人类在线历史的共享数字图书馆。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这个网站有一个非常宏大的愿景：</p>\n<blockquote class=\"custom-blockquote multiquote-1\"><span style=\"display: none; color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.5em; letter-spacing: 0; text-align: left; font-weight: normal;\"></span>\n<p style=\"text-indent: 0; padding: 8px 0; color: rgba(0, 0, 0, 1); font-size: 15px; line-height: 1.8em; letter-spacing: 0; text-align: left; font-weight: normal; margin: 0;\">捕捉大小不一的网站，从突发新闻到被遗忘的个人页面，使它们能够为子孙后代保持可访问性。</p>\n</blockquote>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">所以里面收藏了的内容有免费书籍、电影、软件、音乐、网站等。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">截至目前，该网站收集了这么多的数据：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110153451.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">其中网站的数量是最多的，有 1T，超过 1T 的时候，官方还发文庆祝了一下：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110154601.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这个 1T 中的 T 指的是什么呢？</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">Trillion。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">一个非常小众的词汇啊，歪师傅也不认识，所以我去查了一下：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110153943.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这个图片上一眼望去全是 0。</p>\n<blockquote class=\"custom-blockquote multiquote-1\"><span style=\"display: none; color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.5em; letter-spacing: 0; text-align: left; font-weight: normal;\"></span>\n<p style=\"text-indent: 0; padding: 8px 0; color: rgba(0, 0, 0, 1); font-size: 15px; line-height: 1.8em; letter-spacing: 0; text-align: left; font-weight: normal; margin: 0;\">1 Trillion 就是 1,000,000,000,000</p>\n</blockquote>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">反正是数不过来了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">感觉成都都没有这么多 0。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这个网站怎么用呢？</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">很简单。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">拿前面 reddit 中被“夹”了的帖子举例。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">我不是给了吃瓜现场的链接嘛。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">你把链接往“时光机”的这个地方一粘：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110155525.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">你就会看到这个有一个时间轴的页面：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110155608.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">把鼠标浮到有颜色的日期上，就能看到各个时间点的页面快照了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">颜色越深代表那一天的快照越多：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110160303.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">比如，我们看一下这个网站收集到的第一个快照：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110160532.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">点进去，就是我们要找的吃瓜现场。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">发帖后的两小时就被收集到了，速度还是挺快的。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">从数据上看，这个时候已经有 3.7k 个点赞和 255 个评论，已经有要起飞的预兆了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">换个时间的快照，还可以看到点赞和评论的数据变化，比如发帖一天后：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110161244.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">点赞量已经是 71k，评论数来到了 3.8K，直接就是一个起飞的大动作。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这里只是用这个帖子举个例子。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">再举一个例子。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">也是我的真实使用场景。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">有一次我在研究平滑加权轮询负载均衡策略算法为什么是平滑的。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">和各类 AI 讨论了半天，它们也给出了各种参考文献。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">我在其中一个参考文献中看到了这样一个链接：</p>\n<blockquote class=\"custom-blockquote multiquote-1\"><span style=\"display: none; color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.5em; letter-spacing: 0; text-align: left; font-weight: normal;\"></span>\n<p style=\"text-indent: 0; padding: 8px 0; color: rgba(0, 0, 0, 1); font-size: 15px; line-height: 1.8em; letter-spacing: 0; text-align: left; font-weight: normal; margin: 0;\">https://tenfy.cn/2018/11/12/smooth-weighted-round-robin/</p>\n</blockquote>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">我知道这个链接的内容就是我要找的内容，但是这个链接跳转过去已经是 404 了：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110162307.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">于是，时间胶囊就派上用场了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">我直接把这个链接扔它：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110162454.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">找到了这个网页在 2019 年 12 月 10 日的快照：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110162549.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">通过这种方式就找到了原本已经被 404 的网页内容。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">在看一些时间比较久远的文章的时候，参考链接打不开的情况，还是比较常见的。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">所以这个方式是我最常用的一个场景。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">此外，还有另外一个场景，就是偶尔去怀旧一下。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">比如，中文互联网的一滴眼泪：天涯论坛。</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110163342.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这是 20 年前，2006 年 1 月的天涯论坛首页，一股浓烈的早期互联网风格：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110164241.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">在图片的右下角你还能看到“2006 天涯春晚”的字样。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">另外，你不要觉得这只是一个静态页面。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">里面的部分链接还是可以正常跳转的。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">比如，这个链接：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110164612.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">点进去，你可以看到最最古早的一种直播形式：文字直播。</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110164652.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">2006 年 1 月 2 日，《武林外传》开播。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">天涯这个文字直播的时间是 2006 年 1 月 19 日，《武林外传》当时正在全国热播。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">天涯网友在这个页面下提出自己关于《武林外传》的问题，作为天涯的知名写手，宁财神本人会选择部分问题进行回复。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">我截取了几个我觉得有意思的回复：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110165547.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这种行为这算不算是官方剧透了？</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110164929.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">当年祝无双这个角色是真的不让人讨喜啊。幸好当时的网络还不发达，不然我觉得真有可能“网爆祝无双”。</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110165709.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">DVD，一个多么具有年代感的词。</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110170208.png\" /></figure>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110170248.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">写文章的时候，我本来是想截几张图就走的，最多五分钟搞定。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">结果我竟然一页页的翻完了这个帖子，看完之后才发现在这个帖子里面待了半个多小时。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">时间过的还是很快的。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">站在 2026 年，看 2006 的帖子，中间有 20 年的光阴。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">但是就像是 2006 年佟掌柜对要给她干二十年工才能还清债务的小郭说的那样：不要怕，二十年快得很，弹指一挥间。</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/d23d93c0bf069c1024223eee62e71e4c.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">前几天小郭在微博上还回应了正式赎身这个梗。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">去了六里桥、去了同福夹道、去了左家庄站、还去了祥蚨瑞，最后在人来人往的北京街头，一个猝不及防的回眸：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/8457920991b850fa95be10dc56098679.jpg\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这是我的童年回头看了我一眼。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">十几岁的不了解佟掌柜的这句话，三十出头了，一下就理解了：20 年，真的很快呀。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">看到 2006 年的天涯的时候，我依稀想起了一些当年的往事。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">那个时候我才 12 岁，看电视剧是真的在电视机上看，我还记得家里的电视机都是这样的“大屁股”电视机：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260111113117.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">还记得《武林外传》每集开始，唱主题曲的时候，电视上面会显示一个电脑的桌面：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260111122741.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">所以每次开头的时候，我就会叫表妹过来，对她说：你看，我等下把电视变成电脑。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">那个时候表妹才 7 岁，我这个 12 岁的哥哥当然是把她唬的一愣一愣的。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">那个时候电脑也还是一个稀奇的物品，虽然是乡下的学校，但是也还是有一个微机室，去微机室上课必须要带鞋套的那种。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">所以 2006 年的天涯，我肯定是没有看过的，但是在 2026 年看到 2006 的天涯，我还是想起了很多童年往事。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">对了，前几天才给表妹过完 27 岁的生日：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/a1ecdf2dc6a3e232951d8820f6e88cb6.jpg\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">看着这张照片，再想起 7 岁时那个相信哥哥可以把电视变成电脑给她看《武林外传》的妹妹。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">“二十年快得很，弹指一挥间”。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">你说这不叫时间胶囊，叫什么？</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">再看一下 10 年前，2016 年 1 月 1 日的天涯，彼时的天涯可以说是如日中天，非常多的网友天天泡在论坛里面，谈古论今，激扬文字。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这是那天的天涯首页截图：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110212425.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">热帖榜第一的是一个关于纯电动汽车的帖子，我进去看了一下：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110212718.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这个帖子的点击量是 10w，有 816 个回复。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">可见这确实是当时的一个非常热门的话题。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">按照作者的观点，纯电汽车代替燃油汽车，还很长的路要走。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">站在 10 年后的今天，其实我们已经知道答案了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">但是，当我看到这个回复的时候，我还是佩服天涯网友的眼光：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110213134.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">除了天涯，还可以考古很多其他的网站。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">比如，B 站：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110221101.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">从 2011 年开始有了网页快照，我随便点开一看，满满的历史感：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110221037.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">而这是 2016 年，10 年前的 B 站首页：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110221728.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">当时还有一个专门的鬼畜区：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110221819.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">而这里的一些视频甚至还是可以播放的。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">比如这个“启蒙作品”：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110222457.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">现在在 B 站有 160w 的播放：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110222627.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">在这个视频的评论区，你能找到大量来“考古”的人：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110222829.png\" /></figure>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110222903.png\" /></figure>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110222940.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">二十年都弹指一挥间了，别说区区十年了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">从 B 站怀旧完成后，随便，我也去磨房、马蜂窝、穷游网看了一圈，随便选了 2012 年到 2016 年间的一些页面，感谢它们陪我度过了一整个美好的大学生活。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">是我当时认识、感知、体验这个的广阔世界的一个重要窗口。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">感谢磨房 4 年的陪伴：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110223656.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">感谢马蜂窝 4 年的陪伴：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110224704.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">感谢穷游网 4 年的陪伴：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110224951.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">如果你也有想要寻找的记忆，可以尝试在这个网站上去找一找。</p>\n<h2><span class=\"prefix\" style=\"display: none;\"></span><span class=\"content\">存档</span><span class=\"suffix\" style=\"display: none;\"></span><span> </span></h2>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">既然已经聊到“archive”了，那就顺便再分享一个“archive.today”。</p>\n<blockquote class=\"custom-blockquote multiquote-1\"><span style=\"display: none; color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.5em; letter-spacing: 0; text-align: left; font-weight: normal;\"></span>\n<p style=\"text-indent: 0; padding: 8px 0; color: rgba(0, 0, 0, 1); font-size: 15px; line-height: 1.8em; letter-spacing: 0; text-align: left; font-weight: normal; margin: 0;\">https://archive.ph/</p>\n</blockquote>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110230731.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这个网站和前面的“互联网档案馆”最大的一个差异是“互联网档案馆”是它主动去做“网页快照”，什么时候做，什么页面做，并不一定。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">而“archive.today”是一个你可以去主动存档的网站。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">比如，还是说回 reddit 上的那个帖子。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">帖子下面有这样的一个回复：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110230932.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">这个回复中的超链接就是回复者找到的关于这个“爆料”是 AI 生成的证据。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">点过去是这样的：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110231156.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">他提供的是一个网页存档。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">为什么他要这么做呢？</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">你想想，如果他提供一个原始链接，但是这个原始链接突然有一天找不到了，岂不是很尴尬？</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">但是先在“archive.today”上存档一下，然后把这个存档后的链接贴出来，就稳当多了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">以后你要保存证据的话，你就可以使用这个网站。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">另外，这个网站还有一个骚操作。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">反而是骚操作让这个网站的打开率更高一点。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">国外的一些网站可能有些文章是要付费才能看到的。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">比如纽约时报：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110234836.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">但是，如果你一不小心把付费文章的链接贴在这个网站上去搜索。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">有一些“好事之人”已经帮你把文章在这个网站上做了快照了，这些人可以称之为“赛博菩萨”，因为这些“菩萨”，你就可能看到免费的原文了：</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260110234948.png\" /></figure>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">在这里叠个甲啊，偶尔看到一两篇的话可以这样操作一下，就当时是试看了。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">如果经常要看的话，还是充点钱吧。</p>\n<p style=\"color: rgba(0, 0, 0, 1); font-size: 16px; line-height: 1.8em; letter-spacing: 0; text-align: left; text-indent: 0; margin: 0; padding: 8px 0;\">对了，多说一句，上面提到的神奇的网站既然叫做时光胶囊，还有一些赛博菩萨，这些魔法世界中才有的东西，那肯定需要你会对应的魔法咒语才能访问到。如果你不会魔法，强行访问，那你肯定要撞到墙上。</p>\n<figure style=\"margin: 10px 0; padding: 0; display: flex;\"><img alt=\"\" src=\"https://why-image-1300252878.cos.ap-chengdu.myqcloud.com/img/20220716/20260111150634.png\" /></figure>\n</section>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-12 20:27</span>&nbsp;\n<a href=\"https://www.cnblogs.com/thisiswhy\">why技术</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "为什么有的人说“越老思维越固化”？怎么才能具备成长型思维？",
      "link": "https://www.cnblogs.com/jiujuan/p/19473443",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jiujuan/p/19473443\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 18:25\">\n    <span>为什么有的人说“越老思维越固化”？怎么才能具备成长型思维？</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"一前言为什么有的人说越老思维越固化\">一：前言：为什么有的人说越老思维越固化</h2>\n<p>在日常生活中，我们常常听到这样的说法：\"人老了就变得固执了\"、\"年轻人的想法就是灵活\"、\"中年之后学习能力就下降了\"。</p>\n<p>这些观念根深蒂固，以至于许多人将年龄增长与思维固化画上等号，认为随着岁月的推移，改变和成长变得越来越不可能。</p>\n<p>然而，斯坦福大学心理学家卡罗尔·德韦克的研究彻底颠覆了这一传统认知，她提出的\"成长型思维\"概念揭示了一个令人振奋的事实：<br />\n<strong>大脑具有终身可塑性，思维模式的选择取决于我们对待能力和挑战的态度，而非不可改变的生理宿命</strong>。</p>\n<p>成长型思维的核心信念：能力是可塑的。</p>\n<blockquote>\n<p>认为人的才智和能力不是天生的固定值，才智和能力是可以通过后天努力，通过刻意练习和策略改进，并长期坚持，是能够不断发展提高的。</p>\n</blockquote>\n<p>与之相对的是\"固定型思维\"——相信人的基本能力是天生固定值、难以改变的。</p>\n<p>这两种思维模式并非简单的性格标签，而是影响着我们在人生阶段做出选择。一个拥有成长型思维的人，会将挫折视为学习机会，将批评当作反馈，将努力视为通向不断成长的必经之路。而固定型思维的人，回避挑战，将失败视为自身不足，呆在自己的舒适区。</p>\n<h2 id=\"二两种思维的科学分析\">二：两种思维的科学分析</h2>\n<h3 id=\"神经科学的角度来看\">神经科学的角度来看</h3>\n<p>成长型思维的革命性在于它对\"能力\"这一概念的重新定义。传统观念往往将能力视为某种固定的容器——你生来有多大容量，这辈子基本就局限在这个范围内。德韦克的研究表明，这种解释是片面的。事实上，人类的大脑和认知能力更像是一片生态林，而非一个有限的容器。这片生态林能够不断扩展、深化和重新组织，其潜力远超我们通常所想象的范围。</p>\n<p>从神经科学的角度来看，大脑的可塑性是终身的。</p>\n<p>虽然大脑在青少年时期确实经历着剧烈的重塑过程，神经元之间的突触连接经历着大规模的修剪、连接和强化，但这并不意味着成年后大脑就失去了改变的能力。</p>\n<p>新的神经连接可以在任何年龄形成，新的技能可以被学习，新的习惯可以被培养。剑桥大学的一项研究追踪了成年人在学习新技能时的大脑变化，发现即使是五十岁以上的成年人，在学习复杂技能（如杂耍或新的语言）时，大脑皮层的结构也会发生可测量的变化。这意味着大脑定型论本身就是一个需要被抛弃的固定型思维。</p>\n<h3 id=\"固定型思维心理角度来看\">固定型思维心理角度来看</h3>\n<p>理解固定型思维的运作机制，对于打破它至关重要。</p>\n<p>固定型思维并非简单的不愿意改变，而是一套复杂的心理防御系统。</p>\n<p>这个系统在我们面对挑战、挫折和不确定性时自动启动，其核心功能是保护我们的自我概念和自尊。当我们拥有固定型思维时，每一次失败都不仅仅是一次挫折，而是对自我价值的否定；每一次挑战都不仅仅是学习机会，而是暴露自身不足的风险。</p>\n<p>因此，固定型思维的人会倾向于选择安全的、能够证明自己能力的任务，回避那些可能导致失败的挑战。</p>\n<p>这种思维模式的形成往往可以追溯到童年时期的经历。<br />\n当父母和老师过度强调结果而非过程，当成功被定义为\"赢\"而失败被定义为\"丢脸\"，当努力被视为天赋不足的标志时，儿童就逐渐形成了关于能力和成功的固定型信念。</p>\n<p>这些信念一旦内化，就会成为过滤世界信息的透镜——同样的经历会被不同思维模式的人做出截然不同的解读。</p>\n<h2 id=\"三两种不同思维的表现\">三：两种不同思维的表现</h2>\n<p>一个拥有成长型思维的人遇到不会的事情，会想：\"我还没掌握这个，但我可以通过练习来提高\"，而一个拥有固定型思维的则可能想：\"我就是不擅长这个，这是天生的\"。</p>\n<p>这种认知差异会在多年后产生巨大的累积效应，影响学业、职业、人际关系和以后的生活态度。</p>\n<p>在学习与成长维度上，拥有成长型思维的人将错误视为宝贵的反馈，将困难视为成长的催化剂。他们相信\"不会\"只是\"暂时还不会\"，因此更愿意走出舒适区，接受挑战。</p>\n<p>在人际关系中，成长型思维帮助人们将冲突视为理解和深化关系的机会，而非证明对方是错的证据。</p>\n<p><strong>需要强调的是，成长型思维并不意味着盲目乐观或忽视现实</strong>。</p>\n<p>成长型思维是关于过程的思维——它承认当前的状态，认可努力的价值，相信改进的可能性，同时脚踏实地采取行动。</p>\n<p>两种思维对比图：</p>\n<p><img alt=\"成长型思维和固定型思维对比图\" src=\"https://img2024.cnblogs.com/blog/650581/202601/650581-20260112193839386-1115149159.png\" /><br />\n（图片来自：卡罗尔·德韦克《终身成长》）</p>\n<h2 id=\"四是什么阻碍了成长型思维形成\">四：是什么阻碍了成长型思维形成？</h2>\n<h3 id=\"早期教育的影响\">早期教育的影响</h3>\n<p>童年经历对思维模式的塑造具有重大的影响，这种影响往往在我们还无法用语言表达的时候就已经开始。</p>\n<p>心理学家识别出几种典型的教育行为，它们在无意中强化了固定型思维。</p>\n<p>第一种是过度赞美天赋而非努力。当孩子完成某件事时，如果成人的反馈集中在“你真聪明”、“你真有天赋”上，孩子就会将这些外在的、不可控的特质与自我价值联系起来。这意味着当他们遇到困难时，会担心失去聪明的标签，从而回避挑战。</p>\n<p>第二种是将失败与身份等同。如果孩子在失败时听到的是“你怎么这么笨”、“你不行”这样的话，他们就会将失败内化为自我定义的一部分，而不是一个可以改变的状态。</p>\n<p>教育环境中的评价体系也起着关键作用。当学校和社会高度强调标准化考试成绩、当学生被排名和比较、当正确答案被过度强调时，学生就学会了关注结果而非过程，关注证明自己而非提升自己。</p>\n<p>这种环境下，即使口头上强调过程比结果重要，实际行动传递的信息却截然相反。孩子们是非常敏锐的信息接收者，他们能够从成人行为的细微差别中，判断什么才是真正被重视的。</p>\n<p>研究表明，在强调努力和进步的环境中成长的孩子，不仅学业表现更好，而且面对挫折时更加坚韧，更愿意追求挑战性的目标。</p>\n<p>家庭氛围中的隐性信息同样重要。</p>\n<p>如果父母本身持有固定型思维——例如经常表达\"我就是不擅长数学\"或\"人老了学不了新东西\"——孩子就会在耳濡目染中吸收这些观念。</p>\n<p>更微妙的是，父母如何对待自己的错误和失败，会直接影响孩子对待这些问题的方式。如果父母在犯错时表现出自责、羞耻或防御，孩子就会将这些情绪反应模式内化。</p>\n<p>相反，如果父母能够平静地承认错误并讨论改进方法，孩子就学会了以成长型的方式对待失误。</p>\n<h3 id=\"大脑发育与认知变化\">大脑发育与认知变化</h3>\n<p>大脑发育的复杂性为理解思维模式提供了神经生物学的基础。</p>\n<p>前额叶皮层——负责执行功能、计划、冲动控制和抽象思维的大脑区域——是人类大脑中最晚成熟的区域之一。它的发育从青春期开始，一直延续到二十五岁左右。</p>\n<p>这意味着年轻人在判断、规划和调节情绪方面的能力确实与成年人存在差异，这在一定程度上解释了为什么青少年有时会做出冲动的决定。然而，这并不意味着年轻人的思维就是固化的，恰恰相反，青少年大脑的高度可塑性使得这个阶段是培养成长型思维的绝佳时机。</p>\n<p>随着年龄增长，大脑确实会经历一些变化，成年后，大脑仍然保持着可塑性，尽管其运作方式与年轻时期有所不同。</p>\n<p>老年大脑在某些方面甚至具有优势：研究表明，老年人在整合复杂信息、情绪调节和利用已有知识方面往往表现出色。</p>\n<p>所谓的认知衰退在很大程度上是可以被生活方式因素所影响的——包括认知活动、社会参与、体育锻炼和饮食等。更重要的是，大脑的可塑性是区域特异性的，这意味着特定类型的训练可以在相应区域增强功能，即使在其他区域可能出现年龄相关变化的情况下也是如此。</p>\n<p>值得强调的是，神经可塑性是双向的。大脑不仅会因为不使用而\"生锈\"，也会因为积极使用而发展。这被称为\"用进废退\"原则，在认知心理学和神经科学中有着充分的实证支持。</p>\n<p>成长型思维本质上是一种对大脑可塑性的信念的一种“利用”。当我们相信改变是可能的，我们就会投入努力去实现改变，而这种投入本身又会促进实际的神经变化，形成良性循环。</p>\n<h3 id=\"心理防御机制与舒适区依赖\">心理防御机制与舒适区依赖</h3>\n<p>从心理学角度来看，固定型思维在很大程度上是一种防御机制。它保护我们免受失败和挫折带来的痛苦感觉，维护我们的自我形象和自尊。</p>\n<p>当我们害怕失败时，固定型思维告诉我们：\"这不是真正的我，只是我还没展现出我的潜力\"，或者\"如果我尝试了但失败了，那才说明我真的不行\"。通过回避挑战，我们避免了失败的风险，但也同时拒绝了成长的机会。</p>\n<p>这种防御机制在短期内确实能够减少焦虑和不适，但在长期中却导致了能力的停滞和潜力的浪费。</p>\n<p>舒适区是另一个重要的概念。我们都有一种倾向，就是停留在让我们感到安全和熟悉的范围内。</p>\n<p>这种倾向本身并非问题，它是人类进化过程中形成的正常机制——在远古时代，冒险离开熟悉的环境往往意味着危险。</p>\n<p>然而，在现代社会中，过度依赖舒适区会导致成长停滞。</p>\n<p>舒适区的问题不在于它本身，而在于我们用它作为回避成长挑战的借口。我们可能满足于够用的技能，回避精通所需的大量练习；满足于过得去的关系，回避建立更深连接所需的脆弱和坦诚；满足于可行的解决方案，回避寻找更优方案所需的创新和风险承担。</p>\n<p>身份认同的固化是固定型思维的深层表现。</p>\n<p>当我们将自己定义为\"我是某种人\"时，这种定义就成为了自我限制的框架。\"我是一个不擅长数学的人\"、\"我是一个内向的人\"、\"我是一个没有创造力的人\"——这些自我定义一旦形成，就会指导我们选择性地接触那些\"符合\"我们身份的信息和经验，回避那些可能挑战我们身份的可能性。</p>\n<p>成长型思维要求我们打破这种身份固化，认识到身份不是固定的标签，而是可以不断扩展和重新定义的。</p>\n<h2 id=\"五怎么突破障碍有哪些方法\">五：怎么突破障碍？有哪些方法？</h2>\n<h3 id=\"认知重构从内在对话开始\">认知重构：从内在对话开始</h3>\n<p>成长型思维的核心在于我们如何解读自己的经历——特别是那些困难和挑战的时刻。</p>\n<p>认知重构是一种系统性的方法，帮助我们识别和改变那些强化固定型思维的内在对话。</p>\n<p>这个过程始于对自我思维的观察。我们需要学会注意到自己什么时候在使用固定型思维的语言——\"我就是不擅长这个\"、\"这证明我不行\"、\"如果我尝试了但失败了太丢人了\"。一旦识别出这些思维模式，我们就可以有意识地挑战和重构它们。</p>\n<p>认知重构的具体技术包括几个层次。</p>\n<ul>\n<li>\n<p>首先是证据检验——当我们产生\"我不擅长这个\"的思维时，问自己：这个结论的证据是什么？有什么证据与这个结论矛盾？通常我们会发现，固定型思维的结论是基于少数负面经历，而忽视了正面经历和改变的可能性。</p>\n</li>\n<li>\n<p>其次是归因重塑——将失败归因于可控因素（如努力程度、策略选择）而非不可控因素（如天赋、运气），这样失败就变成了可以改进的问题，而非固定的身份特征。</p>\n</li>\n<li>\n<p>第三是框架转换——将\"我做不到\"转换为\"我暂时还做不到\"，将\"这太难了\"转换为\"这需要更多时间和练习\"，将\"失败是坏事\"转换为\"失败提供了学习的机会\"。</p>\n</li>\n</ul>\n<p>建立支持性的自我对话是认知重构的持续实践。</p>\n<p>当我们犯错或遇到困难时，内在的批评声音往往会加剧痛苦和防御，而鼓励和支持的声音则能促进学习和成长。成长型思维要求我们将这种支持性的声音内化，成为面对挑战时的默认反应。这需要刻意练习，就像培养任何新习惯一样。</p>\n<h3 id=\"行为改变行动塑造思维\">行为改变：行动塑造思维</h3>\n<p>思维和行动是相互影响的——改变其中一个往往会导致另一个的改变。仅仅在认知上理解成长型思维是不够的，我们需要通过行为来强化和深化它。</p>\n<p>选择挑战性任务是培养成长型思维最直接的行为方式。当我们主动选择那些需要努力和坚持才能完成的任务时，我们就在用行动向自己证明：能力是可以发展的，努力是有效的。这种行为产生的数据比任何言语都更有说服力。</p>\n<p>具体来说，可以从小处开始。选择一个你认为自己\"不擅长\"的领域，投入时间和精力去学习和练习。注意观察这个过程中的变化——最初的感觉、逐渐的进步、遇到的困难和突破。</p>\n<p>这种亲身体验比理论更能说服你：能力确实是可以发展的。重要的是选择适度挑战的任务——太容易的任务无法促进成长，太难的任务可能导致挫败。最佳的选择是那些需要努力但不是不可能完成的任务。</p>\n<p>建立反馈循环是另一个重要的行为策略。成长型思维需要及时、准确的反馈来调整努力的方向。仅仅努力是不够的——我们需要知道努力是否有效，哪里需要调整。</p>\n<p>这意味着主动寻求反馈，无论是来自老师、同事、朋友还是自我反思。记录进展、回顾目标达成情况、诚实地评估自己的表现——这些都是建立有效反馈循环的方法。没有反馈，我们可能只是在重复无效的努力，或者错过了改进的机会。</p>\n<h3 id=\"习惯养成将成长习惯化\">习惯养成：将成长习惯化</h3>\n<p>成长型思维如果只是偶尔的觉悟，其效果是有限的。</p>\n<p>真正的持久改变需要将成长型思维转化为自动的习惯。这需要理解习惯的运作机制——习惯由触发、行为和奖励三部分组成。设计支持成长型思维的习惯，意味着将成长导向的行为与积极的情感体验联系起来，使其逐渐自动化。</p>\n<p>具体的方法包括建立定期反思的习惯。</p>\n<p>每周或每月花时间回顾自己的学习和成长：遇到了什么挑战？从中学到了什么？下次可以如何改进？这种反思不仅提供反馈，还强化了成长是重要的、值得关注的这一信念。</p>\n<p>记录成长日志也是类似的方法——记录那些困难时刻、如何应对的、从中获得了什么。这些记录成为可见的成长证据，可以用来对抗固定型思维的消极结论。</p>\n<p>将成长语言融入日常交流也很重要。</p>\n<p>注意自己使用的语言，特别是对他人（特别是孩子和学生）的反馈。用\"你通过努力提高了\"代替\"你真聪明\"，用\"你还没掌握这个技巧，但我们一起来找方法\"代替\"你不行\"。</p>\n<p>这种语言不仅影响他人，也逐渐改变自己的思维习惯。在孩子教育中，这一点尤为重要——父母和老师使用的语言类型可以塑造孩子的思维模式。</p>\n<h3 id=\"环境设计创造支持性生态系统\">环境设计：创造支持性生态系统</h3>\n<p>环境对思维模式的影响往往被低估。</p>\n<p>我们所处的物理环境、社交环境和文化环境持续不断地向我们传递信息，影响我们的思维和行为。</p>\n<p>设计一个支持成长型思维的环境，意味着有意识地创造那些鼓励学习、接受挑战、重视成长的条件。这不仅包括物理环境的安排，更重要的是社交环境的构建。</p>\n<p>物理环境的设计可以促进成长型思维。展示自己或他人成长过程的物品（如早期的作品和现在的作品的对比）、放置激励性的书籍和资料、创造可以专注学习和创造的空间——这些都在无形中传递着成长的信息。相反，过分强调成就和成功的环境可能强化固定型思维，因为它们强调的是结果而非过程。</p>\n<p>社交环境的设计同样关键。寻找和建立那些重视成长、支持改变的社群——学习小组、兴趣社群、导师关系等。在这些社群中，分享困难和失败是被鼓励的，努力和进步是被认可的，新尝试是被支持的。相反，那些强化比较、竞争和聪明标签的社交环境会强化固定型思维。</p>\n<h2 id=\"六终身成长与意义\">六：终身成长与意义</h2>\n<h3 id=\"更大视角下的终身成长\">更大视角下的终身成长</h3>\n<p>成长型思维不是一个需要在某个年龄阶段完成的任务，而是贯穿整个生命周期的持续实践。</p>\n<p>真正理解成长型思维意味着认识到：无论处于什么年龄，无论已经取得了什么成就，总有更多可以学习、发展和贡献的空间。</p>\n<p>这种认识本身就具有解放性——它解除了\"已经太晚了\"或\"已经足够好了\"的自我限制，让生命保持开放和可能。</p>\n<p>从整合的视角来看，人生的各个阶段不是彼此割裂的，而是相互联系、相互丰富的。</p>\n<p>青年时期的热情和冒险、中年时期的深度和智慧、老年时期的沉淀和超越——这些品质可以相互渗透，创造出独特的生命形态。</p>\n<p>一个拥有成长型思维的年轻人可以培养老年人的沉稳和远见；一个拥有成长型思维的中年人可以保持年轻人的好奇和开放；一个拥有成长型思维的老年人可以贡献年轻人的活力和创造力。</p>\n<p>这种跨年龄的品质整合，是成长型思维在生命层面的最高体现。</p>\n<h3 id=\"成长型思维与意义建构\">成长型思维与意义建构</h3>\n<p>成长型思维的深层价值不仅在于提高能力和成就，更在于建构有意义的人生。</p>\n<p>当我们相信改变是可能的，我们就不会被当前的困境所定义，就能够看到未来的可能性，就有能力在困难中找到意义。</p>\n<p>这种意义建构的能力是心理健康和人生满意度的核心要素。研究表明，那些能够在挫折中寻找意义的人，比那些只关注结果的人更有可能从逆境中恢复，并在长期中取得更好的结果。</p>\n<p>成长型思维也意味着对生命本身的开放态度。</p>\n<p>每一个人生阶段都有其独特的挑战和机遇，每一个经历——无论是成功的还是失败的——都可以成为成长的素材。当我们用这种眼光看待生活时，生活就不再是必须熬过去的阶段，而是值得活出来的旅程。这种态度不是盲目的乐观，而是基于对人类潜力的深刻理解和信任。</p>\n<h2 id=\"七最后重新定义可能性\">七：最后：重新定义可能性</h2>\n<p>\"越老越固执\"与其说是一个不可避免的生理命运，不如说是一个自我实现的预言。</p>\n<p>它之所以流行，是因为它反映了许多人的真实体验——他们确实感到随着年龄增长，改变变得更加困难。然而，这种体验的原因是复杂的，可能包括习惯的强化、身份的固化、社会期望的内化，以及大脑某些加工速度的自然变化。但这些原因并不意味着成长型思维是不可能的，它们只是意味着我们需要更刻意地培养和锻炼这种思维模式。</p>\n<p>成长型思维是一种选择——选择相信改变是可能的，选择将挑战视为机会，选择用过程而非结果来定义自己。这种选择不受年龄限制。虽然不同年龄阶段面临的挑战不同，但成长的可能性是普遍的。一个二十岁的人可以选择维持现状，拒绝学习和改变；一个七十岁的人可以选择拥抱新事物，持续成长和发展。决定因素不是年龄，而是对成长的态度和承诺。</p>\n<p>最终，成长型思维是一种关于人的可能性的信念。</p>\n<p>它告诉我们：人不应该被任何标签所定义——无论这个标签是年龄、能力、背景还是过去的经历。这种信念不是否认限制，而是相信在限制之内和之外，都存在着未被发掘的潜力。</p>\n<p>培养和维持成长型思维，就是选择活在这种可能性的空间中，不断扩展自己的边界，同时接纳和珍视自己独特的生命旅程。在这个意义上，成长型思维不仅是一种认知策略，更是一种生命态度——一种对生命的开放、好奇和勇敢的态度。</p>\n<h2 id=\"参考\">参考</h2>\n<ul>\n<li>《终身成长》<a href=\"https://book.douban.com/subject/27154533/\" rel=\"noopener nofollow\" target=\"_blank\">https://book.douban.com/subject/27154533/</a></li>\n<li>《不强势的父母：教育孩子要懂心理学》<a href=\"https://book.douban.com/subject/37233750/\" rel=\"noopener nofollow\" target=\"_blank\">https://book.douban.com/subject/37233750/</a></li>\n<li>《认知觉醒》<a href=\"https://book.douban.com/subject/35193035/\" rel=\"noopener nofollow\" target=\"_blank\">https://book.douban.com/subject/35193035/</a></li>\n<li>如何从固定型思维转变为成长型思维？-知乎问答，<a href=\"https://www.zhihu.com/question/21099982\" rel=\"noopener nofollow\" target=\"_blank\">https://www.zhihu.com/question/21099982</a></li>\n<li>《可塑的我：自我发展心理学的35堂必修课》<a href=\"https://book.douban.com/subject/35388653/\" rel=\"noopener nofollow\" target=\"_blank\">https://book.douban.com/subject/35388653/</a></li>\n</ul>\n\n</div>\n<div id=\"MySignature\">\n    == just do it ==\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-12 18:25</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jiujuan\">九卷</a>&nbsp;\n阅读(<span id=\"post_view_count\">49</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "基于 nano-vLLM 学习大模型推理关键功能",
      "link": "https://www.cnblogs.com/cswuyg/p/19471225",
      "published": "",
      "description": "<div class=\"postTitle\">\n\t\t<h1><a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/cswuyg/p/19471225\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 12:38\">\n    <span>基于 nano-vLLM 学习大模型推理关键功能</span>\n    \n\n</a>\n</h1>\n\t</div>\n\t<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<blockquote>注：本文已于2025.12.31 发表于知乎和公众号</blockquote>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>1. 背景</span></h1>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>如果要向一位完全不了解大模型推理技术的开发者介绍这个领域，我应该从哪里讲起？</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>大模型推理的最简流程可以概括为：输入一串文本 → 文本通过词典映射表转换成一串数字序号 → 序号再经过 embedding 层的计算，变成一组能代表语义的浮点数向量 → 这组向量送入推理系统，经过层层的矩阵乘法、加法和各类专用函数的运算，得到新的输出向量 → 对输出向量做概率筛选，选出概率最高的那个数值对应的序号 → 最后再通过词典映射表 “翻译” 回文字，得到最终输出的一个词。</span></div>\n</div>\n<div class=\"Image-captionContainer\">\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"204\" src=\"https://pic1.zhimg.com/80/v2-52738e1009b99b73843b74399250f825_1440w.png?source=ccfced1a\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"877\" />\n<div class=\"ImageEdit-button css-187js3w\" style=\"text-align: center;\">图 1&nbsp;</div>\n<div class=\"ImageEdit-button css-187js3w\"><span>这是对大模型推理最朴素的理解，上述流程看似简单，但背后的推理计算环节对普通开发者而言仍是一个 “黑盒”。如果想更进一步拆解推理引擎的底层加速原理，nano-vllm 会是一个极佳的入门切入点。</span></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>2. 简介</span></h1>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><a class=\"Link ztext-link\" href=\"https://github.com/GeeeekExplorer/nano-vllm\" rel=\"noopener nofollow\" target=\"_blank\">nano-vLLM</a><span> 代码量仅约 1200 行，却实现了生产级推理框架的核心技术原型，具体包括：</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>连续批处理（Continuous Batching）</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>KV 缓存（Prefix KV Cache / Paged KV Cache）</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>高性能编译与执行优化（Torch Compilation、Triton、CUDA Graph）</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>张量并行（Tensor Parallelism）</span></div>\n</li>\n</ul>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>该框架极具入门学习价值，本文将先介绍 nano-vLLM 的基本组成架构，再对部分核心技术要点展开深入解析。</span></div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>3. 系统架构</span></h1>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>nano-vLLM 的架构非常有层次感。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>3.1. 整体架构概览</span></h2>\n<div class=\"Image-captionContainer\">\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"403\" src=\"https://picx.zhimg.com/80/v2-41cd766bd06373d580df6302b2ba3203_1440w.png?source=ccfced1a\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"655\" />\n<div class=\"ImageEdit-button css-187js3w\" style=\"text-align: center;\">图 2, 来自：https://deepwiki.com/GeeeekExplorer/nano-vllm</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>三层结构</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>接口层：User Interface Layer</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>推理引擎中控层：Inference Engine Layer</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>显存管理和模型执行层：Memory Management &amp; Model Execution Layer</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>3.2. 类层面架构</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>从类设计层面观察 nano-vLLM 的架构。</span></div>\n</div>\n<div class=\"Image-captionContainer\">\n<div>\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div>\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"433\" src=\"https://pic1.zhimg.com/80/v2-8612f2dcf6e20d6dd1fccb4ebb494bd2_1440w.png?source=ccfced1a\" style=\"display: block; margin-left: auto; margin-right: auto;\" width=\"409\" />\n<div class=\"ImageEdit-button css-187js3w\" style=\"text-align: center;\">&nbsp;</div>\n<div class=\"ImageDelete-button css-5sjb75\" style=\"text-align: center;\">&nbsp;图 3</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>上图中四种颜色代表系统的四个组成部分</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>浅蓝色，入口和推理引擎中控层</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>浅绿色，模型推理</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>浅红色，KV Cache 管理</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>浅紫色，权重加载和矩阵计算的封装</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>3.3. 源码层面划分</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>源码规划上也较为简洁。目录结构如下：</span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_Highlighter\">\n<pre class=\"brush:bash;gutter:true;\">nanovllm/\n├── engine\n├── layers\n├── models\n└── utils</pre>\n</div>\n</div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>engine，引擎的入口、中控，同时 KV Cache 比较简单，代码也放在这个目录下。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>layers，模型推理的通用组件，内部包括：linear、layernorm、rotary_embedding、attention、activation 等基础功能的封装，可以被不同模型使用。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>models，模型的实现，依赖 layers 的组件实现不同模型的推理。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>utils，不同层都可能会用到的工具函数。</span></div>\n</li>\n</ul>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>4. 连续批处理</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>4.1. 概念理解</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）定义</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>连续批处理 (Continuous Batching)：是一种迭代级（Iteration-level）的调度策略。它以“Token 生成步骤”为调度粒度。通过动态地在每一轮迭代中替换已完成的任务，消除了由于生成长度不一导致的 GPU 计算气泡，极大地提升了系统的吞吐量。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）朴素理解</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>一个请求需要执行多轮，不同请求需要执行的轮数不同，系统一轮最多只能同时执行一批 N 个请求，当一个批次里的请求参差不齐的完成时，每完成一个请求就将其用新请求替代掉。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>对比传统批处理和连续批处理：</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>传统批处理 (Static Batching)：必须等待 Batch 中生成序列最长的那个请求完成，整个 Batch 才会释放。在此期间，生成序列短请求完成后槽位会空转。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>连续批处理 (Continuous Batching)：请求完成即退出，新请求立即补位，槽位始终满载。</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>4.2. 最基础的连续批处理</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>最简单的连续批处理，不考虑 prefill 和 decode 的差异，示例代码：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> time\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> threading\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> queue\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> random\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 初始化线程安全的等待队列</span>\nwaiting_queue =<span style=\"color: rgba(0, 0, 0, 1);\"> queue.Queue()\nMAX_BATCH_SIZE </span>= 3\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 模拟用户请求线程 (生产者) ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> user_request_producer():\n    request_id </span>= 1\n    <span style=\"color: rgba(0, 0, 255, 1);\">while</span><span style=\"color: rgba(0, 0, 0, 1);\"> True:\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 模拟用户随机到达：每 1~2 秒来一个新请求</span>\n        time.sleep(random.uniform(1, 2<span style=\"color: rgba(0, 0, 0, 1);\">))\n        \n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 每个请求需要的 Token 长度随机（3到8之间）</span>\n        req = {<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">id</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">REQ-{request_id}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: random.randint(3, 8<span style=\"color: rgba(0, 0, 0, 1);\">)}\n        waiting_queue.put(req)\n        \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n[用户端] 送入新请求: {req['id']} (预计长度: {req['remain']})</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n        request_id </span>+= 1\n        <span style=\"color: rgba(0, 0, 255, 1);\">if</span> request_id &gt; 5<span style=\"color: rgba(0, 0, 0, 1);\">:\n          </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span>\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 核心推理循环 (消费者/执行器) ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> inference_loop():\n    running_batch </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []\n    \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">--- 推理引擎已启动 ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    iteration </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> 0\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">while</span><span style=\"color: rgba(0, 0, 0, 1);\"> True:\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> A. 补位逻辑：只要 Batch 没满且队列里有货，就拉进来</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">while</span> len(running_batch) &lt;<span style=\"color: rgba(0, 0, 0, 1);\"> MAX_BATCH_SIZE:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">try</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 使用 block=False，如果队列空了直接报错进 except，不阻塞推理逻辑</span>\n                new_req = waiting_queue.get(block=<span style=\"color: rgba(0, 0, 0, 1);\">False)\n                running_batch.append(new_req)\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">  &gt;&gt;&gt; [调度] {new_req['id']} 进入 Batch</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">except</span><span style=\"color: rgba(0, 0, 0, 1);\"> queue.Empty:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">break</span>\n\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> B. 推理逻辑：如果当前 Batch 有任务，就执行一次 Step</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">if</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_batch:\n            iteration </span>+= 1\n            <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">=</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>*20 + f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{iteration=}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> + <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">=</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>*20<span style=\"color: rgba(0, 0, 0, 1);\">)\n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 模拟 GPU 推理耗时 (Step 耗时)</span>\n            time.sleep(1.2<span style=\"color: rgba(0, 0, 0, 1);\">) \n            \n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 当前 Batch 状态展示</span>\n            active_ids = [f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{r['id']}(剩{r['remain']-1})</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> <span style=\"color: rgba(0, 0, 255, 1);\">for</span> r <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_batch]\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[GPU推理] 处理中: {active_ids}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n            \n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 每一个请求的剩余长度减 1</span>\n            finished_this_step =<span style=\"color: rgba(0, 0, 0, 1);\"> []\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_batch:\n                req[</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>] -= 1\n                <span style=\"color: rgba(0, 0, 255, 1);\">if</span> req[<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>] &lt;=<span style=\"color: rgba(0, 0, 0, 1);\"> 0:\n                    finished_this_step.append(req)\n            \n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> C. 剔除逻辑：做完的立刻踢出，下一轮循环开头就会有新请求补进来</span>\n            <span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> finished_this_step:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">  &lt;&lt;&lt; [完成] {req['id']} 生成完毕，释放位置</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n                running_batch.remove(req)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n            </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 如果 Batch 和 队列都空了，稍微歇会，避免 CPU 空转</span>\n            time.sleep(0.5<span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 启动程序 ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(128, 0, 128, 1);\">__name__</span> == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">__main__</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 启动用户请求线程</span>\n    t = threading.Thread(target=user_request_producer, daemon=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    t.start()\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 主线程执行推理循环</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">try</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        inference_loop()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">except</span><span style=\"color: rgba(0, 0, 0, 1);\"> KeyboardInterrupt:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n服务已停止</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<p><span>核心逻辑：</span></p>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>存储结构：代码的核心有两个队列，waiting_queue 负责存储请求线程不断接收到的新请求，running_queue 负责存储已经运行但还没有结束的请求。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>迭代循环：生产者持续往 waiting_queue 写入新请求，迭代循环持续从 waiting_queue 获取新请求加入到 running_queue，同时清理 running_queue 里已经完成的请求。</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>4.3. prefill 优先的连续批处理</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>prefill 优先的批处理，需要区分 prefll 和 decode，优先处理新请求，示例代码：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> time\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> queue\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> random\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> threading\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 核心队列</span>\nwaiting_queue =<span style=\"color: rgba(0, 0, 0, 1);\"> queue.Queue()  \nrunning_queue </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []             \n\nMAX_BATCH_SIZE </span>= 4\n\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> user_request_producer():\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\n    修改点：模拟爆发式请求到达，以触发多请求 Prefill\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span>\n    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 第一波：爆发式到达 (3个请求同时进入队列)</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n[用户] --- 爆发式请求到达 (3个请求) ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> i <span style=\"color: rgba(0, 0, 255, 1);\">in</span> range(1, 4<span style=\"color: rgba(0, 0, 0, 1);\">):\n        req </span>= {<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">id</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">REQ-{i}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: random.randint(2, 5<span style=\"color: rgba(0, 0, 0, 1);\">)}\n        waiting_queue.put(req)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[用户] 请求 {req['id']} 进入等待队列</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 延迟一会儿，再来第二波单点请求</span>\n    time.sleep(5<span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n[用户] --- 延迟请求到达 (1个请求) ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    req </span>= {<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">id</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">REQ-4</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>: 3<span style=\"color: rgba(0, 0, 0, 1);\">}\n    waiting_queue.put(req)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[用户] 请求 {req['id']} 进入等待队列</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> inference_loop():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">--- 连续批处理引擎：多请求 Prefill 模式 ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    iteration </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> 0\n    \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">while</span><span style=\"color: rgba(0, 0, 0, 1);\"> True:\n        current_batch </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []\n        is_prefill_stage </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> False\n        \n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 调度：构建当前批次</span>\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 只要 waiting_queue 非空，就尽可能填满 MAX_BATCH_SIZE</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(0, 0, 255, 1);\">not</span><span style=\"color: rgba(0, 0, 0, 1);\"> waiting_queue.empty():\n            is_prefill_stage </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> True\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">while</span> <span style=\"color: rgba(0, 0, 255, 1);\">not</span> waiting_queue.empty() <span style=\"color: rgba(0, 0, 255, 1);\">and</span> len(current_batch) &lt;<span style=\"color: rgba(0, 0, 0, 1);\"> MAX_BATCH_SIZE:\n                req </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> waiting_queue.get()\n                current_batch.append(req)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">elif</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_queue:\n            is_prefill_stage </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> False\n            current_batch </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> list(running_queue)\n        \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(0, 0, 255, 1);\">not</span><span style=\"color: rgba(0, 0, 0, 1);\"> current_batch:\n            time.sleep(</span>0.5<span style=\"color: rgba(0, 0, 0, 1);\">)\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">continue</span>\n\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 执行：模拟推理</span>\n        iteration += 1\n        <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n{'='*15} Iteration {iteration} {'='*15}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n        \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span><span style=\"color: rgba(0, 0, 0, 1);\"> is_prefill_stage:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[PREFILL] 批量生成中: {[r['id'] for r in current_batch]}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n            time.sleep(</span>1.5<span style=\"color: rgba(0, 0, 0, 1);\">) \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[DECODE ] 批量生成中: {[f'{r['id']}(剩{r['remain']})' for r in current_batch]}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n            time.sleep(</span>0.4<span style=\"color: rgba(0, 0, 0, 1);\">) \n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 统一状态更新</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> current_batch:\n            req[</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span>] -= 1\n\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 统一判断生命周期</span>\n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 注意：为了避免在遍历列表时删除元素，我们先收集要删除的对象</span>\n        to_remove_from_running =<span style=\"color: rgba(0, 0, 0, 1);\"> []\n        \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> current_batch:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> req[<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">remain</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span>] &lt;=<span style=\"color: rgba(0, 0, 0, 1);\"> 0:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">  &lt;&lt;&lt; [完成] {req['id']} 退出系统</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> running_queue:\n                    to_remove_from_running.append(req)\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span><span style=\"color: rgba(0, 0, 0, 1);\"> is_prefill_stage:\n                    running_queue.append(req)\n                    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">  -&gt; {req['id']} Prefill 完成，转入 running_queue</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n                </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n                    </span><span style=\"color: rgba(0, 0, 255, 1);\">pass</span>\n        \n        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 真正的从 running_queue 移除</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">for</span> req <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> to_remove_from_running:\n            running_queue.remove(req)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(128, 0, 128, 1);\">__name__</span> == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">__main__</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    t </span>= threading.Thread(target=user_request_producer, daemon=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    t.start()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">try</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        inference_loop()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">except</span><span style=\"color: rgba(0, 0, 0, 1);\"> KeyboardInterrupt:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">pass</span></pre>\n</div>\n<p><span>叠加上 prefill 优先之后的连续批处理代码也较为简单，主要是维护三个变量：waiting_queue、running_queue、current_batch。</span></p>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5. KV Cache</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.1. 概念理解</span></h2>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.1.1. KV Cache 的用途</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>KV Cache 有两层用途。一是用在同一个请求的 Decode 阶段，复用之前已经计算过的 KV 结果以避免重复计算；二是用在不同请求之间，使具有相同前缀的请求可以共享一部分 KV 数据，这就是 Prefix KV Cache。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.1.2. PagedAttention 技术</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在 Cache 的存储层面，PagedAttention 实现了显存的按需申请。由于 KV Cache 空间不再一次性预分配，请求序列对应的物理地址是离散的。PagedAttention 的核心在于，它能够直接读取这些物理离散的块来完成注意力计算，这背后实现了一层从“逻辑连续地址”到“物理离散地址”的映射。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>对于没有接触过非 PagedAttention 实现的读者来说，这种设计似乎理所当然：按需申请、分页管理、地址映射、局部性原理——这些都是计算机科学中非常常规的思维，甚至很难想到不这么写的理由。那么，为什么 PagedAttention 会被认为是一项里程碑式的先进技术呢？</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>首先，在 PagedAttention 出现之前，业界普遍认为 KV Cache 在显存中必须物理连续，否则会因访存不连续导致性能大幅下降。其次，当时的注意力算子（如标准的 FlashAttention）并不支持二次寻址映射。PagedAttention 证明了即便物理存储不连续，性能依然可以保持极高。其代码实现最关键的点在于重构了 CUDA 内核，使其原生支持 KV Cache 二次寻址。一个序列的 KV Cache 不需要物理连续，也正是不同序列间能够灵活复用 Prefix KV Cache 的技术前提。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>总的来说，虽然分页虚拟内存在 CPU 领域是常识，但在 GPU 算子领域其发展相对缓慢。实现一套既能分页管理、又不损失算力利用率的 Attention Kernel 是 PagedAttention 的核心所在。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.2. Prefix KV Cache 的实现</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Cache 的管理较为简单，只有 BlockManager 类，负责维护显存池各个 block 的的状态。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.2.1. 功能细节</span></h3>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>使用 hash 来识别是否有可复用前缀，以 block 为基本单元</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>链式 hash，每个 block 的 hash 计算输入为前序 block 的 hash 值加上本 block 的 token id</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>每一个 block 有对应的 meta 信息对象，记录 block 被复用的引用计数，确保复用时不会被释放</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>为避免 hash 碰撞出现错误，block meta 信息还需要记录原始的 token id</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在获取 KV Cache 空间时需要考虑是否跨 block</span></div>\n</li>\n</ul>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.2.2. 内存池</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在进程启动时，一次性申请内存池的空间：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre>kv_cache =<span style=\"color: rgba(0, 0, 0, 1);\"> torch.empty(\n    </span>2,                          <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> K 和 V</span>\n    num_layers,                 <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 层数</span>\n    num_blocks,                 <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 总块数</span>\n    block_size,                 <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 每块 token 数</span>\n    num_kv_heads // tp_size,    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> KV head 数（考虑张量并行）</span>\n    head_dim                    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> head 维度</span>\n)</pre>\n</div>\n<p><span>上述申请显存代码中的 num_blocks 是根据可用于 KV Cache 的显存算出来的：</span></p>\n<div class=\"cnblogs_code\">\n<pre>block_bytes = 2 * hf_config.num_hidden_layers * self.block_size * num_kv_heads * head_dim *<span style=\"color: rgba(0, 0, 0, 1);\"> hf_config.torch_dtype.itemsize\nconfig.num_kvcache_blocks </span>= int(total * config.gpu_memory_utilization - used - peak + current) // block_bytes</pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>上述代码中的 block_bytes 并不是指 block 的大小，而是把计算 blocks 数的所有除数乘到了一起，除数包括：block 大小、k 和 v、模型层数。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><span>total * config.gpu_memory_utilization - used - peak + current<span> 这部分则是根据最高的显存利用率算出来可用显存，减去当前模型加载完后使用了的部分，再减去模型预热时使用的激活显存：<span><span>peak - current<span>。</span></span></span></span></span></span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>申请到内存池后，按层共享视图给各个层的 Attention 对象，代码看起来比较 tricky，但在 python 里倒比较常见：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">for</span> module <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> self.model.modules():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> hasattr(module, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">k_cache</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>) <span style=\"color: rgba(0, 0, 255, 1);\">and</span> hasattr(module, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">v_cache</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n        module.k_cache </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> self.kv_cache[0, layer_id]\n        module.v_cache </span>= self.kv_cache[1<span style=\"color: rgba(0, 0, 0, 1);\">, layer_id]\n        layer_id </span>+= 1</pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>遍历模型中的所有 nn.Module 子模块，通过检查是否存在 k_cache 和 v_cache 属性来识别 Attention 层。对于每个 Attention 层，将其 k_cache 和 v_cache 属性替换为指向全局 KV Cache 显存池的张量视图，这样所有层共享同一块连续的显存空间，但每层只能访问自己对应的切片。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>5.2.3. KV Cache 写入</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在 attention 子层的 forward 前做 KV Cache 的写入，使用的 store_kvcache_kernel 函数是 triton.jit 实现的，代码也比较简洁：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">@triton.jit\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> store_kvcache_kernel(\n    key_ptr,\n    key_stride,\n    value_ptr,\n    value_stride,\n    k_cache_ptr,\n    v_cache_ptr,\n    slot_mapping_ptr,\n    D: tl.constexpr,\n):\n    idx </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> tl.program_id(0)\n    slot </span>= tl.load(slot_mapping_ptr +<span style=\"color: rgba(0, 0, 0, 1);\"> idx)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> slot == -1: <span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\">\n    key_offsets </span>= idx * key_stride +<span style=\"color: rgba(0, 0, 0, 1);\"> tl.arange(0, D)\n    value_offsets </span>= idx * value_stride +<span style=\"color: rgba(0, 0, 0, 1);\"> tl.arange(0, D)\n    key </span>= tl.load(key_ptr +<span style=\"color: rgba(0, 0, 0, 1);\"> key_offsets)\n    value </span>= tl.load(value_ptr +<span style=\"color: rgba(0, 0, 0, 1);\"> value_offsets)\n    cache_offsets </span>= slot * D +<span style=\"color: rgba(0, 0, 0, 1);\"> tl.arange(0, D)\n    tl.store(k_cache_ptr </span>+<span style=\"color: rgba(0, 0, 0, 1);\"> cache_offsets, key)\n    tl.store(v_cache_ptr </span>+<span style=\"color: rgba(0, 0, 0, 1);\"> cache_offsets, value)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> store_kvcache(key: torch.Tensor, value: torch.Tensor, k_cache: torch.Tensor, v_cache: torch.Tensor, slot_mapping: torch.Tensor):\n    N, num_heads, head_dim </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> key.shape\n    D </span>= num_heads *<span style=\"color: rgba(0, 0, 0, 1);\"> head_dim\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">assert</span> key.stride(-1) == 1 <span style=\"color: rgba(0, 0, 255, 1);\">and</span> value.stride(-1) == 1\n    <span style=\"color: rgba(0, 0, 255, 1);\">assert</span> key.stride(1) == head_dim <span style=\"color: rgba(0, 0, 255, 1);\">and</span> value.stride(1) ==<span style=\"color: rgba(0, 0, 0, 1);\"> head_dim\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">assert</span> k_cache.stride(1) == D <span style=\"color: rgba(0, 0, 255, 1);\">and</span> v_cache.stride(1) ==<span style=\"color: rgba(0, 0, 0, 1);\"> D\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">assert</span> slot_mapping.numel() ==<span style=\"color: rgba(0, 0, 0, 1);\"> N\n    store_kvcache_kernel[(N,)](key, key.stride(0), value, value.stride(0), k_cache, v_cache, slot_mapping, D)</span></pre>\n</div>\n<p><span>使用 stride 函数来确认显存是否连续，因为在 store_kvcache_kernel 的实现里会按照显存连续来读取指定位置的值。</span></p>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>显存连续和不连续的例子：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n\nkey </span>= torch.randn(2, 3, 4<span style=\"color: rgba(0, 0, 0, 1);\">)\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span><span style=\"color: rgba(0, 0, 0, 1);\">(key.stride())  \nkey_t </span>= key.transpose(1, 2<span style=\"color: rgba(0, 0, 0, 1);\">)\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">转置后（非连续）:</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">, key_t.stride())  \nkey_t </span>= key_t.contiguous()  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 重新分配，使其连续</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">contiguous 后:</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">, key_t.stride())\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 输出：</span><span style=\"color: rgba(0, 128, 0, 1);\">\n#</span><span style=\"color: rgba(0, 128, 0, 1);\"> (12, 4, 1)</span><span style=\"color: rgba(0, 128, 0, 1);\">\n#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 转置后（非连续）: (12, 1, 4)</span><span style=\"color: rgba(0, 128, 0, 1);\">\n#</span><span style=\"color: rgba(0, 128, 0, 1);\"> contiguous后: (12, 3, 1)</span></pre>\n</div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6. cuda graph</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6.1. 概念理解</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>CUDA Graph 是一种将一系列 CUDA 操作录制成图的技术，在重复执行的固定操作序列场景下可以显著提升推理性能，主要基于这几方面：</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>减少 CPU 与 GPU 之间的频繁同步和指令下发开销，降低传统独立操作带来的控制流交互损耗；</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>减少执行过程中的 CPU 干预，GPU 自主批量执行图内操作，最大化 GPU 利用率，降低延迟、提升吞吐。</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>规避多次独立 CUDA Kernel 的启动固定开销，多个 Kernel 打包后仅需一次调度触发，大幅提升小 Kernel 密集场景的执行效率；</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>可配合显存池实现显存资源复用，减少 “少量多次” 显存申请 / 释放的开销，同时驱动会基于图内显存访问模式优化带宽利用率；</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>CUDA 驱动可获取操作序列的全局视图，基于完整的依赖关系进行全局优化（如 Kernel 顺序调整、资源合并等）；</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6.2. 功能细节</span></h2>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>录制时使用的张量内存地址，在重放时必须保持不变，也就是后面多次 replay 都会使用捕获时申请的变量空间</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>捕获后的 graph 对象记录在成员变量里，供下次推理时选择</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>重放时选择比请求 batch size 大的最小 graph batch size</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>捕获时不同的 batch size 共享相同的静态显存空间，并让多个批次共享显存池，使得虽然有多个 batch size，但只会使用 Max Batch Size 的显存空间</span></div>\n</li>\n</ul>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6.3. 示例代码</span></h2>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch.nn as nn\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 基础配置</span>\ndevice = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\nD </span>= 512                 <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 维度</span>\ngraph_bs = [1, 8, 32]   <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 预定义的桶（分桶尺寸）</span>\nNUM_LAYERS = 100        <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 极深模型，增加 Kernel 数量以放大 Graph 优势</span>\niters = 50              <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 性能测试迭代次数</span>\nmax_bs =<span style=\"color: rgba(0, 0, 0, 1);\"> max(graph_bs)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 定义深层模型 (产生约 400 个 Kernel)</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> UltraDeepModel(nn.Module):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> <span style=\"color: rgba(128, 0, 128, 1);\">__init__</span><span style=\"color: rgba(0, 0, 0, 1);\">(self):\n        super().</span><span style=\"color: rgba(128, 0, 128, 1);\">__init__</span><span style=\"color: rgba(0, 0, 0, 1);\">()\n        self.blocks </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> nn.ModuleList()\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(NUM_LAYERS):\n            block </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> nn.ModuleDict({\n                </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">ln</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">: nn.LayerNorm(D).to(device),\n                </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">linear</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">: nn.Linear(D, D).to(device),\n                </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">act</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">: nn.ReLU()\n            })\n            self.blocks.append(block)\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> forward(self, x):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> block <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> self.blocks:\n            identity </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> x\n            x </span>= block[<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">ln</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">](x)\n            x </span>= block[<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">linear</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">](x)\n            x </span>= block[<span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">act</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">](x)\n            x </span>= x +<span style=\"color: rgba(0, 0, 0, 1);\"> identity \n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> x\n\nmodel </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> UltraDeepModel().eval()\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 静态缓冲区准备</span>\nstatic_input = torch.empty(max_bs, D, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\nstatic_output </span>= torch.empty(max_bs, D, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\n\ngraphs </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> {}\ngraph_pool </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> None\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 录制阶段 (从大到小，共享内存池)</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">--- 开始录制分桶 CUDA Graphs ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n</span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> bs <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> reversed(sorted(graph_bs)):\n    current_input </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> static_input[:bs]\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> Warmup</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span> range(5<span style=\"color: rgba(0, 0, 0, 1);\">):\n        _ </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> model(current_input)\n    \n    g </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> torch.cuda.CUDAGraph()\n    with torch.cuda.graph(g, pool</span>=<span style=\"color: rgba(0, 0, 0, 1);\">graph_pool):\n        static_output[:bs] </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> model(current_input)\n    \n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> graph_pool <span style=\"color: rgba(0, 0, 255, 1);\">is</span><span style=\"color: rgba(0, 0, 0, 1);\"> None:\n        graph_pool </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> g.pool()\n    graphs[bs] </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> g\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">✅ 已录制桶 BS={bs}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 5. 辅助函数：根据实际 BS 匹配最近的桶</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> get_bucket_bs(actual_bs):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> b <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> sorted(graph_bs):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> actual_bs &lt;=<span style=\"color: rgba(0, 0, 0, 1);\"> b:\n            </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> b\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> None\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 6. 性能对比测试 (包含 Padding 逻辑)</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span> benchmark(actual_test_bs=7<span style=\"color: rgba(0, 0, 0, 1);\">):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n--- 性能测试开始: 实际请求 BS={actual_test_bs} ---</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 生成测试数据</span>\n    test_data = torch.randn(actual_test_bs, D, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\n    \n    start_event </span>= torch.cuda.Event(enable_timing=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    end_event </span>= torch.cuda.Event(enable_timing=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 段落 A: Standard Eager Mode (直接跑 7 个) ---</span>\n    torch.cuda.nvtx.range_push(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Eager_Mode</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    start_event.record()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(iters):\n        _ </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> model(test_data)\n    end_event.record()\n    torch.cuda.synchronize()\n    eager_time </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> start_event.elapsed_time(end_event)\n    torch.cuda.nvtx.range_pop()\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 段落 B: CUDA Graph Mode (Padding 对齐到 8) ---</span>\n    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 路由逻辑</span>\n    bucket_bs =<span style=\"color: rgba(0, 0, 0, 1);\"> get_bucket_bs(actual_test_bs)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> bucket_bs <span style=\"color: rgba(0, 0, 255, 1);\">is</span><span style=\"color: rgba(0, 0, 0, 1);\"> None:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">❌ 错误: 实际 BS={actual_test_bs} 超过了最大分桶 {max_bs}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\">\n\n    torch.cuda.nvtx.range_push(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Graph_Mode_Bucket_{bucket_bs}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 数据对齐 (Padding): 将 7 条数据拷入 8 的静态区域</span>\n    <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> static_input 的前 7 行被覆盖，第 8 行保持不变（即 Padding 位）</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">    static_input[:actual_test_bs].copy_(test_data)\n    \n    start_event.record()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(iters):\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 重放分桶 8 的图</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">        graphs[bucket_bs].replay()\n    end_event.record()\n    torch.cuda.synchronize()\n    graph_time </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> start_event.elapsed_time(end_event)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 4. 结果截断 (Slicing): 从静态区拿回前 7 条</span>\n    final_res =<span style=\"color: rgba(0, 0, 0, 1);\"> static_output[:actual_test_bs]\n    torch.cuda.nvtx.range_pop()\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 打印结果</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">匹配到的桶: {bucket_bs} (Padding 浪费率: {(bucket_bs-actual_test_bs)/bucket_bs*100:.1f}%)</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{'Mode':&lt;20} | {'Avg Time (ms)':&lt;15}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> * 40<span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{'Eager Mode':&lt;20} | {eager_time/iters:&gt;15.4f}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{'Graph Mode':&lt;20} | {graph_time/iters:&gt;15.4f}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> * 40<span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">🚀 加速比: {eager_time/graph_time:.2f}x</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">最终输出形状: {final_res.shape}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> <span style=\"color: rgba(128, 0, 128, 1);\">__name__</span> == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">__main__</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 测试不同的输入 BS</span>\n    benchmark(actual_test_bs=7)  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 触发对齐到 8</span>\n    benchmark(actual_test_bs=1)  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 精确匹配到 1</span></pre>\n</div>\n<p><span>执行：nsys profile --trace=cuda,osrt,nvtx python3 cu2.py</span></p>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>输出：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">Collecting data...\n</span>--- 开始录制分桶 CUDA Graphs ---<span style=\"color: rgba(0, 0, 0, 1);\">\n✅ 已录制桶 BS</span>=32<span style=\"color: rgba(0, 0, 0, 1);\">\n✅ 已录制桶 BS</span>=8<span style=\"color: rgba(0, 0, 0, 1);\">\n✅ 已录制桶 BS</span>=1\n\n--- 性能测试开始: 实际请求 BS=7 ---<span style=\"color: rgba(0, 0, 0, 1);\">\n匹配到的桶: </span>8 (Padding 浪费率: 12.5%<span style=\"color: rgba(0, 0, 0, 1);\">)\nMode                 </span>|<span style=\"color: rgba(0, 0, 0, 1);\"> Avg Time (ms)  \n</span>----------------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\nEager Mode           </span>|          7.9331<span style=\"color: rgba(0, 0, 0, 1);\">\nGraph Mode           </span>|          1.0136\n----------------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\n🚀 加速比: </span>7<span style=\"color: rgba(0, 0, 0, 1);\">.83x\n最终输出形状: torch.Size([</span>7, 512<span style=\"color: rgba(0, 0, 0, 1);\">])\n\n</span>--- 性能测试开始: 实际请求 BS=1 ---<span style=\"color: rgba(0, 0, 0, 1);\">\n匹配到的桶: </span>1 (Padding 浪费率: 0.0%<span style=\"color: rgba(0, 0, 0, 1);\">)\nMode                 </span>|<span style=\"color: rgba(0, 0, 0, 1);\"> Avg Time (ms)  \n</span>----------------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\nEager Mode           </span>|          8.1803<span style=\"color: rgba(0, 0, 0, 1);\">\nGraph Mode           </span>|          0.8011\n----------------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\n🚀 加速比: </span>10<span style=\"color: rgba(0, 0, 0, 1);\">.21x\n最终输出形状: torch.Size([</span>1, 512])</pre>\n</div>\n<p><span>查看 nsys：</span></p>\n</div>\n<div class=\"Image-captionContainer\">\n<div>\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div>\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\" style=\"text-align: center;\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"394\" src=\"https://picx.zhimg.com/80/v2-fa4eefbebda0472fa1f21d8bd01e4c17_1440w.png?source=ccfced1a\" width=\"753\" /></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div style=\"text-align: center;\">图 4</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>可以看到，在 cuda graph 的时候，SM 使用更充分。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>6.4. Q&amp;A</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）为什么推理时 cuda graph 的选择要采用向上对齐的分桶策略，即：批次相等或稍大的，而不是选择批次最大的？</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>虽然在建图（Capture）阶段，系统会按照最大批次（Max Batch Size）预先申请并锁定静态显存空间，此时即便选择最大批次执行也不会产生额外的显存容量浪费，但会引入以下两个维度的性能损耗：</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>显存带宽的无效占用，大模型推理（尤其是 Decoding 阶段）属于典型的访存密集型任务，其瓶颈在于模型权重从显存到计算单元的搬运速度。即便大部分批次位置是 Padding（空数据），CUDA Graph 依然会严格执行录制时的内存寻址定义，搬运完整批次的数据。使用过大的批次会导致 GPU 浪费极其宝贵的带宽去搬运“无效数据”，从而增加单次推理的耗时，推高推理延迟（Latency）。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>计算资源的无效占用，GPU 调度器会根据图定义的规模预分配硬件资源（如 SM 核心、寄存器、共享显存等）。虽然 Padding 部分的计算逻辑极快，但这些资源在整个 CUDA Graph 执行完成前无法被释放。这会导致 GPU 硬件处于“虚假繁忙”状态，阻塞了其他潜在任务（如多流并行等）获取硬件资源，削弱了系统整体的并发吞吐能力（Throughput）。</span></div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7. Torch Compilation</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7.1. 概念理解</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>torch.compile 能够将 PyTorch 张量计算相关的 Python 逻辑，转化为更高效的中间表示（在 CUDA 设备上，通常是 Triton 内核代码，也支持原生 CUDA 内核）。相较于传统的即时执行（Eager Mode），这种方式通过优化计算内核本身带来显著的运行效率提升；此外，当输入张量形状、数据类型固定时，torch.compile 还会自动启用 CUDA Graph 优化，进一步放大性能收益。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>在 torch.compile 问世之前，PyTorch 开发者若想追求高性能，仅有两种核心选择：一是使用 Eager Mode 接受其原生性能上限，二是手动编写 Triton 或 CUDA 底层内核代码（该方式开发门槛高、周期长、维护成本高）。而有了 torch.compile 后，开发者只需编写简洁易懂的 PyTorch Python 业务逻辑，无需关注底层硬件适配与内核实现，即可获得接近手写 Triton/CUDA 的优异性能，大幅平衡了开发效率与运行性能。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7.2. 使用方法</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>应用 torch.compile 非常简单，核心有两类使用方式：</span></div>\n</div>\n<ul class=\"public-DraftStyleDefault-ul\">\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>装饰器方式：在 PyTorch 函数上直接添加 @torch.compile 装饰器，定义时即完成编译声明；</span></div>\n</li>\n<li class=\"Editable-styled public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>显式调用方式：通过 compiled_obj = torch.compile(target) 显式编译目标对象，后续调用 compiled_obj 即可使用优化后的逻辑；</span></div>\n</li>\n</ul>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>另外，可以对模型实例的直接编译：对于 PyTorch 模型（nn.Module 子类实例），可直接传入 torch.compile 完成整体编译，无需单独修饰 forward 方法。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>示例代码：</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>方式 1：装饰器方式（适用于函数 / 模型方法）</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch.nn as nn\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 对普通PyTorch函数使用装饰器</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">@torch.compile\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> my_tensor_func(x, y):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> torch.matmul(x, y) +<span style=\"color: rgba(0, 0, 0, 1);\"> torch.relu(y)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 对模型的forward方法使用装饰器</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> MyModel(nn.Module):\n    @torch.compile  </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 修饰forward方法，自动编译模型推理逻辑</span>\n    <span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> forward(self, x):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> nn.Linear(10, 20)(x)</pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>方式 2：显式调用方式（适用于函数 / 模型，灵活性更高）</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch.nn as nn\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 显式编译普通函数</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> my_tensor_func(x, y):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> torch.matmul(x, y) +<span style=\"color: rgba(0, 0, 0, 1);\"> torch.relu(y)\ncompiled_func </span>= torch.compile(my_tensor_func)  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 生成编译后的函数</span>\noutput = compiled_func(torch.randn(32, 10), torch.randn(10, 20))  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 调用编译后的函数</span>\n\n<span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 显式编译模型（与方式3本质一致，更强调“先编译后使用”的显式流程）</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> MyModel(nn.Module):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> forward(self, x):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> nn.Linear(10, 20<span style=\"color: rgba(0, 0, 0, 1);\">)(x)\nmodel </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> MyModel()\ncompiled_model </span>= torch.compile(model)  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 直接编译整个模型实例</span>\noutput = compiled_model(torch.randn(32, 10))  <span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 调用编译后的模型</span></pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>方式 3：直接编译模型实例（深度学习中最常用，简化写法）</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch.nn as nn\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> MyModel(nn.Module):\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> forward(self, x):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> nn.Linear(10, 20<span style=\"color: rgba(0, 0, 0, 1);\">)(x)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 直接编译模型实例，一步到位（无需装饰器，最简洁常用）</span>\nmodel =<span style=\"color: rgba(0, 0, 0, 1);\"> torch.compile(MyModel())\noutput </span>= model(torch.randn(32, 10))</pre>\n</div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7.3. 性能对比</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>下面以一个简单的例子对比 Eager Mode 和 Compiled Mode：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> torch\n</span><span style=\"color: rgba(0, 0, 255, 1);\">import</span><span style=\"color: rgba(0, 0, 0, 1);\"> time\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 确保使用的是 GPU</span>\ndevice = <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> <span style=\"color: rgba(0, 0, 255, 1);\">if</span> torch.cuda.is_available() <span style=\"color: rgba(0, 0, 255, 1);\">else</span> <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cpu</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">if</span> device == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cpu</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">警告：CUDA 不可用，将使用 CPU 运行（torch.compile 的优势在 GPU 上最明显）。</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> complex_operation_eager(x, y):\n    z </span>= x *<span style=\"color: rgba(0, 0, 0, 1);\"> y\n    z </span>= z +<span style=\"color: rgba(0, 0, 0, 1);\"> x\n    z </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> torch.relu(z)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> z.sum()\n\n@torch.compile\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> complex_operation_graph(x, y):\n    z </span>= x *<span style=\"color: rgba(0, 0, 0, 1);\"> y\n    z </span>= z +<span style=\"color: rgba(0, 0, 0, 1);\"> x\n    z </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> torch.relu(z)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> z.sum()\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 准备数据</span>\nx = torch.randn(10000, 10000, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\ny </span>= torch.randn(10000, 10000, device=<span style=\"color: rgba(0, 0, 0, 1);\">device)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 热身 (Warm up)</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">正在编译并进行多次热身以稳定 GPU 状态...</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 增加热身循环</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">for</span> i <span style=\"color: rgba(0, 0, 255, 1);\">in</span> range(3<span style=\"color: rgba(0, 0, 0, 1);\">): \n    complex_operation_graph(x, y)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> i ==<span style=\"color: rgba(0, 0, 0, 1);\"> 0:\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-&gt; 首次编译完成，正在进行后续预热...</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\ntorch.cuda.synchronize()\n</span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">预热完毕，开始正式测试。</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> benchmark(func, x, y, label, iterations=100<span style=\"color: rgba(0, 0, 0, 1);\">):\n    start_event </span>= torch.cuda.Event(enable_timing=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    end_event </span>= torch.cuda.Event(enable_timing=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n    \n    start_event.record()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(iterations):\n        func(x, y)\n    end_event.record()\n    \n    torch.cuda.synchronize()\n    \n    elapsed_time_ms </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> start_event.elapsed_time(end_event)\n    avg_time_s </span>= (elapsed_time_ms / 1000) /<span style=\"color: rgba(0, 0, 0, 1);\"> iterations\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{label} 平均耗时: {avg_time_s:.6f} 秒</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> avg_time_s\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 3. 执行测试并计算加速比</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">with torch.no_grad():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> * 30<span style=\"color: rgba(0, 0, 0, 1);\">)\n    eager_time </span>= benchmark(complex_operation_eager, x, y, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Eager Mode   </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    compile_time </span>= benchmark(complex_operation_graph, x, y, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Compiled Mode</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">-</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> * 30<span style=\"color: rgba(0, 0, 0, 1);\">)\n    \n    </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 计算加速比逻辑</span>\n    speedup = eager_time /<span style=\"color: rgba(0, 0, 0, 1);\"> compile_time\n    improvement </span>= (speedup - 1) * 100\n    \n    <span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">性能提升结果:</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">加速比 (Speedup): {speedup:.2f}x</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">运行速度提升了: {improvement:.1f}%</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<p><span>运行输出：</span></p>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">正在编译并进行多次热身以稳定 GPU 状态...\n</span>-&gt;<span style=\"color: rgba(0, 0, 0, 1);\"> 首次编译完成，正在进行后续预热...\n预热完毕，开始正式测试。\n</span>------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\nEager Mode    平均耗时: </span>0.005690<span style=\"color: rgba(0, 0, 0, 1);\"> 秒\nCompiled Mode 平均耗时: </span>0.001528<span style=\"color: rgba(0, 0, 0, 1);\"> 秒\n</span>------------------------------<span style=\"color: rgba(0, 0, 0, 1);\">\n性能提升结果:\n加速比 (Speedup): </span>3<span style=\"color: rgba(0, 0, 0, 1);\">.72x\n运行速度提升了: </span>272.3%</pre>\n</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>性能有数倍的提升。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>7.4. Q&amp;A</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）既然 torch.compile 有这么大的好处，为什么不能给所有的张量操作函数都加上 @torch.compile？</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>有几方面的原因：首先，存在编译开销，会导致首次运行显著变慢；然后，编译器生成的 Triton 内核（或其他后端代码）通常是针对特定张量形状、数据类型和设备配置优化的，如果输入张量的这些属性频繁变化，会反复触发重新编译（即 “编译缓存失效”），反而抵消性能收益；第三，torch.compile 自身会带来额外的显存开销（用于存储编译后的中间表示、内核缓存等），过多无差别使用可能导致显存不足（OOM）；最后，并非所有代码都能被成功图化优化，如果张量操作中调用了非 PyTorch 原生的第三方库（或纯 Python 原生逻辑），会导致计算图中断，此时编译器无法继续优化后续逻辑，还需要将控制权交回给 Python 解释器，产生不必要的上下文切换开销，可能导致负优化。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）torch.compile 支持生成原生 CUDA 内核代码，但通常来说，编译器自动生成的通用原生 CUDA 代码优化粒度不够精细；而 Triton 内置了极强的 Autotuning（自动调优）能力，针对深度学习张量计算场景做了深度适配，因此在绝大多数深度学习任务中，Triton 内核的性能通常优于自动生成的原生 CUDA 内核。</span></div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>8. Torch Compilation、Trition、CUDA Graph 三者的区别和联系</span></h1>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>8.1. 核心区别</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）Torch Compilation：PyTorch 高层一站式性能优化入口（用户态抽象接口）</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>作为面向开发者的顶级优化封装，torch.compile 无需开发者关注底层硬件细节与优化实现，其核心定位是对 PyTorch 张量计算逻辑（函数 /nn.Module 模型）进行端到端自动优化，屏蔽了底层内核生成与执行优化的复杂性，是绝大多数 PyTorch 开发者的首选性能优化工具。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）Triton：高性能 GPU 内核专用 DSL </span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Triton 既是开发者手动编写高性能内核的领域专用语言（DSL），也是 torch.compile 自动化生成代码的核心目标后端。其中，triton.jit 是 Triton 框架提供的即时编译装饰器，定位为高性能跨平台 GPU 内核的手动开发入口，抽象层级低于 torch.compile、高于原生 CUDA C++。它允许开发者以 Python 风格语法编写 GPU 内核逻辑，无需手动处理线程调度、寄存器分配等底层细节，最终编译为高效 GPU 机器码，用于满足定制化算子的高性能需求。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）CUDA Graph：GPU 底层静态任务流执行优化技术</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>CUDA Graph 是一种静态任务流调度技术，旨在消除主机端（Host）与设备端（Device）之间的交互延迟。它并非 “内核生成工具”，也非 “用户态编程接口”，而是针对 CPU-GPU 交互瓶颈的底层执行优化技术，抽象层级最低。其核心作用是固化连续的 CUDA 内核调用序列与内存配置，通过 “录制 - 重放” 模式消除重复内核启动、CPU-GPU 频繁通信的开销，仅优化执行流程，不改变内核本身的计算性能。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>8.2. 核心联系</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）torch.compile 依赖 triton.jit 实现高性能内核生成</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>torch.compile 的默认底层编译器（Inductor）在 CUDA 设备上，会自动将 PyTorch 计算逻辑转化为 Triton 内核代码，并隐式调用 triton.jit 完成编译，生成高性能 GPU 内核（开发者无需手动编写 Triton 代码，也无需感知 triton.jit 的存在）。此外，torch.compile 也支持生成原生 CUDA 内核，作为 Triton 内核的可选补充方案。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）torch.compile 集成 CUDA Graph 实现执行层二次优化</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>当输入张量的形状、数据类型等属性固定时，torch.compile 会自动启用 CUDA Graph 优化，将编译生成的 Triton/CUDA 内核调用序列录制为 CUDA 图。后续重复执行该逻辑时，直接在 GPU 上重放该图，进一步放大性能收益，实现 “内核计算优化” 与 “执行流程优化” 的协同增效。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）triton.jit 自定义内核可与 CUDA Graph 手动协同</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>开发者手动通过 triton.jit 编写并编译的自定义内核，在批量重复执行（输入形状固定）的场景下，可手动集成 CUDA Graph 完成 “录制 - 重放” 流程，消除 CPU 对 GPU 的调度开销，实现内核计算性能与执行效率的双重极致优化。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（4）三者协同构建极致性能计算链路</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>典型极致性能链路：手动编写 triton.jit 定制内核 → 嵌入 PyTorch 模型 / 函数 → 通过 torch.compile 进行上层计算图优化（算子融合、内存复用等） → torch.compile 自动启用 CUDA Graph 优化执行流程 → 实现 GPU 计算性能最大化。</span></div>\n</div>\n<h1 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9. TP 模式</span></h1>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>TP 模式将矩阵计算按行、列拆分到多颗 GPU 上执行，涉及两个关键点：权重参数怎么加载、多核计算之间如何协同，下面做介绍。</span></div>\n</div>\n<h2 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.1. 加载权重参数</span></h2>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>权重参数与矩阵计算强相关，因此权重参数的加载逻辑通常与矩阵计算逻辑一同封装在同一个类中，实现功能的内聚性。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.1.1. 关键技术点</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）参数文件中，权重矩阵以 Key-Value 键值对形式存储，读取时同样采用 Key-Value 方式解析。其中 key 对应权重矩阵在模型中的归属位置，例如：模型第 0 层 MLP 子层的 down proj 权重对应的 key 为 model.layers.0.mlp.down_proj.weight。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）参数文件由训练流程写入、推理流程读取，训练与推理两侧必须严格对齐 key 的命名规则。模型参数加载时，会根据参数文件中的 key 名称，在 nn.Module 对象中匹配并调用对应的 weight_loader 方法完成加载。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）模型结构包含多个层级，每一层内部又包含多个子模块，不同子模块对应各自专属的参数加载方法。PyTorch 的 nn.Module 通过特殊方法 __setattr__，将模型结构中的各个子模块构建为树形结构；树形结构中每个叶子节点的路径，与参数文件中的 key 一一映射，通过该路径找到叶子节点后，即可获取对应的参数对象 nn.Parameter，而该参数对象绑定了其所属子模块的 weight_loader 方法。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（4）矩阵乘法 A * B 遵循「A 的行 × B 的列」计算规则，在模型推理中，B 为权重矩阵，实际访问时以列维度为主。为提升读取效率、避免缓存（Cache）频繁失效，权重矩阵 B 通常以转置形式存储。TP（Tensor Parallel）worker 加载权重时，需适配该转置存储特性 —— 即权重矩阵第 0 维对应原始矩阵的列数据，第 1 维对应原始矩阵的行数据。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>从上述技术点可得出核心对应关系：参数文件中的模型结构以一个个 key 表示，这些 key 按层级关系可构建为一棵路径树；代码中的模型结构以有包含关系的类对象表示，这些类对象同样构成一棵与参数文件路径树完全对应的树。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.1.2. 实操举例（FFN 层 up proj 权重加载）</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）假设 TP size=2，up proj 权重矩阵的原始形状为 [1024, 3072]，下面介绍一个 TP worker 如何加载权重。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）首先，构造模型对象时，会初始化 ColumnParallelLinear 对象，并设定核心参数：input_size=1024，output_size=3072/2=1536（按 TP 尺寸做均分）。这两个参数最终用于初始化 nn.Parameter 对象，对应代码为 <span><span>self.weight = nn.Parameter(torch.empty(output_size, input_size))<span>，需注意此处初始化的张量以 output_size 为行维度、input_size 为列维度。</span></span></span></span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）随后，启动模型权重加载流程：先从参数文件中读取所有 key-value 键值对，再通过 key 在 nn.Module 树形结构中查找对应的 nn.Parameter 对象，匹配到后调用其绑定的 weight_loader 函数，执行具体的参数加载操作。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（4）参数加载阶段，针对列并行模式，需要对权重张量的第 0 维度进行拆分，再根据当前进程的 tp_rank（TP 进程编号），确定本进程需要加载的权重区间，完成分片权重的加载。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>注：代码实现中，会将 gate 矩阵与 up 矩阵进行合并加载到显存中，因此实际加载流程会在此基础上增加几步额外步骤。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.1.3. 构造树形结构示例代码</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>下面 demo 代码展示多个有层级的对象如何通过特殊方法 __setattr__ 构造树形结构.</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> MiniModule:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> <span style=\"color: rgba(128, 0, 128, 1);\">__init__</span>(self, name=<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">root</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n        self._name </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> name\n        self._modules </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> {}\n        self._parameters </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> {}\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> <span style=\"color: rgba(128, 0, 128, 1);\">__setattr__</span><span style=\"color: rgba(0, 0, 0, 1);\">(self, name, value):\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span><span style=\"color: rgba(0, 0, 0, 1);\"> isinstance(value, MiniModule):\n            self._modules[name] </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> value\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">elif</span> name.endswith(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n            self._parameters[name] </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> value\n\n        super().</span><span style=\"color: rgba(128, 0, 128, 1);\">__setattr__</span><span style=\"color: rgba(0, 0, 0, 1);\">(name, value)\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">def</span> get_all_paths(self, prefix=<span style=\"color: rgba(128, 0, 0, 1);\">\"\"</span><span style=\"color: rgba(0, 0, 0, 1);\">):\n        </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span><span style=\"color: rgba(128, 0, 0, 1);\">递归遍历并收集所有参数的完整路径</span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\n        paths </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> []\n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 1. 先收集当前层级的参数路径</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">for</span> p_name <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> self._parameters:\n            full_path </span>= f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{prefix}.{p_name}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> <span style=\"color: rgba(0, 0, 255, 1);\">if</span> prefix <span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\"> p_name\n            paths.append(full_path)\n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> 2. 递归进入子模块，传递更新后的前缀</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">for</span> m_name, m_obj <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> self._modules.items():\n            new_prefix </span>= f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">{prefix}.{m_name}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span> <span style=\"color: rgba(0, 0, 255, 1);\">if</span> prefix <span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\"> m_name\n            paths.extend(m_obj.get_all_paths(new_prefix))\n\n        </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span><span style=\"color: rgba(0, 0, 0, 1);\"> paths\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> q_weight_loader():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">this is q_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> down_weight_loader():\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">this is down_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 构造树形结构 ---</span>\nmodel = MiniModule(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Qwen3</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nmodel.layers </span>= MiniModule(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Layers</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nmodel.layers.attention </span>= MiniModule(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">Attention</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nmodel.layers.attention.q_weight_loader </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> q_weight_loader\nmodel.layers.mlp </span>= MiniModule(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">MLP</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nmodel.layers.mlp.down_weight_loader </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> down_weight_loader\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 打印所有路径 ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">遍历模型的所有参数路径：</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nall_paths </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> model.get_all_paths()\n</span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> path <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> all_paths:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">路径: {path}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n</span><span style=\"color: rgba(0, 128, 0, 1);\">#</span><span style=\"color: rgba(0, 128, 0, 1);\"> --- 模拟 nano-vllm 的访问逻辑 ---</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">def</span><span style=\"color: rgba(0, 0, 0, 1);\"> mock_get_parameter(root, path):\n    parts </span>= path.split(<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">.</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    curr </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> root\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">for</span> part <span style=\"color: rgba(0, 0, 255, 1);\">in</span> parts[:-1<span style=\"color: rgba(0, 0, 0, 1);\">]:\n        curr </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> curr._modules[part]\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">return</span> curr._parameters[parts[-1<span style=\"color: rgba(0, 0, 0, 1);\">]]\n\ntarget </span>= <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">layers.attention.q_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>\n<span style=\"color: rgba(0, 0, 255, 1);\">print</span>(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">\\n模拟查找路径 '{target}':</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\nloader </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> mock_get_parameter(model, target)\nloader()</span></pre>\n</div>\n<p><span>输出结果：</span></p>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">遍历模型的所有参数路径：\n路径: layers.attention.q_weight_loader\n路径: layers.mlp.down_weight_loader\n\n模拟查找路径 </span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(128, 0, 0, 1);\">layers.attention.q_weight_loader</span><span style=\"color: rgba(128, 0, 0, 1);\">'</span><span style=\"color: rgba(0, 0, 0, 1);\">:\nthis is q_weight_loader</span></pre>\n</div>\n<h2><span>9.2. 多 GPU 之间的计算协同</span></h2>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.2.1. 功能细节</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>nano-vllm 只考虑单机内的多 GPU 协同，协同过程如下：</span></div>\n</div>\n<div class=\"Image-captionContainer\">\n<div>\n<div class=\"Image-resizerContainer css-ym3v7r\">\n<div class=\"css-79elbk\">\n<div>\n<div class=\"ImageDelete-Container css-xi606m\">\n<div class=\"ImageDelete-Wrapper css-1gomreu\" style=\"text-align: center;\"><img class=\"Image FocusPlugin--unfocused Image--isBlock css-1phd9a0\" height=\"351\" src=\"https://pic1.zhimg.com/80/v2-7f054d91f4badab37cab087059b847da_1440w.png?source=ccfced1a\" width=\"500\" /></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div style=\"text-align: center;\">图 5</div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（1）进程隔离与独立加载： 采用多进程模式，一个 GPU 对应一个独立进程。各进程并发读取权重文件，并根据自己的 tp_rank 按照预设的切分策略（如 ColumnParallel 的行切分或 RowParallel 的列切分），将属于自己的那部分数据从 safetensors 加载到显存中。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（2）控制面协同（Control Plane）： Rank 0 负责全局调度，通过共享内存将推理请求（Tokens、Sampling Params 等）同步给其他 Rank。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>（3）数据面通信（Data Plane）： 在矩阵计算的关键节点，利用通信原语（如 all-reduce 处理分块求和、all-gather 处理序列拼接）完成张量并行的结果汇总，使分布在不同显卡上的计算结果在数学上等价于单卡计算。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<h3 class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>9.2.2. 示例代码</span></h3>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>下面写一个简单的数据通信例子：</span></div>\n</div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\">\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 0, 1);\">import torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport </span><span style=\"color: rgba(0, 0, 255, 1);\">time</span><span style=\"color: rgba(0, 0, 0, 1);\">\n\ndef setup(rank, world_size):\n    dist.init_process_group(\n        </span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">nccl</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">tcp://127.0.0.1:2333</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>, world_size=world_size, rank=<span style=\"color: rgba(0, 0, 0, 1);\">rank\n    )\n    torch.cuda.set_device(rank)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef test_all_reduce(rank, world_size):\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"\n</span>    All-<span style=\"color: rgba(0, 0, 0, 1);\">Reduce 子函数：将所有 GPU 的数据进行归约操作（求和），结果同步到所有 GPU\n\n    使用场景：\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> RowParallelLinear 的输出聚合\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> 需要所有 GPU 都得到相同的聚合结果\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"\n</span>    print(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] ===== All-Reduce 示例 =====</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> rank == <span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        data </span>= torch.tensor([<span style=\"color: rgba(128, 0, 128, 1);\">1.0</span>, <span style=\"color: rgba(128, 0, 128, 1);\">2.0</span>], device=f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda:{rank}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        data </span>= torch.tensor([<span style=\"color: rgba(128, 0, 128, 1);\">3.0</span>, <span style=\"color: rgba(128, 0, 128, 1);\">4.0</span>], device=f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda:{rank}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Reduce before: {data}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    dist.all_reduce(data, op</span>=<span style=\"color: rgba(0, 0, 0, 1);\">dist.ReduceOp.SUM)\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Reduce after:  {data}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\ndef test_all_gather(rank, world_size):\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"\n</span>    All-<span style=\"color: rgba(0, 0, 0, 1);\">Gather 子函数：收集所有 GPU 的数据到每个 GPU 上\n\n    使用场景：\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> VocabParallelEmbedding 的输出收集\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> ParallelLMHead 的 logits 收集\n    </span>-<span style=\"color: rgba(0, 0, 0, 1);\"> 需要每个 GPU 都获得所有 GPU 的完整数据\n    </span><span style=\"color: rgba(128, 0, 0, 1);\">\"\"\"\n</span>    print(f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] ===== All-Gather 示例 =====</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    # Rank </span><span style=\"color: rgba(128, 0, 128, 1);\">0</span> 产生 [<span style=\"color: rgba(128, 0, 128, 1);\">10</span>, <span style=\"color: rgba(128, 0, 128, 1);\">20</span>], Rank <span style=\"color: rgba(128, 0, 128, 1);\">1</span> 产生 [<span style=\"color: rgba(128, 0, 128, 1);\">30</span>, <span style=\"color: rgba(128, 0, 128, 1);\">40</span><span style=\"color: rgba(0, 0, 0, 1);\">]\n    local_data </span>=<span style=\"color: rgba(0, 0, 0, 1);\"> torch.tensor(\n        [</span><span style=\"color: rgba(128, 0, 128, 1);\">10.0</span> + rank * <span style=\"color: rgba(128, 0, 128, 1);\">20</span>, <span style=\"color: rgba(128, 0, 128, 1);\">20.0</span> + rank * <span style=\"color: rgba(128, 0, 128, 1);\">20</span>], device=f<span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">cuda:{rank}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">\n    )\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Gather local data: {local_data}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    gathered_list </span>= [torch.zeros_like(local_data) <span style=\"color: rgba(0, 0, 255, 1);\">for</span> _ <span style=\"color: rgba(0, 0, 255, 1);\">in</span><span style=\"color: rgba(0, 0, 0, 1);\"> range(world_size)]\n    dist.all_gather(gathered_list, local_data)\n\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Gather result: {gathered_list}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\n    gathered_tensor </span>= torch.<span style=\"color: rgba(0, 0, 255, 1);\">cat</span>(gathered_list, dim=<span style=\"color: rgba(128, 0, 128, 1);\">0</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">[Rank {rank}] All-Gather concatenated: {gathered_tensor}</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n\ndef tp_demo(rank, world_size):\n    setup(rank, world_size)\n\n    test_all_reduce(rank, world_size)\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">time</span>.<span style=\"color: rgba(0, 0, 255, 1);\">sleep</span>(<span style=\"color: rgba(128, 0, 128, 1);\">5</span><span style=\"color: rgba(0, 0, 0, 1);\">)\n    test_all_gather(rank, world_size)\n\n    cleanup()\n\ndef run_demo():\n    world_size </span>= <span style=\"color: rgba(128, 0, 128, 1);\">2</span><span style=\"color: rgba(0, 0, 0, 1);\">\n    mp.spawn(tp_demo, args</span>=(world_size,), nprocs=world_size, <span style=\"color: rgba(0, 0, 255, 1);\">join</span>=<span style=\"color: rgba(0, 0, 0, 1);\">True)\n\n</span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> __name__ == <span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">__main__</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">if</span> torch.cuda.device_count() &gt;= <span style=\"color: rgba(128, 0, 128, 1);\">2</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        run_demo()\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">else</span><span style=\"color: rgba(0, 0, 0, 1);\">:\n        print(f</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span><span style=\"color: rgba(128, 0, 0, 1);\">需要至少 2 张 GPU 来运行此 TP 示例</span><span style=\"color: rgba(128, 0, 0, 1);\">\"</span>)</pre>\n</div>\n<h1><span>10. 其他</span></h1>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>本文基于 nano-vllm 项目，聚焦讲解大模型推理加速领域中最基础的若干核心技术点。需要说明的是，大模型推理加速的技术体系十分丰富，本项目并未覆盖全部内容，例如：计算通信重叠（Overlap）、多 token 预测（MTP，Multi‑Token Prediction）、多流、多进程服务（MPS，Multi-Process Service）、数据并行（DP）、流水线并行（PP）、上下文并行（CP）、专家并行（EP）以及 PD 分离等进阶技术方向，可作为后续深入学习的拓展内容。</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>&nbsp;</span></div>\n</div>\n<div class=\"Editable-unstyled\">\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>注1：在本文的总结过程中，除了查看源代码，也借助 deepwiki 以及其他 AI 工具辅助。&nbsp;<br /><br /></span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>本文:&nbsp;<a href=\"https://www.cnblogs.com/cswuyg/p/19471225\" target=\"_blank\">https://www.cnblogs.com/cswuyg/p/19471225</a></span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>知乎:&nbsp;<a href=\"https://zhuanlan.zhihu.com/p/1989806890381746916\" rel=\"noopener nofollow\" target=\"_blank\">https://zhuanlan.zhihu.com/p/1989806890381746916</a></span></div>\n<div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>公众号:&nbsp;<a href=\"https://mp.weixin.qq.com/s/6mAZ49iP1SCKt5ZdWf6ErQ\" rel=\"noopener nofollow\" target=\"_blank\">https://mp.weixin.qq.com/s/6mAZ49iP1SCKt5ZdWf6ErQ</a></span></div>\n\n\n\n</div>\n</div>\n<div class=\"clear\"></div>\n\n\t<div class=\"postDesc\">posted on \n<span id=\"post-date\">2026-01-12 12:38</span>&nbsp;\n<a href=\"https://www.cnblogs.com/cswuyg\">-银光-</a>&nbsp;\n阅读(<span id=\"post_view_count\">94</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "上周热点回顾（1.5-1.11）",
      "link": "https://www.cnblogs.com/cmt/p/19469697",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/cmt/p/19469697\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 08:38\">\n    <span>上周热点回顾（1.5-1.11）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p>热点随笔：</p>\n<p> · <a href=\"https://www.cnblogs.com/knqiufan/archive/2026/01/07/19449849.html\" target=\"_blank\">Claude Code 完全指南：使用方式、技巧与最佳实践</a> (<a href=\"https://www.cnblogs.com/knqiufan/\" target=\"_blank\">knqiufan</a>) <br />\n · <a href=\"https://www.cnblogs.com/shanyou/archive/2026/01/05/19441004.html\" target=\"_blank\">从 TIOBE 2025 年度语言到 2026 年 C# 智能体生态的全面崛起</a>\n(<a href=\"https://www.cnblogs.com/shanyou/\" target=\"_blank\">张善友</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/thisiswhy/archive/2026/01/08/19458678.html\" target=\"_blank\">可怕，看到一个冷血的算法。</a>\n(<a href=\"https://www.cnblogs.com/thisiswhy/\" target=\"_blank\">why技术</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/xueweihan/archive/2026/01/06/19445015.html\" target=\"_blank\">嫌 AI 写的界面太丑？装上这个开源插件，秒变资深设计师</a>\n(<a href=\"https://www.cnblogs.com/xueweihan/\" target=\"_blank\">削微寒</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/shanyou/archive/2026/01/07/19452660.html\" target=\"_blank\">XAML Studio 已正式开源</a>\n(<a href=\"https://www.cnblogs.com/shanyou/\" target=\"_blank\">张善友</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/yupi/archive/2026/01/09/19460549.html\" target=\"_blank\">干掉 Claude Code，这个开源 AI 编程工具杀疯了？</a>\n(<a href=\"https://www.cnblogs.com/yupi/\" target=\"_blank\">程序员鱼皮</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/jinjiangongzuoshi/archive/2026/01/05/19440715.html\" target=\"_blank\">最近很火爆的Claude Skills到底是个啥？解决什么问题？怎么用！</a>\n(<a href=\"https://www.cnblogs.com/jinjiangongzuoshi/\" target=\"_blank\">狂师</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/kklldog/archive/2026/01/07/19449864.html\" target=\"_blank\">为什么说 IO 操作异步才有意义</a>\n(<a href=\"https://www.cnblogs.com/kklldog/\" target=\"_blank\">Agile.Zhou</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/12lisu/archive/2026/01/08/19456855.html\" target=\"_blank\">强烈推荐 | 阿里开源的这10个神级项目</a>\n(<a href=\"https://www.cnblogs.com/12lisu/\" target=\"_blank\">苏三说技术</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/12lisu/archive/2026/01/06/19447172.html\" target=\"_blank\">用雪花算法就不会产生重复的ID?</a>\n(<a href=\"https://www.cnblogs.com/12lisu/\" target=\"_blank\">苏三说技术</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/ymtianyu/archive/2026/01/05/19442584.html\" target=\"_blank\">从安装到上线：一份 Nginx 实战指南，让你的 Web 应用稳建安全</a>\n(<a href=\"https://www.cnblogs.com/ymtianyu/\" target=\"_blank\">一名程序媛呀</a>)                    <br />\n · <a href=\"https://www.cnblogs.com/tianqing/archive/2026/01/05/19439916.html\" target=\"_blank\">.NET 10 New feature 新增功能介绍-WebSocket功能增强</a>\n(<a href=\"https://www.cnblogs.com/tianqing/\" target=\"_blank\">Eric zhou</a>)                    <br />\n            </p>\n<p>热点新闻：</p>\n<p>\n · <a href=\"https://news.cnblogs.com/n/812350/\" target=\"_blank\">Stack Overflow 彻底凉了，比18年前上线首月问题数量还少</a><br />\n · <a href=\"https://news.cnblogs.com/n/812432/\" target=\"_blank\">终于有实锤了：缺少实体按键的车就是更危险</a><br />\n · <a href=\"https://news.cnblogs.com/n/812455/\" target=\"_blank\">“所有人请注意用脑卫生”：打工人最应该戒掉的6件事</a><br />\n · <a href=\"https://news.cnblogs.com/n/812551/\" target=\"_blank\">毫无征兆！DeepSeek R1爆更86页论文，这才是真正的Open</a><br />\n · <a href=\"https://news.cnblogs.com/n/812321/\" target=\"_blank\">Redis宣布闭源后，中国技术人的“上游时刻”</a><br />\n · <a href=\"https://news.cnblogs.com/n/812667/\" target=\"_blank\">Stack Overflow已死？CEO带队狂赚1.15亿刀，6个月原地反杀</a><br />\n · <a href=\"https://news.cnblogs.com/n/812425/\" target=\"_blank\">8天暴涨400万粉丝后塌房，蛋神是保质期最短的网红嘛？</a><br />\n · <a href=\"https://news.cnblogs.com/n/812328/\" target=\"_blank\">2025，这些互联网巨头赢麻了</a><br />\n · <a href=\"https://news.cnblogs.com/n/812333/\" target=\"_blank\">Claude Code、Cursor 都过时了？！硅谷顶流大牛炸场暴论：AI 编程要练满 2000 小时才算“会用”，一年不用世界级大神也会沦为实习生水平</a><br />\n · <a href=\"https://news.cnblogs.com/n/812428/\" target=\"_blank\">中产「自律三件套」，它第一个塌房？</a><br />\n · <a href=\"https://news.cnblogs.com/n/812541/\" target=\"_blank\">马斯克最新播客长叹：中国听懂了我的话，2026年将在算力上碾压世界</a><br />\n · <a href=\"https://news.cnblogs.com/n/812677/\" target=\"_blank\">因为AI编程，Tailwind CSS差点死了</a></p>\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-12 08:38</span>&nbsp;\n<a href=\"https://www.cnblogs.com/cmt\">博客园团队</a>&nbsp;\n阅读(<span id=\"post_view_count\">394</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "万字长文讲解：团队落地 AI 辅助编程和 AI Specs 实战",
      "link": "https://www.cnblogs.com/whuanle/p/19469026",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/whuanle/p/19469026\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 08:34\">\n    <span>万字长文讲解：团队落地 AI 辅助编程和 AI Specs 实战</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        建设使用 AI 辅助编程的团队，使用 AI Specs 规范化团队协作流程和编码规范，让 AI 落地、实现业务价值。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"ai-编程团队协作\">AI 编程团队协作</h2>\n<h3 id=\"历史背景\">历史背景</h3>\n<p>AI 发展的速度实在太快了，在 GPT-3 横空出世的阶段，那个时候只能使用对话框一问一答，到现在各种 RAG、AI Workflow、AI Agent 等各类技术，使得 AI 可以做更多的事情，实现更加强大的功能。在 cursor、trae 等 AI IDE 出现后，很多人便彻底迷上了这项足以颠覆传统工作模式的技术，尤其在编程领域，这场变革的浪潮来得尤为迅猛 —— 最初，当我们面对复杂的业务功能、晦涩的语法实现或是陌生的框架调用时，无需再耗费大量时间翻阅文档、调试代码，只需通过自然语言向 AI 描述需求，就能借助它的问答式回复获取可用的代码片段，这不仅大大降低了编程的门槛，更让开发者从重复的基础编码工作中得到了初步解放，也为后续对话式代码生成、Vibe Code 乃至 Spec Code 等更先进的编程范式，埋下了充满无限可能的种子。</p>\n<br />\n<p>目前，主流的 AI 辅助编程主要是 AI Agent，在 AI IDE 中输入要做的功能，AI 会自动读取项目文件，经过思考后编写对应的代码，并且会自动修正代码错误，确保代码可以编译通过。这便是大语言模型催生的对话式 AI 代码生成，开启了自然语言交互生成代码的时代，开发者通过多轮对话让 AI 产出代码片段并手动整合，AI 仅作为辅助工具；随后演进的 Vibe Code（氛围编程）范式，让开发者只需描述高阶意图即可驱动 AI 完成全量代码的编写与迭代，人类角色从编码者转向需求引导者与测试者，聚焦快速原型与创意验证。</p>\n<p><img alt=\"image-20260109185650106\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce1adf62.png\" /></p>\n<br />\n<p>但是，AI Agent 模式下的 AI 辅助编程，也存在一些常见的问题，所以在 IT 圈流行着这图：</p>\n<p><img alt=\"896b42e496e1d780c1b2ba5685065dc5\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce1d759f.\" /></p>\n<blockquote>\n<p>感谢热心网友【迪迦】提供的图片。</p>\n</blockquote>\n<br />\n<p>虽然上图有搞笑成分，这里我们将问题分为大语言模型和 AI IDE 两部分，下面介绍常见的 AI 辅助编程问题。</p>\n<p>大语言模型：</p>\n<ul>\n<li><strong>目标漂移</strong>：在多步骤、长流程的编程任务中（如复杂功能开发、项目重构），AI 执行到中后阶段易偏离初始目标，例如原本需求是优化数据库查询性能，后续却无端重写 UI 组件，本质是缺乏有效的目标锚定机制。</li>\n<li><strong>重复犯错</strong>：对已出现过的错误缺乏长期记忆，相同问题会反复出现（如未正确处理异步函数的 <code>await</code> 关键字、文件路径引用错误），无法自动沉淀历史解决方案。</li>\n<li><strong>上下文爆炸</strong>：为让 AI 掌握完整项目信息，需将大量代码、需求文档、历史对话塞进上下文窗口，导致模型处理效率下降、响应延迟，且易忽略关键信息。</li>\n<li><strong>进度丢失</strong>：依赖对话窗口存储任务状态，一旦对话重置、刷新或中断，之前的开发进度、中间决策、已积累的项目认知会全部消失，无法无缝续工。</li>\n<li><strong>幻觉生成</strong>：在缺乏明确参考依据时，可能编造不存在的 API 方法、语法规则或项目配置，生成看似合理但实际无法运行的代码，增加调试成本。</li>\n</ul>\n<p><br />AI IDE：</p>\n<ul>\n<li><strong>上下文管理能力不足</strong>：多数 AI IDE 未建立独立的外部记忆机制，仍依赖模型自带的上下文窗口，无法高效关联项目文件、历史操作记录，难以支撑复杂任务的连贯开发。</li>\n<li><strong>任务追踪与可视化缺失</strong>：缺乏对编程任务的结构化拆解与进度可视化功能，无法像 <code>task_plan.md</code> 那样清晰呈现阶段划分、完成状态、决策依据，开发者难以掌控 AI 的工作轨迹。</li>\n<li><strong>错误记录与复用薄弱</strong>：未集成错误追踪体系，AI 遇到的报错、修复方案无法自动归档，既不便于开发者回溯问题根源，也无法让 AI 后续快速复用已验证的解决方案。</li>\n<li><strong>文件协同与持久化不足</strong>：对 AI 生成的中间产物（如调研笔记、技术选型文档）缺乏统一的存储与管理机制，文件分散且易丢失，无法形成可追溯、可复用的项目知识库。</li>\n<li><strong>人机协作衔接不畅</strong>：开发者难以直接干预 AI 的任务执行流程，例如无法通过编辑结构化计划文件调整开发方向，也缺乏便捷的方式审查 AI 的决策逻辑，导致人机协同效率低下。</li>\n<li><strong>性能与成本失衡</strong>：处理大规模项目时，频繁加载全量上下文会导致 IDE 运行卡顿，且重复处理相同静态内容（如项目框架定义、工具配置）会增加 Token 消耗，提升使用成本。</li>\n</ul>\n<br />\n<p>所以，在目前的技术局限下，要玩 AI 编程，其实还不能为所欲为，同时使用 AI 编程时也会碰到很多阻碍问题，降低了编程的体验和项目代码质量。</p>\n<h3 id=\"ai-编程下的团队协作痛点与核心诉求\">AI 编程下的团队协作痛点与核心诉求</h3>\n<p>前面介绍了 AI 编程本身容易出现的问题和技术局限，回到以团队为单位进行 AI 编程协作时，当团队缺乏标准化协作机制时，AI 编程易陷入 “单兵作战” 的困境，会出现更加多的头疼的问题。</p>\n<p>因为笔者在公司带一个项目组，已经有很多成员使用 AI 写代码，在经过一段时间的观察和思考后，发现了一些问题，所以才想到写这篇文章的，相信这些问题在大家的公司里面也会存在。</p>\n<br />\n<p><strong>代码碎片化</strong></p>\n<blockquote>\n<p>很多同事发现用 AI 写代码，可以早点下班，于是大家都用 AI 写代码，你写你的我写我的，导致成员各自使用 AI 生成代码，风格、逻辑差异显著，模块衔接困难，后期维护成本激增。反正同事用 AI 写的代码，我是压根不想维护。</p>\n</blockquote>\n<br />\n<p><strong>规范失控</strong></p>\n<blockquote>\n<p>大家应该都有感受，在不同提示词、编写不同功能时，AI 编写的代码风格各异，没有统一的规范和风格，AI 生成代码可能偏离团队代码规范、安全标准，埋下质量隐患。</p>\n</blockquote>\n<br />\n<p><strong>知识孤岛</strong></p>\n<blockquote>\n<p>个人使用 AI 积累的经验无法共享，团队整体效率难以提升。你用 cursor，我用 kiro，各写各的代码，没法为团队沉淀知识经验，提示词、功能历史记录、背景上下文等完全没法在团队内共享，每个人都得在本地使用 AI 阅读代码生成上下文记忆。</p>\n</blockquote>\n<br />\n<p><strong>协作低效</strong></p>\n<blockquote>\n<p>AI 写的代码实际上会出现一个简单的功能写一堆代码的情况，也有可能 A 同事写 A 功能时生成了 C，B 同事写 B 功能时又生成了一个类似的 C，项目各种代码错综复杂。导致难以统一任务分配、进行代码评审流程，导致信息传递滞后，冲突频发。</p>\n</blockquote>\n<br />\n<p>除了以上问题在团队编程中需要解决，很多公司都有代码质量规范要求、安全要求、单元测试覆盖率要求，所以基于这些问题和常见公司编程要求，笔者认为团队 AI 编程的核心诉求应聚焦：</p>\n<ul>\n<li>维持一致代码质量</li>\n<li>防止安全漏洞</li>\n<li>减少重复工作量</li>\n<li>标准化协作流程</li>\n<li>加快开发周期</li>\n<li>实现从 “快速原型” 到 “工程化落地” 的跨越。</li>\n</ul>\n<br />\n<p>很多领导对 AI 编程的想法很难评，觉得可以 ”降本增效“ ，一个项目平时需要一个月做完，用 AI 一天就行，代码还能写得比开发还好，这样可以开除很多开发人员，所以号召大家在公司使用 AI 编程。</p>\n<blockquote>\n<p>后面会提到 AI 辅助编程大约可以提高多少速度，但不可能是一天写完一个月的代码。</p>\n</blockquote>\n<p><br />而实际上，除非原因开盲盒，产品做得怎么样取决于 AI 怎么写，前期速度确实很快。但是到了中后期，AI 写的代码难以维护，反而会因为要 AI 写代码，花费大量时间编写提示词，为了实现一个小需求，可能要跟 AI 一直对骂，最终陷入混乱。</p>\n<p>其实，正如笔者前面所说的，大语言模型和 AI IDE 在目前有一些技术局限，而且在团队使用 AI 编程时会带来很多协作问题，如果没有应对这些问题的解决方法，那么使用 AI 编写的代码最终可能是一片混乱，而且生成代码人类可能无法阅读，鬼才知道 AI 写的是啥。</p>\n<p>公司落地 AI 编程，确实可以提高开发速度，缩短开发周期，但是不应该荒谬到认为 ”一个月工作量使用 AI 一天就行“ 。笔者认为，除了提速，应当更多聚焦实现前面提到的 AI 编程的核心诉求。</p>\n<br />\n<p>很多公司天天鼓吹单元测试、代码规范、高性能高质量代码，给开发指定单元测试覆盖率和代码审查等指标，反复折腾开发人员，但是公司存在各种各样的开发管理和技术问题，盲目定制高要求的开发指标，完全解决不了当前的问题，又会使得开发身心疲倦。</p>\n<p>笔者之所以要说这些，是因为代码审查、代码规范、单元测试等，做起来没有那么简单，有多少公司折腾这些，又能最终真正做到？</p>\n<blockquote>\n<p>能不能做好这些，其实跟公司基因有关系，跟落地难度也有关系。</p>\n</blockquote>\n<p><br />其实，只有足够简单，规范才能真正落地。</p>\n<p>所以，笔者一直在思考，如何解决 AI 编程下的团队协作痛点与满足核心诉求，并且真正落地单元测试、代码审查等技术要求。</p>\n<p>这就是本文的重点，AI Spec，到底要怎么做，接下来本文将以架构师的角度，去思考，去研究，我们是架构师，那么应该怎么把 AI 编程落地到团队协作中。</p>\n<h2 id=\"规范开发spec-development\">规范开发(Spec development)</h2>\n<p>前面提到，AI 辅助编程逐渐成为主流，大部分开发人员已经在日常工作中使用 AI 编写代码，但是也会引入很多问题，导致生成很多不可预测的、质量参差不齐的代码。那么为了解决这些问题，出现了 SDD(Spec-Driven Development) 这种概念，强调在使用 AI 编写代码之前，先有 Specification，以便约束 AI 生成高质量的、符合业务需求和技术要求的代码。</p>\n<p>目前社区中主要存在三类 SDD 工具：OpenSpec、Kiro、Spec-Kit。</p>\n<p><br />为了实现 Spec development，选择 SDD 工具时，需要考虑：</p>\n<ul>\n<li>\n<p><strong>工具与流程适配</strong>：选择 Kiro 这类支持规范定制、多场景协作的 AI 工具，避免工具功能与团队流程脱节；</p>\n</li>\n<li>\n<p><strong>重视规范落地</strong>：将团队规则转化为可执行的钩子、Spec 模板，借助 AI 强制落地，而非仅停留在文档层面；</p>\n</li>\n<li>\n<p><strong>构建协作文化</strong>：鼓励成员主动共享 AI 使用经验、参与流程优化，避免 “单兵作战” 思维；</p>\n</li>\n<li>\n<p><strong>平衡自动化与人工</strong>：AI 负责标准化、重复性工作（如测试、规范检查），人类聚焦核心架构与创意决策，实现人机协同增效。</p>\n</li>\n</ul>\n<br />\n<p>在本文，笔者要讲解的是 Kiro 落地的内容。</p>\n<p><br />在搜索资料的过程中，笔者发现几篇写得不错的文章，这里供读者参考，本文就不单独讲解这些 SDD 工具了。</p>\n<blockquote>\n<p>AI 规范驱动开发“三剑客”深度对比：Spec-Kit、Kiro 与 OpenSpec 实战指南 - 技术栈：</p>\n<p><a href=\"https://jishuzhan.net/article/1988226029513670657\" rel=\"noopener nofollow\" target=\"_blank\">https://jishuzhan.net/article/1988226029513670657</a></p>\n<p>Transforming Dev Practices with Kiro’s Spec-Driven Tools | AI Native Dev</p>\n<p><a href=\"https://ainativedev.io/transforming-dev-practices-with-kiros-spec-driven-tools\" rel=\"noopener nofollow\" target=\"_blank\">https://ainativedev.io/transforming-dev-practices-with-kiros-spec-driven-tools</a></p>\n</blockquote>\n<br />\n<p>这里简单介绍一下 Kiro。</p>\n<p>Kiro 是亚马逊云科技推出的 AI IDE，以 “规范驱动开发” 为核心，完美适配团队协作需求。</p>\n<blockquote>\n<p>对国内用户友好，无需特殊网络配置。</p>\n</blockquote>\n<p><img alt=\"img\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce20c847.png\" /></p>\n<h3 id=\"团队对齐ai-编程核心概念解析\">团队对齐：AI 编程核心概念解析</h3>\n<p>在深入团队协作实践前，需先明确支撑 AI 编程的关键技术概念，无论是产品还是测试，也必须了解 LLM、MCP、Agent 等概念，需要团队对一些知识都过关，确保整个团队交流协作不存在知识障碍。</p>\n<blockquote>\n<p>这些概念的知识，笔者就不赘述了。</p>\n</blockquote>\n<br />\n<p>主流大语言模型用于编程的评分，作为架构师需要清晰了解不同模型的特长和优缺点，在编程领域哪个模型最优秀最适合用于编程。</p>\n<blockquote>\n<p>笔者发现几个网站很不错，有各类模型的介绍和评分，地址：</p>\n<p><a href=\"https://apxml.com/zh/leaderboards/coding-llms\" rel=\"noopener nofollow\" target=\"_blank\">Best LLMs for Coding | LLM Leaderboards</a></p>\n<p><a href=\"https://llm-stats.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://llm-stats.com/</a></p>\n<p><a href=\"https://www.aibase.com/zh\" rel=\"noopener nofollow\" target=\"_blank\">https://www.aibase.com/zh</a></p>\n</blockquote>\n<p><img alt=\"image-20260109204244573\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce239a78.png\" /></p>\n<br />\n<p>主流企业使用 AI 辅助编程提升的效率。</p>\n<p>要是领导觉得你一个月写的代码不如 AI 一天写的，那就尴尬了。作为技术人员，必须合理评估使用 AI 编程后，整个团队到底提升了多少效率，编码速度提高了多少。</p>\n<p>比如，根据这篇研究报告，完成任务的编码速度大概提升了 <code>21%</code>，报告地址 <a href=\"https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/\" rel=\"noopener nofollow\" target=\"_blank\">https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/</a></p>\n<p>而根据豆包联网搜索总结的内容来看，提升的开发速度并没有那么夸张。</p>\n<p><img alt=\"image-20260109205849267\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce35ec28.png\" /></p>\n<br />\n<p>总而言之，打造 AI 辅助编程团队，要求团队人员对 AI 有足够的认识，了解基础知识，并且争取在 AI 编程上的认知一致，否则可能会闹出各种笑话，被人引为笑谈。<strong>知识和认知都很重要</strong>，否则团队分分钟散架。</p>\n<h3 id=\"项目模板\">项目模板</h3>\n<p>为什么要使用开发框架，这个事情不用多说，C# 开发应该知道 ABP 框架，Go 开发应该知道 gin、beego，使用框架后大大简化了开发负担。在使用开发框架的情况下，其实团队还需要定制项目模板，以便适配公司技术栈，以及约束公司内的一些技术要求，例如审计属性、微服务通讯方式和鉴权等，好的项目模板可以统一开发模式和习惯。</p>\n<br />\n<p>笔者在写个人开源项目过程中，结合 DDD 、清洁架构、CQRS 的一些概念，辅以 AI 编程，形成了个人开发习惯，所以为了写这篇文章整理了一个模板项目：</p>\n<p><a href=\"https://github.com/whuanle/aispec\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/whuanle/aispec</a></p>\n<br />\n<p>如果以我的开发习惯来定，我会使用模块化 + CQRS，每个业务领域遵循三层模块化架构，无论开发还是做单元测试，都比较简单。</p>\n<pre><code>src/{domain}/\n├── MoAI.{Domain}.Shared/     # 共享层 - DTO、Command、Query 定义\n├── MoAI.{Domain}.Core/       # 核心层 - Handler 实现、业务逻辑\n└── MoAI.{Domain}.Api/        # API 层 - Controller/Endpoint 暴露\n</code></pre>\n<p>依赖关系：<code>Api → Core → Shared</code>，具体参考：</p>\n<p>代码编写约束参考：<a href=\"https://github.com/whuanle/aispec/blob/master/.kiro/steering/cqrs-conventions.md\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/whuanle/aispec/blob/master/.kiro/steering/cqrs-conventions.md</a></p>\n<p><img alt=\"image-20260109211559319\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce3883d6.png\" /></p>\n<p><img alt=\"image-20260109211703315\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce3ac8c4.png\" /></p>\n<br />\n<p>具体设计思路和各类细节就不讲解，本节其实目的是说，无论你使用何种开发框架，是否使用 DDD，还是传统三层结构，团队应该要有一个统一的项目模板以及开发习惯约束，AI 会参考项目已有代码和约束文件，编写符合要求的代码，避免代码天马行空。</p>\n<p>关于模板项目就不展开说了，读者可以在这里找到这个本身用于实战演示的模板项目：<a href=\"https://github.com/whuanle/aispec\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/whuanle/aispec</a></p>\n<h3 id=\"准备定制团队规则与项目索引\">准备定制团队规则与项目索引</h3>\n<p>在使用 AI 编程时，会有一些常见问题：</p>\n<p>1，开发跟 AI 容易陷入盲目对话，AI 始终没有给出想要的代码，导致代码可用率和确定性低下。</p>\n<p>2，开发难以清晰准确向 AI IDE表达目标方案和代码</p>\n<p>3，前置需要大量精力对项目的 Rules、Docs 进行编写和积累</p>\n<p>4，啥都让 AI 写，但是 AI 写出来的方案总是不满意</p>\n<br />\n<p>解决这些问题本身不难，但是要落到团队协作中，作为架构师就要思考更多了，需要保证代码质量和开发效率，避免反复处理这些问题。</p>\n<br />\n<h4 id=\"什么是-steering\">什么是 Steering</h4>\n<p>这里的 steering 其实是 Kiro 里面的概念，目的是让 AI 编程过程中，始终遵循团队已建立的 patterns、libraries、standards。</p>\n<blockquote>\n<p><a href=\"https://kiro.dev/docs/steering/\" rel=\"noopener nofollow\" target=\"_blank\">https://kiro.dev/docs/steering/</a></p>\n</blockquote>\n<br />\n<p>也就是说，基于团队的项目模板，我们要指定一些跟业务无关的技术约束，例如审计属性怎么定，这个开发框架/项目模板怎么使用，不同功能的代码文件怎么命名等，都可以写到 Kiro steering 里面。AI 在写代码时，会一直围绕这些 steering 去写，如果写出的代码不符合 steering 要求，则 AI 会逐步修正代码。</p>\n<br />\n<p>那么，团队要落地代码规范，是不是变简单了？前面提到的 AI 编程的问题和核心诉求，是不是可以解决了？</p>\n<p>是的，所以，要重视项目模板和 steering，在团队开发实现需求之前，就应该定制团队乃至公司研发部门的 steering。</p>\n<h4 id=\"常见-steering-文件策略\">常见 Steering 文件策略</h4>\n<p>后面会提到 Kiro 默认会有的 steering 规则模板，但是从团队的角度考虑，可能还需要这些 steering ：</p>\n<p><strong>API 标准</strong> (<code>api-standards.md</code>) - 定义 REST 约定、错误响应格式、认证流程和版本策略。包括端点命名模式、HTTP 状态码使用和请求/响应示例。</p>\n<p><strong>测试方法</strong> (<code>testing-standards.md</code>) - 建立单元测试模式、集成测试策略、模拟方法和覆盖率期望。记录首选测试库、断言样式和测试文件组织。</p>\n<p><strong>代码风格</strong> (<code>code-conventions.md</code>) - 指定命名模式、文件组织、导入排序和架构决策。包括首选代码结构、组件模式和要避免的反模式示例。</p>\n<p><strong>安全指南</strong> (<code>security-policies.md</code>) - 记录认证要求、数据验证规则、输入清理标准和漏洞预防措施。包括特定于您应用程序的安全编码实践。</p>\n<p><strong>部署流程</strong> (<code>deployment-workflow.md</code>) - 概述构建程序、环境配置、部署步骤和回滚策略。包括 CI/CD 管道详细信息和环境特定要求。</p>\n<br />\n<p>当然，这些 steering 不需要都首选，可以先做一个项目模板，然后让 Kiro 阅读代码并生成即可。</p>\n<p>不同公司团队可能有不同要求，只要看着写就行，可以在摸索的过程中，逐渐积累团队经验，逐渐完善 steering。</p>\n<h3 id=\"核心执行ai-驱动的协作流程设计\">核心执行：AI 驱动的协作流程设计</h3>\n<p>团队协作 AI 编程，把要做的功能丢在对话框，让 AI 写代码就行？</p>\n<p>No！大错特错！</p>\n<br />\n<p>让我们回到 Kiro 的介绍，<code>Agentic AI development from prototype to production</code>，翻译过来是从原型到产品的 Agentic AI 开发。</p>\n<p>注意，要玩 AI 编程，我们是要做从原型到产品整个周期的东西，而不单单是用 AI 写完代码早点下班。</p>\n<p>所以要考虑，团队的角色有哪些，大家应该怎么参与协作，<strong>更重要的是怎么设计新的团队研发流程</strong>。</p>\n<br />\n<p>不同公司的团队组成不一样，所以笔者按照常规团队（产品、UI设计师、前后端开发、测试）组成来讲解后面的内容。</p>\n<blockquote>\n<p>感谢这位作者的文章，给了我很多想法：<a href=\"https://aicodingtools.blog/zh/kiro/kiro-spec-guide\" rel=\"noopener nofollow\" target=\"_blank\">https://aicodingtools.blog/zh/kiro/kiro-spec-guide</a></p>\n</blockquote>\n<br />\n<p>使用 AI 编程后，团队中各个角色要负责的内容会发生变化，要求工作输出的内容能够被 AI 识别并引用，并且在团队内流通。</p>\n<blockquote>\n<p>如果产品输出的需求文档，AI 都不会分析总结，你给开发看？逻辑狗屁不通，AI 怎么写代码？</p>\n<p>产品的需求文档，可以经过开发整理后，作为让 AI 编写代码的提示词和需求约束，大大减轻开发的负担。</p>\n</blockquote>\n<br />\n<p>开发和测试将会大大依赖需求文档（或其它形式的文档），因为需求文档会作为开发、测试和验收依据，在 AI 编程阶段就要让 AI 知道写代码还要考虑怎么测试和验收，不能等让 AI 写完代码，再叫 AI 写单元测试、检查验收能不能过。</p>\n<p><br />另外，前期准备阶段，也依赖架构师或 leader 的架构设计和功能实现设计，而不是说 ”实现一个短链接服务，将长链接缩短为短链接“，技术负责人需要思考和设计，使用何种算法缩短地址、还原地址，怎么存储到数据库，高并发环境下怎么提高并发量和减少对数据库的压力。</p>\n<br />\n<p>所以，要思考，使用 AI 编程落地后，</p>\n<ul>\n<li>\n<p>团队需要哪些角色？</p>\n</li>\n<li>\n<p>每个角色要负责哪些内容？</p>\n</li>\n<li>\n<p>整个研发流程应该分哪些阶段？</p>\n</li>\n</ul>\n<br />\n<p>每个公司情况都是不一样的，所以针对这些问题，笔者没有什么说法和观点，不过后面实战部分会提到 Kiro Specs 是怎么划分研发流程阶段。</p>\n<br />\n<p>笔者设想，未来围绕 AI 辅助编程开发，团队的角色和参与内容可能发生重大改变。</p>\n<p>一是团队开发流程发生改变，不再像以往从产品原型设计、需求会、开发、提测的模式，而是围绕 AI 编程更加靠敏捷开发模式接近。</p>\n<p>二是团队角色负责的内容发生改变，更多围绕产生的资料能够被 AI 吸收，知识可以沉淀，减少信息流通的难度，以及降低产品、设计师、开发、测试之间的知识和职责边界。</p>\n<p>三是可能会出现以 AI 编程为中心的产品研发一体化平台，无论是产品、设计师、开发、测试，都可以围绕这个平台进行符合自己角色的参与，例如产品编写需求文档和验收文档时，可以借助 AI 平台检测需求是否合理、调整设计、细化需求，转化为开发人员便于阅读的文档。产品、设计师通过需求文档借助 AI 平台快速实现产品原型设计。开发人员则可以将需求导入 AI，创建开发任务，逐步与 AI 协作编写代码，还可以借助 AI 快速生成单元测试、集成测试等，最后根据验收文档验证检测最终输出。</p>\n<br />\n<p>而关于团队的研发流程，则会在 <a href=\"#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B\" rel=\"noopener nofollow\">工作流程</a> 一节详细讲解。</p>\n<h3 id=\"总结\">总结</h3>\n<p>作为文章的第二部分，主要是思考怎么建设使用 AI 辅助编程的团队，以及进行团队协作。</p>\n<p>第三部分将会讲解实战，从原型到产品的整个环节，每个步骤应该做什么。</p>\n<h2 id=\"实战\">实战</h2>\n<p>作为文章的第三部分内容，将会以短链接服务为案例，去讲解如何落地实战团队协作和 AI 编程，去设计一个以 AI 辅助开发为核心的项目研发流程。</p>\n<h3 id=\"思考整体架构\">思考整体架构</h3>\n<p>AI 时代，还需要我去设计项目？</p>\n<p>是的，开发人员要自行对技术方案进行调研、上手尝试，然后让 AI 根据你的方案和设计做出来，不要都让 AI 帮你做解决方案。</p>\n<br />\n<p>只需要一个 idea，AI 就可以写代码，可是 AI 写出来的东西可能跟专业人员需要的东西相去甚远，并且还有大量细节满足不了需求，开发人员可能会陷入跟 AI 的对骂中，不断调整提示词，不断等待 AI 生成，最终生成了 90% 的内容，剩下的 10%，开发人员只能对着键盘一点点敲完。</p>\n<p><br />虽然是 AI 编程，但是编程的重点是人的想法和需求，而不是 AI 的天马行空，虽然 AI 实现的项目可能有很多很好的创意和想法，质量可能很好，但是对于专业领域和公司业务项目来说，需求的中心是公司对应的业务，而不是一个 idea。</p>\n<p>而且 AI 有上下文限制，有幻觉等，你只需要一个简单的一个登录功能，可能 AI 把单点登录、OAuth2 等一堆东西给你加上去了（目标偏移）。</p>\n<br />\n<p>所以即使在 AI 时代，我们也要构思项目设计和功能实现的算法和逻辑，让 AI 在这个边界内实现代码和测试。</p>\n<blockquote>\n<p>当然，我们可以利用 AI 去帮助我们验证想法和构思设计，然后将这些内容生成为架构设计和技术方案。</p>\n</blockquote>\n<h4 id=\"短链接服务的架构\">短链接服务的架构</h4>\n<p>由于本文的重点不是怎么实现一个短链接服务，所以笔者这里只简单讲解这个服务的一些算法和实现思路，以便后续使用 AI 写出符合需求的代码。</p>\n<br />\n<p>核心问题1：短链接生成和还原</p>\n<p>核心问题2：短链接存储和查找</p>\n<br />\n<p>笔者的设计是，短链接跟长链接无直接映射关系，也就是不能通过算法转换直接将长链接生成短链接。</p>\n<p>对于每个长链接，创建记录存到数据库时会使用 int64 雪花做主键，存到数据库。</p>\n<p>例如新增一个长链接 <code>https://whuanle.cn</code>，当前雪花id 是 2009194627277520896，经过检查数据库没有重复数据后存储到数据库。</p>\n<p>接着，将雪花 id 使用 base62 生成短链接编码。之所以使用 base62 做缩短编码，是因为<code>[0-9]</code>、<code>[a-z]</code>、<code>[A-Z]</code> 刚刚好是 62 个字符，能够在不使用特殊符号的情况下，使用数字和大小写字母表达值，也就是相当于 62 进制。将 2009194627277520896 使用 base62 编码是得出字符串是 <code>00E3uWKkzx</code>，而存数据只需要一个 int64 就行。</p>\n<p>满足能够将长链接生成短链接，存储空间小，不容易被人碰撞规则的需求。</p>\n<p><img alt=\"image-20260108174726106\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce3d2b7d.png\" /></p>\n<br />\n<p>有了短链接生成和还原算法后，我们继续聊一下数据查找。</p>\n<p>用户访问 <code>/00E3uWKkzx</code> 时，还原得到 2009194627277520896，然后通过这个 id 从数据库查找数据得到  <code>https://whuanle.cn</code>，然后让用户跳转到这个地址即可。</p>\n<p>每次都要查数据库，数据库压力大，而且万一被人攻击，机器人随机拼接的字符串，也要到数据库检查数据在不在，数据库迟早被打崩。</p>\n<br />\n<p>所以第一层是使用 redis 的布隆过滤器，先判断数据是否存在。</p>\n<p><img alt=\"image-20260108180240039\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce4030ea.png\" /></p>\n<br />\n<p>第二层，数据分片，将数据存储到 redis，避免频繁从数据库查找。还可以做本地离线缓存，避免高频度从 redis 查找数据。</p>\n<p>也就是最终是三层缓存。</p>\n<br />\n<p>其它就不多说了，前面提到的算法处理逻辑，将会作为后续 AI 编写功能的依据。</p>\n<h3 id=\"模板项目\">模板项目</h3>\n<p>首选是安装笔者提供的项目模板。</p>\n<pre><code class=\"language-bash\">dotnet new install Maomi.AiSpec.Templates\n</code></pre>\n<p><img alt=\"image-20260109092342372\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce430090.png\" /></p>\n<br />\n<p>执行命令从模板创建新的项目，将 <code>MyShortUri</code> 替换为你需要的项目名称即可。</p>\n<pre><code class=\"language-bash\">dotnet new aispec -n MyShortUri\n</code></pre>\n<p><img alt=\"image-20260109092658516\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce458366.png\" /></p>\n<h3 id=\"后端开发原则和规范\">后端开发原则和规范</h3>\n<p>使用 Kiro 打开 <code>MyShortUri</code> 项目， 在 AGENT STEERING 菜单可以看到已经模板自带四个 steering 文件。</p>\n<p><img alt=\"image-20260109224727421\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce47b764.png\" /></p>\n<br />\n<p>Kiro 通过 <code>.kiro/steering/</code> 目录中的 markdown 文件设置项目约束规定，Kiro 只规定了 product.md、tech.md、structure.md 三个文件，这些基础文件默认包含在每次交互中，形成 Kiro 项目理解的基线。</p>\n<p>cqrs-conventions.md 则是笔者的模板项目的开发规则，约定 AI 生成的代码文件如何存放以及代码格式约束，AI 会生成符合项目要求的代码。</p>\n<p>读者也可以创建一些其它文件，例如 <code>rest-api.md</code> 要求生成的 api 接口需要符合 restapi 格式，将公司研发规范等写入到  <code>.kiro/steering/</code> 目录。</p>\n<h4 id=\"后端代码规范约束\">后端代码规范约束</h4>\n<p><code>cqrs-conventions.md</code> 是笔者自定义的 steering 文件，用来约束 AI 生成的代码必须分为 <code>Command/Query</code>、<code>Api</code>、<code>Handler</code> 三层的 CQRS 结构。</p>\n<p>定义了很多约束规范，这里就不讲解了，自己研究一下。</p>\n<p><img alt=\"image-20260109094451145\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce4a8557.png\" /></p>\n<h4 id=\"我要做什么样的产品\">我要做什么样的产品</h4>\n<p>拉取项目模板后，第一步是修改 <code>product.md</code>，告诉 AI 这是一个短链接项目。</p>\n<blockquote>\n<p><strong>产品概述</strong> (<code>product.md</code>) - 定义产品的目的、目标用户、关键功能和业务目标。</p>\n</blockquote>\n<br />\n<p>一般来说，<strong>产品概述</strong> (<code>product.md</code>)  是产品经理的工作任务，开发只需要将产品经理写的文档拷贝到代码项目即可，不过既然现在没有产品经理，我们可以一句话描述核心需求，让 Kiro 帮助我们生成具体的产品说明，等 AI 生成后，根据实际情况，调整好 <code>product.md</code> 文件。</p>\n<p><img alt=\"image-20260109094959587\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce63e3a9.png\" /></p>\n<br />\n<p>生成效果：</p>\n<p><img alt=\"image-20260109094351449\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce66b451.png\" /></p>\n<h4 id=\"项目结构\">项目结构</h4>\n<p><strong>项目结构</strong> (<code>structure.md</code>) - 概述文件组织、命名约定、导入模式和架构决策，这确保生成的代码无缝融入现有的代码结构里面，这样 AI 不会乱创建文件以及随便找个地方塞代码。</p>\n<p><code>structure.md</code> 要常常随着项目的进展而更新，不要一直不变，每次新增模块后，都要重新生成 <code>structure.md</code> 。</p>\n<p>现在我们点击 Kiro 的 <code>Refine</code> ，重新生成 <code>structure.md</code> 。</p>\n<p><img alt=\"image-20260109095652954\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce6eb390.png\" /></p>\n<h4 id=\"技术约定\">技术约定</h4>\n<p><strong>技术栈</strong> (<code>tech.md</code>) - 记录这个项目选择的框架、库、开发工具和技术约束，当 Kiro 建议实现方案时，它会优先选择已建立的技术栈而非替代方案。</p>\n<p>突然想起来一个事情，某同事写 Go 语言程序需要解决一个打印功能，结果用 AI 写的代码引入了 python，后面被我发现了，我要求用 Go 重写，然后某同事又用 AI 劈里啪啦写了一天，总算用 Go 写出来了。</p>\n<p>所以 <code>tech.md</code> 可以避免这种情况，不然 AI 为了实现一个功能，到处查资料，然后引入一堆依赖，用一种意想不到的方式去实现功能。</p>\n<br />\n<p>现在我们点击 Kiro 的 <code>Refine</code> ，重新生成 <code>tech.md</code> ，例如这个项目使用的技术栈如下：</p>\n<pre><code>| Category | Technology |\n|----------|------------|\n| Framework | ASP.NET Core 9 |\n| Language | C# 12 (nullable reference types enabled) |\n| ORM | Entity Framework Core 9 (MySQL) |\n| CQRS | MediatR |\n| Validation | FluentValidation |\n| Authentication | JWT Bearer tokens |\n| Logging | Serilog |\n| API Docs | OpenAPI/Swagger with Scalar UI |\n| Caching | Redis (StackExchange.Redis) |\n| Module System | Maomi.Core |\n| Code Analysis | StyleCop.Analyzers |\n</code></pre>\n<br />\n<p>作者还可以加上接口设计约定等文件，这里不再赘述。</p>\n<h3 id=\"数据库设计\">数据库设计</h3>\n<p>现在出现了一个使用 AI 做数据库设计方案的方向：Text2SQL。</p>\n<p>Text2SQL 指将自然语言转换为 SQL 的技术，当一个项目从零开始时，我们可以借助 AI 构思、设计项目架构，最后总结输出 SQL，这一步其实比较简单。</p>\n<p>但是后续项目迭代后，需要 AI 去了解整个数据库表结构，需要 AI 根据业务情况设计新的索引、字段约束等，这就要求 AI 需要挖掘数据价值，应对复杂的分析任务，才能给出合理的数据库变更建议。</p>\n<p>可以借助专业的 AI DB 客户端去做，例如 Chat2DB、也可以在 Kiro 装上 MCP 工具读取数据库，由于不是本文重点，这里只讲解思路，就不再详细讲述。</p>\n<br />\n<p>数据库创建表，以便后续实战让 AI 写代码。</p>\n<pre><code class=\"language-csharp\">CREATE TABLE `short_url` (\n  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '唯一ID（对应短链接编码）',\n  `long_url` varchar(2048) NOT NULL COMMENT '原始长链接',\n  `hash` varbinary(32) NOT NULL COMMENT '网址哈希值，方便对比',\n  `create_user_id` int(11) NOT NULL DEFAULT 0 COMMENT '创建人',\n  `create_time` datetime NOT NULL DEFAULT current_timestamp() COMMENT '创建时间',\n  `update_user_id` int(11) NOT NULL DEFAULT 0 COMMENT '更新人',\n  `update_time` datetime NOT NULL DEFAULT current_timestamp() ON UPDATE current_timestamp() COMMENT '更新时间',\n  `is_deleted` bigint(20) NOT NULL DEFAULT 0 COMMENT '软删除',\n  PRIMARY KEY (`id`),\n  KEY `short_url_hash_index` (`hash`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='短链接'\n\nCREATE TABLE `user` (\n  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '用户ID',\n  `user_name` varchar(50) NOT NULL COMMENT '用户名',\n  `email` longtext NOT NULL COMMENT '邮箱',\n  `password` varchar(255) NOT NULL COMMENT '密码',\n  `nick_name` varchar(50) NOT NULL COMMENT '昵称',\n  `password_salt` varchar(255) NOT NULL COMMENT '计算密码值的salt',\n  `is_deleted` bigint(20) NOT NULL COMMENT '软删除',\n  `create_user_id` int(11) NOT NULL COMMENT '创建人',\n  `create_time` datetime NOT NULL DEFAULT utc_timestamp() COMMENT '创建时间',\n  `update_user_id` int(11) NOT NULL COMMENT '最后修改人',\n  `update_time` datetime NOT NULL DEFAULT utc_timestamp() COMMENT '最后更新时间',\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `users_user_name_is_deleted_uindex` (`user_name`,`is_deleted`)\n) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT='用户'\n\n</code></pre>\n<p><img alt=\"image-20260109110209561\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce719e9b.png\" /></p>\n<br />\n<p>修改数据库链接，启动 MysqlScaffold 还原数据库生成代码，将 <code>Database</code> ，目录的文件放到对应位置接口。</p>\n<blockquote>\n<p>您看，有个项目模板多重要，很多时候省下大量的时间，例如笔者这个模板，先设计数据库，然后定制数据库生成代码的步骤，使得生成的实体结构和 数据库上下文类能够符合业务需求，大大提高开发效率。</p>\n</blockquote>\n<p><img alt=\"6f39e423-8590-4a90-bee3-cd0f34a55caa\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce862a66.png\" /></p>\n<h3 id=\"ui-原型设计\">UI 原型设计</h3>\n<p>UI 原型设计就是 UI 设计师根据产品原型设计页面的过程，随着专业的原型设计工具支持 AI 后，产品经理跟 UI 设计师更好地协作，有时候一句话就可以生成一个不错的界面。产品经理可以借助这些 AI 工具快速实现原型页面。</p>\n<p><img alt=\"image-20260109103159002\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ce998d33.png\" /></p>\n<br />\n<p>由于墨刀的 AI 需要付费使用，太贵了，本来想演示 AI 根据设计稿生成页面代码的，最后放弃了，用这钱买了几盒凤爪。</p>\n<p>本节意思读者理解就行，就是说目前市面上有很多 AI 生成设计稿的产品，产品可以不用化那么多时间画原型了，可以专心思考产品设计和流程逻辑，最大程度输出文档，把画原型这些事情留给 AI，然后还可以进一步转换为 UI，设计师也可以借助 AI 快速实现初版界面。</p>\n<h3 id=\"后端开发实战\">后端开发实战</h3>\n<p>基于项目模板和 steering，其实一句话需求，就可以让 Kiro 给我们编写一个模块的后端代码。</p>\n<p>但是 Kiro 提出了 Specs，让团队协作和开发人员早点下班的神器。</p>\n<br />\n<p>Specs 弥合了概念产品需求和技术实施细节之间的差距，确保了一致性并减少了开发迭代。Specs 提供了一种系统化的方法，将需求和想法转化为详细的实施计划，生成验收标准、技术实现和代码生成及测试验收计划。</p>\n<br />\n<p>接下来，我们将会使用 Kiro Specs 生成某个功能的工作流程，最后根据方案生成代码并通过测试验收代码。</p>\n<h4 id=\"实现创建短链接的接口\">实现创建短链接的接口</h4>\n<p>要做短链接服务，第一步是实现一个长链接转短链接的功能，Kiro Specs 要求编码之前需要按照三阶段工作流程进行：需求 → 设计 → 实施。</p>\n<p>在 Kiro 面板中，点击 <strong>Specs</strong> 下的 <code>+</code> 按钮，或者从聊天面板中选择 <strong>Spec</strong>，在对话框内输入要做的功能。</p>\n<pre><code>Create Spec: 实现创建短链接的接口，创建的数据存储到 ShortUrlEntity，使用雪花id赋值id，将长地址使用 SHA-256 生成 32 字节存储到 hash 字段。插入数据时要判断数据库是否存在对应的数据。\n</code></pre>\n<p><img alt=\"a91e33a4-6ae2-48cc-af68-25cffb5a7898\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ceaa6b59.png\" /></p>\n<p><img alt=\"382cf906-3618-4d01-abc1-5b3f6bb39e51\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cebdc604.png\" /></p>\n<br />\n<p>按照引导操作后，最后一步会显示 “保留可选任务（更快完成 MVP）”、“全部设为必需（完整测试覆盖）”。</p>\n<p>如果选择了 “全部设为必需（完整测试覆盖）”，AI 在任务列表加上基于属性的测试要求和执行步骤，Kiro 从提出的需求中提取 <code>属性</code> 并生成测试用例，以便确保 AI 生成的代码符合开发者的意图。</p>\n<blockquote>\n<p>Kiro 文档里面解释 <code>属性</code>：对于任何一组具有特定前提条件的输入，某些预期行为是成立的。Kiro 从格式化的需求中提取属性 (例如，“THE System SHALL 允许经过认证的用户查看活动车辆列表”)，确定哪些属性可以进行逻辑测试，然后在你选择运行它们时生成数百或数千个随机测试用例。</p>\n</blockquote>\n<br />\n<p><img alt=\"image-20260109115335208\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cec13d1c.png\" /></p>\n<p><br />结果一顿操作后，需求已经下发到 <code>.kiro/specs/create-short-url</code>，Kiro 会生成三个关键文件，这三个文件构成一个 spec。</p>\n<ul>\n<li><strong>requirements.md</strong>：使用结构化的 EARS 记号法捕获用户故事和验收标准</li>\n<li><strong>design.md</strong>：记录技术架构、序列图和实施考虑因素</li>\n<li><strong>tasks.md</strong>：提供详细的实施计划，包含离散的、可跟踪的任务</li>\n</ul>\n<p><img alt=\"image-20260109115648904\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ced335de.png\" /></p>\n<br />\n<p>我们可以先编辑 requirements.md、design.md，确保 AI 生成的具体业务要求和测试说明、技术栈和实现思路符合我们的要求，最后打开 tasks.md 文件，点击任务旁边的 <code>Start task</code> 按钮开始实现代码。</p>\n<p><img alt=\"0dce46d5-0884-4a84-ac8d-792c18df1988\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ced5de13.png\" /></p>\n<br />\n<p>由于创建短链接存储代码时，使用的算法跟我的设想相差甚远，所以这里也可以重新生成代码，不过建议好好写  requirements.md、design.md，不要在对话框调整代码逻辑，只有沉淀的文档才是最重要的。</p>\n<p><img alt=\"image-20260109130137218\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ceeab3a2.png\" /></p>\n<h3 id=\"工作流程\">工作流程</h3>\n<p>前面利用做 ”实现创建短链接的接口“ 这个需求，上手了 Kiro specs，各位应该大概了解怎么玩了，但是回到团队协作下，我们要好好构思，研发团队应该怎么做 specs。</p>\n<p>所以，在这一节中，讲解 Kiro specs 的一些概念，以便回答 [核心执行：AI 驱动的协作流程设计](#核心执行：AI 驱动的协作流程设计) 中提到的问题，怎么指定新的团队研发流程。</p>\n<h4 id=\"需求阶段\">需求阶段</h4>\n<p>Kiro specs 提出在需求阶段用结构化 EARS 表示法定义用户故事和验收标准，也就是 requirements.md 应该撰写的内容。requirements.md 文件以用户故事的形式写成，其中包含 EARS 表示法中的验收标准。</p>\n<p>核心工作如下：</p>\n<ul>\n<li>定义用户故事</li>\n<li>编写验收标准</li>\n<li>采用 EARS 符号进行需求规范</li>\n</ul>\n <br />\n<p>与传统需求编写方法的比较，EARS 符号表示需求有很大优势，尤其在使用 AI 编程时。</p>\n<table>\n<thead>\n<tr>\n<th><strong>方面</strong></th>\n<th><strong>传统要求</strong></th>\n<th><strong>EARS 符号</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>明晰</strong></td>\n<td>通常含糊不清或冗长</td>\n<td>简洁明了</td>\n</tr>\n<tr>\n<td><strong>标准化</strong></td>\n<td>不同团队之间存在很大差异</td>\n<td>所有需求的统一语法</td>\n</tr>\n<tr>\n<td><strong>易于理解</strong></td>\n<td>对于非技术利益相关者来说很困难</td>\n<td>所有利益相关者都能轻松理解</td>\n</tr>\n<tr>\n<td><strong>可追溯分析仪</strong></td>\n<td>维持起来很困难</td>\n<td>通过结构化语法增强可追溯性</td>\n</tr>\n</tbody>\n</table>\n <br />\n<p>回想我们公司，产品经理使用飞书文档写产品文档，逻辑混乱，阅读起来非常头疼，常常在编码开发过程中反复调整，在提测和验收环节存在各种各样的问题，上线后一堆 bug。研发流程和上线后的软件，存在各种各样大大小小的问题，研发怪产品经理文档写得烂，产品经理怪研发没有理解需求，测试人员喷研发代码写得烂。</p>\n<p>而领导们忙着定制各种 ”规范“，要求产品经理写产品文档要按照某些排版格式去做，做了各种研发流程要求，试图通过指定一系列所谓的规范去彻底解决研发团队内的问题。</p>\n <br />\n<p>EARS 表示法，做了深入了解后，发现这个真的很适合团队落地，既可以解决产品经理到编码、提测后的一些常见问题，又能便于 AI 理解需求。AI 可以很容易基于 EARS 需求表示法，生成对应的编码需求和测试用例，确保代码的逻辑跟需求一致，并且生成对应的验收文档，作为最终代码验收依据。</p>\n<p>requirements.md 文档分为多个部分，其中 <strong>Requirements</strong> 部分应当使用 EARS 表示法编写，每个 requirement 都遵循以下模式。</p>\n<pre><code>当[条件/事件]发生时\n系统应[预期行为]\n</code></pre>\n<p><img alt=\"image-20260110103653398\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639ceed27e6.png\" /></p>\n <br />\n<h4 id=\"设计阶段\">设计阶段</h4>\n<p>design.md 文件是记录技术架构、顺序图和实现注意事项的地方，通过 design.md 可以全面了解系统的工作方式，包括组件及其交互。Kiri 的规范为设计文档提供了一种结构化的方法，使得理解和协作复杂系统变得更加容易。</p>\n<p>设计阶段可以参考前面 AI 生成的 <code>design.md</code>，包括了下图中的内容。</p>\n<p><img alt=\"image-20260110104444972\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cef01f19.png\" /></p>\n<h4 id=\"实施阶段\">实施阶段</h4>\n<p>tasks.md 文件提供了一个详细的实施计划，其中包含离散的、可跟踪的任务和子任务。每个任务都有明确的定义，包括清晰的描述、预期结果以及任何必要的资源或依赖关系。每个步骤都可以点击，AI 执行任务后会实时刷新到 task.md 里面。</p>\n<p>AI 生成的 task.md 如下，基本也是分三步：分解任务、定义任务输出结果、设置任务上下依赖关系，当然最后可能还有验收实施的说明</p>\n<p><img alt=\"image-20260109131506945\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cef30fb1.png\" /></p>\n<h4 id=\"执行阶段\">执行阶段</h4>\n<p>其实就是我们在 task.md 手动点击各个 task 的过程，我们应该一直跟踪任务进度，随时调整更新补充 specs 三个文档，完善内容，让 AI 输出的效果更加好。</p>\n<h3 id=\"使用-hook-自动构建单元测试\">使用 Hook 自动构建单元测试</h3>\n<p>当在工作空间中创建、保存或删除与特定全局模式匹配的文件时，会触发文件钩子。这些钩子需要一个模式数组来指定要监视的文件。</p>\n <br />\n<p>这里笔者使用单元测试来做 Hook，让读者了解 Kiro 的 Hook 怎么玩。</p>\n<p>单元测试是项目最重要的一部分，由于笔者这个模板项目采用整洁架构，所以做测试也是相当简单。</p>\n<p>主要分为三部分考虑：</p>\n<p>对于无业务相关的框架、工具、算法代码，单独设计验证的单元测试即可。</p>\n<p>对于模型类，要验证 api 请求时参数限制，要识别字段长度范围等规则。</p>\n<p>对于 Api、Handler 可以一起测，编写集成测试，直接使用 TestWebHost 模拟请求。</p>\n <br />\n<p>现在我们编写对应的提示词，让 Kiro 生成 Hook。</p>\n<pre><code>给 Api 编写单元测试，使用 EFCore内存数据库模拟，redis 使用mock替代对于无业务相关的框架、工具、算法代码，单独设计验证的单元测试即可。对于模型类，要验证 api 请求时参数限制，要识别字段长度范围等规则。对于 Api、Handler 可以一起测，编写集成测试，直接使用 TestWebHost 模拟请求。\n</code></pre>\n<p><img alt=\"image-20260109125656799\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf060a20.png\" /></p>\n<p><img alt=\"image-20260109130251045\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf08c52b.png\" /></p>\n<p><img alt=\"image-20260109125731788\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf0b69fe.png\" /></p>\n <br />\n<p>现在使用 spec 加入新的功能：</p>\n<pre><code>Create Spec: 用户访问短链接后，先将短链接使用 base62 还原为雪花id，先从redis布隆过滤器里面过滤，如果查找不到则直接404。然后从 key `short_url:{雪花id}` 里面找，找不到就查数据库然后重新塞到 redis（超时时间30分钟）。\n</code></pre>\n <br />\n<p>创建完成后，执行 Task list。</p>\n <br />\n<p>可以等待 Hook 自动触发，或在对话界面告诉 AI 手动执行 <code>api-unit-test-gen.kiro.hook</code>。</p>\n<p>最终生成单元测试如下：</p>\n<p><img alt=\"image-20260109134821100\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf234a04.png\" /></p>\n<p><img alt=\"image-20260109135655490\" src=\"https://www.whuanle.cn/wp-content/uploads/2026/01/post-22312-69639cf25f739.png\" /></p>\n<h3 id=\"最后\">最后</h3>\n<p>希望可以通过本文帮助您在公司内建设一个 AI 辅助编程团队。</p>\n\n</div>\n<div id=\"MySignature\">\n    痴者工良(https://whuanle.cn)\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-12 08:34</span>&nbsp;\n<a href=\"https://www.cnblogs.com/whuanle\">痴者工良</a>&nbsp;\n阅读(<span id=\"post_view_count\">599</span>)&nbsp;\n评论(<span id=\"post_comment_count\">1</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "Java中线程安全问题的原因和解决方案",
      "link": "https://www.cnblogs.com/xi-yongqi/p/19468853",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n\t\t\t<a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/xi-yongqi/p/19468853\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 23:41\">\n    <span>Java中线程安全问题的原因和解决方案</span>\n    \n\n</a>\n\n\t\t</h1>\n\t\t<div class=\"clear\"></div>\n\t\t<div class=\"postBody\">\n\t\t\t<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h3 id=\"线程安全问题的核心原因\">线程安全问题的核心原因</h3>\n<ul>\n<li>线程安全问题本质是多个线程并发访问共享且可变的资源时，操作的原子性、可见性或有序性被破坏，导致程序执行结果不符合预期。</li>\n</ul>\n<ol>\n<li>根本原因：共享可变资源</li>\n</ol>\n<ul>\n<li>共享资源：多个线程都能访问到的资源（如成员变量、静态变量、共享内存区域）；</li>\n<li>可变资源：资源的状态（值）可以被修改（如int计数器、HashMap的元素）；</li>\n<li>经典的i++ 操作。它在底层分为“读取-修改-写入”三个步骤。如果两个线程同时读取 i=1，各自加1后写回，结果是2而不是3。</li>\n</ul>\n<ol start=\"2\">\n<li>直接原因：三大特性被破坏</li>\n</ol>\n<ul>\n<li>Java内存模型（JMM）定义的多线程并发三大核心特性，任何一个被破坏都会引发线程安全问题：\n<ul>\n<li>原子性：一个操作（如count++）包含“读 - 改 - 写”三步，非原子操作会被多线程交错执行；</li>\n<li>可见性：线程修改共享变量后，不会立即同步到主内存，其他线程读取的仍是旧值；</li>\n<li>有序性：JVM的指令重排序优化，会导致多线程下执行顺序混乱（如未加volatile的双重检查锁单例）。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"线程安全问题的解决方案\">线程安全问题的解决方案</h3>\n<ul>\n<li>核心思路：要么避免共享可变资源（从根源消除问题），要么控制并发访问规则（保证三大特性）。</li>\n</ul>\n<h5 id=\"方案1避免共享可变资源优先推荐\">方案1：避免共享可变资源（优先推荐）</h5>\n<ul>\n<li>栈封闭（局部变量）：局部变量存储在线程私有栈中，每个线程有独立副本，天然线程安全。</li>\n</ul>\n<pre><code class=\"language-java\">public class StackClosedDemo {\n    // 每个线程调用该方法时，都会创建独立的count副本\n    public void calculate() {\n        int count = 0;\n        count++; // 无线程安全问题\n        System.out.println(Thread.currentThread().getName() + \": \" + count);\n    }\n\n    public static void main(String[] args) {\n        StackClosedDemo demo = new StackClosedDemo();\n        // 10个线程各自操作自己的局部变量\n        for (int i = 0; i &lt; 10; i++) {\n            new Thread(demo::calculate, \"Thread-\" + i).start();\n        }\n    }\n}\n</code></pre>\n<ul>\n<li>不可变对象:对象创建后状态不可修改（如String、Integer），即使共享也无法修改值。</li>\n</ul>\n<pre><code class=\"language-java\">// 自定义不可变类（final类+final成员变量+无setter）\npublic final class ImmutableUser {\n    private final String name;\n    private final int age;\n\n    public ImmutableUser(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    // 仅提供getter，无setter\n    public String getName() { return name; }\n    public int getAge() { return age; }\n}\n</code></pre>\n<ul>\n<li>ThreadLocal（线程本地存储）:为每个线程提供独立的变量副本，线程操作自身副本，互不干扰。</li>\n</ul>\n<pre><code class=\"language-java\">public class ThreadLocalDemo {\n    // 每个线程有独立的Integer副本，初始值为0\n    private static ThreadLocal&lt;Integer&gt; threadLocal = ThreadLocal.withInitial(() -&gt; 0);\n\n    public void increment() {\n        threadLocal.set(threadLocal.get() + 1);\n        System.out.println(Thread.currentThread().getName() + \": \" + threadLocal.get());\n    }\n\n    public static void main(String[] args) {\n        ThreadLocalDemo demo = new ThreadLocalDemo();\n        // 3个线程各自操作自己的副本\n        for (int i = 0; i &lt; 3; i++) {\n            new Thread(() -&gt; {\n                for (int j = 0; j &lt; 2; j++) {\n                    demo.increment();\n                }\n            }, \"Thread-\" + i).start();\n        }\n    }\n}\n// 输出（顺序可能不同）：\n// Thread-0: 1、Thread-0: 2\n// Thread-1: 1、Thread-1: 2\n// Thread-2: 1、Thread-2: 2\n</code></pre>\n<h4 id=\"方案2同步加锁控制并发访问\">方案2：同步/加锁（控制并发访问）</h4>\n<h5 id=\"互斥同步阻塞同步这是最常见的方案通过加锁来保证同一时刻只有一个线程操作资源\">互斥同步（阻塞同步）:这是最常见的方案，通过加锁来保证同一时刻只有一个线程操作资源。</h5>\n<ul>\n<li>synchronized 关键字：Java 原生支持，使用简单。可修饰方法或代码块。属于不可中断的锁。</li>\n</ul>\n<pre><code class=\"language-java\">public class SynchronizedDemo {\n    private int count = 0;\n\n    // 同步实例方法，锁是this对象\n    public synchronized void increment() {\n        count++;\n    }\n\n    public static void main(String[] args) throws InterruptedException {\n        SynchronizedDemo demo = new SynchronizedDemo();\n        // 1000个线程执行increment\n        for (int i = 0; i &lt; 1000; i++) {\n            new Thread(demo::increment).start();\n        }\n        Thread.sleep(1000);\n        System.out.println(\"最终count：\" + demo.count); // 输出1000\n    }\n}\n</code></pre>\n<h5 id=\"reentrantlock显式锁比synchronized灵活可中断可超时公平锁需手动释放锁必须在finally中\">ReentrantLock（显式锁）:比synchronized灵活（可中断、可超时、公平锁），需手动释放锁（必须在finally中）。</h5>\n<pre><code class=\"language-java\">import java.util.concurrent.locks.Lock;\nimport java.util.concurrent.locks.ReentrantLock;\n\npublic class ReentrantLockDemo {\n    private int count = 0;\n    private Lock lock = new ReentrantLock(); // 默认非公平锁\n\n    public void increment() {\n        lock.lock(); // 加锁\n        try {\n            count++;\n        } finally {\n            lock.unlock(); // 释放锁，避免死锁\n        }\n    }\n\n    public static void main(String[] args) throws InterruptedException {\n        ReentrantLockDemo demo = new ReentrantLockDemo();\n        for (int i = 0; i &lt; 1000; i++) {\n            new Thread(demo::increment).start();\n        }\n        Thread.sleep(1000);\n        System.out.println(\"最终count：\" + demo.count); // 输出1000\n    }\n}\n</code></pre>\n<h4 id=\"方案3volatile关键字保证可见性有序性\">方案3：volatile关键字（保证可见性/有序性）</h4>\n<ul>\n<li>保证可见性：强制失效工作内存，直接读写主内存。</li>\n<li>保证有序性：禁止指令重排序。</li>\n<li>注意：它不保证原子性（不能解决i++问题）。</li>\n</ul>\n<pre><code class=\"language-java\">public class VolatileDemo {\n    private volatile boolean stop = false; // 保证可见性和有序性\n\n    public void runThread() {\n        new Thread(() -&gt; {\n            int i = 0;\n            while (!stop) { // 能立即感知stop的修改\n                i++;\n            }\n            System.out.println(\"线程停止，i=\" + i);\n        }).start();\n    }\n\n    public static void main(String[] args) throws InterruptedException {\n        VolatileDemo demo = new VolatileDemo();\n        demo.runThread();\n        Thread.sleep(3000);\n        demo.stop = true; // 修改后，线程立即停止\n    }\n}\n</code></pre>\n\n</div>\n<div class=\"clear\"></div>\n\n\t\t</div>\n\t\t<div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-11 23:41</span>&nbsp;\n<a href=\"https://www.cnblogs.com/xi-yongqi\">我会替风去</a>&nbsp;\n阅读(<span id=\"post_view_count\">157</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "飞书 .NET SDK 事件处理的幂等性与去重机制",
      "link": "https://www.cnblogs.com/mudtools/p/19469184",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/mudtools/p/19469184\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 23:00\">\n    <span>飞书 .NET SDK 事件处理的幂等性与去重机制</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>飞书事件处理过程中如何让你的应用不再\"重复劳动\"，如何用三层防护筑起安全墙，结合内存与 Redis 双重保障，让你的飞书应用稳如磐石——不再重复处理，告别混乱状态。</p>\n</blockquote>\n<hr />\n<h2 id=\"为什么需要去重\">为什么需要\"去重\"？</h2>\n<p>想象一下这样的场景：</p>\n<p>你在飞书里收到一条消息，应用收到通知后创建了待办事项。但因为网络不稳定，飞书以为你没收到，又发了一遍同样的通知——结果呢？你的应用又创建了一次待办，同一个任务出现了两次。</p>\n<p>这就是我们所说的\"重复处理\"问题。</p>\n<hr />\n<h3 id=\"什么时候会出现这种情况\">什么时候会出现这种情况？</h3>\n<p>在飞书事件驱动的世界里，以下情况都可能导致<strong>同一事件被多次送达</strong>：</p>\n<ul>\n<li>📡 <strong>网络波动</strong>：飞书服务器没收到你的确认，于是重发</li>\n<li>🔄 <strong>服务重启</strong>：内存清空，之前的事件又来了</li>\n<li>👥 <strong>多实例运行</strong>：多个实例同时收到同一事件</li>\n<li>🔌 <strong>断线重连</strong>：WebSocket 重连后可能重复消息</li>\n</ul>\n<h3 id=\"真实案例一分钟内的混乱\">真实案例：一分钟内的混乱</h3>\n<pre><code>时间线：\n─────────────────────────────────────────────────────────────\n09:00:00  飞书推送：收到一条新消息\n09:00:01  实例A 接收并处理 → 创建待办 ✅\n09:00:05  飞书没收到确认，再次推送\n09:00:06  实例B 接收并处理 → 又创建待办 ❌\n─────────────────────────────────────────────────────────────\n</code></pre>\n<p><strong>后果有多严重？</strong></p>\n<table>\n<thead>\n<tr>\n<th>问题</th>\n<th>实际影响</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>📉 数据重复</td>\n<td>用户收到两条相同的待办</td>\n</tr>\n<tr>\n<td>💰 资金损失</td>\n<td>订单重复扣款，退钱都退不完</td>\n</tr>\n<tr>\n<td>📧 骚扰用户</td>\n<td>同一条通知发十次</td>\n</tr>\n<tr>\n<td>🔄 状态混乱</td>\n<td>数据库里说\"已处理\"，实际只做了一半</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"三层防护像保安一样层层把关\">三层防护：像保安一样层层把关</h2>\n<p>Mud.Feishu SDK 设计了一套<strong>三层递进式防护机制</strong>，就像小区的三道岗哨，从外到内层层把关，确保不会有\"坏人\"（重复事件）混进来。</p>\n<div class=\"mermaid\">graph TB\n    subgraph AppLayer[\"应用层去重（业务键）\"]\n        AppHandler[\"IdempotentFeishuEventHandler&lt;T&gt;\"]\n        AppDesc[\"基于业务主键（消息ID、订单ID等）去重\"]\n    end\n\n    subgraph DispatchLayer[\"分发层去重（EventId）\"]\n        EventDedup[\"IFeishuEventDeduplicator / IFeishuEventDistributedDeduplicator\"]\n        EventDesc[\"基于飞书事件ID去重，24小时窗口期\"]\n    end\n\n    subgraph ProtocolLayer[\"协议层去重（SeqID/Nonce）\"]\n        WS_Dedup[\"WebSocket: IFeishuSeqIDDeduplicator\"]\n        Webhook_Dedup[\"Webhook: IFeishuNonceDistributedDeduplicator\"]\n        ProtoDesc[\"基于消息序列号去重，过滤重复消息\"]\n    end\n\n    AppHandler --&gt;|处理器内部业务逻辑| EventDedup\n    EventDedup --&gt;|事件路由与分发| WS_Dedup\n    EventDedup --&gt;|事件路由与分发| Webhook_Dedup\n\n    style AppLayer fill:#e1f5ff\n    style DispatchLayer fill:#fff4e1\n    style ProtocolLayer fill:#ffe1e1\n</div><h3 id=\"这三层分别负责什么\">这三层分别负责什么？</h3>\n<p>可以把这三层想象成工厂流水线上的三个质检员：</p>\n<table>\n<thead>\n<tr>\n<th>质检员</th>\n<th>检查什么？</th>\n<th>在哪检查？</th>\n<th>过滤范围</th>\n<th>适合什么时候用？</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>协议层</strong></td>\n<td>消息编号（SeqID）</td>\n<td>消息刚到达时</td>\n<td>每条消息</td>\n<td>过滤网络重复，最外层防护</td>\n</tr>\n<tr>\n<td><strong>分发层</strong></td>\n<td>事件ID（EventId）</td>\n<td>事件分发前</td>\n<td>每个事件</td>\n<td>过滤飞书重发，中间层防护</td>\n</tr>\n<tr>\n<td><strong>应用层</strong></td>\n<td>业务键（TaskId）</td>\n<td>业务处理时</td>\n<td>每次业务操作</td>\n<td>防止逻辑重复，最后一道防线</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"分发层事件的身份证检查\">分发层：事件的\"身份证检查\"</h2>\n<p>这是最核心的一层，就像门口的保安，每个进来的事件都要先出示身份证（EventId），保安查过记录后才能放行。</p>\n<pre><code class=\"language-csharp\">public interface IFeishuEventDeduplicator\n{\n    /// &lt;summary&gt;\n    /// 尝试标记事件为处理中\n    /// &lt;/summary&gt;\n    /// &lt;returns&gt;\n    /// true: 已处理或正在处理中（跳过）\n    /// false: 新事件（继续处理）\n    /// &lt;/returns&gt;\n    bool TryMarkAsProcessing(string eventId);\n\n    /// &lt;summary&gt;\n    /// 标记事件为已完成\n    /// &lt;/summary&gt;\n    void MarkAsCompleted(string eventId);\n\n    /// &lt;summary&gt;\n    /// 回滚处理中状态（异常时调用）\n    /// &lt;/summary&gt;\n    void RollbackProcessing(string eventId);\n\n    /// &lt;summary&gt;\n    /// 检查事件是否已处理\n    /// &lt;/summary&gt;\n    bool IsProcessed(string eventId);\n}\n</code></pre>\n<h3 id=\"事件的一生三种状态流转\">事件的一生：三种状态流转</h3>\n<p>每个事件在去重系统里都像过安检一样，经历三种状态的变化：</p>\n<div class=\"mermaid\">stateDiagram-v2\n    [*] --&gt; Pending: 收到新事件\n    Pending --&gt; Processing: TryMarkAsProcessing()\n    Processing --&gt; Completed: 处理成功\n    Processing --&gt; Pending: 超时/异常（Rollback）\n    Completed --&gt; Pending: 缓存过期（24小时后）\n\n    note right of Processing\n        处理中超时（默认5分钟）\n        允许重新处理\n    end note\n\n    note right of Completed\n        24小时后自动清理\n        支持TTL配置\n    end note\n</div><h3 id=\"真实场景websocket-是怎么去重的\">真实场景：WebSocket 是怎么去重的？</h3>\n<p>来看看 WebSocket 收到事件后做了什么（<code>FeishuEventMessageHandler.cs#104-147</code>）：</p>\n<pre><code class=\"language-csharp\">// 1. 去重检查\nbool isProcessing = false;\n\nif (_options.EnableDistributedDeduplication &amp;&amp; _distributedDeduplicator != null)\n{\n    // 优先使用分布式去重（Redis）\n    isProcessing = await _distributedDeduplicator.TryMarkAsProcessedAsync(\n        eventData.EventId, \n        cancellationToken: cancellationToken);\n}\nelse if (_options.EnableEventDeduplication &amp;&amp; _deduplicator != null)\n{\n    // 使用内存去重\n    isProcessing = _deduplicator.TryMarkAsProcessing(eventData.EventId);\n}\n\n// 2. 跳过已处理事件\nif (isProcessing)\n{\n    _logger.LogDebug(\"事件 {EventId} 已在处理中或已处理，跳过\", eventData.EventId);\n    return;\n}\n\n// 3. 处理事件\ntry\n{\n    await _eventHandlerFactory.HandleEventParallelAsync(\n        eventData.EventType, \n        eventData, \n        cancellationToken);\n\n    // 4. 处理成功，标记为已完成\n    if (_options.EnableEventDeduplication &amp;&amp; _deduplicator != null)\n    {\n        _deduplicator.MarkAsCompleted(eventData.EventId);\n    }\n}\ncatch (Exception ex)\n{\n    // 5. 处理失败，回滚状态\n    if (_options.EnableEventDeduplication &amp;&amp; _deduplicator != null)\n    {\n        _deduplicator.RollbackProcessing(eventData.EventId);\n    }\n    throw;\n}\n</code></pre>\n<h3 id=\"方案一内存去重适合开发和小规模应用\">方案一：内存去重（适合开发和小规模应用）</h3>\n<p><strong>就像在大脑里记笔记</strong>：把每个事件ID记在内存里，收到新事件时先查查笔记，如果有就跳过。</p>\n<p><strong>特点</strong>：</p>\n<ul>\n<li>🧠 <strong>全靠脑子记</strong>：所有数据存在内存里</li>\n<li>⏰ <strong>记24小时</strong>：超过时间就自动忘掉</li>\n<li>🧹 <strong>定期打扫</strong>：每5分钟清理一次过期的记录</li>\n<li>⏱️ <strong>超时保护</strong>：处理超过5分钟还没完成，就允许重试</li>\n</ul>\n<p><strong>核心代码</strong>（<code>FeishuEventDeduplicator.cs#86-135</code>）：</p>\n<pre><code class=\"language-csharp\">public bool TryMarkAsProcessing(string eventId)\n{\n    lock (_lock)\n    {\n        // 检查是否已存在\n        if (_eventCache.TryGetValue(eventId, out var entry))\n        {\n            // 如果已处理，返回 true（跳过）\n            if (entry.Status == DeduplicationStatus.Completed)\n                return true;\n\n            // 如果已在处理中，检查是否超时\n            if (entry.Status == DeduplicationStatus.Processing)\n            {\n                if (DateTimeOffset.UtcNow - entry.ProcessedAt &gt; _processingTimeout)\n                {\n                    // 处理中超时，允许重新处理\n                    _logger?.LogWarning(\"事件 {EventId} 处理中超时，允许重新处理\", eventId);\n                    _eventCache.Remove(eventId);\n                    // 继续处理\n                }\n                else\n                {\n                    // 仍在处理中，跳过\n                    return true;\n                }\n            }\n        }\n\n        // 标记为处理中\n        _eventCache[eventId] = new EventCacheEntry\n        {\n            ProcessedAt = DateTimeOffset.UtcNow,\n            EventId = eventId,\n            Status = DeduplicationStatus.Processing\n        };\n\n        return false; // 未处理，新事件\n    }\n}\n</code></pre>\n<h3 id=\"方案二redis-分布式去重生产环境首选\">方案二：Redis 分布式去重（生产环境首选）</h3>\n<p><strong>就像共享笔记本</strong>：用 Redis 把去重记录记在外部，所有实例都能查到。就算重启服务，笔记本还在，不会忘记。</p>\n<p><strong>特点</strong>：</p>\n<ul>\n<li>📒 <strong>写在共享笔记本上</strong>：所有实例一起看</li>\n<li>🔄 <strong>自动翻页</strong>：到期自动清理，不用管</li>\n<li>🤝 <strong>大家一起用</strong>：多实例部署没问题</li>\n<li>⚡ <strong>原子操作</strong>：SETNX + EXPIRE 确保不会写错</li>\n</ul>\n<p><strong>核心代码</strong>（<code>RedisFeishuEventDistributedDeduplicator.cs#53-89</code>）：</p>\n<pre><code class=\"language-csharp\">public async Task&lt;bool&gt; TryMarkAsProcessedAsync(\n    string eventId, \n    TimeSpan? ttl = null, \n    CancellationToken cancellationToken = default)\n{\n    var actualTtl = ttl ?? _defaultCacheExpiration;\n    var redisKey = $\"{_keyPrefix}{eventId}\";\n\n    // 使用 SETNX + EXPIRE 实现原子性去重\n    // 仅当键不存在时设置，并设置过期时间\n    var setResult = await _database.StringSetAsync(\n        redisKey,\n        \"1\",\n        actualTtl,\n        When.NotExists);\n\n    if (!setResult)\n    {\n        _logger?.LogDebug(\"事件 {EventId} 已处理过，跳过\", eventId);\n        return true; // 已处理\n    }\n\n    _logger?.LogDebug(\"事件 {EventId} 标记为已处理，TTL: {Ttl}\", eventId, actualTtl);\n    return false; // 未处理，新事件\n}\n</code></pre>\n<h3 id=\"两种方案怎么选\">两种方案怎么选？</h3>\n<table>\n<thead>\n<tr>\n<th>对比项</th>\n<th>内存去重（自己记）</th>\n<th>Redis 去重（共享笔记本）</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>速度</strong></td>\n<td>⚡ 像闪电一样快（直接读内存）</td>\n<td>🚀 很快（但需要网络）</td>\n</tr>\n<tr>\n<td><strong>可靠性</strong></td>\n<td>⚠️ 重启就忘光</td>\n<td>✅ 永远记着，重启也还在</td>\n</tr>\n<tr>\n<td><strong>多实例</strong></td>\n<td>❌ 每个人各记各的</td>\n<td>✅ 大家一起看同一本笔记</td>\n</tr>\n<tr>\n<td><strong>麻烦程度</strong></td>\n<td>🟢 零依赖，开箱即用</td>\n<td>🟡 需要部署 Redis</td>\n</tr>\n<tr>\n<td><strong>适合谁</strong></td>\n<td>🏠 开发环境、单机运行</td>\n<td>🏢 生产环境、多实例部署</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"协议层消息的排队号检查\">协议层：消息的\"排队号检查\"</h2>\n<h3 id=\"websocket-的-seqid-是什么\">WebSocket 的 SeqID 是什么？</h3>\n<p>飞书 WebSocket 用的是 ProtoBuf 二进制协议，每条消息都带着一个<strong>递增的序号</strong>，就像去银行办事拿的排队号。通过记住已经处理过的排队号，就能在消息层面就把重复的挡在外面。</p>\n<h4 id=\"seqid-去重怎么工作\">SeqID 去重怎么工作？</h4>\n<pre><code class=\"language-csharp\">public interface IFeishuSeqIDDeduplicator\n{\n    /// &lt;summary&gt;\n    /// 尝试标记 SeqID 为已处理\n    /// &lt;/summary&gt;\n    /// &lt;returns&gt;\n    /// true: 已处理过（跳过）\n    /// false: 新消息（继续处理）\n    /// &lt;/returns&gt;\n    bool TryMarkAsProcessed(ulong seqId);\n\n    /// &lt;summary&gt;\n    /// 检查 SeqID 是否已处理\n    /// &lt;/summary&gt;\n    bool IsProcessed(ulong seqId);\n\n    /// &lt;summary&gt;\n    /// 异步检查 SeqID 是否已处理\n    /// &lt;/summary&gt;\n    Task&lt;bool&gt; IsProcessedAsync(ulong seqId);\n\n    /// &lt;summary&gt;\n    /// 清空缓存\n    /// &lt;/summary&gt;\n    void ClearCache();\n\n    /// &lt;summary&gt;\n    /// 获取缓存中的 SeqID 数量\n    /// &lt;/summary&gt;\n    int GetCacheCount();\n\n    /// &lt;summary&gt;\n    /// 获取已处理的最大 SeqID\n    /// &lt;/summary&gt;\n    ulong GetMaxProcessedSeqId();\n}\n</code></pre>\n<h4 id=\"实现机制\">实现机制</h4>\n<div class=\"mermaid\">graph LR\n    A[收到ProtoBuf消息] --&gt; B{SeqID检查}\n    B --&gt;|已处理| C[跳过消息]\n    B --&gt;|未处理| D[记录SeqID]\n    D --&gt; E[处理消息]\n    E --&gt; F[发送ACK确认]\n    D --&gt; G[更新最大SeqID]\n    G --&gt; H[定时清理过期SeqID]\n</div><h4 id=\"核心实现\">核心实现</h4>\n<p><strong>内存实现</strong>（<code>FeishuSeqIDDeduplicator.cs</code>）：</p>\n<pre><code class=\"language-csharp\">public bool TryMarkAsProcessed(ulong seqId)\n{\n    lock (_lock)\n    {\n        // 检查是否已存在\n        if (_processedSeqIds.Contains(seqId))\n        {\n            _logger?.LogDebug(\"SeqID {SeqId} 已处理过，跳过\", seqId);\n            return true; // 已处理\n        }\n\n        // 记录新 SeqID\n        _processedSeqIds.Add(seqId);\n        _seqIdTimestamps[seqId] = DateTimeOffset.UtcNow;\n\n        // 更新最大 SeqID\n        if (seqId &gt; _maxProcessedSeqId)\n        {\n            _maxProcessedSeqId = seqId;\n        }\n\n        return false; // 未处理，新消息\n    }\n}\n</code></pre>\n<p><strong>使用位置</strong>（<code>BinaryMessageProcessor.cs#186-194</code>）：</p>\n<pre><code class=\"language-csharp\">// SeqID 去重检查\nif (_seqIdDeduplicator != null &amp;&amp; _seqIdDeduplicator.TryMarkAsProcessed(frame.SeqID))\n{\n    _logger.LogDebug(\"SeqID {SeqID} 已处理过，跳过\", frame.SeqID);\n    eventArgs.SkipReason = $\"SeqID {frame.SeqID} 已处理过\";\n    BinaryMessageReceived?.Invoke(this, eventArgs);\n    \n    // 仍然发送ACK确认\n    await SendAckMessageAsync(frame, true, cancellationToken);\n    return;\n}\n</code></pre>\n<h4 id=\"特性总结\">特性总结</h4>\n<ul>\n<li>✅ <strong>基于 HashSet 高效查找</strong>：O(1) 查询复杂度</li>\n<li>✅ <strong>自动跟踪最大 SeqID</strong>：支持顺序性验证</li>\n<li>✅ <strong>24小时过期机制</strong>：定期清理历史数据</li>\n<li>✅ <strong>内存友好</strong>：仅存储 SeqID，不存储完整消息</li>\n</ul>\n<h3 id=\"webhook-的-nonce防重放攻击的秘密武器\">Webhook 的 Nonce：防重放攻击的秘密武器</h3>\n<p>飞书 Webhook 的请求里带有一个 <code>nonce</code>（随机数），这就像一次性密码——用过的密码就不能再用了，这样就能防止\"重放攻击\"（坏人拿同一个请求重复发送）。</p>\n<h4 id=\"防重放攻击流程\">防重放攻击流程</h4>\n<pre><code>攻击者尝试重放请求：\n─────────────────────────────────────────────────────────────\n原始请求：Nonce=\"abc123\", Timestamp=\"1234567890\"\n✅ 正常处理（首次接收）\n\n重放请求：Nonce=\"abc123\", Timestamp=\"1234567890\"\n❌ 拒绝处理（Nonce 已记录）\n─────────────────────────────────────────────────────────────\n</code></pre>\n<h4 id=\"redis-实现\">Redis 实现</h4>\n<pre><code class=\"language-csharp\">public class RedisFeishuNonceDistributedDeduplicator : IFeishuNonceDistributedDeduplicator\n{\n    private readonly IDatabase _database;\n    private readonly TimeSpan _defaultTtl = TimeSpan.FromMinutes(5); // 5分钟过期\n\n    public async Task&lt;bool&gt; TryMarkAsProcessedAsync(string nonce, CancellationToken cancellationToken = default)\n    {\n        var redisKey = $\"{_keyPrefix}{nonce}\";\n\n        // SETNX + EXPIRE：5分钟内重复 nonce 会被拒绝\n        var setResult = await _database.StringSetAsync(\n            redisKey,\n            \"1\",\n            _defaultTtl,\n            When.NotExists);\n\n        if (!setResult)\n        {\n            _logger?.LogWarning(\"检测到重复的 Nonce: {Nonce}，可能为重放攻击\", nonce);\n            return true; // 已处理，拒绝\n        }\n\n        return false; // 首次处理\n    }\n}\n</code></pre>\n<hr />\n<h2 id=\"应用层给每个业务操作发身份证\">应用层：给每个业务操作发\"身份证\"</h2>\n<h3 id=\"为什么还需要这一层\">为什么还需要这一层？</h3>\n<p>就算前面两层都挡住了，业务逻辑还是有可能重复执行，比如：</p>\n<ul>\n<li>🔀 同一事件触发了多个处理器（并行处理）</li>\n<li>🔄 处理器内部多次访问同一个资源</li>\n<li>📡 第三方接口重试导致重复调用</li>\n</ul>\n<h3 id=\"实际的幂等性实现方式\">实际的幂等性实现方式</h3>\n<p>在实际的 Mud.Feishu SDK 中，业务层幂等性主要通过以下两种方式实现：</p>\n<p><strong>方式1：使用 <code>DefaultFeishuObjectEventHandler&lt;T&gt;</code>（推荐）</strong><br />\nSDK 提供了 <code>DefaultFeishuObjectEventHandler&lt;T&gt;</code> 基类，专门用于处理对象类型的事件。你只需要继承这个基类，并在业务逻辑中检查数据是否已存在即可。</p>\n<p><strong>方式2：使用 <code>IdempotentFeishuEventHandler&lt;T&gt;</code></strong><br />\nSDK 还提供了一个 <code>IdempotentFeishuEventHandler&lt;T&gt;</code> 基类，提供了基于业务键的自动去重能力。你需要重写 <code>GetBusinessKey()</code> 方法和 <code>HandleEventInternalAsync()</code> 方法，基类会自动处理业务去重。</p>\n<hr />\n<h3 id=\"手把手教你用三个实际案例\">手把手教你用：三个实际案例</h3>\n<h4 id=\"案例-1消息处理防止重复创建待办\">案例 1：消息处理——防止重复创建待办</h4>\n<pre><code class=\"language-csharp\">public class MessageEventHandler : DefaultFeishuObjectEventHandler&lt;MessageReceiveResult&gt;\n{\n    private readonly IMessageService _messageService;\n\n    public MessageEventHandler(\n        ILogger&lt;MessageEventHandler&gt; logger,\n        IMessageService messageService)\n        : base(logger)\n    {\n        _messageService = messageService;\n    }\n\n    protected override async Task ProcessBusinessLogicAsync(\n        EventData eventData,\n        ObjectEventResult&lt;MessageReceiveResult&gt;? messageData,\n        CancellationToken cancellationToken = default)\n    {\n        if (eventData == null)\n            throw new ArgumentNullException(nameof(eventData));\n\n        // 检查消息是否已处理（业务层幂等性）\n        var messageId = messageData?.Object.MessageId;\n        if (string.IsNullOrEmpty(messageId))\n        {\n            _logger.LogWarning(\"消息ID为空，跳过处理\");\n            return;\n        }\n\n        var alreadyProcessed = await _messageService.IsMessageProcessedAsync(messageId, cancellationToken);\n        if (alreadyProcessed)\n        {\n            _logger.LogInformation(\"消息 {MessageId} 已处理，跳过\", messageId);\n            return;\n        }\n\n        // 处理消息\n        await _messageService.ProcessMessageAsync(messageData!.Object, cancellationToken);\n    }\n}\n</code></pre>\n<h4 id=\"案例-2部门处理避免重复创建\">案例 2：部门处理——避免重复创建</h4>\n<pre><code class=\"language-csharp\">public class DepartmentCreatedEventHandler : DefaultFeishuObjectEventHandler&lt;DepartmentCreatedResult&gt;\n{\n    private readonly IDepartmentService _departmentService;\n\n    public DepartmentCreatedEventHandler(\n        ILogger&lt;DepartmentCreatedEventHandler&gt; logger,\n        IDepartmentService departmentService)\n        : base(logger)\n    {\n        _departmentService = departmentService;\n    }\n\n    protected override async Task ProcessBusinessLogicAsync(\n        EventData eventData,\n        ObjectEventResult&lt;DepartmentCreatedResult&gt;? departmentData,\n        CancellationToken cancellationToken = default)\n    {\n        if (eventData == null)\n            throw new ArgumentNullException(nameof(eventData));\n\n        if (departmentData?.Object == null)\n        {\n            _logger.LogWarning(\"部门数据为空，跳过处理\");\n            return;\n        }\n\n        // 检查部门是否已存在（业务层幂等性）\n        var departmentId = departmentData.Object.DepartmentId;\n        var exists = await _departmentService.ExistsAsync(departmentId, cancellationToken);\n\n        if (exists)\n        {\n            _logger.LogInformation(\"部门 {DepartmentId} 已存在，跳过创建\", departmentId);\n            return;\n        }\n\n        // 创建部门\n        await _departmentService.CreateAsync(departmentData.Object, cancellationToken);\n    }\n}\n</code></pre>\n<h4 id=\"案例-3用户创建避免重复注册\">案例 3：用户创建——避免重复注册</h4>\n<pre><code class=\"language-csharp\">public class UserCreatedEventHandler : DefaultFeishuObjectEventHandler&lt;UserCreatedResult&gt;\n{\n    private readonly IUserService _userService;\n\n    public UserCreatedEventHandler(\n        ILogger&lt;UserCreatedEventHandler&gt; logger,\n        IUserService userService)\n        : base(logger)\n    {\n        _userService = userService;\n    }\n\n    protected override async Task ProcessBusinessLogicAsync(\n        EventData eventData,\n        ObjectEventResult&lt;UserCreatedResult&gt;? userData,\n        CancellationToken cancellationToken = default)\n    {\n        if (eventData == null)\n            throw new ArgumentNullException(nameof(eventData));\n\n        if (userData?.Object == null)\n        {\n            _logger.LogWarning(\"用户数据为空，跳过处理\");\n            return;\n        }\n\n        // 检查用户是否已存在（业务层幂等性）\n        var userId = userData.Object.UserId;\n        var exists = await _userService.ExistsAsync(userId, cancellationToken);\n\n        if (exists)\n        {\n            _logger.LogInformation(\"用户 {UserId} 已存在，跳过创建\", userId);\n            return;\n        }\n\n        // 创建用户\n        await _userService.CreateAsync(userData.Object, cancellationToken);\n    }\n}\n</code></pre>\n<h3 id=\"怎么设计业务键有套路吗\">怎么设计业务键？有套路吗？</h3>\n<table>\n<thead>\n<tr>\n<th>业务场景</th>\n<th>业务键长什么样？</th>\n<th>为什么这么设计？</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>收到消息</td>\n<td><code>im.message.receive_v1:om_xxxxxxxxxx</code></td>\n<td>事件类型 + 消息ID，一眼看出是哪条消息</td>\n</tr>\n<tr>\n<td>创建部门</td>\n<td><code>contact.department.created_v3:od_xxxxxxxxxx</code></td>\n<td>事件类型 + 部门ID，避免和其他ID冲突</td>\n</tr>\n<tr>\n<td>创建用户</td>\n<td><code>contact.user.created_v3:ou_xxxxxxxxxx</code></td>\n<td>事件类型 + 用户ID，唯一标识用户</td>\n</tr>\n<tr>\n<td>删除部门</td>\n<td><code>contact.department.deleted_v3:od_xxxxxxxxxx</code></td>\n<td>事件类型 + 部门ID，处理删除事件</td>\n</tr>\n<tr>\n<td>消息已读</td>\n<td><code>im.message.message_read_v1:ou_xxxxxxxxxx:om_xxxxxxxxxx</code></td>\n<td>事件类型 + 用户ID + 消息ID</td>\n</tr>\n</tbody>\n</table>\n<p><strong>四条黄金法则</strong>：</p>\n<ol>\n<li>🔑 <strong>唯一性</strong>：一个操作对应一个键，不能有两个操作撞车</li>\n<li>👀 <strong>可读性</strong>：看日志时能快速知道这是什么</li>\n<li>🧱 <strong>稳定性</strong>：别用时间戳这种会变的字段做键</li>\n<li>✂️ <strong>简洁性</strong>：别写太长，省内存、好查询</li>\n</ol>\n<hr />\n<h2 id=\"配置实战开发-vs-生产环境\">配置实战：开发 vs 生产环境</h2>\n<h3 id=\"websocket-怎么配置\">WebSocket 怎么配置？</h3>\n<pre><code class=\"language-csharp\">// 使用建造者模式配置 WebSocket\nbuilder.Services.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        // 事件去重配置\n        options.EnableEventDeduplication = true;           // 启用事件去重\n        options.EnableDistributedDeduplication = false;    // 单机场景使用内存去重\n        options.EventDeduplicationCacheExpirationMs = 86400000;  // 24小时过期\n        options.EventDeduplicationCleanupIntervalMs = 300000;    // 5分钟清理间隔\n    });\n</code></pre>\n<h4 id=\"生产环境必备配置一定要看\">生产环境必备配置（一定要看！）</h4>\n<pre><code class=\"language-csharp\">// 1. 配置 Redis（在 appsettings.json 中）\n// {\n//   \"Feishu\": {\n//     \"Redis\": {\n//       \"ConnectionString\": \"your-redis-server\"\n//     }\n//   }\n// }\n\n// 2. 注册 Redis 服务和去重服务\nbuilder.Services\n    .AddFeishuRedis()\n    .AddFeishuRedisDeduplicators();\n\n// 3. 启用分布式去重\nbuilder.Services.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        options.EnableDistributedDeduplication = true;   // 启用分布式去重\n        options.EventDeduplicationCacheExpirationMs = 86400000;  // 24小时\n    });\n</code></pre>\n<h3 id=\"webhook-配置\">Webhook 配置</h3>\n<pre><code class=\"language-csharp\">// 注册 Webhook 服务\nbuilder.Services.AddFeishuWebhookServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        options.VerificationToken = \"your_verification_token\";\n        options.EncryptKey = \"your_encrypt_key\";\n        options.EventHandlingTimeoutMs = 30000;          // 30秒超时\n        options.MaxConcurrentEvents = 10;                // 最大并发数\n    });\n\n// 注册 Redis 去重\nbuilder.Services\n    .AddFeishuRedis()\n    .AddFeishuRedisEventDeduplicator();\n</code></pre>\n<h3 id=\"一张表看懂开发与生产的区别\">一张表看懂开发与生产的区别</h3>\n<table>\n<thead>\n<tr>\n<th>配置项</th>\n<th>开发环境</th>\n<th>生产环境</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>EnableEventDeduplication</code></td>\n<td><code>true</code></td>\n<td><code>true</code></td>\n<td>始终启用内存去重</td>\n</tr>\n<tr>\n<td><code>EnableDistributedDeduplication</code></td>\n<td><code>false</code></td>\n<td><code>true</code></td>\n<td>生产环境启用Redis去重</td>\n</tr>\n<tr>\n<td><code>EventDeduplicationCacheExpirationMs</code></td>\n<td><code>3600000</code> (1小时)</td>\n<td><code>86400000</code> (24小时)</td>\n<td>生产环境延长窗口期</td>\n</tr>\n<tr>\n<td><code>EventDeduplicationCleanupIntervalMs</code></td>\n<td><code>60000</code> (1分钟)</td>\n<td><code>300000</code> (5分钟)</td>\n<td>生产环境降低清理频率</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"完整配置从零到上线一次搞定\">完整配置：从零到上线一次搞定</h3>\n<pre><code class=\"language-csharp\">public void ConfigureServices(IServiceCollection services)\n{\n    // 配置 Redis（appsettings.json 中配置）\n    services\n        .AddFeishuRedis()\n        .AddFeishuRedisDeduplicators();\n\n    // 注册 WebSocket 服务\n    services.AddFeishuWebSocketServiceBuilder(configuration)\n        .ConfigureOptions(wsOptions =&gt;\n        {\n            wsOptions.EnableEventDeduplication = true;\n            wsOptions.EnableDistributedDeduplication = true;\n            wsOptions.EventDeduplicationCacheExpirationMs = 86400000;\n            wsOptions.AutoReconnect = true;\n            wsOptions.MaxReconnectAttempts = 5;\n            wsOptions.HeartbeatIntervalMs = 30000;\n        })\n        .AddHandler&lt;MessageEventHandler&gt;()\n        .AddHandler&lt;DepartmentCreatedEventHandler&gt;()\n        .Build();\n\n    // 注册 Webhook 服务\n    services.AddFeishuWebhookServiceBuilder(configuration)\n        .ConfigureOptions(webhookOptions =&gt;\n        {\n            webhookOptions.VerificationToken = \"your_token\";\n            webhookOptions.EncryptKey = \"your_key\";\n            webhookOptions.EventHandlingTimeoutMs = 30000;\n            webhookOptions.MaxConcurrentEvents = 20;\n        })\n        .AddHandler&lt;UserCreatedEventHandler&gt;()\n        .Build();\n}\n</code></pre>\n<h3 id=\"还有个贴心功能配置错误自动提醒\">还有个贴心功能：配置错误自动提醒</h3>\n<p>SDK 会自动检查配置，如果你把去重功能全关了，它会直接报错提醒你：</p>\n<pre><code class=\"language-csharp\">// FeishuWebSocketOptions.Validate() 自动执行\n// SDK 会在服务启动时检查配置，如果去重功能全关闭会抛出警告\n</code></pre>\n<hr />\n<h2 id=\"踩坑指南五个常见问题和解决方案\">踩坑指南：五个常见问题和解决方案</h2>\n<h3 id=\"问题-1服务重启重复事件又来了\">问题 1：服务重启，重复事件又来了</h3>\n<p><strong>现场是这样的</strong>：</p>\n<p><strong>现象</strong>：</p>\n<pre><code>09:00 服务接收 EventId=\"evt_123\" ✅ 处理成功\n09:05 服务重启（内存缓存清空）\n09:06 飞书重发 EventId=\"evt_123\" ❌ 重复处理\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>🧠 内存去重就像短期记忆，重启就忘光了</li>\n<li>📡 飞书没收到你的确认，以为你没收到，继续发</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>方案1：启用 Redis 分布式去重（推荐）</strong></p>\n<pre><code class=\"language-csharp\">// 配置 Redis（appsettings.json 中配置连接信息）\nservices\n    .AddFeishuRedis()\n    .AddFeishuRedisDeduplicators();\n\nservices.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        options.EnableDistributedDeduplication = true; // 使用Redis\n        options.EnableEventDeduplication = false;    // 禁用内存去重\n    });\n</code></pre>\n<p>✅ <strong>方案2：实现去重状态持久化</strong></p>\n<pre><code class=\"language-csharp\">public class PersistentEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly IDatabase _database;\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        // 尝试写入数据库\n        var exists = _database.EventExists(eventId);\n        if (!exists)\n        {\n            _database.MarkProcessing(eventId);\n            return false;\n        }\n        return true;\n    }\n}\n</code></pre>\n<hr />\n<h3 id=\"问题-2多实例一起跑重复处理挡不住\">问题 2：多实例一起跑，重复处理挡不住</h3>\n<p><strong>现场是这样的</strong>：</p>\n<pre><code>实例A 和 实例B 同时启动\n飞书推送 EventId=\"evt_123\"\n实例A 收到 → 处理 ✅\n实例B 收到 → 处理 ❌ (重复！)\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>🧠 每个实例都有自己独立的\"小本本\"</li>\n<li>🚫 实例之间互相看不到对方的记录</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>必须使用 Redis 分布式去重</strong></p>\n<pre><code class=\"language-csharp\">// 所有实例连接到同一个 Redis（在配置文件中配置）\nservices\n    .AddFeishuRedis()\n    .AddFeishuRedisDeduplicators();\n\nservices.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        options.EnableDistributedDeduplication = true;\n    });\n</code></pre>\n<p><strong>一张图看懂区别</strong>：</p>\n<div class=\"mermaid\">graph TB\n    subgraph Wrong[\"❌ 错误架构（内存去重）\"]\n        A1[\"实例A&lt;br/&gt;内存缓存A&lt;br/&gt;evt_123 ✅\"]\n        A2[\"实例B&lt;br/&gt;内存缓存B&lt;br/&gt;evt_123 ❌\"]\n    end\n\n    subgraph Right[\"✅ 正确架构（Redis去重）\"]\n        B1[\"实例A\"]\n        B2[\"实例B\"]\n        Redis[\"Redis 去重&lt;br/&gt;evt_123 ✅\"]\n    end\n\n    B1 --&gt; Redis\n    B2 --&gt; Redis\n\n    style Wrong fill:#ffebee\n    style Right fill:#e8f5e9\n    style Redis fill:#e3f2fd\n</div><hr />\n<h3 id=\"问题-3处理超时重复事件趁虚而入\">问题 3：处理超时，重复事件趁虚而入</h3>\n<p><strong>现场是这样的</strong>：</p>\n<pre><code>09:00 开始处理 EventId=\"evt_123\"\n09:01 处理超时（超时30秒）\n09:01 回滚 Processing 状态\n09:02 飞书重发 EventId=\"evt_123\"\n09:02 重复处理 ❌ (实际已成功！)\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>⏱️ 超时后系统以为没完成，把\"处理中\"标记撤回了</li>\n<li>✅ 但业务可能已经做完了，只是响应慢了一点点</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>方案1：增加最终一致性检查</strong></p>\n<pre><code class=\"language-csharp\">public class DepartmentCreatedEventHandler : DefaultFeishuObjectEventHandler&lt;DepartmentCreatedResult&gt;\n{\n    private readonly IDepartmentService _departmentService;\n\n    protected override async Task ProcessBusinessLogicAsync(\n        EventData eventData,\n        ObjectEventResult&lt;DepartmentCreatedResult&gt;? departmentData,\n        CancellationToken ct)\n    {\n        if (departmentData?.Object == null)\n            return;\n\n        var departmentId = departmentData.Object.DepartmentId;\n\n        // 先检查部门是否已存在（幂等性）\n        var existingDepartment = await _departmentService.GetAsync(departmentId, ct);\n        if (existingDepartment != null)\n        {\n            _logger.LogInformation(\"部门 {DepartmentId} 已存在，跳过创建\", departmentId);\n            return;\n        }\n\n        // 创建部门\n        await _departmentService.CreateAsync(departmentData.Object, ct);\n    }\n}\n</code></pre>\n<p>✅ <strong>方案2：实现处理续期机制</strong></p>\n<pre><code class=\"language-csharp\">public class LongRunningTaskHandler : DefaultFeishuEventHandler\n{\n    private readonly IFeishuEventDeduplicator _deduplicator;\n\n    protected override async Task ProcessBusinessLogicAsync(EventData eventData, CancellationToken ct)\n    {\n        var eventId = eventData.EventId;\n\n        // 定期续期处理时间（每30秒）\n        var renewalTask = Task.Run(async () =&gt;\n        {\n            while (!ct.IsCancellationRequested)\n            {\n                await Task.Delay(30000, ct);\n                _deduplicator.RenewProcessing(eventId);\n            }\n        }, ct);\n\n        try\n        {\n            await ProcessLongRunningTask(eventData, ct);\n        }\n        finally\n        {\n            await renewalTask;\n        }\n    }\n}\n</code></pre>\n<hr />\n<h3 id=\"问题-4redis-宕机去重防护全失效\">问题 4：Redis 宕机，去重防护全失效</h3>\n<p><strong>现场是这样的</strong>：</p>\n<pre><code>Redis 宕机\n服务无法调用 TryMarkAsProcessedAsync()\n返回 false（允许处理）\n多个实例重复处理同一事件 ❌\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>🚫 Redis 挂了，代码为了不阻塞选择\"允许处理\"</li>\n<li>❌ 结果重复事件全涌进来</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>方案1：实现降级到内存去重</strong></p>\n<pre><code class=\"language-csharp\">public class HybridEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly IFeishuEventDistributedDeduplicator _redis;\n    private readonly IFeishuEventDeduplicator _memory;\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        try\n        {\n            // 优先使用 Redis\n            return _redis.TryMarkAsProcessedAsync(eventId).GetAwaiter().GetResult();\n        }\n        catch (Exception ex)\n        {\n            _logger.LogError(ex, \"Redis去重失败，降级到内存去重\");\n            // 降级到内存去重\n            return _memory.TryMarkAsProcessing(eventId);\n        }\n    }\n}\n</code></pre>\n<p>✅ <strong>方案2：配置熔断机制</strong></p>\n<pre><code class=\"language-csharp\">public class CircuitBreakerEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly CircuitBreaker _circuitBreaker;\n    private int _failureCount;\n    private DateTime _lastFailureTime;\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        if (_circuitBreaker.IsOpen())\n        {\n            _logger.LogWarning(\"Redis去重熔断，拒绝处理\");\n            return true; // 拒绝处理，保护下游\n        }\n\n        try\n        {\n            var result = _redis.TryMarkAsProcessedAsync(eventId).GetAwaiter().GetResult();\n            _circuitBreaker.RecordSuccess();\n            return result;\n        }\n        catch (Exception ex)\n        {\n            _circuitBreaker.RecordFailure();\n            throw;\n        }\n    }\n}\n</code></pre>\n<hr />\n<h3 id=\"问题-5缓存越积越多内存快爆了\">问题 5：缓存越积越多，内存快爆了</h3>\n<p><strong>现场是这样的</strong>：</p>\n<pre><code>服务运行24小时\n去重缓存条目：100万+\n内存占用：超过500MB\nGC压力增大，性能下降\n</code></pre>\n<p><strong>为什么？</strong></p>\n<ul>\n<li>📈 高频事件（比如接收消息）一天能产生几十万条记录</li>\n<li>🕐 清理间隔太长，旧记录堆积如山</li>\n</ul>\n<p><strong>怎么解决？</strong></p>\n<p>✅ <strong>方案1：缩短缓存过期时间</strong></p>\n<pre><code class=\"language-csharp\">services.AddFeishuWebSocketServiceBuilder(configuration)\n    .ConfigureOptions(options =&gt;\n    {\n        // 根据实际业务调整\n        options.EventDeduplicationCacheExpirationMs = 3600000;  // 缩短为1小时\n        options.EventDeduplicationCleanupIntervalMs = 60000;    // 提高清理频率为1分钟\n    });\n</code></pre>\n<p>✅ <strong>方案2：使用 LRU 缓存</strong></p>\n<pre><code class=\"language-csharp\">public class LRUEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly LRUCache&lt;string, EventCacheEntry&gt; _cache;\n    private const int MaxCacheSize = 10000; // 最多保留1万条\n\n    public LRUEventDeduplicator()\n    {\n        _cache = new LRUCache&lt;string, EventCacheEntry&gt;(MaxCacheSize);\n    }\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        if (_cache.TryGetValue(eventId, out var entry))\n            return true;\n\n        _cache.Add(eventId, new EventCacheEntry { /* ... */ });\n        return false;\n    }\n}\n</code></pre>\n<p>✅ <strong>方案3：业务层优化</strong></p>\n<pre><code class=\"language-csharp\">// 某些高频事件不需要去重\npublic class SystemEventHandler : DefaultFeishuEventHandler\n{\n    // 不继承 IdempotentFeishuEventHandler\n    // 直接处理，减少业务层去重压力\n\n    protected override Task ProcessBusinessLogicAsync(EventData eventData, CancellationToken ct)\n    {\n        // 仅记录日志，不进行业务操作\n        _logger.LogInformation(\"系统事件：{EventType}\", eventData.EventType);\n        return Task.CompletedTask;\n    }\n}\n</code></pre>\n<hr />\n<h2 id=\"给系统做体检监控与调试\">给系统做体检：监控与调试</h2>\n<h3 id=\"采集什么数据最有效\">采集什么数据最有效？</h3>\n<pre><code class=\"language-csharp\">public class DeduplicatorMetrics\n{\n    public long TotalProcessed { get; set; }\n    public long DuplicateSkipped { get; set; }\n    public long ProcessingTimeout { get; set; }\n    public long CacheHitCount { get; set; }\n    public long CacheMissCount { get; set; }\n\n    public double DuplicateRate =&gt; \n        TotalProcessed &gt; 0 ? (double)DuplicateSkipped / TotalProcessed : 0;\n\n    public double CacheHitRate =&gt;\n        (CacheHitCount + CacheMissCount) &gt; 0 \n            ? (double)CacheHitCount / (CacheHitCount + CacheMissCount) \n            : 0;\n}\n</code></pre>\n<h3 id=\"暴露一个接口随时查看健康状态\">暴露一个接口，随时查看健康状态</h3>\n<pre><code class=\"language-csharp\">[ApiController]\n[Route(\"api/[controller]\")]\npublic class DeduplicatorController : ControllerBase\n{\n    private readonly IFeishuEventDeduplicator _deduplicator;\n\n    [HttpGet(\"stats\")]\n    public ActionResult&lt;DeduplicatorStats&gt; GetStats()\n    {\n        return Ok(new DeduplicatorStats\n        {\n            CacheCount = _deduplicator.GetCacheCount(),\n            MaxProcessedSeqId = _seqIdDeduplicator?.GetMaxProcessedSeqId(),\n            DuplicateRate = _metrics.DuplicateRate,\n            CacheHitRate = _metrics.CacheHitRate\n        });\n    }\n\n    [HttpGet(\"check/{eventId}\")]\n    public ActionResult&lt;bool&gt; CheckEventId(string eventId)\n    {\n        var isProcessed = _deduplicator.IsProcessed(eventId);\n        return Ok(new { eventId, isProcessed });\n    }\n}\n</code></pre>\n<h3 id=\"日志怎么写才容易排查问题\">日志怎么写才容易排查问题？</h3>\n<pre><code class=\"language-csharp\">public class EnhancedEventDeduplicator : IFeishuEventDeduplicator\n{\n    private readonly ILogger&lt;EnhancedEventDeduplicator&gt; _logger;\n\n    public bool TryMarkAsProcessing(string eventId)\n    {\n        lock (_lock)\n        {\n            if (_eventCache.TryGetValue(eventId, out var entry))\n            {\n                // 详细记录去重命中原因\n                _logger.LogInformation(\n                    \"[Deduplication] EventId={EventId} 已处理，\" +\n                    \"Status={Status}, ProcessedAt={ProcessedAt}\",\n                    eventId,\n                    entry.Status,\n                    entry.ProcessedAt);\n\n                return true;\n            }\n\n            _logger.LogDebug(\"[Deduplication] EventId={EventId} 新事件\", eventId);\n            // ...\n            return false;\n        }\n    }\n}\n</code></pre>\n<hr />\n<h2 id=\"快速回顾与行动清单\">快速回顾与行动清单</h2>\n<h3 id=\"三层防护一表览\">三层防护一表览</h3>\n<table>\n<thead>\n<tr>\n<th>去重层级</th>\n<th>去重依据</th>\n<th>实现方式</th>\n<th>推荐配置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>协议层</strong></td>\n<td>SeqID / Nonce</td>\n<td><code>IFeishuSeqIDDeduplicator</code> / <code>IFeishuNonceDistributedDeduplicator</code></td>\n<td>始终启用</td>\n</tr>\n<tr>\n<td><strong>分发层</strong></td>\n<td>EventId</td>\n<td><code>IFeishuEventDeduplicator</code> / <code>IFeishuEventDistributedDeduplicator</code></td>\n<td>生产环境启用 Redis</td>\n</tr>\n<tr>\n<td><strong>应用层</strong></td>\n<td>数据存在性检查</td>\n<td><code>DefaultFeishuObjectEventHandler&lt;T&gt;</code> 中的业务逻辑</td>\n<td>按需实现</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"还想了解更多\">还想了解更多？</h3>\n<ul>\n<li>📖 <a href=\"https://open.feishu.cn/document/server-docs/event-subscription-guide/overview\" rel=\"noopener nofollow\" target=\"_blank\">飞书官方文档：事件订阅</a></li>\n<li>💻 <a href=\"https://github.com/mudtools/MudFeishu\" rel=\"noopener nofollow\" target=\"_blank\">MudFeishu GitHub 仓库</a></li>\n<li>💻 <a href=\"https://gitee.com/mudtools/MudFeishu\" rel=\"noopener nofollow\" target=\"_blank\">MudFeishu Gitee 仓库</a></li>\n<li>🔒 <a href=\"https://redis.io/topics/distlock\" rel=\"noopener nofollow\" target=\"_blank\">Redis 分布式锁最佳实践</a></li>\n</ul>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-11 23:00</span>&nbsp;\n<a href=\"https://www.cnblogs.com/mudtools\">玩泥巴的|mudtools.cn</a>&nbsp;\n阅读(<span id=\"post_view_count\">263</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "迭代器 iteration、iter 与 多线程 concurrent 交叉实践（详细）",
      "link": "https://www.cnblogs.com/io-T-T/p/19469168",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/io-T-T/p/19469168\" id=\"cb_post_title_url\" title=\"发布于 2026-01-11 22:52\">\n    <span>迭代器 iteration、iter 与 多线程 concurrent 交叉实践（详细）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        迭代器 多线程 concurrent iter python\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"迭代器iterationiter-与-多线程-concurrent-交叉实践详细\">迭代器<code>iteration</code>、<code>iter</code> 与 多线程 <code>concurrent</code> 交叉实践（详细）</h2>\n<h3 id=\"实践及简介说明\">实践及简介说明</h3>\n<p>​\t由于在实际运用重，迭代器（或生成器）经常与多线程一并使用。本实践旨在对迭代器（及生成器）、多线程库（主要为<code>concurrent</code>）进行交叉实践说明，用来使读者更加理解迭代器和多线程在实际的应用。因此本篇轻教程，重实践，当然也会简要说明相关知识。</p>\n<h3 id=\"基础知识相关介绍简要\">基础知识相关介绍（简要）</h3>\n<blockquote>\n<p>详细介绍建议观看<a href=\"https://space.bilibili.com/1318868/?spm_id_from=333.788.upinfo.detail.click\" rel=\"noopener nofollow\" target=\"_blank\">Hucci写代码</a>博主教的基础知识，详细易懂</p>\n<p><a href=\"https://www.bilibili.com/video/BV1jt421c7yN\" rel=\"noopener nofollow\" target=\"_blank\">【Python】从迭代器到生成器：小内存也能处理大数据</a></p>\n<p><a href=\"https://www.bilibili.com/video/BV1tVsyzUEtX/?\" rel=\"noopener nofollow\" target=\"_blank\">在Python中用多线程之前，需要先搞懂这些</a></p>\n</blockquote>\n<h3 id=\"你为什么需要学习迭代器多线程\">你为什么需要学习迭代器、多线程</h3>\n<p>学这个教程/实践之前，你需要先看看自己有没有这个需求，当然你感兴趣也可以学。</p>\n<h4 id=\"迭代器生成器的优劣\">迭代器（生成器）的优劣</h4>\n<p><strong>优势：</strong></p>\n<ul>\n<li>使用时生成数据，缓解爆缓存问题（RAM、GRAM）【主要】</li>\n<li>统一标准接口，无需关系数据的数据结构调用问题，使用 <code>for _iter in iteration</code> 即可调用</li>\n</ul>\n<p><strong>劣势：</strong></p>\n<ul>\n<li>单次使用</li>\n<li>不支持索引</li>\n<li>单向遍历（向前）</li>\n</ul>\n<h4 id=\"多线程的优劣python限定\">多线程的优劣（Python限定）</h4>\n<p><strong>优势</strong>：</p>\n<ul>\n<li>并行执行，提高CPU的使用率</li>\n<li>充分利用I/O的中断时间，从而提高代码的执行速度</li>\n</ul>\n<p><strong>劣势：</strong></p>\n<ul>\n<li>调试难度大</li>\n<li>老版本（好像是3.12之前），只有一个GIL锁，导致进程会出现抢占现象。</li>\n<li>少数据量带来的提升可能较少</li>\n</ul>\n<h3 id=\"01-环境来源数据\">01-环境来源数据</h3>\n<p>我用的是<code>kaggle</code>的开源数据，由于很多都是<code>csv</code>类型，因此我也以简单的<code>csv</code>数据集多线程读取进行说明，你们也可以用其他数据集，最好文本行长度&gt;5k, 才能展现多线程、迭代器的优势，数量级低还是用单线程、串行吧，容易理解好维护。</p>\n<blockquote>\n<p><a href=\"https://www.kaggle.com/datasets/dansbecker/powerlifting-database/data\" rel=\"noopener nofollow\" target=\"_blank\">powerlifting-database</a></p>\n</blockquote>\n<h3 id=\"02-任务介绍及编码思路\">02-任务介绍及编码思路</h3>\n<p>空有编码能力，但是没有编码思路是大忌吧。因此先概述任务及编码思路。</p>\n<h4 id=\"21-任务\">2.1 任务</h4>\n<p>本次实践，任务有以下几点：</p>\n<ol>\n<li>了解并构造迭代器类</li>\n<li>使用迭代器类对数据集（<strong>csv数据</strong>）进行数据划分</li>\n<li>使用多线程库（<code>concurrent</code>）来模拟多线程处理数据。</li>\n</ol>\n<h4 id=\"22-编码思路\">2.2 编码思路</h4>\n<p>想要达成多线程与迭代器的结合，首先需要构造合适的数据，因此需要：</p>\n<ol>\n<li>构建迭代器类（框架）</li>\n</ol>\n<p>然后需要将数据集导入进来，完善迭代器类，还需要对输入的数据做一下分块，方便后续使用：</p>\n<ol start=\"2\">\n<li>数据导入+分块+完善迭代器类</li>\n</ol>\n<p>数据来源解决了，也分块了，接下来是将数据与多线程联动了</p>\n<ol start=\"3\">\n<li>引入多线程库，处理切分的数据。</li>\n</ol>\n<h3 id=\"03-构建迭代器类\">03 构建迭代器类</h3>\n<h4 id=\"31迭代器的最小框架\">3.1迭代器的最小框架</h4>\n<blockquote>\n<p>可参考 <a href=\"https://www.cnblogs.com/Ravenna/p/15676404.html\" target=\"_blank\">python迭代器简单理解 __iter__和__next__方法</a></p>\n</blockquote>\n<p>​\t成为一个最小迭代器类通常需要包含以下几个类接口：</p>\n<img alt=\"image-20260111171210573\" class=\"lazyload\" />\n<ol>\n<li><code>__init__</code>：类初始化接口</li>\n<li><code>__iter__</code>: 获取迭代对象的接口</li>\n<li><code>__next__</code>：数据迭代接口。</li>\n</ol>\n<h4 id=\"32--构建迭代器类\">3.2  构建迭代器类</h4>\n<p>为了照顾更多的读者，我这里使用传统接口对<code>csv</code>进行读写，有<code>panda</code>能力的读者可以使用<code>pd</code>快捷读取并构造。</p>\n<pre><code class=\"language-python\">class read_file_batch:\n    def __init__(self,file_path:str, batch_size:int):\n        self.file_fp = open(file_path, encoding='utf-8', mode='r+')\n        self.batch_size = batch_size\n    def __iter__(self):\n        return self\t\t\t\t\t#返回这个迭代器本身，我们是以类作为迭代对象，因此返回他自己吧\n    def __next__(self):\t\t\t\t#数据划分\n        batch_res = []\n        for i in range(self.batch_size):\t\n            line = self.file_fp.readline()\t#最底层获取行数据的交互接口\n            if line:                        #如果行非空\n                batch_res.append(line)\n            else:                           #读到最后就算空的了\n                self.file_fp.close()\n                raise StopIteration\n\n        return batch_res\n</code></pre>\n<ul>\n<li>\n<p>调用尝试：</p>\n<p>可以调试一下，感受一下分批的迭代及数据的输入输出。</p>\n<pre><code class=\"language-python\">if __name__ == '__main__':\n    your_file_path:str = r\"E:\\powerlifting-database\\openpowerlifting.csv\"\n    iteration = read_file_batch(your_file_path, batch_size=1000)\n    count = 0\n    for lines in iteration:\n        print('-'*50+f\"count:{count}\"+'-'*50)\n        count += 1\n        print(lines)\n</code></pre>\n</li>\n</ul>\n<h3 id=\"04-引入多线程处理批量数据\">04 引入多线程处理批量数据</h3>\n<h4 id=\"41-多线程写法\">4.1 多线程写法</h4>\n<p>其实蛮简单的，只要把接口函数写好、数据处理一下，就行了。</p>\n<ol>\n<li>\n<p>导入库：</p>\n<pre><code class=\"language-python\">from concurrent.futures import ThreadPoolExecutor as Excecutor\n</code></pre>\n</li>\n<li>\n<p>使用上下文管理工具管理线程（确保安全及释放）：</p>\n<pre><code class=\"language-python\">with Excecutor(max_workers=8) as executor:\n</code></pre>\n</li>\n<li>\n<p>使用map接口进行多线程指令执行：</p>\n<pre><code class=\"language-python\">executor.map(\n    fn= function, iter_element1, iter_element2, iter_element3\n)#function是你对应的函数名， iter_element1-3是函数依赖的输入数据\n</code></pre>\n</li>\n<li>\n<p>汇总：</p>\n<pre><code class=\"language-python\">from concurrent.futures import ThreadPoolExecutor as Excecutor\nwith Excecutor(max_workers=8) as executor:\n\texecutor.map(\n    fn= function, iter_element1, iter_element2, iter_element3\n)#function是你对应的函数名， iter_element1-3是函数依赖的输入数据\n</code></pre>\n</li>\n</ol>\n<h4 id=\"42-引入多线程处理数据\">4.2 引入多线程处理数据</h4>\n<ol>\n<li>\n<p>首先，模拟一个处理行数据的函数用户处理行信息：</p>\n<pre><code class=\"language-python\">def process_dealing(line:str):\n    '''\n    模拟处理某一行的数据\n    :param line:处理每一行\n    '''\n    print(f'line top 20 info is :{line[:20]}')\n    pass\n</code></pre>\n</li>\n<li>\n<p>在迭代器的迭代过程中对数据进行处理。</p>\n<pre><code class=\"language-python\">if __name__ == '__main__':\n    your_file_path:str = r\"E:\\powerlifting-database\\openpowerlifting.csv\"\n    iteration = read_file_batch(your_file_path, batch_size=1000)\n    count = 0\n    with Excecutor(max_workers=8) as executor:\n\n        for lines in iteration:\n            print('-'*50+f\"count:{count}\"+'-'*50)\n            count += 1\n            print(lines)\n            executor.map(process_dealing, lines)\n            #第一个参数是处理的函数，后面是*iteration 就是你函数依赖的参数的可迭代对象，这里就是放入list[str]\n</code></pre>\n</li>\n<li>\n<p>结束了，根据你的需要可以对数据进行处理了，其实最难的步骤在于如何构造迭代器（生成器），多线程其实俩步就走完了。</p>\n</li>\n</ol>\n<h3 id=\"05-非必须使用迭代器多线程前后对比\">05 （非必须）使用迭代器、多线程前后对比</h3>\n<pre><code class=\"language-python\">from concurrent.futures import ThreadPoolExecutor as Excecutor\nimport time\nimport tracemalloc  #跟踪内存接口，内置\n\ndef process_dealing(line:str):\n    '''\n    模拟处理某一行的数据\n    :param line:处理每一行\n    '''\n    # print(f'line top 20 info is :{line[:20]}')\n    pass\n\nclass read_file_batch:\n    def __init__(self,file_path:str, batch_size:int):\n        self.file_fp = open(file_path, encoding='utf-8', mode='r+')\n        self.batch_size = batch_size\n    def __iter__(self):\n        return self\n    def __next__(self):                     #数据划分,对迭代器进行迭代，实际上是获取他的next，因此在这里划分数据\n        batch_res = []\n        for i in range(self.batch_size):    #取batch个\n            line = self.file_fp.readline()\n            if line:                        #如果行非空\n                batch_res.append(line)\n            else:                           #读到最后就算空的了\n                self.file_fp.close()\n                raise StopIteration\n\n        return batch_res                    #返回迭代数据\n\ndef main_concurrent(file_path,batch_size:int):\n    start_malloc = tracemalloc.start()\n    start_time = time.time()\n    your_file_path:str = file_path\n    iteration = read_file_batch(your_file_path, batch_size=batch_size)\n    count = 0\n    with Excecutor(max_workers=8) as executor:\n\n        for lines in iteration:                         #返回是batch行,每一行是一个大的str\n            print('-'*50+f\"count:{count}\"+'-'*50+f\"RAM cost:{round(tracemalloc.get_tracemalloc_memory() / 1024**2,4)} M\")\n            count += 1\n            print(lines[:50])                           #节约时间，只取前50字符\n            executor.map(process_dealing, lines)\n            #第一个参数是处理的函数，后面是*iteration 就是你函数依赖的参数的可迭代对象，这里就是放入list[str]\n    print(f'spend time:{round(time.time()-start_time,4)}')\n\ndef main_not_concurrent(file_path,batch_size:int=1000):\n\n    start_time = time.time()\n    start_trace = tracemalloc.start()\n\n    with open(file=file_path,encoding='utf-8',mode='r+') as fp:\n        data = fp.readlines()\n        for idx,line in enumerate(data):\n            if idx % batch_size == 0:\n                print('-'*50 +str(idx)+'-'*50+f\"RAM cost:{round(tracemalloc.get_tracemalloc_memory() / 1024**2,4)} M\")\n                print(line[:50])\n                process_dealing(line)\n    print(f'spend time:{round(time.time()-start_time,4)}')\n\n\nif __name__ == '__main__':\n    file_path = r'E:\\openpowerlifting.csv'\n    batch_size = 1000\n    main_concurrent(file_path,batch_size)\n    # main_not_concurrent(file_path,batch_size)\n    # 其实能看出来这种数据量还是直接缓存在内存比较好，但使用多线程+迭代器确实减少了很多内存\n</code></pre>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-11 22:52</span>&nbsp;\n<a href=\"https://www.cnblogs.com/io-T-T\">io_T_T</a>&nbsp;\n阅读(<span id=\"post_view_count\">101</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "一个高性能的 .NET MQTT 客户端与服务器库",
      "link": "https://www.cnblogs.com/dotnet-org-cn/p/19473369",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/dotnet-org-cn/p/19473369\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 18:10\">\n    <span>一个高性能的 .NET MQTT 客户端与服务器库</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"前言\">前言</h2>\n<p>在物联网（IoT）蓬勃发展的今天，MQTT 协议已经成为设备通信的事实标准。无论是智能家居、工业自动化还是车联网，MQTT 都扮演着至关重要的角色。今天，我要为大家介绍一个完全使用 C# 实现的高性能 MQTT 库</p>\n<p>这个库不仅提供了完整的 MQTT 客户端实现，还包含了一个功能齐全的 Broker 服务器，支持桥接、集群等企业级特性。</p>\n<h2 id=\"核心特性\">核心特性</h2>\n<h3 id=\"协议支持\">协议支持</h3>\n<ul>\n<li><strong>MQTT 3.1.1</strong> - 完整支持</li>\n<li><strong>MQTT 5.0</strong> - 完整支持（包括用户属性、消息过期、主题别名等新特性）</li>\n<li><strong>MQTT-SN</strong> - 基于 UDP 的轻量级 MQTT 变体，适合受限设备</li>\n<li><strong>CoAP</strong> - 约束应用协议网关支持</li>\n</ul>\n<h3 id=\"性能特性\">性能特性</h3>\n<ul>\n<li>高性能异步实现</li>\n<li>零不必要的内存分配</li>\n<li>缓冲区池技术</li>\n<li>支持 10000+ 并发连接</li>\n</ul>\n<h3 id=\"企业级功能\">企业级功能</h3>\n<ul>\n<li>Broker 桥接（多 Broker 消息同步）</li>\n<li>集群支持（去中心化 P2P 架构）</li>\n<li>灵活的认证与授权机制</li>\n<li>TLS/SSL 加密传输</li>\n<li>持久会话与离线消息存储</li>\n</ul>\n<h3 id=\"框架支持\">框架支持</h3>\n<ul>\n<li>.NET 6.0</li>\n<li>.NET 8.0</li>\n<li>.NET 10.0</li>\n</ul>\n<hr />\n<h2 id=\"技术实现\">技术实现</h2>\n<p>本项目采用了大量现代 .NET 高性能技术，下面详细介绍核心技术点。</p>\n<h3 id=\"内存管理技术\">内存管理技术</h3>\n<h4 id=\"spant-和-memoryt---零拷贝处理\">Span&lt;T&gt; 和 Memory&lt;T&gt; - 零拷贝处理</h4>\n<p>项目使用 <code>ref struct</code> 实现的二进制读写器，完全在栈上分配，避免堆内存压力：</p>\n<pre><code class=\"language-csharp\">// 零拷贝的二进制读取器\npublic ref struct MqttBinaryReader\n{\n    private readonly ReadOnlySpan&lt;byte&gt; _buffer;\n    private int _position;\n\n    // 零拷贝切片操作\n    [MethodImpl(MethodImplOptions.AggressiveInlining)]\n    public ReadOnlySpan&lt;byte&gt; ReadBytes(int count)\n    {\n        var span = _buffer.Slice(_position, count);\n        _position += count;\n        return span;\n    }\n}\n</code></pre>\n<p><strong>技术优势</strong>：</p>\n<ul>\n<li><code>ref struct</code> 只能在栈上分配，无 GC 压力</li>\n<li><code>ReadOnlySpan&lt;byte&gt;</code> 支持零拷贝切片</li>\n<li>避免大量字节数组复制操作</li>\n</ul>\n<h4 id=\"arraypoolt---缓冲区复用\">ArrayPool&lt;T&gt; - 缓冲区复用</h4>\n<p>使用共享内存池减少频繁的内存分配：</p>\n<pre><code class=\"language-csharp\">// 从共享池租借缓冲区\nvar buffer = ArrayPool&lt;byte&gt;.Shared.Rent(1024);\ntry\n{\n    await stream.ReadAsync(buffer.AsMemory(0, length), cancellationToken);\n    // 处理数据...\n}\nfinally\n{\n    ArrayPool&lt;byte&gt;.Shared.Return(buffer);  // 归还缓冲区\n}\n</code></pre>\n<h4 id=\"stackalloc---小缓冲区栈分配\">stackalloc - 小缓冲区栈分配</h4>\n<p>对于小型临时缓冲区，直接在栈上分配：</p>\n<pre><code class=\"language-csharp\">// 4 字节的可变长度编码缓冲区，栈分配\nSpan&lt;byte&gt; remainingLengthBytes = stackalloc byte[4];\nvar size = EncodeRemainingLength(length, remainingLengthBytes);\n</code></pre>\n<h3 id=\"异步编程模型\">异步编程模型</h3>\n<h4 id=\"asyncawait--configureawait\">async/await + ConfigureAwait</h4>\n<p>所有 IO 操作均采用异步模式，并使用 <code>ConfigureAwait(false)</code> 优化：</p>\n<pre><code class=\"language-csharp\">public async Task&lt;MqttConnectResult&gt; ConnectAsync(CancellationToken cancellationToken = default)\n{\n    // 建立 TCP 连接\n    await _tcpClient.ConnectAsync(host, port, cancellationToken).ConfigureAwait(false);\n\n    // TLS 握手\n    if (Options.UseTls)\n    {\n        await sslStream.AuthenticateAsClientAsync(sslOptions, cancellationToken).ConfigureAwait(false);\n    }\n\n    // 发送 CONNECT 报文\n    await SendPacketAsync(connectPacket, cancellationToken).ConfigureAwait(false);\n}\n</code></pre>\n<h4 id=\"channelt---高性能事件队列\">Channel&lt;T&gt; - 高性能事件队列</h4>\n<p>Broker 使用有界通道实现非阻塞的事件分发：</p>\n<pre><code class=\"language-csharp\">public sealed class MqttBrokerEventDispatcher\n{\n    private readonly Channel&lt;BrokerEvent&gt; _eventChannel;\n\n    public MqttBrokerEventDispatcher(int capacity = 10000)\n    {\n        // 有界通道，队列满时丢弃最旧事件\n        _eventChannel = Channel.CreateBounded&lt;BrokerEvent&gt;(new BoundedChannelOptions(capacity)\n        {\n            FullMode = BoundedChannelFullMode.DropOldest,\n            SingleReader = true,\n            SingleWriter = false\n        });\n    }\n\n    // 非阻塞事件发送\n    public void Dispatch&lt;TEventArgs&gt;(BrokerEventType type, TEventArgs args, EventHandler&lt;TEventArgs&gt;? handler)\n    {\n        _eventChannel.Writer.TryWrite(new BrokerEvent(type, args, handler));\n    }\n}\n</code></pre>\n<h4 id=\"taskcompletionsource---请求响应模式\">TaskCompletionSource - 请求/响应模式</h4>\n<p>实现 QoS 1/2 的确认等待机制：</p>\n<pre><code class=\"language-csharp\">private readonly Dictionary&lt;ushort, TaskCompletionSource&lt;object?&gt;&gt; _pendingPackets = new();\n\nprivate async Task&lt;object?&gt; WaitForPacketAsync(ushort packetId, CancellationToken cancellationToken)\n{\n    var tcs = new TaskCompletionSource&lt;object?&gt;(TaskCreationOptions.RunContinuationsAsynchronously);\n    _pendingPackets[packetId] = tcs;\n\n    using var cts = CancellationTokenSource.CreateLinkedTokenSource(cancellationToken);\n    cts.CancelAfter(TimeSpan.FromSeconds(30));  // 30 秒超时\n\n    using var registration = cts.Token.Register(() =&gt; tcs.TrySetCanceled());\n    return await tcs.Task.ConfigureAwait(false);\n}\n</code></pre>\n<h4 id=\"semaphoreslim---发送同步\">SemaphoreSlim - 发送同步</h4>\n<p>确保报文发送的串行化：</p>\n<pre><code class=\"language-csharp\">private readonly SemaphoreSlim _sendLock = new(1, 1);\n\nprivate async Task SendPacketBytesAsync(byte[] packet, CancellationToken cancellationToken)\n{\n    await _sendLock.WaitAsync(cancellationToken).ConfigureAwait(false);\n    try\n    {\n        await _stream.WriteAsync(packet.AsMemory(), cancellationToken).ConfigureAwait(false);\n        await _stream.FlushAsync(cancellationToken).ConfigureAwait(false);\n    }\n    finally\n    {\n        _sendLock.Release();\n    }\n}\n</code></pre>\n<h3 id=\"编译器优化\">编译器优化</h3>\n<h4 id=\"methodimpl-特性\">MethodImpl 特性</h4>\n<p>针对不同场景使用合适的编译器优化指令：</p>\n<pre><code class=\"language-csharp\">// 强制内联 - 用于频繁调用的短方法\n[MethodImpl(MethodImplOptions.AggressiveInlining)]\npublic ushort ReadUInt16()\n{\n    var value = (ushort)((_buffer[_position] &lt;&lt; 8) | _buffer[_position + 1]);\n    _position += 2;\n    return value;\n}\n\n// 最积极的优化 - 用于热路径\n[MethodImpl(MethodImplOptions.AggressiveOptimization)]\nprivate static async Task&lt;int&gt; DecodeRemainingLengthAsync(Stream stream, CancellationToken ct)\n{\n    // 可变长度解码实现...\n}\n\n// 禁止内联 - 避免异常处理代码膨胀热路径\n[MethodImpl(MethodImplOptions.NoInlining)]\nprivate void ThrowIfDisposed()\n{\n    if (_disposed) throw new ObjectDisposedException(nameof(MqttClient));\n}\n</code></pre>\n<h3 id=\"网络编程\">网络编程</h3>\n<h4 id=\"多层传输抽象\">多层传输抽象</h4>\n<p>支持 TCP、UDP 等多种传输方式：</p>\n<pre><code class=\"language-csharp\">public interface ITransportConnection : IAsyncDisposable\n{\n    string ConnectionId { get; }\n    TransportType TransportType { get; }\n    EndPoint? RemoteEndPoint { get; }\n    bool IsConnected { get; }\n\n    ValueTask&lt;int&gt; ReadAsync(Memory&lt;byte&gt; buffer, CancellationToken cancellationToken = default);\n    ValueTask WriteAsync(ReadOnlyMemory&lt;byte&gt; buffer, CancellationToken cancellationToken = default);\n    ValueTask FlushAsync(CancellationToken cancellationToken = default);\n}\n</code></pre>\n<h4 id=\"tlsssl-支持\">TLS/SSL 支持</h4>\n<p>使用 SslStream 实现加密传输，支持 TLS 1.2 和 TLS 1.3：</p>\n<pre><code class=\"language-csharp\">var sslOptions = new SslClientAuthenticationOptions\n{\n    TargetHost = Options.Host,\n    EnabledSslProtocols = SslProtocols.Tls12 | SslProtocols.Tls13,\n    ClientCertificates = Options.ClientCertificate != null\n        ? new X509CertificateCollection { Options.ClientCertificate }\n        : null\n};\n\nawait sslStream.AuthenticateAsClientAsync(sslOptions, cancellationToken);\n</code></pre>\n<h3 id=\"协议序列化\">协议序列化</h3>\n<h4 id=\"工厂模式--延迟初始化\">工厂模式 + 延迟初始化</h4>\n<p>协议处理器采用单例 + 延迟初始化模式：</p>\n<pre><code class=\"language-csharp\">public static class MqttProtocolHandlerFactory\n{\n    private static readonly Lazy&lt;IMqttProtocolHandler&gt; _v311Handler =\n        new(() =&gt; new V311ProtocolHandler());\n    private static readonly Lazy&lt;IMqttProtocolHandler&gt; _v500Handler =\n        new(() =&gt; new V500ProtocolHandler());\n\n    public static IMqttProtocolHandler GetHandler(MqttProtocolVersion version)\n    {\n        return version switch\n        {\n            MqttProtocolVersion.V311 =&gt; _v311Handler.Value,\n            MqttProtocolVersion.V500 =&gt; _v500Handler.Value,\n            _ =&gt; throw new NotSupportedException()\n        };\n    }\n}\n</code></pre>\n<h4 id=\"可变长度整数编码\">可变长度整数编码</h4>\n<p>MQTT 协议特有的可变长度编码，1-4 字节可表示 0 到 268,435,455：</p>\n<pre><code class=\"language-csharp\">public uint ReadVariableByteInteger()\n{\n    uint value = 0;\n    int multiplier = 1;\n    byte encodedByte;\n\n    do\n    {\n        encodedByte = _buffer[_position++];\n        value += (uint)((encodedByte &amp; 0x7F) * multiplier);\n        multiplier *= 128;\n    } while ((encodedByte &amp; 0x80) != 0);\n\n    return value;\n}\n</code></pre>\n<h3 id=\"并发数据结构\">并发数据结构</h3>\n<h4 id=\"concurrentdictionary---线程安全集合\">ConcurrentDictionary - 线程安全集合</h4>\n<p>用于管理客户端会话和订阅：</p>\n<pre><code class=\"language-csharp\">private readonly ConcurrentDictionary&lt;string, MqttClientSession&gt; _sessions = new();\nprivate readonly ConcurrentDictionary&lt;string, MqttApplicationMessage&gt; _retainedMessages = new();\n\npublic int ConnectedClients =&gt; _sessions.Count;\npublic IEnumerable&lt;MqttClientSession&gt; Sessions =&gt; _sessions.Values;\n</code></pre>\n<h3 id=\"设计模式应用\">设计模式应用</h3>\n<table>\n<thead>\n<tr>\n<th>模式</th>\n<th>应用场景</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>工厂模式</strong></td>\n<td>协议处理器创建</td>\n<td><code>MqttProtocolHandlerFactory</code></td>\n</tr>\n<tr>\n<td><strong>策略模式</strong></td>\n<td>不同协议版本实现</td>\n<td><code>V311ProtocolHandler</code> / <code>V500ProtocolHandler</code></td>\n</tr>\n<tr>\n<td><strong>建造者模式</strong></td>\n<td>报文构建</td>\n<td><code>IPublishPacketBuilder</code> / <code>IConnectPacketBuilder</code></td>\n</tr>\n<tr>\n<td><strong>观察者模式</strong></td>\n<td>事件系统</td>\n<td><code>MessageReceived</code> / <code>ClientConnected</code></td>\n</tr>\n<tr>\n<td><strong>装饰器模式</strong></td>\n<td>传输层 TLS</td>\n<td><code>SslStream</code> 装饰 <code>NetworkStream</code></td>\n</tr>\n<tr>\n<td><strong>单例模式</strong></td>\n<td>协议处理器缓存</td>\n<td>全局共享的处理器实例</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"技术栈总结\">技术栈总结</h3>\n<table>\n<thead>\n<tr>\n<th>类别</th>\n<th>技术</th>\n<th>作用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>内存管理</strong></td>\n<td><code>Span&lt;T&gt;</code>, <code>Memory&lt;T&gt;</code>, <code>ref struct</code>, <code>ArrayPool&lt;T&gt;</code>, <code>stackalloc</code></td>\n<td>零拷贝、栈分配、缓冲区复用</td>\n</tr>\n<tr>\n<td><strong>异步编程</strong></td>\n<td><code>async/await</code>, <code>Channel&lt;T&gt;</code>, <code>TaskCompletionSource</code>, <code>SemaphoreSlim</code></td>\n<td>高效并发、非阻塞事件处理</td>\n</tr>\n<tr>\n<td><strong>编译优化</strong></td>\n<td><code>AggressiveInlining</code>, <code>AggressiveOptimization</code>, <code>NoInlining</code></td>\n<td>JIT 编译器优化提示</td>\n</tr>\n<tr>\n<td><strong>网络层</strong></td>\n<td><code>TcpClient</code>, <code>TcpListener</code>, <code>SslStream</code>, 传输抽象</td>\n<td>多协议支持、安全传输</td>\n</tr>\n<tr>\n<td><strong>并发集合</strong></td>\n<td><code>ConcurrentDictionary</code>, <code>ConcurrentQueue</code></td>\n<td>线程安全的数据结构</td>\n</tr>\n<tr>\n<td><strong>序列化</strong></td>\n<td>自定义二进制读写器、可变长度编码</td>\n<td>高效的协议解析</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"性能优化建议\">性能优化建议</h2>\n<h3 id=\"客户端优化\">客户端优化</h3>\n<ol>\n<li><strong>选择合适的 QoS</strong>：大多数场景 QoS 1 就足够了，QoS 2 开销较大</li>\n<li><strong>批量发送</strong>：如果有大量消息，考虑合并后发送</li>\n<li><strong>合理设置 KeepAlive</strong>：根据网络环境调整，一般 60 秒即可</li>\n<li><strong>使用持久会话</strong>：如果需要接收离线消息，设置 <code>CleanSession = false</code></li>\n</ol>\n<h3 id=\"broker-优化\">Broker 优化</h3>\n<ol>\n<li><strong>调整最大连接数</strong>：根据服务器性能设置 <code>MaxConnections</code></li>\n<li><strong>限制消息大小</strong>：设置 <code>MaxMessageSize</code> 防止恶意大消息</li>\n<li><strong>离线消息限制</strong>：设置 <code>MaxOfflineMessagesPerClient</code> 防止内存溢出</li>\n<li><strong>使用集群</strong>：高可用场景使用集群部署</li>\n</ol>\n<hr />\n<h2 id=\"客户端使用指南\">客户端使用指南</h2>\n<h3 id=\"基础连接\">基础连接</h3>\n<pre><code class=\"language-csharp\">using System.Net.MQTT;\n\n// 配置客户端选项\nvar options = new MqttClientOptions\n{\n    Host = \"localhost\",\n    Port = 1883,\n    ClientId = \"my-iot-device\",\n    CleanSession = true\n};\n\n// 创建客户端\nusing var client = new MqttClient(options);\n\n// 连接到 Broker\nvar result = await client.ConnectAsync();\n\nif (result.IsSuccess)\n{\n    Console.WriteLine(\"连接成功！\");\n}\n</code></pre>\n<h3 id=\"订阅主题\">订阅主题</h3>\n<pre><code class=\"language-csharp\">// 订阅单个主题\nawait client.SubscribeAsync(\"sensors/temperature\", MqttQualityOfService.AtLeastOnce);\n\n// 使用通配符订阅多个主题\nawait client.SubscribeAsync(\"sensors/#\", MqttQualityOfService.AtLeastOnce);  // 多级通配符\nawait client.SubscribeAsync(\"sensors/+/status\", MqttQualityOfService.AtMostOnce);  // 单级通配符\n</code></pre>\n<h3 id=\"接收消息\">接收消息</h3>\n<pre><code class=\"language-csharp\">client.MessageReceived += (sender, e) =&gt;\n{\n    Console.WriteLine($\"收到消息:\");\n    Console.WriteLine($\"  主题: {e.Message.Topic}\");\n    Console.WriteLine($\"  内容: {e.Message.PayloadAsString}\");\n    Console.WriteLine($\"  QoS: {e.Message.QualityOfService}\");\n};\n</code></pre>\n<h3 id=\"发布消息\">发布消息</h3>\n<pre><code class=\"language-csharp\">// 简单发布\nawait client.PublishAsync(\"sensors/temperature\", \"25.5\");\n\n// 指定 QoS 发布\nawait client.PublishAsync(\"sensors/humidity\", \"60%\", MqttQualityOfService.AtLeastOnce);\n\n// 发布保留消息\nawait client.PublishAsync(\"device/status\", \"online\", MqttQualityOfService.AtLeastOnce, retain: true);\n\n// 使用完整的消息对象\nvar message = MqttApplicationMessage.Create(\n    topic: \"sensors/data\",\n    payload: \"{\\\"temp\\\": 25.5, \\\"humidity\\\": 60}\",\n    qos: MqttQualityOfService.ExactlyOnce,\n    retain: false\n);\nawait client.PublishAsync(message);\n</code></pre>\n<h3 id=\"遗嘱消息last-will\">遗嘱消息（Last Will）</h3>\n<p>遗嘱消息会在客户端异常断开时自动发布：</p>\n<pre><code class=\"language-csharp\">var options = new MqttClientOptions\n{\n    Host = \"localhost\",\n    ClientId = \"my-device\",\n    WillMessage = MqttApplicationMessage.Create(\n        topic: \"devices/my-device/status\",\n        payload: \"offline\",\n        qos: MqttQualityOfService.AtLeastOnce,\n        retain: true\n    )\n};\n</code></pre>\n<h3 id=\"tls-加密连接\">TLS 加密连接</h3>\n<pre><code class=\"language-csharp\">var options = new MqttClientOptions\n{\n    Host = \"secure-broker.example.com\",\n    Port = 8883,\n    UseTls = true,\n    // 可选：客户端证书\n    ClientCertificate = new X509Certificate2(\"client.pfx\", \"password\")\n};\n</code></pre>\n<h3 id=\"自动重连\">自动重连</h3>\n<pre><code class=\"language-csharp\">var options = new MqttClientOptions\n{\n    Host = \"localhost\",\n    AutoReconnect = true,\n    ReconnectDelayMs = 5000  // 5秒后重连\n};\n\nclient.Connected += (s, e) =&gt; Console.WriteLine(\"已连接\");\nclient.Disconnected += (s, e) =&gt; Console.WriteLine(\"连接断开，正在重连...\");\n</code></pre>\n<h3 id=\"完整客户端示例\">完整客户端示例</h3>\n<pre><code class=\"language-csharp\">using System.Net.MQTT;\n\nvar options = new MqttClientOptions\n{\n    Host = \"localhost\",\n    Port = 1883,\n    ClientId = $\"client-{Guid.NewGuid():N}\",\n    Username = \"user\",\n    Password = \"password\",\n    CleanSession = true,\n    KeepAliveSeconds = 60,\n    AutoReconnect = true\n};\n\nusing var client = new MqttClient(options);\n\n// 设置事件处理\nclient.Connected += (s, e) =&gt; Console.WriteLine(\"[事件] 已连接到 Broker\");\nclient.Disconnected += (s, e) =&gt; Console.WriteLine(\"[事件] 连接已断开\");\nclient.MessageReceived += (s, e) =&gt;\n{\n    Console.WriteLine($\"[消息] {e.Message.Topic}: {e.Message.PayloadAsString}\");\n};\n\n// 连接\nvar result = await client.ConnectAsync();\nif (!result.IsSuccess)\n{\n    Console.WriteLine($\"连接失败: {result.ReasonCode}\");\n    return;\n}\n\n// 订阅\nawait client.SubscribeAsync(\"test/#\", MqttQualityOfService.AtLeastOnce);\n\n// 发布测试消息\nfor (int i = 0; i &lt; 10; i++)\n{\n    await client.PublishAsync(\"test/counter\", i.ToString());\n    await Task.Delay(1000);\n}\n\n// 断开连接\nawait client.DisconnectAsync();\n</code></pre>\n<hr />\n<h2 id=\"服务器broker使用指南\">服务器（Broker）使用指南</h2>\n<h3 id=\"启动基础-broker\">启动基础 Broker</h3>\n<pre><code class=\"language-csharp\">using System.Net.MQTT.Broker;\n\nvar options = new MqttBrokerOptions\n{\n    Port = 1883,\n    AllowAnonymous = true,\n    EnableRetainedMessages = true,\n    MaxConnections = 10000\n};\n\nusing var broker = new MqttBroker(options);\n\n// 启动服务器\nawait broker.StartAsync();\nConsole.WriteLine(\"MQTT Broker 已启动，监听端口 1883\");\n\n// 保持运行\nawait Task.Delay(Timeout.Infinite);\n\n// 停止服务器\nawait broker.StopAsync();\n</code></pre>\n<h3 id=\"配置认证\">配置认证</h3>\n<pre><code class=\"language-csharp\">// 使用简单认证器\nbroker.Authenticator = new SimpleAuthenticator()\n    .AddUser(\"admin\", \"admin123\")\n    .AddUser(\"device1\", \"device1pass\")\n    .AddUser(\"device2\", \"device2pass\");\n\nvar options = new MqttBrokerOptions\n{\n    Port = 1883,\n    AllowAnonymous = false  // 禁用匿名访问\n};\n</code></pre>\n<h3 id=\"自定义认证器\">自定义认证器</h3>\n<pre><code class=\"language-csharp\">public class MyAuthenticator : IMqttAuthenticator\n{\n    public Task&lt;MqttAuthenticationResult&gt; AuthenticateAsync(\n        MqttAuthenticationContext context,\n        CancellationToken cancellationToken)\n    {\n        // 从数据库验证用户\n        if (ValidateFromDatabase(context.Username, context.Password))\n        {\n            return Task.FromResult(MqttAuthenticationResult.Success());\n        }\n\n        return Task.FromResult(MqttAuthenticationResult.Failure(\n            MqttConnectReasonCode.BadUserNameOrPassword));\n    }\n}\n\nbroker.Authenticator = new MyAuthenticator();\n</code></pre>\n<h3 id=\"broker-事件处理\">Broker 事件处理</h3>\n<pre><code class=\"language-csharp\">// 客户端连接事件\nbroker.ClientConnected += (s, e) =&gt;\n{\n    Console.WriteLine($\"[连接] 客户端 {e.Session.ClientId} 已连接\");\n    Console.WriteLine($\"  地址: {e.Session.RemoteEndpoint}\");\n    Console.WriteLine($\"  当前连接数: {broker.ConnectedClients}\");\n};\n\n// 客户端断开事件\nbroker.ClientDisconnected += (s, e) =&gt;\n{\n    Console.WriteLine($\"[断开] 客户端 {e.Session.ClientId} 已断开\");\n};\n\n// 客户端订阅事件\nbroker.ClientSubscribed += (s, e) =&gt;\n{\n    Console.WriteLine($\"[订阅] {e.Session.ClientId} 订阅了 {e.TopicFilter}\");\n};\n\n// 消息发布事件\nbroker.MessagePublished += (s, e) =&gt;\n{\n    Console.WriteLine($\"[消息] {e.Message.Topic}: {e.Message.PayloadAsString}\");\n    Console.WriteLine($\"  来自: {e.SourceClientId}\");\n};\n\n// 消息发布前拦截（可以阻止消息发布）\nbroker.MessagePublishing += (s, e) =&gt;\n{\n    // 检查敏感主题\n    if (e.Message.Topic.StartsWith(\"admin/\") &amp;&amp; e.SourceClientId != \"admin\")\n    {\n        e.Cancel = true;  // 阻止非管理员发布到 admin 主题\n    }\n};\n</code></pre>\n<h3 id=\"tls-配置\">TLS 配置</h3>\n<pre><code class=\"language-csharp\">var options = new MqttBrokerOptions\n{\n    // 普通端口\n    Port = 1883,\n\n    // TLS 端口\n    UseTls = true,\n    TlsPort = 8883,\n    ServerCertificate = new X509Certificate2(\"server.pfx\", \"password\"),\n    RequireClientCertificate = false\n};\n</code></pre>\n<hr />\n<h2 id=\"高级功能\">高级功能</h2>\n<h3 id=\"broker-桥接\">Broker 桥接</h3>\n<p>桥接功能允许将多个 Broker 连接起来，实现消息的跨 Broker 同步。</p>\n<pre><code class=\"language-csharp\">var broker = new MqttBroker(new MqttBrokerOptions { Port = 2883 });\n\n// 添加桥接到父 Broker\nvar bridge = broker.AddBridge(new MqttBridgeOptions\n{\n    Name = \"parent-bridge\",\n    RemoteHost = \"parent-broker.example.com\",\n    RemotePort = 1883,\n    ClientId = \"bridge-client-1\",\n\n    // 上行规则：本地消息 -&gt; 远程 Broker\n    UpstreamRules =\n    {\n        new MqttBridgeRule { LocalTopicFilter = \"sensor/#\", Enabled = true },\n        new MqttBridgeRule { LocalTopicFilter = \"device/+/data\", Enabled = true }\n    },\n\n    // 下行规则：远程消息 -&gt; 本地\n    DownstreamRules =\n    {\n        new MqttBridgeRule { LocalTopicFilter = \"commands/#\", Enabled = true },\n        new MqttBridgeRule { LocalTopicFilter = \"config/#\", Enabled = true }\n    }\n});\n\n// 桥接事件\nbridge.Connected += (s, e) =&gt; Console.WriteLine(\"桥接已连接\");\nbridge.MessageForwarded += (s, e) =&gt;\n{\n    var direction = e.Direction == BridgeDirection.Upstream ? \"上行\" : \"下行\";\n    Console.WriteLine($\"[桥接-{direction}] {e.OriginalTopic}\");\n};\n\n// 获取统计信息\nvar stats = bridge.GetStatistics();\nConsole.WriteLine($\"上行消息: {stats.UpstreamMessageCount}\");\nConsole.WriteLine($\"下行消息: {stats.DownstreamMessageCount}\");\n</code></pre>\n<h3 id=\"集群部署\">集群部署</h3>\n<p>集群功能实现了去中心化的 P2P 架构，任何节点都可以独立运行，支持自动故障检测和恢复。</p>\n<pre><code class=\"language-csharp\">var broker = new MqttBroker(new MqttBrokerOptions { Port = 1883 });\n\n// 启用集群\nbroker.EnableCluster(new MqttClusterOptions\n{\n    NodeId = \"node-1\",\n    ClusterName = \"my-cluster\",\n    ClusterPort = 11883,\n    SeedNodes = new List&lt;string&gt;\n    {\n        \"node2.example.com:11883\",\n        \"node3.example.com:11883\"\n    },\n    HeartbeatIntervalMs = 5000,\n    NodeTimeoutMs = 15000,\n    EnableDeduplication = true  // 防止消息重复\n});\n\n// 集群事件\nbroker.Cluster!.PeerJoined += (s, e) =&gt;\n    Console.WriteLine($\"节点加入: {e.Peer.NodeId}\");\n\nbroker.Cluster!.PeerLeft += (s, e) =&gt;\n    Console.WriteLine($\"节点离开: {e.Peer.NodeId}\");\n\nbroker.Cluster!.MessageForwarded += (s, e) =&gt;\n    Console.WriteLine($\"消息转发: {e.Topic}\");\n\nawait broker.StartAsync();\n</code></pre>\n<h3 id=\"mqtt-sn-网关\">MQTT-SN 网关</h3>\n<p>MQTT-SN 是基于 UDP 的轻量级协议，适合资源受限的嵌入式设备：</p>\n<pre><code class=\"language-csharp\">var options = new MqttBrokerOptions\n{\n    Port = 1883,\n    EnableMqttSn = true,\n    MqttSnPort = 1885\n};\n</code></pre>\n<h3 id=\"coap-网关\">CoAP 网关</h3>\n<p>CoAP 网关允许 CoAP 设备与 MQTT 生态系统互通：</p>\n<pre><code class=\"language-csharp\">var options = new MqttBrokerOptions\n{\n    Port = 1883,\n    EnableCoAP = true,\n    CoapPort = 5683,\n    CoapMqttPrefix = \"mqtt\"\n};\n\n// CoAP 客户端可以通过以下方式访问 MQTT 主题：\n// GET coap://broker:5683/mqtt/sensors/temperature\n// PUT coap://broker:5683/mqtt/sensors/temperature (发布消息)\n</code></pre>\n<hr />\n<h2 id=\"qos-服务质量\">QoS 服务质量</h2>\n<p>MQTT 定义了三种服务质量级别：</p>\n<table>\n<thead>\n<tr>\n<th>QoS</th>\n<th>名称</th>\n<th>说明</th>\n<th>适用场景</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>At Most Once</td>\n<td>最多一次，不保证送达</td>\n<td>传感器数据，丢失可接受</td>\n</tr>\n<tr>\n<td>1</td>\n<td>At Least Once</td>\n<td>至少一次，可能重复</td>\n<td>重要数据，可处理重复</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Exactly Once</td>\n<td>恰好一次，保证送达且不重复</td>\n<td>计费、订单等关键数据</td>\n</tr>\n</tbody>\n</table>\n<pre><code class=\"language-csharp\">// QoS 0 - 最多一次\nawait client.PublishAsync(\"sensor/temp\", \"25\", MqttQualityOfService.AtMostOnce);\n\n// QoS 1 - 至少一次\nawait client.PublishAsync(\"alert/fire\", \"detected\", MqttQualityOfService.AtLeastOnce);\n\n// QoS 2 - 恰好一次\nawait client.PublishAsync(\"order/create\", orderJson, MqttQualityOfService.ExactlyOnce);\n</code></pre>\n<hr />\n<h2 id=\"主题通配符\">主题通配符</h2>\n<table>\n<thead>\n<tr>\n<th>通配符</th>\n<th>说明</th>\n<th>示例</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>+</code></td>\n<td>匹配单个层级</td>\n<td><code>sensor/+/temp</code> 匹配 <code>sensor/room1/temp</code></td>\n</tr>\n<tr>\n<td><code>#</code></td>\n<td>匹配多个层级</td>\n<td><code>sensor/#</code> 匹配 <code>sensor/room1/temp</code> 和 <code>sensor/room1/humidity</code></td>\n</tr>\n</tbody>\n</table>\n<pre><code class=\"language-csharp\">// 订阅所有房间的温度\nawait client.SubscribeAsync(\"sensor/+/temperature\", MqttQualityOfService.AtLeastOnce);\n\n// 订阅所有传感器数据\nawait client.SubscribeAsync(\"sensor/#\", MqttQualityOfService.AtLeastOnce);\n</code></pre>\n<hr />\n<h2 id=\"mqtt-50-新特性\">MQTT 5.0 新特性</h2>\n<p>如果你使用 MQTT 5.0 协议，可以利用以下新特性：</p>\n<pre><code class=\"language-csharp\">var options = new MqttClientOptions\n{\n    Host = \"localhost\",\n    ProtocolVersion = MqttProtocolVersion.V500  // 使用 MQTT 5.0\n};\n\n// 创建带有 MQTT 5.0 属性的消息\nvar message = MqttApplicationMessage.CreateWithProperties(\n    topic: \"request/data\",\n    payload: \"{\\\"query\\\": \\\"temperature\\\"}\",\n    qos: MqttQualityOfService.AtLeastOnce,\n    retain: false,\n\n    // MQTT 5.0 特有属性\n    responseTopic: \"response/client1\",           // 响应主题\n    correlationData: Encoding.UTF8.GetBytes(\"req-123\"),  // 关联数据\n    messageExpiryInterval: 60,                   // 消息60秒后过期\n    contentType: \"application/json\",             // 内容类型\n    userProperties: new List&lt;MqttUserProperty&gt;   // 用户自定义属性\n    {\n        new MqttUserProperty(\"version\", \"1.0\"),\n        new MqttUserProperty(\"source\", \"sensor-hub\")\n    }\n);\n\nawait client.PublishAsync(message);\n</code></pre>\n<hr />\n<h2 id=\"总结\">总结</h2>\n<p>是一个功能完整、性能优秀的 .NET MQTT 库，具有以下优势：</p>\n<ul>\n<li><strong>完整的协议支持</strong>：MQTT 3.1.1、MQTT 5.0、MQTT-SN、CoAP 全覆盖</li>\n<li><strong>高性能设计</strong>：异步 IO、零分配、缓冲区池</li>\n<li><strong>企业级特性</strong>：桥接、集群、认证授权</li>\n<li><strong>易于使用</strong>：简洁的 API 设计，丰富的示例代码</li>\n<li><strong>现代化</strong>：支持最新的 .NET 版本</li>\n</ul>\n<p>无论你是构建 IoT 平台、实现设备通信，还是搭建消息中间件，这个库都能满足你的需求。</p>\n<h2 id=\"相关链接\">相关链接</h2>\n<p>源码地址：<a href=\"https://github.com/hnlyf1688/mqtt\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/hnlyf1688/mqtt</a></p>\n\n</div>\n<div id=\"MySignature\">\n    中国.NET协会（<a href=\"http://www.dotnet.org.cn\" target=\"_blank\">http://www.dotnet.org.cn</a>）<br />\n腾讯企鹅群：45132984<a href=\"http://wp.qq.com/wpa/qunwpa?idkey=70cd6c09355ad0beae9424fc818c26b5cdc2c57bd6ceac797a3fce483cddb069\" target=\"_blank\"><img alt=\"中国.NET协会\" border=\"0\" src=\"http://pub.idqqimg.com/wpa/images/group.png\" title=\"中国.NET协会\" /></a><br />\n博客园地址：<a href=\"http://http://www.cnblogs.com/dotnet-org-cn\">http://http://www.cnblogs.com/dotnet-org-cn</a><br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;国内唯一一个以非盈利的.NET协会，致力打造国内具有权威性、价值性的.NET协会。\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-12 18:10</span>&nbsp;\n<a href=\"https://www.cnblogs.com/dotnet-org-cn\">中国.NET研究协会</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "技术面：MySQL篇（InnoDB事务执行过程、事务隔离级别、事务并发异常）",
      "link": "https://www.cnblogs.com/jimoer/p/19472951",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jimoer/p/19472951\" id=\"cb_post_title_url\" title=\"发布于 2026-01-12 17:12\">\n    <span>技术面：MySQL篇（InnoDB事务执行过程、事务隔离级别、事务并发异常）</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"技术面：MySQL篇（InnoDB事务执行过程、事务隔离级别、事务并发异常）\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/772743/202601/772743-20260112171227414-206366530.png\" />\n        MySQL的InnoDB引擎下更新操作时事务的执行过程？什么是脏读、幻读、不可重复读？MySQL中的事务隔离级别？MySQL的InnoDB引擎是如何解决脏读、幻读、不可重复读？\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"mysql的innodb引擎下更新操作时事务的执行过程\">MySQL的InnoDB引擎下更新操作时事务的执行过程</h2>\n<p>MySQL数据库在InnoDB中一次update的操作过程基本如下：<br />\n<img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/772743/202601/772743-20260112170353829-586528926.png\" /></p>\n<ol>\n<li><strong>首先将数据加载到Buffer Pool里</strong>：当InnoDB需要更新一条记录时，首先会在Buffer Pool中查找该记录是否在内存中。若没在内存中，则从磁盘读取该页到Buffer Pool中。</li>\n<li><strong>记录UndoLog</strong>:在修改操作前，InnoDB会在Undo Log中记录修改前的数据。Undo Log是用来保证事务原子性和一致性的一种机制，用于在发生事务回滚等情况时，将修改操作回滚到修改前的状态，以达到事务的原子性和一致性。<code>Undo Log 首先写入内存中的 Undo 缓冲区，随后由后台线程定期刷盘，无需等待事务提交。</code>。</li>\n<li><strong>在Buffer Pool中更新数据</strong>:当执行update语句时，InnoDB会先更新已经读取到Buffer Pool中的数据，而不是直接写入磁盘。同时，InnoDB会将修改后的数据页状态设置为\"脏页\"（<code>DirtyPage</code>）状态，表示该页已经被修改但尚未写入磁盘。</li>\n<li><strong>记录RedoLog Buffer</strong>:InnoDB在Buffer Pool中记录修改操作的同时，InnoDB会先将修改操作写入到Redo Log Buffer 中。</li>\n<li><strong>提交事务</strong>:在执行完所有修改操作后，事务被提交。在提交事务时，InnoDB会将Redo Log写入磁盘，以保证事务持久性（commit 时会保证 write+fsync，但 log buffer 中的内容可能在 prepare 阶段就已被后台线程提前刷盘）。</li>\n<li><strong>写入磁盘</strong>:在提交后，InnoDB会将Buffer Pool中的脏页写入磁盘，以保证数据的持久性。但是这个写入过程并不是立即执行的，是有一个后台线程异步执行的，所以可能会延迟写入，总之MySQL会选择合适的时机把数据写入磁盘做持久化。</li>\n<li><strong>记录Binlog</strong>:在提交过程中，MySQL Server层将事务提交的信息记录到Binlog中。Binlog是MySQL用来实现主从复制的一种机制，用于将主库上的事务同步到从库上。在Binlog中记录的信息包括:事务开始的时间、数据库名、表名、事务ID、SQL语句等。</li>\n</ol>\n<h2 id=\"什么是脏读幻读不可重复读\">什么是脏读、幻读、不可重复读</h2>\n<p>在整个事务的执行过程中，当出现并发、以及兼顾性能等情况，会出现一系列的问题，但是针对不同的情况，MySQL 也给出了针对性方案。下面就针对<strong>脏读</strong>、<strong>幻读</strong>、<strong>不可重复读</strong>三种异常情况来说明一下。</p>\n<h3 id=\"脏读\">脏读</h3>\n<p><strong>脏读是指读到了其他事务还没提交的数据。</strong><br />\n<img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/772743/202601/772743-20260112170446694-1151197701.png\" /><br />\n脏读产生的过程是当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交(<code>commit</code>)到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是脏数据，依据脏数据所做的操作可能是不正确的。</p>\n<h3 id=\"不可重复读\">不可重复读</h3>\n<p>不可重复读是指在对某数据进行读取过程中，有其他事务对数据进行了修改(<code>UPDATE</code>、<code>DELETE</code>)，导致第二次读取的结果不同。<br />\n<img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/772743/202601/772743-20260112170511717-1573903593.png\" /><br />\n不可重复读的发生过程，在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。比如事务T1读取某一数据，事务T2读取并修改了该数据，T1为了对读取值进行检验而再次读取该数据，便得到了不同的结果。</p>\n<h3 id=\"幻读\">幻读</h3>\n<p>幻读是指事务在做范围查询过程中，有另外一个事务对范围内新增了记录(<code>INSERT</code>)，导致范围查询的结果条数不一致。<br />\n<img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/772743/202601/772743-20260112170540845-429365170.png\" /></p>\n<p><strong>幻读也常被看作不可重复读在‘范围查询’场景下的特殊表现</strong></p>\n<p>例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中符合条件的的“全部数据行”。<br />\n同时，第二个事务也修改这个表中的数据，这种修改是向表中插入\"一行新数据\"。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。</p>\n<h2 id=\"mysql中的事务隔离级别\">MySQL中的事务隔离级别</h2>\n<p><strong>脏读</strong>、<strong>幻读</strong>、<strong>不可重复读</strong>这三种异常情况，产生的主要原因还是由于数据库支持多个事务同时执行时，互相操作相同的数据。那么事务之间能看到哪些数据呢？<br />\n这就是由数据库的事务隔离级别来决定的。</p>\n<p><strong>事务隔离级别，主要用于定义事务处理过程中，不同事务之间可见性和相互影响程度的一套标准。</strong></p>\n<blockquote>\n<p>业内标准SQL-92（也叫 SQL2）是 ANSI/ISO 在 1992 年发布的第二版 SQL 标准，它对 1986 年的 SQL-86 做了大幅扩充，奠定了今天主流关系数据库（Oracle、DB2、SQL Server、PostgreSQL、MySQL 等）所支持的核心功能。</p>\n</blockquote>\n<p>SQL-92定义了<strong>4</strong>种隔离级别来解决<code>脏读</code>、<code>幻读</code>、<code>不可重复读</code>等这些异常情况，从高到底依次为:<strong><code>可串行化(Serializable)</code></strong>、<strong><code>可重复读(Repeatable reads)</code></strong>、<strong><code>读已提交(Read committed)</code></strong>、<strong><code>读未提交(Read uncommitted)</code></strong>。</p>\n<p><strong>读未提交(Read uncommitted)</strong> ，最低级的隔离级别，就是说这种隔离级别下，一个事务可以读取到另一个事务未提交的数据。这种事务隔离级别下，会产生，<strong>脏读、幻读、不可重复读</strong>。</p>\n<p><strong>读已提交(Read committed)</strong>，这种的事务隔离级别下，一个事务如果在修改数据，还未提交事务，其他事务是不能够读取该数据的。因此这种事务隔离级别可以防止<strong>脏读</strong>。</p>\n<p><strong>可重复读(Repeatable reads)</strong>，这种事务隔离级别下，比读已提交更严谨，不但可以防止<strong>脏读</strong>，还能够防止<strong>不可重复读</strong>，<code>但是没办法彻底解决幻读问题</code>。</p>\n<p><strong>可串行化(Serializable)</strong>，这种事务隔离级别下，所有的事务都是按照顺序串行执行，但是事务并发执行速度也是最慢的。此隔离级别可以防止<strong>脏读、幻读、不可重复读</strong>。<br />\n<img alt=\"image\" src=\"https://img2024.cnblogs.com/blog/772743/202601/772743-20260112170645904-710641449.png\" /><br />\n以上是SQL-92标准中对事务隔离级别的定义，但是不同的数据库在实际实现的过程中，是有一些细微差异的。</p>\n<h2 id=\"mysql的innodb引擎是如何解决脏读幻读不可重复读\">MySQL的InnoDB引擎是如何解决脏读、幻读、不可重复读</h2>\n<h3 id=\"解决脏读\">解决脏读</h3>\n<p>脏读是在一个事务中读取到了另一个事务中未提交的数据，在【<strong>读已提交</strong>】隔离级别下，事务只能读取到其他事务已经提交的数据版本。这就是靠<code>MVCC（多版本并发控制）</code>来实现的。</p>\n<p><strong>InnoDB下，系统会检查每个数据行的版本，只有当该版本是由已提交事务修改的，才对当前事务可见。</strong></p>\n<p>当事务在【<strong>读已提交</strong>】隔离级别下执行读取操作时，InnoDB获取当前最新的<strong>全局事务ID</strong>，这个ID表示在当前时刻所有已提交事务的最新状态。InnoDB会检查每个数据行的版本，如果该版本是由一个小于或等于当前事务ID的事务修改的，并且该事务已提交，则这个版本是可见的。这保证了事务只能看到在它开始之前已经提交的数据版本。</p>\n<h4 id=\"实现原理\">实现原理</h4>\n<p>通过行的隐藏字段如：<strong>DB_TRX_ID</strong>（事务ID）和<strong>ReadView</strong>来判断数据版本的可见性。<br />\n<code>DB_TRX_ID</code> 6字节，是全局事务ID，记录最后修改该行的事务ID。<br />\n<code>DB_ROLL_PTR</code>：7字节，回滚指针，指向undo log中的历史版本<br />\n<code>DB_ROW_ID</code>：6字节，行ID（当表没有主键时使用）</p>\n<h5 id=\"readview\">ReadView</h5>\n<p><code>ReadView</code> 是 InnoDB 用来实现 <strong>MVCC（多版本并发控制）</strong> 的核心数据结构，它相当于事务在执行 快照读（普通 SELECT） 时生成的一张“瞬时照片”，记录了当前系统中所有活跃事务（已开启但未提交）的 ID 列表，从而决定当前事务能看到哪些版本的数据。</p>\n<p><strong>ReadView的结构</strong></p>\n<pre><code class=\"language-bash\">struct read_view_t{\n\ttrx_id_t\tlow_limit_id; /*!&lt; 大于等于此ID的事务不可见 */\n\ttrx_id_t\tup_limit_id; /*!&lt; 小于此ID的事务可见 */\n\ttrx_id_t*\ttrx_ids; /*!&lt; 创建时的活跃事务ID列表 */\n\ttrx_id_t\tcreator_trx_id; /*!&lt; 创建该ReadView的事务ID */\n};\n</code></pre>\n<p>例如：</p>\n<pre><code class=\"language-bash\">trx_ids = [20,28,34,40);\nlow_limit_id =40;\nup_limit_id = 20;\ncreator_trx_id = 38\n</code></pre>\n<p>上面这种情况下：<br />\n<code>trx_id &lt; 20</code>：说明改trx_id的事务在这个ReadView生成前已提交，那么该事务的结果是<strong>可见</strong>的<br />\n<code>trx_id &gt; 20 &amp; trx_id &lt; 40</code>：说明该trx_id的事务在这个ReadView生成时，是活跃状态，那么这个记录对于当前事务（创建<code>ReadView</code>的事务）来说不可见。<br />\n<code>trx_id ≥ 41</code>：未来事务，对于创建<code>ReadView</code>的事务来说，不可见。</p>\n<p><strong>不同隔离级别下ReadView 的行为</strong></p>\n<table>\n<thead>\n<tr>\n<th>隔离级别</th>\n<th>ReadView 创建时机</th>\n<th>是否复用</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>READ UNCOMMITTED（读未提交）</strong></td>\n<td>不创建 ReadView，直接读最新版本</td>\n<td>-</td>\n</tr>\n<tr>\n<td><strong>READ COMMITTED（读已提交）</strong></td>\n<td>每次执行 SELECT 时都创建一个新的 ReadView</td>\n<td>不复用</td>\n</tr>\n<tr>\n<td><strong>REPEATABLE READ（可重复读）</strong></td>\n<td>事务中<strong>第一次 SELECT</strong> 时创建 ReadView，之后整个事务复用</td>\n<td>复用</td>\n</tr>\n<tr>\n<td><strong>SERIALIZABLE（串行化）</strong></td>\n<td>退化为加锁机制，不依赖 MVCC</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"解决不可重复读\">解决不可重复读</h3>\n<p>不可重复读在同一事务中，多次读取同一数据返回的结果不同。<br />\n解决方案：<code>InnoDB</code>主要是通过<code>MVCC</code>和<code>ReadView</code>机制解决不可重复读问题。</p>\n<p>核心实现：<br />\n<strong>在【可重复读】REPEATABLE READ，这种事务隔离级别下，当使用快照读，读取数据时，只会在第一次读取的时候生成一个ReadView，后续事务中所有快照读都用同一个快照，这样就不会发生不可重复读的问题了。</strong></p>\n<p>即使其他事务修改了数据并提交，当前事务仍然看到原来的版本。</p>\n<h3 id=\"解决幻读\">解决幻读</h3>\n<p>幻读在事务的范围查询中，多次查询出来的数量不一致。<br />\n解决方案：<br />\n<strong><code>InnoDB</code>的RR隔离级别（可重复读）通过<code>MVCC</code> + 间隙锁（<code>Next-Key Lock</code>）的组合机制解决幻读问题。</strong></p>\n<blockquote>\n<p><strong>这种方式只是在一定程度上解决了幻读，但是并没有完全避免，当一个事务中，对同一段范围查询，既有快照读又有当前读时，可能出现“先快照、后当前”导致的幻读，需要业务层注意。</strong></p>\n</blockquote>\n<h4 id=\"分场景来说明是如何解决幻读的\">分场景来说明是如何解决幻读的</h4>\n<h5 id=\"快照读场景\">快照读场景：</h5>\n<p><strong>MVCC机制</strong>：普通<code>SELECT</code>语句使用快照读，基于<code>ReadView</code>机制可以避免幻读<br />\n<strong>实现原理</strong>：事务只能看到<code>ReadView</code>创建之前存在的数据，新插入的数据对当前事务不可见。</p>\n<h5 id=\"当前读场景\">当前读场景：</h5>\n<p><strong><code>Next-Key Lock</code>机制</strong>：对于<code>SELECT ... FOR UPDATE、UPDATE、DELETE</code>等当前读操作</p>\n<p><strong>技术细节</strong>：</p>\n<ul>\n<li>不仅锁定读取到的记录，还会锁定记录之间的间隙（<code>Gap Lock</code>）</li>\n<li>防止其他事务在查询范围内插入新数据。</li>\n<li>通过锁定索引记录和记录之间的间隙来避免幻读。</li>\n</ul>\n<h2 id=\"总结\">总结</h2>\n<p>MySQL InnoDB 的 UPDATE 语句看似一条简单命令，背后却要走完“<code>加载→写 Undo→改内存→写 Redo→两阶段提交→Binlog→异步刷脏</code>”整条链路；<br />\n<strong>其中 Redo 保证崩溃恢复，Binlog 保证主从复制，Undo 提供回滚与 MVCC 多版本。</strong></p>\n<p>SQL-92 的 4 种隔离级别，在 InnoDB 里被细化为 MVCC + 锁的混合方案：</p>\n<ul>\n<li><strong>READ UNCOMMITTED 直接读最新版，可能脏读；</strong></li>\n<li><strong>READ COMMITTED 每次新建 ReadView，解决脏读；</strong></li>\n<li><strong>REPEATABLE READ 复用首次 ReadView，解决不可重复读，并用 Next-Key Lock 在当前读场景下堵住幻读；</strong></li>\n<li><strong>SERIALIZABLE 退化为纯加锁，彻底串行。</strong></li>\n</ul>\n<p>理解这套“<strong>日志先行、MVCC 多版本、锁补边界</strong>”的设计，才能在业务开发中正确选择隔离级别、合理使用当前读与索引，避免“<strong>锁等待</strong>”或“<strong>幻读</strong>”带来的意外结果。</p>\n\n</div>\n<div id=\"MySignature\">\n    <div style=\"float: left; letter-spacing: 1px; font-size: 13px;\"><p><strong>作者：</strong><a href=\"http://www.cnblogs.com/jimoer/\">纪莫</a>\n<br />欢迎任何形式的转载，但请务必注明出处。<br />\n限于本人水平，如果文章和代码有表述不当之处，还请不吝赐教。</p>\n<!-- 分享图标开始 -->\n<p>欢迎扫描二维码关注公众号：<strong>Jimoer</strong></p>\n<p>文章会同步到公众号上面，大家一起成长，共同提升技术能力。</p>\n<p><strong>声援博主：</strong>如果您觉得文章对您有帮助，可以点击文章右下角【<a href=\"\" style=\"font-size: 14pt;\">推荐</a>】一下。</p>\n<p>您的鼓励是博主的最大动力！</p>\n</div>\n<div style=\"float: right;\">\n<img alt=\"微信公众号\" src=\"https://img2018.cnblogs.com/blog/772743/201909/772743-20190904004009398-659676330.png\" /></div>\n<!-- 分享图标结束 -->\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-01-12 17:12</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jimoer\">纪莫</a>&nbsp;\n阅读(<span id=\"post_view_count\">0</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}